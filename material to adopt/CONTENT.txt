Let’s zoom in and really chew through these slides. I’ll go topic by topic and squeeze out every bit of structure.

I’ll use this rough roadmap:

1. Preliminaries: inner product, lines, segments
2. Affine combinations and affine sets (several equivalent characterizations)
3. Convex combinations, convex sets (properties, equivalent views)
4. Cones and convex cones
5. Hyperplanes and halfspaces (several proofs of convexity)
6. Balls and ellipsoids + principal axes (eigendecomposition / SVD / Cholesky)
7. Norms, norm balls, norm cones (multiple proofs of convexity)
8. Polyhedra (Ax ≤ b, Cx = d) and their convexity
9. Positive semidefinite cone (S^n_+) (two proofs of convexity)
10. Operations that preserve convexity; intersection example with trigonometric polynomials

I’ll keep looping back to the same pattern:

> “This set is convex/affine/conic because it is the preimage or image of something simple under an affine or linear map, or an intersection of simple convex sets.”

---

## 0. Preliminaries

We’re working in (\mathbb{R}^n) with the standard inner product and norms.

* Inner product:
  [
  \langle x,y\rangle = x^T y = \sum_{i=1}^n x_i y_i.
  ]
* Euclidean norm:
  [
  |x|_2 = \sqrt{\langle x,x\rangle}.
  ]

**Line through two points (x_1,x_2):**

[
L(x_1,x_2) = {x \mid x = x_2 + \theta (x_1 - x_2),\ \theta \in \mathbb{R}}.
]

**Line segment:**

[
[x_1,x_2] = {x \mid x = \theta x_1 + (1-\theta)x_2,\ 0\le\theta\le 1}.
]

* Same formula, but (\theta) restricted to ([0,1]) instead of all (\mathbb{R}).

---

## 1. Affine combinations and affine sets

### 1.1 Affine combinations

Given points (x_1,\dots,x_k\in\mathbb{R}^n), an **affine combination** is

[
x = \sum_{i=1}^k \theta_i x_i, \quad \sum_{i=1}^k \theta_i = 1.
]

Coefficients (\theta_i) can be negative; only the “sum to 1” is enforced.

Why “sum to 1”? Because if you rewrite:

[
x - x_k = \sum_{i=1}^{k-1} \theta_i(x_i - x_k),
]
using (\theta_k = 1 - \sum_{i=1}^{k-1} \theta_i), you see affine combinations are exactly **a point plus a linear combination of direction vectors**.

### 1.2 Affine sets – definition

A set (C\subseteq\mathbb{R}^n) is **affine** if:

> For every (x_1,x_2\in C) and every (\theta\in\mathbb{R}),
> [
> \theta x_1 + (1-\theta)x_2 \in C.
> ]

Equivalently: for any two points in the set, the entire **infinite line** through them is contained in the set, not just the segment.

So affine sets are “flat” without curvature and without boundaries cutting off the line.

### 1.3 Affine sets as “translated subspaces”

Key structural theorem:

> A set (C) is affine iff it can be written as
> [
> C = x_0 + V = {x_0 + v \mid v\in V}
> ]
> for some point (x_0 \in C) and some **linear subspace** (V\subseteq\mathbb{R}^n).

**Proof (⇐: representation ⇒ affine).**

Assume (C = x_0 + V) with (V) a subspace.

Take any (x_1, x_2 \in C). Then there exist (v_1,v_2\in V) with
[
x_1 = x_0 + v_1,\quad x_2 = x_0 + v_2.
]

Now for any (\theta\in\mathbb{R}),
[
\theta x_1 + (1-\theta)x_2
= \theta(x_0 + v_1) + (1-\theta)(x_0 + v_2)
= x_0 + [\theta v_1 + (1-\theta)v_2].
]

Since (V) is a subspace, (\theta v_1 + (1-\theta)v_2 \in V). So (\theta x_1 + (1-\theta)x_2 \in x_0 + V = C). So (C) is affine.

**Proof (⇒: affine ⇒ translated subspace).**

Assume (C) affine and nonempty. Pick some (x_0\in C) and define:

[
V := C - x_0 = {x - x_0 \mid x\in C}.
]

We show (V) is a linear subspace:

1. (0\in V) since (x_0 - x_0 = 0), and (x_0\in C).

2. Closed under addition:
   Let (v_1, v_2\in V). Then (v_i = x_i - x_0) for some (x_i\in C).

   We want to show (v_1 + v_2 \in V). Note:

   * Consider (\theta = 2).
     Since (C) is affine, for any (\theta\in\mathbb{R}):
     [
     \theta x_1 + (1-\theta)x_0 \in C.
     ]
     For (\theta=2):
     [
     2x_1 - x_0 \in C.
     ]
     Similarly (2x_2 - x_0 \in C).

   * Now consider (\frac{1}{2}(2x_1 - x_0) + \frac{1}{2}(2x_2 - x_0)). Since (C) is affine and closed under averaging ((\theta = 1/2)), this point is in (C).

   Compute:
   [
   \frac{1}{2}(2x_1 - x_0) + \frac{1}{2}(2x_2 - x_0)
   = x_1 + x_2 - x_0.
   ]

   So (x_1 + x_2 - x_0 \in C), and hence
   [
   (x_1 + x_2 - x_0) - x_0 = (x_1 - x_0) + (x_2 - x_0) = v_1 + v_2 \in V.
   ]

3. Closed under scalar multiplication:
   Let (v\in V), so (v = x - x_0) for some (x\in C), and let (\alpha\in\mathbb{R}).

   * Again, affine property: for any (\theta\in\mathbb{R}), (\theta x + (1-\theta)x_0\in C).
   * Take (\theta = \alpha). Then (\alpha x + (1-\alpha)x_0\in C).
   * Subtract (x_0):
     [
     [\alpha x + (1-\alpha)x_0] - x_0 = \alpha(x - x_0) = \alpha v \in V.
     ]

So (V) is a subspace and (C = x_0 + V). ∎

This “translated subspace” viewpoint is extremely useful. Example: any solution set of (Ax=b) is exactly one particular solution plus the null space of (A).

### 1.4 Affine sets as solution sets of linear equations

Given a matrix (A\in\mathbb{R}^{m\times n}) and vector (b\in\mathbb{R}^m), the solution set

[
C = {x\mid Ax=b}
]
is affine.

We can describe it as:

* Pick any particular solution (x_p) such that (Ax_p=b).
* Let (V = {v\mid Av=0}) be the nullspace of (A).

Then:

[
C = x_p + V = {x_p + v \mid Av=0}.
]

Conversely, every affine set can be expressed in this form for suitable (A,b). The idea:

1. For affine (C), write (C = x_0 + V) with subspace (V).
2. Choose (A) such that (\text{Null}(A)=V) (always possible: take a basis of (V^\perp) as rows).
3. Then (Ax = Ax_0) for all (x\in C), and only for those.

So “affine set” ⇔ “solution set of linear equations”.

---

## 2. Convex combinations and convex sets

### 2.1 Convex combinations

Given points (x_1,\dots,x_k), a **convex combination** is

[
x = \sum_{i=1}^k \theta_i x_i, \quad \theta_i \ge 0,\quad \sum_{i=1}^k \theta_i = 1.
]

Compare:

* **Affine combination**: (\sum\theta_i=1), signs unrestricted.
* **Convex combination**: (\sum\theta_i=1) and (\theta_i\ge0).

Geometric picture:

* Affine combinations of two points: infinite line.
* Convex combinations of two points: just the segment connecting them.
* Convex combinations of many points: the “filled polygon/polyhedron” formed by them.

The set of **all convex combinations** of points in a set (S) is called the **convex hull**, (\text{conv}(S)).

### 2.2 Convex sets – main definition

A set (C\subseteq\mathbb{R}^n) is **convex** if:

> For all (x_1,x_2\in C) and all (\theta\in[0,1]),
> [
> \theta x_1 + (1-\theta)x_2 \in C.
> ]

Equivalent phrasing: with any pair of points, the **entire line segment** joining them lies in the set.

Geometrically: no “dent” or “hole” along straight lines.

### 2.3 Equivalent formulations

Some useful equivalences:

1. **Finite convex combinations.**

   (C) is convex iff for any finite set of points (x_1,\dots,x_k \in C) and any convex weights (\theta_i) (nonnegative, sum to 1), the convex combination (\sum_i\theta_i x_i) also lies in (C).

   Proof sketch (⇐ is trivial, ⇒ is by induction on (k) using the 2-point definition repeatedly).

2. **Intersection with lines.**

   A closed-form geometric fact:
   (C) is convex iff for every line (L), the intersection (C\cap L) is either empty or a (possibly degenerate) interval.

   Intuition: on each line, a convex set looks like an interval, because you can’t “skip” intermediate points along a line without violating convexity.

   One direction:
   If (C) convex, then on any line pick two points (p,q\in C\cap L). The entire segment [p,q] lies in C and is contained in L, so (C\cap L) contains the interval between any two of its points → it’s an interval/segment/ray/empty.

   Conversely, if the intersection with any line is an interval, then in particular for any two points (x_1,x_2\in C), the whole segment between them (the line segment is intersection of C with the line through them) lies in (C), so C is convex.

This “line intersection” view is helpful intuition but the main working definition is the “two points + θ” one.

---

## 3. Cones and convex cones

### 3.1 Cones

A set (K\subseteq\mathbb{R}^n) is a **cone** (or **nonnegative cone**) if

> For all (x\in K) and all (\alpha\ge0),
> [
> \alpha x\in K.
> ]

So if a vector is in (K), then the whole ray from the origin through that vector is in (K).

Examples:

* The nonnegative orthant (\mathbb{R}^n_+ = {x \mid x_i\ge0}).
* Any linear subspace is a cone (closed under all real scalings, including negative).

### 3.2 Convex cones

A set is a **convex cone** if it is both a cone and convex:

> (x,y\in K,\ \theta\in[0,1]) ⇒ (\theta x + (1-\theta)y \in K) and
> (x\in K,\ \alpha\ge0) ⇒ (\alpha x\in K).

Equivalent formulation:

> (K) is a convex cone ⇔ for any (x_i\in K) and any (\theta_i\ge0),
> (\sum_i \theta_i x_i\in K).

**Proof that these two definitions are equivalent.**

* (Convex + cone ⇒ closed under nonnegative linear combos)

  For two vectors:

  * Take (x_1, x_2\in K) and (\theta_1,\theta_2\ge0).
  * If (\theta_1 + \theta_2 = 0), then (\theta_1 x_1 + \theta_2 x_2 = 0\in K) since cones contain 0.
  * If (\theta_1 + \theta_2 > 0), write
    [
    \theta_1 x_1 + \theta_2 x_2
    = (\theta_1+\theta_2)\left(\frac{\theta_1}{\theta_1+\theta_2} x_1
    + \frac{\theta_2}{\theta_1+\theta_2} x_2\right).
    ]
    The bracket is a convex combination of (x_1,x_2) (weights sum to 1, nonnegative), so it lies in (K) by convexity; multiplying by (\theta_1+\theta_2\ge0) keeps it in (K) by cone property.

  For more than two vectors, you iterate this trick or use induction.

* (Closed under nonnegative combos ⇒ convex + cone)

  * Cone: choose only one nonzero coefficient, say (\theta_1\ge0), and (x=\theta_1 x_1).
  * Convex: use coefficients (\theta_i\ge0) with (\sum\theta_i=1).

So you can think of convex cones as “all nonnegative linear combinations” of some generating set (possibly infinite).

---

## 4. Hyperplanes and halfspaces

### 4.1 Hyperplanes

Given nonzero (a\in\mathbb{R}^n) and scalar (b), the **hyperplane** is:

[
H = {x\mid a^T x = b}.
]

a) **Affine-set view**

This is the set of solutions to one linear equation, so from Section 1.4 it is an affine set.

Let’s be explicit: pick any one point (x_0) with (a^T x_0 = b). The set of all solutions is

[
H = x_0 + {v \mid a^T v = 0} = x_0 + \text{Null}(a^T).
]

The nullspace of (a^T) is a subspace of dimension (n-1). So this is an ((n-1))-dimensional flat, i.e. hyperplane.

b) **Convexity**

All affine sets are convex, so (H) is convex. But you can also check directly:

Take (x_1,x_2\in H), so (a^T x_1 = a^T x_2 = b). For (\theta\in[0,1]),

[
a^T (\theta x_1 + (1-\theta)x_2) =
\theta a^T x_1 + (1-\theta)a^T x_2
= \theta b + (1-\theta)b
= b.
]

So segment stays in (H).

### 4.2 Halfspaces

A **(closed) halfspace** with normal (a\neq0) and offset (b) is:

[
C = {x\mid a^T x \le b}.
]

Geometric: that’s the region on one side of the hyperplane (a^T x = b).

**Proof of convexity – version 1 (direct).**

Take (x_1,x_2\in C), so (a^T x_1 \le b), (a^T x_2 \le b). For (\theta\in[0,1]):

[
a^T (\theta x_1 + (1-\theta)x_2)
= \theta a^T x_1 + (1-\theta)a^T x_2
\le \theta b + (1-\theta)b
= b.
]

So convex.

**Proof of convexity – version 2 (preimage of a convex set).**

Define the linear functional (f(x) = a^T x). Then

[
C = f^{-1}((-\infty, b]) =
{x \mid f(x)\in(-\infty,b]}.
]

* The set ((-\infty, b]\subset\mathbb{R}) is convex.
* Preimage of a convex set under an affine map is convex. (Easy check: if (u,v) map inside a convex subset of (\mathbb{R}), then the line segment between them also maps inside that subset.)

So (C) is convex as a preimage of a convex set.

This “preimage under linear/affine map” trick will keep coming back.

---

## 5. Balls and ellipsoids

### 5.1 Euclidean balls

A closed Euclidean ball of radius (r>0) centered at (x_c) is:

[
B(x_c,r) = {x \mid |x - x_c|_2 \le r}.
]

We already gave a proof of convexity; here’s a slightly different view.

**Proof of convexity – version 2 (affine image of a simpler set).**

Write (x = x_c + z). Then

[
B(x_c,r) = x_c + B(0,r) = {x_c + z \mid |z|_2 \le r}.
]

So the structure is just the ball around origin translated by (x_c). Translation is an affine map (T(z) = x_c + z). Affine images of convex sets are convex (proof below), so:

* It suffices to show (B(0,r) = {z\mid|z|_2\le r}) convex.
* That’s the special case of “norm balls are convex”, handled later.

### 5.2 Ellipsoids in quadratic form

Fix (x_c\in\mathbb{R}^n) and symmetric positive definite (P\succ0). Define

[
\mathcal{E} = {x \mid (x - x_c)^T P^{-1}(x - x_c) \le 1}.
]

We can interpret this in several equivalent ways.

#### Interpretation 1: ellipsoid as norm ball

Define a quadratic norm
[
|z|_{P^{-1}} = \sqrt{z^T P^{-1} z}.
]

Then

[
\mathcal{E} = {x \mid |x - x_c|_{P^{-1}} \le 1}
]
is just a quadratic norm ball → convex.

Also, this shows **ellipsoids are just norm balls under a linear change of coordinates**.

#### Interpretation 2: ellipsoid as linear image of Euclidean ball

Since (P\succ0), it has a symmetric square root (P^{1/2}) and inverse square root (P^{-1/2}), such that:

[
P^{1/2}P^{1/2} = P,
\quad
P^{-1/2}P^{-1/2} = P^{-1},
\quad
P^{1/2}P^{-1/2}=I.
]

Let (z = P^{-1/2}(x - x_c)). Then

[
(x - x_c)^T P^{-1}(x - x_c)
= (P^{-1/2}(x - x_c))^T (P^{-1/2}(x - x_c))
= |z|_2^2.
]

So the ellipsoid in x-coordinates corresponds to the **unit Euclidean ball** in z-coordinates:

[
\mathcal{E} = {x_c + P^{1/2}z \mid |z|_2\le 1}.
]

So (\mathcal{E}) is the image of the unit Euclidean ball under the affine map (z\mapsto x_c + P^{1/2}z). Because affine images of convex sets are convex, ellipsoids are convex.

#### Affine images of convex sets are convex (general fact)

Let (C\subseteq\mathbb{R}^n) be convex, and let (T:\mathbb{R}^n\to\mathbb{R}^m) be affine:
[
T(x) = Ax + b.
]

Then (T(C) = {T(x)\mid x\in C}) is convex.

Proof:

Take any (y_1,y_2\in T(C)). Then there exist (x_1,x_2\in C) with (y_1=T(x_1),y_2=T(x_2)). For (\theta\in[0,1]),

[
\begin{aligned}
\theta y_1 + (1-\theta)y_2
&= \theta T(x_1) + (1-\theta)T(x_2) \
&= \theta (Ax_1 + b) + (1-\theta)(Ax_2 + b) \
&= A(\theta x_1 + (1-\theta)x_2) + b \
&= T(\theta x_1 + (1-\theta)x_2).
\end{aligned}
]

Since (C) is convex, (\theta x_1 + (1-\theta)x_2\in C), hence its image under (T) lies in (T(C)). So (\theta y_1 + (1-\theta)y_2 \in T(C)). ∎

#### Principal axes via eigendecomposition (ellipsoid orientation)

With (P = Q\Lambda Q^T), where (Q) orthogonal and (\Lambda) diagonal with entries (\lambda_i>0):

Let (y = Q^T (x - x_c)).

Then:

[
(x - x_c)^T P^{-1}(x - x_c)
= y^T\Lambda^{-1}y
= \sum_{i=1}^n \frac{y_i^2}{\lambda_i}.
]

The ellipsoid becomes:

[
\mathcal{E}
= \left{ x_c + Qy ,\middle|,
\sum_{i=1}^n \frac{y_i^2}{\lambda_i} \le 1
\right}.
]

So:

* In y-coordinates, the ellipsoid is axis-aligned.
* The semi-axis along direction (e_i) has length (\sqrt{\lambda_i}).
* In x-coordinates, those axes are rotated by (Q); principal directions are the eigenvectors (q_i).

So eigenvalues = squared radii, eigenvectors = principal axis directions.

#### Alternate decomposition: Cholesky

Instead of eigen, you can factor (P = LL^T) (Cholesky). Then set (z = L^{-T}(x-x_c)):

[
(x - x_c)^T P^{-1}(x - x_c)
= (x-x_c)^T (L^{-T}L^{-1})(x-x_c)
= |L^{-1}(x-x_c)|_2^2.
]

Again, ellipsoid is
[
\mathcal{E} = {x_c + Ly\mid |y|_2\le1}.
]

Same story: ellipsoid = linear image of ball.

---

## 6. Norms, norm balls, norm cones

### 6.1 Norm axioms revisited

A function (|\cdot|: \mathbb{R}^n\to\mathbb{R}) is a norm if:

1. (|x|\ge0) (nonnegativity), and (|x|=0) iff (x=0).
2. (|\alpha x| = |\alpha||x|) (absolute homogeneity).
3. (|x+y|\le|x|+|y|) (triangle inequality).

Examples you care about:

* (\ell_2) norm: Euclidean.
* (\ell_p) norms:
  (|x|_p = (\sum |x_i|^p)^{1/p}) for (p\ge1).
* (\ell_\infty): (|x|_\infty = \max_i |x_i|).
* Quadratic norm: (|x|_A = \sqrt{x^T A x}) for (A) SPD.

### 6.2 Norm balls are convex – several proofs

**Proof 1 (triangle inequality).**

Let (B(x_c,r) = {x\mid|x-x_c|\le r}).

Take (x_1,x_2\in B(x_c,r)). Then (|x_i-x_c|\le r). For (\theta\in[0,1]):

[
\begin{aligned}
|\theta x_1 + (1-\theta)x_2 - x_c|
&= |\theta(x_1 - x_c) + (1-\theta)(x_2 - x_c)| \
&\le \theta|x_1 - x_c| + (1-\theta)|x_2 - x_c| \
&\le \theta r + (1-\theta)r = r.
\end{aligned}
]

So convex combination stays inside ball.

**Proof 2 (preimage of convex interval).**

Define (f(x) = |x - x_c|). Then a norm is a convex function (nontrivial but standard; triangle inequality is exactly convexity of the norm).

* The sublevel set ({x\mid f(x)\le r}) of any convex function is convex.
* So norm balls are convex as sublevel sets of convex functions.

This viewpoint is very general: most constraint sets in convex optimization are sublevel sets of convex functions.

### 6.3 Norm cones are convex cones

Norm cone associated with (|\cdot|):

[
K = {(x,t)\in\mathbb{R}^{n+1} \mid |x| \le t}.
]

We already gave a proof, but here’s a slightly structured version.

**Cone property.**

Take ((x,t)\in K) and (\alpha\ge0). Then

[
|\alpha x| = \alpha|x| \le \alpha t.
]

So ((\alpha x,\alpha t)\in K). Hence cone.

**Convexity.**

Take ((x_1,t_1),(x_2,t_2)\in K). Then (|x_i|\le t_i). For (\theta\in[0,1]):

[
\begin{aligned}
|\theta x_1 + (1-\theta)x_2|
&\le \theta|x_1| + (1-\theta)|x_2| \
&\le \theta t_1 + (1-\theta)t_2.
\end{aligned}
]

So ((\theta x_1 + (1-\theta)x_2,\ \theta t_1 + (1-\theta)t_2)\in K). Convex.

Special case: **second-order cone** when (|\cdot| = |\cdot|_2):

[
{(x,t)\mid |x|_2 \le t}
]
= that “ice-cream cone” you see in 3D.

---

## 7. Polyhedra

Definition:

[
P = {x\in\mathbb{R}^n \mid Ax \preceq b,\ Cx = d},
]
where:

* (Ax \preceq b) means componentwise: (a_i^T x \le b_i) for each row (a_i^T).
* (Cx=d) means linear equalities.

So polyhedron = intersection of finitely many halfspaces and hyperplanes.

**Convexity – direct reasoning.**

We’ve already done the pieces:

* Each set (H_i = {x\mid a_i^T x \le b_i}) is a halfspace ⇒ convex.
* Each set (E_j = {x\mid c_j^T x = d_j}) is a hyperplane ⇒ affine ⇒ convex.

Then
[
P = \bigcap_i H_i \cap \bigcap_j E_j.
]

Intersection of convex sets is convex (we’ll prove in the next section), so (P) is convex.

The drawings you had (triangle formed by three lines with shading ⇔ intersection of 3 halfspaces) are exactly polyhedron examples in (\mathbb{R}^2).

---

## 8. Positive semidefinite cone (S^n_+)

### 8.1 Definitions

* (S^n): symmetric (n\times n) real matrices.
* (S^n_+ = {X\in S^n \mid X \succeq 0}): positive semidefinite (PSD) matrices.
* (X\succeq0) ⇔ (z^T X z \ge 0) for all (z\in\mathbb{R}^n).
* (S^n_{++} = {X\mid X\succ0}): positive definite matrices, (z^T X z>0) for all nonzero (z).

### 8.2 Convex cone – Proof 1 (quadratic forms)

1. **Cone property.**
   If (X\succeq0) and (\alpha\ge0):

   [
   z^T (\alpha X) z = \alpha z^T X z \ge 0,
   ]
   so (\alpha X\succeq0).

2. **Convexity.**
   If (X_1,X_2\succeq0) and (\theta\in[0,1]). For any (z),

   [
   z^T (\theta X_1 + (1-\theta)X_2) z
   = \theta z^T X_1 z + (1-\theta) z^T X_2 z \ge 0.
   ]

   So the combination is PSD. Thus (S^n_+) convex.

So (S^n_+) is a convex cone.

### 8.3 Convex cone – Proof 2 (eigendecomposition viewpoint)

Take (X \in S^n_+). Then (X) has eigendecomposition (X = Q\Lambda Q^T) with (\Lambda) diagonal, eigenvalues (\lambda_i \ge 0).

* Cone property:
  (\alpha X = Q(\alpha \Lambda)Q^T), eigenvalues (\alpha\lambda_i\ge0) for (\alpha\ge0) ⇒ still PSD.

* Convexity:
  If (X_1 = Q_1\Lambda^{(1)}Q_1^T), (X_2 = Q_2\Lambda^{(2)}Q_2^T) are PSD, the combination (\theta X_1 + (1-\theta)X_2) is symmetric; its Rayleigh quotient is a convex combination of nonnegative numbers, so eigenvalues are ≥0. (You can also argue again via quadratic forms).

This doesn’t add much over Proof 1, but it links to spectral properties: PSD = “all eigenvalues ≥ 0”; convex combinations preserve this property.

---

## 9. Operations that preserve convexity

### 9.1 Intersection

We already did this, but let’s restate in clean form.

**Theorem.**
Let ({C_i}_{i\in I}) be a collection of convex sets in (\mathbb{R}^n). Then

[
C = \bigcap_{i\in I} C_i
]
is convex.

**Proof.**

Take (x_1,x_2\in C). Then (x_1,x_2\in C_i) for every (i\in I).

For any (\theta\in[0,1]), since (C_i) is convex,
[
\theta x_1 + (1-\theta)x_2 \in C_i \quad \forall i.
]

So (\theta x_1 + (1-\theta)x_2) lies in every (C_i), thus it lies in (C). ∎

This is how you prove polyhedra (finite intersections of halfspaces) and many other sets are convex.

### 9.2 Affine images and preimages

Two more meta-operations:

* **Affine image**: if (C) convex and (T(x)=Ax+b) affine, then (T(C)) convex (we proved earlier).
* **Affine preimage**: if (D) convex and (T) affine, then (T^{-1}(D) = {x\mid T(x)\in D}) is convex.

Proof of preimage fact:

Take (x_1,x_2\in T^{-1}(D)). Then (T(x_1),T(x_2)\in D). For (\theta\in[0,1]):

[
T(\theta x_1 + (1-\theta)x_2)
= \theta T(x_1) + (1-\theta)T(x_2).
]

Since (D) convex, the RHS is in (D). So the LHS is in (D), meaning (\theta x_1 + (1-\theta)x_2 \in T^{-1}(D)). ∎

You’ve already seen:

* Halfspaces as preimages of intervals under linear functionals.
* Norm cones as preimages of epigraphs of norms.

These three operations (intersection, affine image, affine preimage) plus basic convex sets generate 90% of feasible regions in convex optimization.

---

## 10. Trigonometric polynomial example (intersection of infinitely many halfspaces)

Final slide’s example:

[
S = {x\in\mathbb{R}^m \mid |p_x(t)| \le 1 \text{ for all } |t|\le \pi/3},
]
where
[
p_x(t) = x_1\cos t + x_2\cos 2t + \cdots + x_m\cos mt.
]

You can think of:

* Parameter space: coefficients (x\in\mathbb{R}^m).
* For each (x), you get a real function (p_x(t)).
* (S) is the set of coefficient vectors that yield functions staying within ([-1,1]) on that interval.

### 10.1 Writing S as intersection of halfspaces

First, fix a single (t_0\in[-\pi/3,\pi/3]). Consider

[
|p_x(t_0)| \le 1.
]

Write (c(t_0) = (\cos t_0,\cos2t_0,\dots,\cos mt_0)^T), so

[
p_x(t_0) = c(t_0)^T x.
]

The constraint becomes:

[
|c(t_0)^T x| \le 1
\quad\Longleftrightarrow\quad
-1 \le c(t_0)^T x \le 1.
]

These are two linear inequalities:

[
c(t_0)^T x \le 1,\quad -c(t_0)^T x \le 1.
]

Each inequality describes a halfspace in (\mathbb{R}^m). The intersection of the two is a (possibly unbounded) strip – convex.

Now enforce this for *every* (t_0\in[-\pi/3,\pi/3]). Then

[
S = \bigcap_{|t|\le\pi/3} {x \mid |c(t)^T x| \le 1}.
]

Each inner set is convex (intersection of two halfspaces). Intersection over all (t) (uncountable index set) is still convex by the intersection theorem.

So:

> (S) is convex as intersection of convex sets.

### 10.2 Alternate view: S as sublevel set of a convex function

Define the function (F:\mathbb{R}^m\to\mathbb{R}):

[
F(x) = \sup_{|t|\le\pi/3} |p_x(t)|.
]

Then

[
S = {x \mid F(x) \le 1}.
]

Now:

* For any fixed (t), the map (x \mapsto p_x(t) = c(t)^T x) is linear, hence convex and concave.
* (x \mapsto |p_x(t)|) is convex as absolute value of a linear function.
* The supremum of convex functions (over any index set) is convex.

So (F) is a convex function, and (S) is its sublevel set ⇒ convex.

This gives an **alternate proof** of convexity, using “sublevel set of a convex function” instead of “intersection of many convex sets”. They’re equivalent here:
[
F(x) \le 1 \iff \forall t: |p_x(t)| \le 1.
]

---

## 11. Quick structural summary

What you now have, in a very explicit, modular way:

* **Affine sets**

  * Defined by containing whole lines between points.
  * Equivalently: translated subspaces, or solution sets of linear equations (Ax=b).

* **Convex sets**

  * Defined by containing all line segments between points.
  * Equivalently: closed under convex combinations; intersection with any line is interval.

* **Cones and convex cones**

  * Cones: closed under nonnegative scaling.
  * Convex cones: closed under nonnegative linear combinations.

* **Key examples and why they’re convex/affine/conic**

  * Hyperplanes: affine (solutions of one linear equation).
  * Halfspaces: preimages of convex intervals under linear maps.
  * Balls and ellipsoids: norm balls / affine images of Euclidean balls.
  * Norm balls: sublevel sets of norms (convex functions).
  * Norm cones: epigraphs of norms → convex cones.
  * Polyhedra: finite intersections of halfspaces/hyperplanes → convex.
  * PSD cone (S^n_+): closed under conic combinations via quadratic forms.
  * Example set of trigonometric polynomials: intersection of infinite halfspaces OR sublevel set of convex function.

* **Operations preserving convexity**

  * Intersection of convex sets.
  * Affine images and preimages.
  * Sublevel sets of convex functions (very important for constraint sets).

These tools are exactly what you use later to prove: “this weird-looking constraint set is convex”, by expressing it as intersection/preimage/sublevel set of these fundamental building blocks.



We’ll structure it like this:

1. Intersection of convex sets

   1. Trig-polynomial example (S) in detail (including the picture for (m=2))
2. Convex combinations and the convex hull

   1. Two definitions and full equivalence
   2. Extra facts: Carathéodory, examples, extreme points
3. Affine maps (f(x)=Ax+b): what they *really* do

   1. Image of a convex set is convex (several perspectives)
   2. Preimage of a convex set is convex (again, several perspectives)
   3. Concrete geometric examples (your drawings)
4. Examples: norm balls under affine maps, hyperbolic cone, LMIs

   1. Image / preimage of norm balls = ellipsoids etc.
   2. Hyperbolic cone as SOC preimage and as cone of inflated ellipses
   3. LMI solution sets as PSD-cone preimages, with a low-dimensional example

---

## 1. Intersection of convex sets

We already did the textbook proof, but let’s strip it to its skeleton and then add alternative views.

### 1.1 Core argument

We’re given a family of convex sets ({C_\alpha}_{\alpha\in A}) in (\mathbb{R}^n). Define

[
C := \bigcap_{\alpha\in A} C_\alpha.
]

We want to show (C) is convex.

**Mechanics:**

* Take arbitrary (x,y\in C).
* “Being in the intersection” means:
  (x\in C_\alpha) and (y\in C_\alpha) for **every** (\alpha\in A).
* Because each (C_\alpha) is convex, for any (\theta\in[0,1]),
  [
  \theta x + (1-\theta)y \in C_\alpha \quad\forall\alpha.
  ]
* Therefore (\theta x + (1-\theta)y) lies in *all* the (C_\alpha), hence in their intersection:
  [
  \theta x + (1-\theta)y \in C.
  ]

Done. That’s exactly what your blue arrows (x,y\in\cap E_\alpha \Rightarrow x,y\in E_\alpha \forall\alpha \Rightarrow \theta x+(1-\theta)y\in E_\alpha \forall\alpha \Rightarrow \theta x+(1-\theta)y\in\cap E_\alpha) encode.

### 1.2 Intersection viewpoint via indicator functions (alternate perspective)

Define the **indicator function** of a set (C):

[
\delta_C(x) =
\begin{cases}
0, & x\in C,\
+\infty, & x\notin C.
\end{cases}
]

A set (C) is convex iff (\delta_C) is a convex function in the sense

[
\delta_C(\theta x + (1-\theta)y)
\le \theta\delta_C(x) + (1-\theta)\delta_C(y).\tag{*}
]

* If both (x,y) are in (C), RHS is (0), so (*) says the middle point must also have indicator 0 → in (C).
* If either point is outside, RHS is (+\infty), and the inequality is trivially satisfied.

Now for intersections:

[
\delta_{\cap_\alpha C_\alpha}(x)
= \sup_\alpha \delta_{C_\alpha}(x),
]
because the indicator is (0) only if **all** indicators are 0.

The supremum of convex functions is convex. So the indicator of intersection is convex ⇒ intersection is convex.

Same result, different language. This perspective becomes useful later when you start manipulating convex problems via conjugates and duality.

---

## 1.3 The trigonometric polynomial set (S)

The example:

[
S = {x\in\mathbb{R}^m \mid |p_x(t)|\le 1 \text{ for all } |t|\le \pi/3},
]
where
[
p_x(t) = x_1\cos t + x_2\cos 2t + \cdots + x_m\cos mt.
]

Your blue notes:

* For each fixed (t), define
  [
  E_t = \left{x\in\mathbb{R}^m \mid
  \left|x_1\cos t + \dots + x_m\cos mt\right| \le 1\right}.
  ]
* Then
  [
  S = \bigcap_{|t|\le\pi/3} E_t.
  ]

Exactly right.

### 1.3.1 Convexity of each (E_t)

Fix one time (t_0). Define (c(t_0) = (\cos t_0,\dots,\cos mt_0)^T).

Then
[
p_x(t_0) = c(t_0)^T x.
]

Constraint (|p_x(t_0)|\le1) becomes:

[
-1 \le c(t_0)^T x \le 1
\quad\Longleftrightarrow\quad
\begin{cases}
c(t_0)^T x \le 1,\
-c(t_0)^T x \le 1.
\end{cases}
]

Geometrically, in (x)-space:

* (c(t_0)^T x = 1) is a hyperplane.
* (c(t_0)^T x \le 1) is the halfspace “one side” of that hyperplane.
* (-c(t_0)^T x \le 1) is the opposite halfspace defined by the hyperplane (c(t_0)^T x = -1).
* The intersection is an infinite **slab** between two parallel hyperplanes.

This slab is convex because it is intersection of two convex halfspaces.

So each (E_{t_0}) is convex.

### 1.3.2 Now (S = \bigcap_{|t|\le\pi/3} E_t)

Since each (E_t) is convex, and intersection of any family of convex sets is convex, (S) is convex.

That’s the intersection proof.

### 1.3.3 Alternate proof via supremum of convex functions

For each fixed (t),

* (x\mapsto c(t)^T x) is linear.
* (x\mapsto |c(t)^T x|) is convex (absolute value of linear function).

Define
[
F(x) = \sup_{|t|\le\pi/3} |c(t)^T x|.
]

Supremum of convex functions is convex. So (F) is convex.

Then
[
S = {x \mid F(x) \le 1}
]
is a sublevel set of a convex function ⇒ convex.

Both perspectives (intersection of convex sets vs sublevel of sup) are equivalent because:

[
F(x)\le 1 \iff |c(t)^T x|\le 1\ \forall t\in[-\pi/3,\pi/3].
]

### 1.3.4 What does (S) look like for (m=2)?

When the slide draws “the set (S) for (m=2)”, we have

[
p_x(t) = x_1 \cos t + x_2 \cos 2t,
\quad x = (x_1,x_2)\in\mathbb{R}^2.
]

We want all (x) such that
[
|x_1\cos t + x_2\cos 2t| \le 1
\quad\forall t\in[-\pi/3,\pi/3].
]

For each fixed (t), this is a *strip* between two parallel lines. As (t) varies over ([- \pi/3,\pi/3]), you intersect all those strips. The resulting region is:

* Symmetric with respect to the origin (since if (x) is allowed, then (-x) is allowed: the function (p_{-x}(t) = -p_x(t)) just flips sign and still has |·| ≤1).
* A convex polygon-ish shape (or a rounded convex set if you treat time as continuous), bounded because large coefficients blow (|p_x(t)|) above 1 at some (t).

So in the picture:

* Each straight line drawn is something like (c(t)^T x = 1) or (=-1) for some (t).
* The region (S) is the convex polygon where all these strips overlap.

You can think of it as:

“Every direction in the space of trigonometric polynomials gives you a line; restricting the max amplitude to 1 cuts down the coefficients to lie inside this convex cage.”

---

## 2. Convex combination and convex hull

We now revisit convex combinations and convex hulls and go deeper.

### 2.1 Convex combination recap

Given (x_1,\dots,x_k\in\mathbb{R}^n), any vector

[
x = \sum_{i=1}^k \theta_i x_i,\quad
\theta_i\ge0,\quad
\sum_{i=1}^k \theta_i = 1,
]

is a convex combination.

Key property: convex combinations **preserve convexity**:

* (C) is convex ⇔ it contains every convex combination of its points.

You can also think of ((\theta_1,\dots,\theta_k)) as “weights” or “probabilities”. A convex combination is like the expectation of a random point that equals (x_i) with probability (\theta_i).

### 2.2 Convex hull – two equivalent definitions

Let (S\subseteq\mathbb{R}^n).

**Definition 1 (via combinations).**

[
\operatorname{conv}(S)
= \Big{\text{all finite convex combinations of points in }S\Big}.
]

Explicitly:
[
\operatorname{conv}(S)
= \left{\sum_{i=1}^k \theta_i x_i
;\middle|;
k\ge1,\ x_i\in S,\ \theta_i\ge0,\ \sum_i\theta_i=1
\right}.
]

**Definition 2 (via “smallest convex set”).**

[
\operatorname{conv}(S)
= \bigcap{C\subseteq\mathbb{R}^n \mid C\text{ is convex and } S\subseteq C}.
]

That is, intersect *all* convex sets that contain (S). The intersection is convex (intersection property), and it is the **smallest convex set containing (S)** in the sense of inclusion.

We proved equivalence already; I’ll just emphasize the logic:

* First show: set of all convex combinations, call it (H_1), is itself convex and contains (S).
* Then: *any* convex set (C) containing (S) must also contain every convex combination of points in (S). So (H_1\subseteq C) for all such (C) ⇒ (H_1\subseteq \bigcap C = H_2).
* But (H_1) is one of the convex sets that contain (S), so (H_2\subseteq H_1).
* Hence (H_1=H_2). QED.

This is nice:

* Definition 1 is *constructive* (“how to build the hull”).
* Definition 2 is *universal property* (“what the hull *is* abstractly”).

Both are important.

### 2.3 Concrete examples of convex hulls

1. **Two points (x_1,x_2)**

   (\operatorname{conv}{x_1,x_2} = {\theta x_1 + (1-\theta)x_2 \mid 0\le\theta\le1}): the line segment between them.

2. **Three non-collinear points in (\mathbb{R}^2)**

   (\operatorname{conv}{x_1,x_2,x_3}) is the filled triangle (including edges and vertices).

3. **A set of points forming a polygon**

   If (S) is a finite point set in the plane, (\operatorname{conv}(S)) is a convex polygon; some points will be on the boundary (extreme points), others inside (not needed for the hull).

4. **Convex hull of a circle**

   Take (S) = unit circle in (\mathbb{R}^2). Then (\operatorname{conv}(S)) is the filled disk: all points with (|x|_2\le1).

   Why? Every interior point is a convex combination (in fact, average) of finitely many points on the circle; and any convex set containing the circle must also contain the disk.

5. **Convex hull of a nonconvex blob**

   Picture on the slide: some wiggly nonconvex shape. The convex hull “fills in” all the dents and holes and wraps a tight convex “rubber band” around the set.

### 2.4 Extra fact: Carathéodory’s theorem (very useful later)

> **Carathéodory’s theorem.**
> In (\mathbb{R}^n), any point in (\operatorname{conv}(S)) can be written as a convex combination of at most (n+1) points of (S).

This is *not* required for your slides, but it’s good to know. Consequences:

* In (\mathbb{R}^2): every point in the convex hull is a convex combination of at most 3 points. So you never need more than a triangle worth of points.
* In (\mathbb{R}^3): at most 4 points (a tetrahedron).

Practically: if you’re searching over convex combinations, you can always restrict to (n+1) points.

### 2.5 Extreme points (intuition pointer)

An **extreme point** of a convex set (C) is a point that **cannot** be written as a nontrivial convex combination of other points in (C). For polyhedra, these are the “vertices”. For a convex polygon, the corners.

For finite (S), the extreme points of (\operatorname{conv}(S)) are exactly the points in (S) that lie on the “convex hull boundary”; interior points of (S) are not extreme.

This matters later for linear programming: optimal solutions live at extreme points.

---

## 3. Affine maps (f(x)=Ax+b): what they do to convex sets

Affine maps are what you get with “linear transform + translation”. They preserve straight-line structure: lines ↦ lines, segments ↦ segments, convex combos ↦ convex combos.

### 3.1 Characterization: affine maps preserve convex combinations

A map (f:\mathbb{R}^n\to\mathbb{R}^m) is **affine** iff

[
f!\left(\sum_{i=1}^k \theta_i x_i\right)
= \sum_{i=1}^k \theta_i f(x_i)
\quad\text{for all }x_i\text{ and all }\theta_i\ge0,\ \sum_i\theta_i=1.
]

For (f(x)=Ax+b), check:

[
\begin{aligned}
f!\left(\sum_i \theta_i x_i\right)
&= A\left(\sum_i \theta_i x_i\right) + b
= \sum_i \theta_i A x_i + b \
&= \sum_i \theta_i (Ax_i + b)
\quad\text{because }\sum_i\theta_i = 1,\
&= \sum_i \theta_i f(x_i).
\end{aligned}
]

So affine maps preserve convex combinations. Conversely, any map that preserves convex combinations must be affine.

That’s why they play nicely with convexity.

### 3.2 Image of a convex set is convex (again, with this perspective)

Let (S\subseteq\mathbb{R}^n) be convex.

Take two points in the image: (y_1 = f(x_1)), (y_2 = f(x_2)) with (x_1,x_2\in S). For (\theta\in[0,1]):

[
\theta y_1 + (1-\theta)y_2
= \theta f(x_1) + (1-\theta)f(x_2)
= f(\theta x_1 + (1-\theta)x_2).
]

Since (S) is convex, (\theta x_1 + (1-\theta)x_2\in S), so its image is in (f(S)). Thus (f(S)) is convex.

So the one-liner explanation:

> **Affine maps commute with convex combinations.**
> Convex sets are exactly the sets closed under convex combinations.
> So images of convex sets under affine maps are convex.

### 3.3 Preimage of a convex set is convex, via convex combinations

Let (C\subseteq\mathbb{R}^m) be convex.

Take (x_1,x_2\in f^{-1}(C)). That means:
[
f(x_1)\in C,\quad f(x_2)\in C.
]

By convexity of (C),

[
\theta f(x_1) + (1-\theta)f(x_2)\in C.
]

But
[
\theta f(x_1) + (1-\theta)f(x_2)
= f(\theta x_1 + (1-\theta)x_2),
]
since (f) is affine and preserves convex combos.

So
[
f(\theta x_1 + (1-\theta)x_2)\in C
\quad\Rightarrow\quad
\theta x_1 + (1-\theta)x_2 \in f^{-1}(C).
]

Hence (f^{-1}(C)) is convex.

### 3.4 Geometric examples (your sketches)

1. **Projection**
   (f:\mathbb{R}^3\to\mathbb{R}^2), (f(x,y,z)=(x,y)).

   * A 3D convex set (say, a polyhedron) projects to a 2D convex set (its shadow on a plane).
   * This is extensively used in optimization: many feasible sets are projections of simpler sets in higher dimension (e.g., linear programming polyhedra, epigraphs).

2. **Slice / Section**
   Preimage of a convex set can be seen as a “slice” of space. Example:
   [
   f(x)=c^T x,\quad C=(-\infty,1].
   ]
   Then
   [
   f^{-1}(C) = {x\mid c^T x \le 1},
   ]
   a halfspace. That’s literally cutting space with a hyperplane.

3. **Perspective example**
   Imagine a 3D convex object and a light source projecting a shadow on a plane: that’s an affine image.
   On the other hand, taking the set of points casting rays through a window is like a preimage of a convex window region under a projection direction.

4. **Simple 2D example (f(x,y)=x/y)**

   Your lower-right doodle: (f:\mathbb{R}^2\to\mathbb{R}), (f(x,y)=x/y) (division is not affine!). This is a nice counterexample direction:

   * The preimage of ((-\infty,1]) under this map is ({(x,y)\mid x\le y}). This set is actually convex.
   * But in general, nonlinear maps don’t preserve convexity so cleanly: you can find convex sets whose images under nonlinear maps become nonconvex.

   Moral: the theorems “image/preimage preserve convexity” **rely** on the map being affine.

---

## 4. Examples slide: norm balls, hyperbolic cone, LMI

Now we apply the machinery to concrete forms.

### 4.1 Image and preimage of norm balls

Let (|\cdot|) be any norm on (\mathbb{R}^n), and let (B = {z\mid|z|\le1}) be its unit ball.

Take an affine map (f(x)=Ax+b).

#### 4.1.1 Image: ({Ax+b \mid |x|\le1} = f(B))

* Since (B) is convex and (f) is affine, (f(B)) is convex.
* In Euclidean case ((|\cdot|_2)), if (A) is invertible, this is an ellipsoid:
  [
  f(B) = {y\mid (y-b)^T (AA^T)^{-1} (y-b)\le 1}.
  ]
* If (A) is rank-deficient, you get a “flattened” ellipsoid living in an affine subspace (lower-dimensional ellipsoid).

So this is a general way to get ellipsoids and their degenerate versions.

#### 4.1.2 Preimage: ({x\mid|Ax + b|\le1} = f^{-1}(B))

* Since (B) convex and (f) affine, this preimage is convex, by the preimage theorem.

* For (|\cdot|_2), we can write it explicitly:

  [
  |Ax + b|_2^2 \le 1
  \quad\Longleftrightarrow\quad
  (Ax + b)^T (Ax + b) \le 1.
  ]

  Expand:
  [
  x^T A^T A x + 2 b^T A x + b^T b - 1 \le 0.
  ]

  This is a **quadratic inequality** in (x). If (A) has full column rank (so (A^TA \succ 0)), the set is an ellipsoid. If not, you can get cylinders, strips, etc., but always convex.

* For other norms ((\ell_1, \ell_\infty)), the set is still convex but geometry changes: under (\ell_\infty) you’ll get something like the preimage of a cube, which is a polyhedron.

This pattern is essentially:

> “Any set defined by a norm of an affine expression being bounded is convex.”

You’ll see constraints like (|Ax-b|_2\le \epsilon) *everywhere* (least squares with robustness, etc.).

---

### 4.2 Hyperbolic cone

Given (P\in S_+^n) and (c\in\mathbb{R}^n):

[
K_{\text{hyp}} = {x\mid x^T P x \le (c^T x)^2,\ c^T x \ge 0}.
]

Let’s unpack:

1. Quadratic form (x^T P x) is nonnegative because (P\succeq0).
2. Right side is the square of a linear form (c^T x).
3. The inequality is homogeneous of degree 2. So the set is cone-shaped.

#### 4.2.1 Rewrite with a square root (P^{1/2})

Since (P\succeq0), take (P^{1/2}) with (P^{1/2}P^{1/2} = P). Then:

[
x^T P x = x^T P^{1/2}P^{1/2}x = |P^{1/2}x|_2^2.
]

Inequality:

[
|P^{1/2}x|_2^2 \le (c^T x)^2,\quad c^T x \ge 0.
]

Given the second constraint, this is equivalent to

[
|P^{1/2}x|_2 \le c^T x.
]

So

[
K_{\text{hyp}} = {x \mid |P^{1/2}x|_2 \le c^T x}.
]

Note that if (|P^{1/2}x|\le c^T x), then automatically (c^T x\ge0) because the left side is nonnegative. So the explicit (c^T x\ge0) is redundant.

#### 4.2.2 Show it’s a cone

Take (x\in K_{\text{hyp}}) and (\lambda\ge0). Then:

[
|P^{1/2}(\lambda x)|_2 = \lambda|P^{1/2}x|_2 \le \lambda c^T x = c^T (\lambda x).
]

So (\lambda x\in K_{\text{hyp}}). That’s the cone property.

#### 4.2.3 As preimage of SOC (convexity proof)

Define linear map (F:\mathbb{R}^n\to\mathbb{R}^{n+1}):

[
F(x) = (P^{1/2}x,\ c^T x).
]

Let the second-order cone (K_{\text{soc}}\subset\mathbb{R}^{n+1}) be:

[
K_{\text{soc}} = {(u,t)\mid |u|_2 \le t}.
]

Then
[
K_{\text{hyp}} = F^{-1}(K_{\text{soc}}).
]

* (K_{\text{soc}}) is a convex cone (norm cone).
* (F) is linear (affine with (b=0)).
* Preimage of a convex set under a linear/affine map is convex.

Therefore:

> (K_{\text{hyp}}) is a convex cone.

This is the “clean” proof.

#### 4.2.4 Geometry in small dimension

Take (n=2) for intuition. Suppose (P\succ0) and (c\neq0).

* Fix a value (t = c^T x > 0).
* The inequality (x^T P x \le t^2) defines an ellipsoid (centered at origin).
* Intersect this ellipsoid with the hyperplane (c^T x = t): that intersection is an ellipse (or an ellipsoidal slice).

As (t) increases, these slices scale outwards. The union of all these slices for (t\ge0) forms a kind of cone whose cross-sections (perpendicular to (c)) are ellipses.

So the “hyperbolic cone” is: cone made of ellipsoids stacked along the direction of (c).

---

### 4.3 Solution set of a linear matrix inequality (LMI)

General form on the slide:

[
\mathcal{L} = \left{x\in\mathbb{R}^m ,\middle|,
x_1A_1 + \cdots + x_m A_m \preceq B
\right},\quad A_i,B\in S^p.
]

This is an **LMI**: the matrix (B - \sum x_i A_i) is required to be PSD.

#### 4.3.1 Rewrite as PSD cone preimage

Define affine map (F:\mathbb{R}^m\to S^p):

[
F(x) = B - \sum_{i=1}^m x_i A_i.
]

Then

[
\mathcal{L} = F^{-1}(S_+^p),
]
where (S_+^p) is the PSD cone (convex cone).

* (F) is affine.
* (S_+^p) is convex.

So by the preimage theorem:

> (\mathcal{L}) is convex.

This is the standard meta-argument for “LMI constraints define convex feasible sets.”

#### 4.3.2 Direct convexity check using PSD cone convexity

Take (x,y\in\mathcal{L}). That means:

[
B - \sum_i x_i A_i \succeq 0,\quad
B - \sum_i y_i A_i \succeq 0.
]

For (\theta\in[0,1]), define (z = \theta x + (1-\theta)y). We want to check:

[
B - \sum_i z_i A_i \succeq 0.
]

Compute:

[
\begin{aligned}
B - \sum_i z_i A_i
&= B - \sum_i (\theta x_i + (1-\theta)y_i)A_i \
&= B - \theta\sum_i x_i A_i - (1-\theta)\sum_i y_i A_i \
&= \theta\left(B - \sum_i x_i A_i\right)

* (1-\theta)\left(B - \sum_i y_i A_i\right).
  \end{aligned}
  ]

Each term in parentheses is PSD because (x,y) are feasible. Since (S_+^p) is a convex cone, their convex combination is also PSD. So the matrix is PSD, and (z\in\mathcal{L}).

So again: (\mathcal{L}) is convex.

#### 4.3.3 Concrete tiny example

Take (p=2) and:

[
A_1 = \begin{pmatrix}1 & 0\ 0 & 0\end{pmatrix},\quad
A_2 = \begin{pmatrix}0 & 0\ 0 & 1\end{pmatrix},\quad
B = \begin{pmatrix}1 & 0\ 0 & 1\end{pmatrix} = I_2.
]

Then the LMI is:

[
x_1 A_1 + x_2 A_2 \preceq I_2
\quad\Longleftrightarrow\quad
\begin{pmatrix}x_1 & 0\0 & x_2\end{pmatrix}
\preceq
\begin{pmatrix}1 & 0\0 & 1\end{pmatrix}.
]

That is:
[
\begin{pmatrix}1-x_1 & 0\0 & 1-x_2\end{pmatrix}
\succeq 0.
]

This is PSD iff both diagonal entries are nonnegative:

[
1 - x_1 \ge 0,\quad 1 - x_2 \ge 0,
]
i.e.
[
x_1 \le 1,\quad x_2 \le 1.
]

There’s **no lower bound** from the PSD condition here; so (\mathcal{L}) is just the halfspace region “below and left of (1,1)” in (\mathbb{R}^2):

[
\mathcal{L} = {(x_1,x_2) \mid x_1\le1,\ x_2\le1}.
]

That region is obviously convex. This shows how fairly mundane convex sets can be expressed as LMI sets; conversely, more subtle LMIs give nontrivial convex sets that are *not* polyhedra.

---

## Big conceptual summary for this batch

You now have a very detailed mental map of:

1. **Intersection as a convexity-preserving operation.**
   Used to build sets like the trig polynomial (S) by intersecting infinite families of halfspaces (or equivalently: sublevel of sup of convex functions).

2. **Convex hull** as both:

   * The set of all finite convex combinations of points in (S).
   * The smallest convex set containing (S) (intersection of all convex supersets).
     Bonus: it only needs at most (n+1) points in (\mathbb{R}^n) (Carathéodory).

3. **Affine maps** as convex-structure-preserving transformations:

   * They preserve convex combinations (key characterization).
   * So images of convex sets are convex; preimages of convex sets are convex.
   * This explains why projections, scalings, translations, and linear constraints all “play nicely” with convexity.

4. **Prototype convex sets generated via affine maps:**

   * Images and preimages of norm balls → ellipsoids and ellipsoidal regions.
   * Hyperbolic cone → preimage of second-order cone under linear map.
   * LMI solution sets → preimages of the PSD cone under an affine map.

Over and over, you’ll look at some constraint set in later lectures and immediately try to express it as:

> intersection / affine (pre)image / sublevel set of some basic convex object (ball, halfspace, SOC, PSD cone, etc.).

That pattern is basically the core skill of “recognizing convexity” in optimization.



I’ll go through everything in this batch:

1. Perspective map (P(x,t)=x/t)

   * Geometric meaning
   * Prove: image of convex set is convex
   * Prove: inverse image of convex set is convex
   * Decode the μ–formula on the slide in full detail
2. Linear–fractional map (f(x) = \dfrac{Ax+b}{c^T x + d})

   * Show it is “affine + perspective”
   * Use that to prove images/preimages of convex sets are convex
   * Explain why your attempt to check “f preserves convex combos” fails (because it doesn’t, in general)
   * Work through the 2D example (f(x) = \dfrac{x}{x_1+x_2+1})
3. Proper cones

   * Definition (closed, solid, pointed) and why each condition matters
   * Check examples: (\mathbb{R}^n_+), (S^n_+), nonnegative polynomials
4. Generalized inequalities defined by a proper cone

   * Definition of (\preceq_K) and (\prec_K)
   * Examples: componentwise inequality, matrix inequality
   * Prove main properties: addition, scaling, transitivity, antisymmetry, limit/closure, basic facts for strict inequality

I’ll be explicit with all algebra and also keep connecting to the little sketches you added.

---

## 1. Perspective map (P(x,t) = x/t)

### 1.1 Definition and domain

The **perspective map** is

[
P:\mathbb{R}^{n+1}\to\mathbb{R}^n,\qquad
P(x,t) = \frac{x}{t},
]
with **domain**
[
\operatorname{dom}P = {(x,t)\in\mathbb{R}^{n+1}\mid t>0}.
]

So for a point in (\mathbb{R}^{n+1}), you’re looking at “horizontal coordinates (x)” divided by “height (t)”.

Geometric picture (important intuition):

* Think of ((x,t)) as a point in (\mathbb{R}^{n}\times\mathbb{R}), where last coordinate is “height”.
* Imagine a light source at the origin looking at the plane (t=1).
* (P(x,t)) is the intersection of the ray from the origin through ((x,t)) with the plane (t=1).

  * The ray is ({\lambda(x,t)\mid \lambda\ge0}).
  * At the plane (t=1), we need (\lambda t = 1 \Rightarrow \lambda=1/t).
  * So intersection point is ((\lambda x, \lambda t) = (x/t, 1)).
  * The “projected” (n)-dimensional point is (x/t).

So perspective = central projection from origin onto the plane (t=1), ignoring the last coordinate.

This is why perspective transformations show up all over projective geometry and computer graphics.

---

### 1.2 Claim: image of a convex set under (P) is convex

> If (S\subseteq\mathbb{R}^{n+1}) is convex and (S\subseteq\operatorname{dom}P) (so every point has (t>0)), then (P(S)\subseteq\mathbb{R}^n) is convex.

This is exactly the statement the slide 2.24 proves.

#### Setup

Let (S\subseteq\mathbb{R}^{n+1}) be convex. Take two points in the image:

[
y_1, y_2 \in P(S).
]

By definition of the image, there exist ((x_1,t_1), (x_2,t_2) \in S) with (t_1,t_2>0) such that

[
y_1 = \frac{x_1}{t_1},\quad y_2 = \frac{x_2}{t_2}.
]

We must show:
[
\theta y_1 + (1-\theta)y_2 \in P(S)\quad \forall\theta\in[0,1].
]

So pick any (\theta\in[0,1]), define

[
y = \theta y_1 + (1-\theta)y_2.
]

We want to write (y) as (x/t) for some ((x,t)\in S).

#### Trick: find ((x,t)) whose perspective is (y)

Goal: find (\mu\in[0,1]) such that if we set

[
(x,t) = \mu(x_1,t_1) + (1-\mu)(x_2,t_2),
]
then
[
\frac{x}{t} = y.
]

That is exactly what the slide does.

Let’s work this out step-by-step.

1. First write down (y) explicitly:

   [
   y = \theta \frac{x_1}{t_1} + (1-\theta)\frac{x_2}{t_2}.
   ]

2. We want to express this as (x/t) with
   [
   (x,t) = \mu(x_1,t_1) + (1-\mu)(x_2,t_2)
   ]
   for some (\mu).

   Then
   [
   x = \mu x_1 + (1-\mu) x_2,\qquad
   t = \mu t_1 + (1-\mu) t_2.
   ]

   So
   [
   \frac{x}{t} = \frac{\mu x_1 + (1-\mu)x_2}{\mu t_1 + (1-\mu)t_2}.
   ]

   We want this to equal (y).

3. Equate this desired expression with (y):

   [
   \theta \frac{x_1}{t_1} + (1-\theta)\frac{x_2}{t_2}
   = \frac{\mu x_1 + (1-\mu)x_2}{\mu t_1 + (1-\mu)t_2}.
   ]

   Multiply both sides by (\mu t_1 + (1-\mu) t_2):

   [
   \left(\mu t_1 + (1-\mu)t_2\right)\left(
   \theta \frac{x_1}{t_1} + (1-\theta)\frac{x_2}{t_2}
   \right)
   = \mu x_1 + (1-\mu)x_2.
   ]

   This needs to hold as a vector equality for *all* (x_1,x_2), so we match coefficients of (x_1) and (x_2) separately.

   Coefficient in front of (x_1):

   * LHS: (\left(\mu t_1 + (1-\mu)t_2\right)\theta \frac{1}{t_1}).
   * RHS: (\mu).

   So we want
   [
   \left(\mu t_1 + (1-\mu)t_2\right)\theta \frac{1}{t_1} = \mu.
   ]

   Similarly for (x_2):

   * LHS: (\left(\mu t_1 + (1-\mu)t_2\right)(1-\theta)\frac{1}{t_2}).
   * RHS: (1-\mu).

   So:
   [
   \left(\mu t_1 + (1-\mu)t_2\right)(1-\theta)\frac{1}{t_2} = 1-\mu.
   ]

4. Solve for (\mu) from the first equation.

   The equation
   [
   \left(\mu t_1 + (1-\mu)t_2\right)\theta \frac{1}{t_1} = \mu
   ]
   is
   [
   \theta\frac{1}{t_1}(\mu t_1 + (1-\mu)t_2) = \mu.
   ]

   Expand:
   [
   \theta (\mu + (1-\mu)\tfrac{t_2}{t_1}) = \mu.
   ]

   Solve for (\mu). A cleaner way (matching the slide) is to guess the final formula:

   Set
   [
   \mu = \frac{\theta / t_1}{\theta / t_1 + (1-\theta)/t_2}.
   ]

   Let’s verify this works.

   Compute denominator:
   [
   D := \theta/t_1 + (1-\theta)/t_2 > 0,
   ]
   because (t_1,t_2>0).

   Then (\mu = (\theta/t_1)/D), (1-\mu = ((1-\theta)/t_2)/D).

   Now compute (t):

   [
   t = \mu t_1 + (1-\mu) t_2
   = \frac{\theta}{t_1 D} t_1 + \frac{1-\theta}{t_2 D} t_2
   = \frac{\theta + (1-\theta)}{D}
   = \frac{1}{D}.
   ]

   So (t = 1/D). Then

   [
   \mu t = \mu \frac{1}{D} = \frac{\theta/t_1}{D} \cdot \frac{1}{D} = \frac{\theta}{t_1 D^2},
   ]
   which is messy, but we don’t actually need it. Instead, compute (x/t):

   [
   \frac{x}{t}
   = \frac{\mu x_1 + (1-\mu)x_2}{t}
   = \frac{\mu x_1 + (1-\mu)x_2}{1/D}
   = D,\mu x_1 + D,(1-\mu)x_2.
   ]

   But (D\mu = \theta/t_1) and (D(1-\mu) = (1-\theta)/t_2). So

   [
   \frac{x}{t}
   = \frac{\theta}{t_1} x_1 + \frac{1-\theta}{t_2} x_2
   = \theta \frac{x_1}{t_1} + (1-\theta)\frac{x_2}{t_2}
   = y.
   ]

   Exactly what we wanted.

So with this choice
[
\mu = \frac{\theta / t_1}{\theta / t_1 + (1-\theta)/t_2},
]
we have:

* ((x,t) = \mu(x_1,t_1) + (1-\mu)(x_2,t_2)) is a convex combination of ((x_1,t_1)) and ((x_2,t_2)).
* Since (S) is convex, ((x,t)\in S).
* Then (y = x/t = P(x,t) \in P(S)).

Thus any convex combination (y) of two points in (P(S)) lies in (P(S)); hence (P(S)) is convex.

That is exactly the algebra hidden in the slide’s short line.

---

### 1.3 Inverse image of convex set under (P) is convex (sketch rigorously)

Now we prove the other statement:

> If (C\subseteq\operatorname{dom}P) is convex in (\mathbb{R}^n), then (P^{-1}(C) = {(x,t)\mid t>0, x/t \in C}) is convex in (\mathbb{R}^{n+1}).

Take two points ((x_1,t_1)) and ((x_2,t_2)) in (P^{-1}(C)). That means:

* (t_1,t_2 >0), and
* (x_1/t_1 \in C), (x_2/t_2 \in C).

Pick (\theta\in[0,1]) and consider convex combination

[
(x,t) = \theta (x_1,t_1) + (1-\theta)(x_2,t_2)
= (\theta x_1 + (1-\theta)x_2,\ \theta t_1 + (1-\theta) t_2).
]

We must show:

* (t>0). That’s immediate: convex combination of positive numbers is positive.
* (\frac{x}{t} \in C).

Now note (t>0), so (x/t) is well-defined. Consider the weights

[
\lambda_1 = \frac{\theta t_1}{t},\quad
\lambda_2 = \frac{(1-\theta) t_2}{t}.
]

These satisfy:

* (\lambda_1,\lambda_2\ge0) because (\theta,(1-\theta), t_1,t_2\ge0).
* (\lambda_1 + \lambda_2 = (\theta t_1 + (1-\theta)t_2)/t = t/t = 1).

So they form a convex weight pair. Now compute (x/t):

[
\begin{aligned}
\frac{x}{t}
&= \frac{\theta x_1 + (1-\theta)x_2}{t} \
&= \frac{\theta t_1}{t}\frac{x_1}{t_1} + \frac{(1-\theta)t_2}{t}\frac{x_2}{t_2} \
&= \lambda_1\frac{x_1}{t_1} + \lambda_2\frac{x_2}{t_2}.
\end{aligned}
]

This is a convex combination (weights (\lambda_1,\lambda_2)) of two points in (C). Since (C) is convex, that combination is in (C). Thus

[
\frac{x}{t} \in C
\quad\Rightarrow\quad
(x,t)\in P^{-1}(C).
]

So (P^{-1}(C)) is convex.

This is the inverse-image version of the same “reweighting” trick we used above.

---

## 2. Linear–fractional map (f(x) = \dfrac{Ax+b}{c^T x + d})

Slide definition:

* Shape: (f:\mathbb{R}^n\to\mathbb{R}^m).
* Data: (A\in\mathbb{R}^{m\times n}), (b\in\mathbb{R}^m), (c\in\mathbb{R}^n), (d\in\mathbb{R}).
* Formula:
  [
  f(x) = \frac{Ax+b}{c^T x + d}.
  ]
* Domain:
  [
  \operatorname{dom}f = {x \in \mathbb{R}^n\mid c^T x + d > 0},
  ]
  because we don’t want to divide by zero, and we also need positivity for the perspective theory.

Your purple/blue scribbles where you try to check
[
\theta f(x) + (1-\theta)f(y) \stackrel{?}{=} f(\theta x + (1-\theta)y)
]
are showing exactly that: *is this map affine?* In general, the answer is no.

But we don’t need (f) to preserve convex combinations directly. Instead we use:

> A linear–fractional map is the composition of an affine map and the perspective map.

Once we know that, we chain our previous results:

* Affine has convex-image/preimage property.
* Perspective has convex-image/preimage property.
* Composition of such maps also preserves that property.

### 2.1 Decompose (f) as affine + perspective

Define an affine map (g:\mathbb{R}^n \to \mathbb{R}^{n+1}):

[
g(x) = \begin{pmatrix}
Ax + b\
c^T x + d
\end{pmatrix}
= (Ax+b,\ c^T x + d).
]

Then apply the perspective map (P) to the result:

[
P(g(x)) = \frac{Ax+b}{c^T x + d}.
]

So:
[
f = P\circ g.
]

Domain consideration:

* (g(x)) has last coordinate (c^T x + d).
* We need this to be (>0) to be in the domain of (P).
* So (\operatorname{dom}f = {x \mid c^T x + d > 0} = g^{-1}(\operatorname{dom}P)).

This is exactly what the slide says: “The composition of the perspective function and an affine function”.

### 2.2 Why (f(S)) is convex for convex (S \subset \operatorname{dom}f)

Take a convex set (S\subseteq\operatorname{dom}f).

1. First apply the affine map (g):
   [
   T = g(S) \subseteq \mathbb{R}^{n+1}.
   ]
   Since (g) is affine and (S) convex, (T) is convex.

   Also, every element of (T) has last coordinate (c^T x+d > 0), so (T\subseteq\operatorname{dom}P).

2. Now apply the perspective map (P):
   [
   f(S) = P(g(S)) = P(T).
   ]
   Since (T) is convex and (T\subseteq\operatorname{dom}P), (P(T)) is convex.

So:

> If (S) is convex and contained in (\operatorname{dom}f), then (f(S)) is convex.

We didn’t need (f) itself to preserve convex combos in the raw sense. We only needed the structure “affine → perspective”.

### 2.3 Why (f^{-1}(C)) is convex for convex (C)

Take a convex set (C\subseteq\mathbb{R}^m). We want to show
[
f^{-1}(C) = {x\in\operatorname{dom}f \mid f(x)\in C}
]
is convex.

Observe:
[
f^{-1}(C)
= {x \mid P(g(x))\in C,\ c^T x + d > 0}.
]

Define
[
D := {P}^{-1}(C) = {(u,t)\in\operatorname{dom}P\mid u/t\in C}.
]

We have:

* Since (C) is convex, (P^{-1}(C)) is convex (preimage under perspective).
* (\operatorname{dom}P) is convex (half-space (t>0)). Intersecting with that keeps convexity.

Now
[
f^{-1}(C) = {x\mid g(x)\in D} = g^{-1}(D).
]

Since (D) is convex and (g) is affine, (g^{-1}(D)) is convex.

So:

> Preimage of a convex set under a linear–fractional map is convex.

### 2.4 Why your “direct convex combination check” fails (and that’s fine)

Your scribble:

[
t f(x) + (1-t) f(y) \stackrel{?}{=} f(tx + (1-t)y).
]

In general, this is false; only affine maps satisfy that.

Quick 1D counterexample:

* Take (f(x) = \frac{1}{x}) on domain ((0,\infty)) (this is a scalar linear–fractional with (A=0,b=1,c=1,d=0)).
* Pick (x=1), (y=3), (t=1/2):

  * Left: (\frac12 f(1) + \frac12 f(3) = \frac12(1) + \frac12(\frac13) = \frac12 + \frac16 = \frac{2}{3}).
  * Right: (f(\frac12\cdot1 + \frac12\cdot3) = f(2) = \frac12).

Not equal. So linear–fractional maps do **not** preserve midpoints or convex combos.

But they *do* send convex sets to convex and preimages of convex sets to convex because of the composition structure.

---

### 2.5 2D linear–fractional example from the slide

Slide: a linear–fractional map from (\mathbb{R}^2\to\mathbb{R}^2),

[
f(x) = \frac{1}{x_1 + x_2 + 1} x
= \frac{x}{x_1 + x_2 + 1},\quad x=(x_1,x_2).
]

Domain:
[
\operatorname{dom}f = {x \mid x_1 + x_2 + 1 > 0}.
]

So the line (x_1 + x_2 + 1 = 0) is a “forbidden line”; near it the map blows up.

Interpretation:

* Define (t(x) = x_1 + x_2 + 1 > 0).
* Then (f(x) = x / t(x)).
* This is a special case of perspective: think of embedding (x) to ((x, t(x))) in (\mathbb{R}^3), then projecting onto plane (t=1).

Properties:

* The line (x_1 + x_2 + 1 = 0) is mapped “to infinity”; as you approach it from the domain side, the denominator → 0+, so the image vector magnitude tends to infinity.
* Lines not parallel to that boundary get mapped to curves, but **images of convex regions** inside the domain remain convex.

If the left picture (C) is convex, then (f(C)) (right picture) looks skewed and nonlinearly warped, but is still convex.

You can even test: pick three points in (C), look at the triangle; its image (f) of that triangle will be convex, but generally not a triangle anymore—its edges become arcs, but the region is still convex.

---

## 3. Proper cones

Now we switch gears: cones and generalized inequalities.

### 3.1 Convex cones recap

A set (K\subseteq\mathbb{R}^n) is a **cone** if

[
x\in K,\ \lambda\ge0 \ \Rightarrow\ \lambda x\in K.
]

It is a **convex cone** if in addition it is convex, i.e.,

[
x,y\in K,\ \theta\in[0,1] \Rightarrow \theta x + (1-\theta)y\in K.
]

Equivalently, a convex cone is closed under all finite **nonnegative linear combinations**:
[
\sum_i \lambda_i x_i\in K \quad\text{whenever } x_i\in K,\ \lambda_i\ge0.
]

### 3.2 Proper cone definition

A **proper cone** (K\subseteq\mathbb{R}^n) is a convex cone that satisfies three extra properties:

1. **Closed**: it contains all its limit points. Formally, if (x_k\in K) and (x_k\to x), then (x\in K).
2. **Solid**: it has **nonempty interior** (in (\mathbb{R}^n)). That is, there exists some (x_0\in K) and radius (r>0) such that the ball (B(x_0,r)\subseteq K).
3. **Pointed**: it contains **no line**. Equivalently, if (x\in K) and (-x\in K), then (x=0). Another equivalent condition: (K\cap(-K) = {0}).

Informal meaning:

* Closed: no limit weirdness.
* Solid: really “lives in” full dimension; it’s not squashed into a lower-dimensional subspace.
* Pointed: it has a “tip” at 0 and opens in some directions but does not contain full lines through the origin.

Why do we care? Because:

> Proper cones induce “nice” partial orders on (\mathbb{R}^n) that behave like the usual (\le) on (\mathbb{R}).

We’ll see that in the next section.

### 3.3 Examples

#### 3.3.1 Nonnegative orthant (\mathbb{R}^n_+)

[
K = \mathbb{R}^n_+ = {x\in\mathbb{R}^n\mid x_i\ge0,\ i=1,\dots,n}.
]

* Cone: if (x_i\ge0) and (\lambda\ge0), then ((\lambda x)_i = \lambda x_i\ge0).
* Convex: sum of nonnegative coordinates stays nonnegative.
* Closed: inequalities (x_i\ge0) define a closed set.
* Solid: interior is ({x\mid x_i>0}), nonempty.
* Pointed: if (x\ge0) and (-x\ge0), then all (x_i\ge0) and (-x_i\ge0) ⇒ (x_i=0) ⇒ (x=0). So no line.

So (\mathbb{R}^n_+) is a proper cone.

#### 3.3.2 Positive semidefinite cone (S^n_+)

[
K = S^n_+ = {X\in S^n\mid X\succeq0}.
]

Recall:

* Cone: if (X\succeq0) and (\lambda\ge0), then (\lambda X\succeq0).
* Convex: if (X,Y\succeq0), then (\theta X + (1-\theta)Y\succeq0).
* Closed: PSD condition (z^T X z \ge0\ \forall z) is closed under limits; equivalently, eigenvalues are continuous.
* Solid: interior consists of *positive definite* matrices (X\succ0). There are many such matrices, e.g. identity.
* Pointed: if (X\succeq0) and (-X\succeq0), then (X=0). (Because any eigenvalue must be ≥0 and ≤0 ⇒ =0.)

So (S^n_+) is a proper cone in the space (S^n) (which we can view as (\mathbb{R}^{n(n+1)/2})).

#### 3.3.3 Nonnegative polynomials on ([0,1])

Set:
[
K = \left{x\in\mathbb{R}^n\mid p_x(t) = x_1 + x_2 t + \dots + x_n t^{n-1} \ge 0 \ \forall t\in[0,1]\right}.
]

So each vector (x) encodes a polynomial; (K) are the coefficient vectors whose polynomial is nonnegative on ([0,1]).

* Cone: if (p_x(t)\ge0) and (\lambda\ge0), then (\lambda p_x(t) \ge 0) ⇒ (x\in K\Rightarrow \lambda x\in K).
* Convex: if (p_x,p_y\ge0), then any nonnegative combination (\alpha p_x + \beta p_y) with (\alpha,\beta\ge0) is ≥0.
* Closed: nontrivial but true; condition “(p_x(t)\ge0\ \forall t\in[0,1])” is closed. (If coefficients converge, the polynomial sequence converges uniformly on compact interval, so limit is ≥0 as pointwise limit of ≥0 functions.)
* Solid: there exist strictly positive polynomials on [0,1], e.g. (p(t)=1). So you can perturb them a bit and stay inside.
* Pointed: if polynomial and its negative are both ≥0 on [0,1], then the polynomial is identically zero. So no line.

So this is again a proper cone, this time in (\mathbb{R}^n), but its geometry is more subtle.

---

## 4. Generalized inequalities via a proper cone

Now the slide defines **generalized inequalities**.

Given a proper cone (K\subseteq\mathbb{R}^n), define:

* **Non-strict inequality**:
  [
  x \preceq_K y \quad\Longleftrightarrow\quad y - x \in K.
  ]
* **Strict inequality**:
  [
  x \prec_K y \quad\Longleftrightarrow\quad y - x \in \operatorname{int}K.
  ]

Intuition:

* Think of (K) as the set of “nonnegative directions”.
* Then (y-x\in K) means “you can travel from (x) to (y) by moving in allowed nonnegative directions only”.

For usual real numbers:

* Take (K = \mathbb{R}_+ = [0,\infty)).
* Then (y-x\ge0) ⇔ (x\le y).
* (y-x>0) ⇔ (x<y).

So (\preceq_{\mathbb{R}_+}) is just ordinary (\le) on (\mathbb{R}).

### 4.1 Examples from the slide

#### 4.1.1 Componentwise inequality

Let (K = \mathbb{R}^n_+). Then
[
x \preceq_{\mathbb{R}^n_+} y
\iff y - x \in \mathbb{R}^n_+
\iff y_i - x_i \ge 0 \ \forall i
\iff x_i \le y_i\ \forall i.
]

So this is the usual “coordinatewise (\le)”.

In many contexts we just write (x\le y) for this and silently mean componentwise.

#### 4.1.2 Matrix inequality (Löwner order)

Let (K = S^n_+). Then
[
X \preceq_{S^n_+} Y
\iff Y - X \succeq 0.
]

So “(X\preceq Y)” means (Y-X) is PSD. Again, very standard in control, SDP, etc. Usually written just (X \preceq Y).

The strict version:
[
X \prec Y \iff Y-X\succ0.
]

You can think of

* eigenvalues of (Y-X) are all positive → (Y) is “bigger” in every quadratic direction.

Your red/blue scribbles with 2×2 matrices and inequalities are exactly playing with this.

---

### 4.2 Basic properties of (\preceq_K)

The slide plus tiny text screenshot lists several properties; let’s prove the key ones.

Assume (K) is a proper cone (at least convex cone, pointed, etc.). We’ll show:

1. **Preservation under addition**:
   [
   x\preceq_K y,\ u\preceq_K v
   \quad\Rightarrow\quad
   x+u \preceq_K y+v.
   ]

2. **Preservation under nonnegative scaling**:
   [
   x\preceq_K y,\ \alpha\ge0
   \quad\Rightarrow\quad
   \alpha x \preceq_K \alpha y.
   ]

3. **Reflexivity**:
   [
   x\preceq_K x.
   ]

4. **Transitivity**:
   [
   x\preceq_K y,\ y\preceq_K z
   \quad\Rightarrow\quad
   x\preceq_K z.
   ]

5. **Antisymmetry** (this is where pointedness matters):
   [
   x\preceq_K y,\ y\preceq_K x
   \quad\Rightarrow\quad
   x=y.
   ]

There are more (limit closure, compatibility with strict inequality); I’ll hit those after these.

---

#### 4.2.1 Preservation under addition

Suppose (x\preceq_K y) and (u\preceq_K v). By definition:

* (y-x\in K).
* (v-u\in K).

Since (K) is a convex cone, it’s closed under addition. So

[
(y-x)+(v-u) = (y+v) - (x+u) \in K.
]

Thus (x+u \preceq_K y+v).

Special important case (the one on slide): (x\preceq_K y) and we add the **same** vector (u) to both sides: since (0\preceq_K 0), we get:

[
x\preceq_K y \Rightarrow x+u\preceq_K y+u.
]

This is exactly like “ordinary” (\le) on (\mathbb{R}): adding the same number preserves inequality.

---

#### 4.2.2 Preservation under nonnegative scaling

Suppose (x\preceq_K y) and (\alpha\ge0). Then (y-x\in K). Since (K) is a cone, (\alpha(y-x) \in K). But

[
\alpha(y-x) = \alpha y - \alpha x.
]

Thus (\alpha x \preceq_K \alpha y).

Again, same feel as standard inequality: if (x\le y) and (\alpha\ge0), then (\alpha x \le \alpha y).

---

#### 4.2.3 Reflexivity

We want (x\preceq_K x) for all (x). By definition:

[
x\preceq_K x \iff x-x\in K \iff 0\in K.
]

And indeed, any cone must contain (0): (0 = 0\cdot x) for any (x\in K), and cones are closed under scaling by nonnegative scalars (including 0). So (0\in K). Done.

---

#### 4.2.4 Transitivity

Assume (x\preceq_K y) and (y\preceq_K z). Then:

* (y-x\in K).
* (z-y\in K).

Add them (stability under addition):

[
(y-x) + (z-y) = z-x \in K.
]

Thus (x\preceq_K z). So (\preceq_K) is transitive.

---

#### 4.2.5 Antisymmetry (needs pointedness)

We want:
[
x\preceq_K y,\ y\preceq_K x \quad\Rightarrow\quad x=y.
]

By definition:

* (y-x\in K).
* (x-y\in K).

So both (y-x) and its negation (-(y-x) = x-y) are in (K). This means (y-x\in K\cap(-K)).

For a **pointed** cone, (K\cap(-K) = {0}). So (y-x=0\Rightarrow x=y).

If (K) were **not** pointed (it contained a line), antisymmetry would fail: you could have nonzero (v) with (v,-v\in K), then (x\preceq_K x+v) and (x+v\preceq_K x) but (x\neq x+v).

This is why we impose “pointed” in the definition of proper cone: it ensures generalized inequality is not degenerate.

---

### 4.3 More properties (including strict inequality)

Let’s denote strict generalized inequality by (x\prec_K y) iff (y-x \in \operatorname{int}K).

Two important facts:

1. If (x\prec_K y) then (x\preceq_K y) and (x\neq y).
2. If (x\prec_K y) and (y\preceq_K z), then (x\prec_K z). Similarly, if (x\preceq_K y) and (y\prec_K z), then (x\prec_K z).

We’ll prove (2), which is what shows up in the BV book.

#### 4.3.1 Compatibility of (\preceq_K) and (\prec_K)

Suppose (x\prec_K y) and (y\preceq_K z). Then:

* (y-x \in \operatorname{int}K).
* (z-y \in K).

We want to show: (z-x \in \operatorname{int}K).

We know (K) is convex with nonempty interior. A standard interior fact:

> If (w\in \operatorname{int}K) and (v\in K), then (w+v\in\operatorname{int}K).

Reason: there is a ball (B(w,r)) contained in (K); adding (v) shifts that ball to (B(w+v,r) = v + B(w,r)), still inside (K); so (w+v) has a small ball around it entirely in (K) ⇒ interior point.

Apply this to our situation:

* Take (w = y-x\in\operatorname{int}K).
* Take (v = z-y\in K).
* Then (w+v = (y-x)+(z-y) = z-x\in\operatorname{int}K).

Therefore (x\prec_K z).

Similarly, if (x\preceq_K y) and (y\prec_K z), we write (z-x = (z-y) + (y-x)) with a similar argument.

So strict and non-strict inequalities behave “monotonically” in the expected way.

#### 4.3.2 Preservation under scaling and addition for strict inequality

You also have:

* If (x\prec_K y) and (\alpha>0), then (\alpha x \prec_K \alpha y).

  Because (y-x\in\operatorname{int}K) and (\alpha(y-x) = \alpha y - \alpha x); and interior of a cone is stable under scaling by positive scalars.

* If (x\prec_K y), then for (u\in\mathbb{R}^n), (x+u \prec_K y+u).

  Because (y+u - (x+u) = y-x\in\operatorname{int}K).

These mirror exactly the (\mathbb{R}) properties: adding the same thing or scaling by positive preserves strict inequalities.

---

### 4.4 Why “proper cone” ⇒ “nice partial order”

Collecting:

* (\preceq_K) is reflexive, transitive, antisymmetric.
  ⇒ it’s a **partial order** on (\mathbb{R}^n).
* It’s compatible with addition and nonnegative scaling (monotone).
  ⇒ analogous to classical order on (\mathbb{R}).
* Strict version (\prec_K) is defined via the interior and behaves like (<).

So a proper cone is exactly what you need to define a well-behaved partial order that plays nicely with linear operations. This is why BV uses proper cones everywhere to define “generalized inequalities” and then works with them almost as if they were usual inequalities.

---

## Wrap-up of this batch

We’ve now fully unpacked the rest of the convex-sets slides:

* **Perspective map (P(x,t)=x/t)**

  * Geometric meaning: central projection onto plane (t=1).
  * Image of convex set is convex: proven with that μ–reweighting trick in full algebraic detail.
  * Preimage of convex set is convex: proven with λ–weights argument.

* **Linear–fractional map (f(x)=(Ax+b)/(c^Tx+d))**

  * Expressed as composition (P\circ g) with an affine (g).
  * Therefore images and preimages of convex sets are convex (using the two-stage argument).
  * Clarified: (f) does not preserve convex combinations in general; your attempted equality fails, but that’s okay—we rely on composition structure instead.
  * Example in 2D shows how a convex region is warped but stays convex.

* **Proper cones**

  * Convex cones that are closed, solid, and pointed.
  * Checked the three main examples: (\mathbb{R}^n_+), (S^n_+), nonnegative polynomials.

* **Generalized inequalities**

  * Defined via a proper cone: (x\preceq_K y\iff y-x\in K), (x\prec_K y\iff y-x\in\operatorname{int}K).
  * Showed examples: componentwise inequality and Loewner matrix order.
  * Proved the main algebraic properties: addition, nonnegative scaling, reflexivity, transitivity, antisymmetry, and compatibility of (\preceq_K) / (\prec_K).

All of this machinery is exactly what you’ll use pretty soon when they start writing constraints like

* (Ax \preceq_K b),
* (X \preceq_{S^n_+} B) (LMIs),
* and inequalities involving linear–fractional expressions.

At that point, instead of re-proving convexity from scratch, you just tick boxes: “affine + perspective + proper cone ⇒ convex feasible set.”

