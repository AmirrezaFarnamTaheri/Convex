# Lecture 01 — Introduction to Convex Optimization

*(what a convex optimization problem is, why it matters, where it shows up, and how to read/write one cleanly)*

## 0) Learning goals

* Read and write convex optimization problems in standard notation.
* Recognize common convex problem families (LP, QP, SOCP, SDP) and how modeling trickery reduces to them.
* Understand the “loss + regularizer + constraints” template and where convexity comes from (at a high level).
* Know the basic workflow: formulate → (optionally) canonicalize → solve with a mature solver → sanity-check.

Scope guard: no deep set theory (that’s Lecture 02) and no convex-function calculus/epigraph tactics beyond quick mentions (that’s Lecture 03). No KKT/duality/algorithms here.

---

## 1) What is a convex optimization problem?

A problem is **convex** if it can be written
[
\begin{aligned}
\min_{x\in\mathbb{R}^n}\quad & f_0(x)\
\text{s.t.}\quad & f_i(x)\le 0,\quad i=1,\dots,m,\
& A x = b,
\end{aligned}
]
where (f_0,\dots,f_m) are convex functions and the equalities are **affine**. The **feasible set** is convex, and **every local minimizer is global**.

Reading tips (notation you’ll see everywhere):

* Componentwise inequalities: (x\ge 0) means (x_i\ge 0\ \forall i).
* Symmetric matrices: (\mathbb{S}^n={X=X^\top}); **PSD** means (X\succeq 0) (Loewner order).
* Norm constraints: (|x|_p\le r) define convex feasible sets.

Why convexity matters: tractability, predictable behavior (no local traps), powerful geometry (Lecture 02), and rich function calculus (Lecture 03).

---

## 2) Canonical convex families (you should spot them on sight)

Think of these as solver dialects. A huge fraction of models reduce to one of these.

### 2.1 Linear Program (LP)

[
\min_x\ c^\top x\quad \text{s.t. }A x\le b,; F x = g.
]
Objective and constraints are affine.

### 2.2 Quadratic Program (QP)

[
\min_x\ \tfrac12 x^\top Q x + q^\top x \quad \text{s.t. } A x\le b,; F x=g,\quad (Q\succeq 0).
]
Convex if (Q\succeq 0). Includes least squares, ridge, and many portfolio/estimation templates.

### 2.3 Second-Order Cone Program (SOCP)

[
\min_x\ c^\top x\quad \text{s.t. } |A_i x + b_i|*2 \le c_i^\top x + d_i,\ i=1,\dots,k,; F x=g.
]
Norm constraints in linear forms. Robust LS, (\ell_2)-or (\ell*\infty)-type constraints, and many “safety margins” reduce to SOCP.

### 2.4 Semidefinite Program (SDP)

[
\min_X\ \langle C, X\rangle \quad \text{s.t. } \mathcal{A}(X)=b,; X\succeq 0.
]
Matrix variable (X) (symmetric). Captures spectral constraints, Lyapunov inequalities, ellipsoids, and more.

(We won’t do algorithms for these here; just recognition and modeling intent.)

---

## 3) Modeling patterns: loss + regularizer + constraints

Most applications fit the blueprint
[
\min_x\ \underbrace{\text{loss}(x)}*{\text{fit data}} ;+; \underbrace{\lambda,\text{regularizer}(x)}*{\text{prior/structure}}\quad \text{s.t.}\quad \text{simple convex constraints}.
]

### 3.1 Typical losses (convex)

* **Least squares**: (\tfrac12|Ax-b|_2^2)
* **(\ell_1) regression**: (|Ax-b|_1)
* **Logistic loss** (classification): (\sum_i \log(1+\exp(-y_i a_i^\top x)))

### 3.2 Typical regularizers (convex)

* **Ridge**: (\tfrac\lambda2|x|_2^2)
* **LASSO**: (\lambda|x|_1) (promotes sparsity)
* **Group norms / nuclear norm** (for structure; we only name them here)

### 3.3 Typical constraints (convex)

* **Box**: (\ell \le x \le u)
* **Simplex**: (\mathbf{1}^\top x=1,\ x\ge 0)
* **Budget/robust**: (|P x|*2 \le r) or (|x|*\infty \le r)
* **Matrix PSD**: (X\succeq 0) (for covariance/gram-matrix models)

We’ll prove why these are convex in Lectures 02–03; for now, trust the label and learn to read/compose them.

---

## 4) Core examples you’ll actually use

### 4.1 Least squares (LS)

[
\min_x \tfrac12|Ax-b|_2^2.
]
Convex QP; solution is the LS projection from Lecture 00. Add **ridge** by (\tfrac\lambda2|x|_2^2) (still a QP).

### 4.2 LASSO

[
\min_x \tfrac12|Ax-b|_2^2 + \lambda|x|_1.
]
Convex (sum of convex terms). Useful for sparse recovery/feature selection.

### 4.3 Logistic regression (regularized)

[
\min_x \sum_{i=1}^m \log\big(1+\exp(-y_i a_i^\top x)\big) + \lambda |x|_2^2.
]
Convex; widely used in classification. With (|x|_1) instead: sparse logistic.

### 4.4 Support vector machine (soft margin)

[
\min_{w,b,\xi\ge 0}\ \tfrac12|w|_2^2 + C\sum_i \xi_i
\quad\text{s.t.}\quad y_i(w^\top x_i+b) \ge 1 - \xi_i.
]
Convex QP; hinge loss lurking inside the constraints.

### 4.5 Chebyshev (minimax) approximation

[
\min_{x,t}\ t \quad \text{s.t.}\ -t \le (Ax-b)*i \le t\ \ \forall i.
]
LP via (\ell*\infty) norm bound; classic for uniform-error fitting.

### 4.6 Robust least squares (SOCP form)

Uncertain (A) in an ellipsoid → a constraint of the form (|(A_0+\Delta)x-b|_2) worst-case ≤ r reduces to (|A_0x-b|_2+\rho|x|_2 \le r), which is SOCP-representable.

*(We’re naming forms and intent here; proofs live in 02–03.)*

---

## 5) “Reading” and “writing” problems cleanly

### 5.1 Variables, parameters, data

* **Variables** are what you optimize over.
* **Parameters** are symbols you may change between runs (e.g., (\lambda)).
* **Data** is fixed for a run (e.g., (A,b)).

Always declare dimensions and units. Keep scaling reasonable (Lecture 00’s condition-number sanity).

### 5.2 Sanity checks before solving

* Is the feasible set nonempty? (Trivial constraints can kill it.)
* Are quantities finite? (Avoid divide-by-zero, log of nonpositive, etc.)
* Is the problem obviously unbounded? (E.g., minimize (c^\top x) with (c\neq 0) and no lower bounding constraint.)

### 5.3 Canonicalization (mental model)

Under the hood, many DSLs rewrite your high-level model into a cone program (LP/QP/SOCP/SDP). You don’t need details today—just know the journey:
“math-like model” → “cone program” → “solver”.

---

## 6) Micro-library of safe rewrites (without diving into 02/03 proofs)

These are modeling moves you can use now; we’ll justify them formally later.

* **Absolute value and (\ell_1):** introduce (u\ge 0), constrain (-u\le x\le u), and replace (|x|) with (u); (|x|_1=\mathbf{1}^\top u).
* **(\ell_\infty) bound:** (|x|_\infty\le t) iff (-t\mathbf{1}\le x\le t\mathbf{1}).
* **(\ell_2) bound:** (|P x + q|_2\le t) is SOCP-ready.
* **Piecewise-linear max:** (t\ge a_i^\top x+b_i\ \forall i) encodes (t\ge \max_i(a_i^\top x+b_i)).
* **Quadratic objectives:** (\tfrac12 x^\top Q x) with (Q\succeq 0) is convex; many solvers accept it directly.

Keep the transformations minimal; over-modeling can slow solvers.

---

## 7) Tiny “Hello, DSL” (pseudocode level)

Just to anchor the workflow—no need to run this now.

```python
# variables
x = Variable(n)

# data/params
A, b = ...     # data
lam = ...      # parameter

# objective: least squares + l1
obj = Minimize( 0.5*sum_squares(A@x - b) + lam*norm1(x) )

# constraints: box and simplex (example)
constraints = [x >= 0, sum(x) == 1]

Problem(obj, constraints).solve()
```

Mentally, that becomes a QP+LP (or SOCP) under the hood, then solved.

---

## 8) Problem set (with short solutions)

**P1.1 — Classify each as convex / not convex; one-liner justification.**
(a) (\min |Ax-b|_2^2 ) s.t. (Fx=g).
(b) (\min -|x|_2) s.t. (Ax\le b).
(c) (\min |x|*1) s.t. (|Bx-c|*\infty\le 1).
(d) (\min x^\top Qx) s.t. (Dx\le e,\ Q\succeq 0).
(e) (\min |x|_2^2) s.t. (x_i\in{0,1}).

*Solutions.* (a) Convex (quadratic + affine). (b) Not convex (concave objective). (c) Convex (norm + norm). (d) Convex (PSD quadratic + linear). (e) Not convex (integrality).

**P1.2 — Chebyshev fitting as an LP.**
Show (\min_{x}|Ax-b|*\infty) is equivalent to
(\min*{x,t} t) s.t. (-t\mathbf{1}\le Ax-b\le t\mathbf{1}).
*Solution.* (|u|_\infty\le t \iff -t\le u_i\le t\ \forall i).

**P1.3 — Ridge vs LASSO geometry (concept).**
State in one line why ridge tends to shrink but not zero coefficients, while LASSO promotes zeros.
*Solution.* Level sets: ridge ball is (\ell_2) (round), LASSO ball is (\ell_1) (pointy at axes); the minimum touches at corners → sparsity.

**P1.4 — Simplex-constrained LS.**
Formulate (\min |Ax-b|_2^2) s.t. (\mathbf{1}^\top x=1,\ x\ge 0). Is it convex?
*Solution.* Yes: QP + affine + nonnegativity.

**P1.5 — Logistic with (\ell_1) regularization (write only).**
Give the objective and constraints for binary labels (y_i\in{\pm1}).
*Solution.* (\min_x \sum_i \log(1+\exp(-y_i a_i^\top x)) + \lambda|x|_1). (No extra constraints.)

---

## 9) What to remember (pocket card)

* “Convex problem” = convex objective + convex (\le 0) constraints + affine equalities.
* Recognize LP/QP/SOCP/SDP by sight; most models reduce to one.
* Blueprint: **loss + regularizer + simple convex constraints**.
* Use safe rewrites: (\ell_1,\ \ell_\infty,\ \ell_2) bounds; max via epigraph variable; PSD as a constraint symbol.
* Workflow: formulate cleanly → solve → sanity-check (feasibility, boundedness, scaling).

---

## 10) Repo drops (ready to generate on request)

* `01-introduction/notes.md` — the lecture above, trimmed into teacher/board flow.
* `01-introduction/pset.md` — P1.1–P1.5 with solutions.
* `01-introduction/quickstart.py` or `cvx_lab.ipynb` — a tiny LS/LASSO demo.

If you want the actual files, I can spit them out immediately so your repo has 00, 01, 02, 03 in order.
