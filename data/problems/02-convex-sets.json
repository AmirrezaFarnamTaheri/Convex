{
  "lecture": "02-convex-sets",
  "problems": [
    {
      "id": "02-001",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "easy",
      "title": "P1 — Epigraph of a norm is convex",
      "statement": "Let \\(S = \\{(x,t) \\in \\mathbb{R}^n \\times \\mathbb{R} \\mid \\|x\\| \\le t\\}\\). Show that \\(S \\subset \\mathbb{R}^{n+1}\\) is convex.\\n\\nRecall: A set \\(C\\) is convex if for all \\(u,v \\in C\\) and \\(\\theta \\in [0,1]\\), the convex combination \\(\\theta u + (1-\\theta)v \\in C\\).",
      "hints": [
        "Start by taking two arbitrary points \\((x_1,t_1), (x_2,t_2) \\in S\\) and a coefficient \\(\\theta \\in [0,1]\\)",
        "Use the definition of \\(S\\): this means \\(\\|x_1\\| \\le t_1\\) and \\(\\|x_2\\| \\le t_2\\)",
        "For the convex combination \\((\\bar{x},\\bar{t}) = \\theta(x_1,t_1) + (1-\\theta)(x_2,t_2)\\), you need to show \\(\\|\\bar{x}\\| \\le \\bar{t}\\)",
        "Apply the triangle inequality: \\(\\|\\theta x_1 + (1-\\theta)x_2\\| \\le \\|\\theta x_1\\| + \\|(1-\\theta)x_2\\|\\)",
        "Use positive homogeneity of norms: \\(\\|\\theta x_1\\| = \\theta\\|x_1\\|\\) for \\(\\theta \\ge 0\\)",
        "Combine with the feasibility conditions \\(\\|x_1\\| \\le t_1\\) and \\(\\|x_2\\| \\le t_2\\)"
      ],
      "solution": "## Solution (Direct, Step by Step)\\n\\nLet \\((x_1,t_1), (x_2,t_2) \\in S\\). By definition of \\(S\\):\\n\\n$$\\|x_1\\| \\le t_1, \\quad \\|x_2\\| \\le t_2$$\\n\\nFix any \\(\\theta \\in [0,1]\\). Consider the convex combination:\\n\\n$$(\\bar{x},\\bar{t}) = \\theta(x_1,t_1) + (1-\\theta)(x_2,t_2) = (\\theta x_1 + (1-\\theta)x_2, \\theta t_1 + (1-\\theta)t_2)$$\\n\\nWe must show \\((\\bar{x},\\bar{t}) \\in S\\), i.e., \\(\\|\\bar{x}\\| \\le \\bar{t}\\).\\n\\n### Step 1 (Triangle inequality)\\n\\n$$\\|\\theta x_1 + (1-\\theta)x_2\\| \\le \\|\\theta x_1\\| + \\|(1-\\theta)x_2\\|$$\\n\\n### Step 2 (Positive homogeneity of norms)\\n\\n$$\\|\\theta x_1\\| = \\theta\\|x_1\\|, \\quad \\|(1-\\theta)x_2\\| = (1-\\theta)\\|x_2\\|$$\\n\\n### Step 3 (Combine with feasibility of the endpoints)\\n\\n$$\\|\\bar{x}\\| \\le \\theta\\|x_1\\| + (1-\\theta)\\|x_2\\| \\le \\theta t_1 + (1-\\theta)t_2 = \\bar{t}$$\\n\\nThus \\((\\bar{x},\\bar{t}) \\in S\\). Since \\((x_1,t_1), (x_2,t_2)\\) and \\(\\theta\\) were arbitrary, \\(S\\) is convex. ∎\\n\\n## Alternative Solution (Epigraph viewpoint)\\n\\nThe map \\(f(x) = \\|x\\|\\) is convex. The **epigraph** of \\(f\\),\\n\\n$$\\text{epi } f = \\{(x,t) \\mid f(x) \\le t\\}$$\\n\\nis always convex for any convex function \\(f\\). Here \\(\\text{epi } f = S\\), therefore \\(S\\) is convex. ∎\\n\\n## Key Insights\\n\\n1. **Geometric interpretation**: The set \\(S\\) is the region \"above\" the norm cone in \\(\\mathbb{R}^{n+1}\\)\\n2. **Epigraph property**: Any convex function has a convex epigraph—this is a fundamental connection between convex functions and convex sets\\n3. **Norm properties used**: Triangle inequality and positive homogeneity (both defining properties of norms)",
      "learningObjectives": [
        "convexity-definition",
        "triangle-inequality",
        "norm-properties",
        "epigraph-concept",
        "convex-combinations",
        "proof-technique"
      ],
      "estimatedTime": 15,
      "relatedProblems": ["02-002", "02-003", "02-005"],
      "notes": "This is a fundamental problem that connects convex sets and convex functions through the epigraph. The direct proof reinforces basic properties of norms and convex combinations. The alternative solution introduces the powerful epigraph viewpoint that will be used extensively in convex analysis."
    },
    {
      "id": "02-002",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "easy",
      "title": "P2 — Convex hull is convex",
      "statement": "Let \\(x_1, \\dots, x_k \\in \\mathbb{R}^n\\). Define\\n\\n$$C = \\left\\{ \\sum_{i=1}^k \\theta_i x_i \\mid \\theta_i \\ge 0, \\sum_{i=1}^k \\theta_i = 1 \\right\\}$$\\n\\nShow that \\(C\\) is convex.",
      "hints": [
        "Pick two arbitrary points \\(u, v \\in C\\) and express them using the definition",
        "For \\(u \\in C\\), write \\(u = \\sum_{i=1}^k \\theta_i x_i\\) with \\(\\theta_i \\ge 0\\) and \\(\\sum \\theta_i = 1\\)",
        "For \\(v \\in C\\), write \\(v = \\sum_{i=1}^k \\phi_i x_i\\) with \\(\\phi_i \\ge 0\\) and \\(\\sum \\phi_i = 1\\)",
        "Form the convex combination \\(\\lambda u + (1-\\lambda)v\\) for \\(\\lambda \\in [0,1]\\)",
        "Expand: \\(\\lambda u + (1-\\lambda)v = \\sum_{i=1}^k [\\lambda\\theta_i + (1-\\lambda)\\phi_i]x_i\\)",
        "Verify that the new coefficients \\(\\lambda\\theta_i + (1-\\lambda)\\phi_i\\) are nonnegative and sum to 1"
      ],
      "solution": "## Solution (Direct Coefficient Bookkeeping)\\n\\nPick two points of \\(C\\):\\n\\n$$u = \\sum_{i=1}^k \\theta_i x_i, \\quad v = \\sum_{i=1}^k \\phi_i x_i$$\\n\\nwith \\(\\theta_i, \\phi_i \\ge 0\\) and \\(\\sum_i \\theta_i = \\sum_i \\phi_i = 1\\).\\n\\nFix \\(\\lambda \\in [0,1]\\). Then:\\n\\n$$\\lambda u + (1-\\lambda)v = \\sum_{i=1}^k [\\lambda\\theta_i + (1-\\lambda)\\phi_i]x_i$$\\n\\n### Nonnegativity\\n\\nEach coefficient \\(\\lambda\\theta_i + (1-\\lambda)\\phi_i \\ge 0\\) because:\\n- \\(\\lambda \\ge 0, \\theta_i \\ge 0 \\implies \\lambda\\theta_i \\ge 0\\)\\n- \\((1-\\lambda) \\ge 0, \\phi_i \\ge 0 \\implies (1-\\lambda)\\phi_i \\ge 0\\)\\n\\n### Sum-to-one\\n\\n$$\\sum_{i=1}^k [\\lambda\\theta_i + (1-\\lambda)\\phi_i] = \\lambda\\sum_i \\theta_i + (1-\\lambda)\\sum_i \\phi_i = \\lambda \\cdot 1 + (1-\\lambda) \\cdot 1 = 1$$\\n\\nHence \\(\\lambda u + (1-\\lambda)v \\in C\\). Therefore \\(C\\) is convex. ∎\\n\\n## Alternative Solution (Induction on \\(k\\))\\n\\n**Base case (\\(k=2\\))**: This is exactly the definition of convexity—the convex hull of two points is the line segment between them, which is clearly convex.\\n\\n**Inductive step**: Assume the claim holds for \\(k-1\\) points. For \\(k\\) points, write any point in \\(C\\) as:\\n\\n$$\\sum_{i=1}^k \\theta_i x_i = \\alpha \\left( \\sum_{i=1}^{k-1} \\frac{\\theta_i}{\\alpha} x_i \\right) + (1-\\alpha)x_k$$\\n\\nwhere \\(\\alpha = \\sum_{i=1}^{k-1} \\theta_i \\in [0,1]\\).\\n\\n**Case 1**: If \\(\\alpha = 0\\), then the point is just \\(x_k \\in C\\).\\n\\n**Case 2**: If \\(\\alpha > 0\\), note that \\(\\theta_i/\\alpha \\ge 0\\) and \\(\\sum_{i=1}^{k-1} (\\theta_i/\\alpha) = 1\\). By the induction hypothesis, the parenthesis \\(\\sum_{i=1}^{k-1} (\\theta_i/\\alpha)x_i\\) lies in the convex hull of \\(\\{x_1,\\dots,x_{k-1}\\}\\).\\n\\nTaking a convex combination (with weights \\(\\alpha\\) and \\(1-\\alpha\\)) with \\(x_k\\) keeps us in the convex hull of all \\(k\\) points.\\n\\nTaking any convex combination of two points in \\(C\\) (each expressible in this form) keeps us in \\(C\\) by the base case. ∎\\n\\n## Key Insights\\n\\n1. **Convex hull definition**: \\(C\\) is precisely the convex hull of \\(\\{x_1,\\dots,x_k\\}\\), denoted \\(\\text{conv}(\\{x_1,\\dots,x_k\\}\\))\\n2. **Coefficient preservation**: Convex combinations preserve the affine combination structure (nonnegativity and sum-to-one)\\n3. **Inductive reasoning**: The convex hull can be built incrementally by adding one point at a time\\n4. **Smallest convex set**: The convex hull is the smallest convex set containing the given points",
      "learningObjectives": [
        "convex-hull",
        "convexity-definition",
        "affine-combinations",
        "proof-by-induction",
        "coefficient-bookkeeping",
        "convex-combinations"
      ],
      "estimatedTime": 20,
      "relatedProblems": ["02-001", "02-008", "02-012"],
      "notes": "This problem establishes that the convex hull—one of the most important constructions in convex geometry—is itself convex. The direct proof emphasizes careful coefficient tracking, while the inductive proof provides geometric intuition about building convex hulls incrementally. This result is foundational for understanding that 'convex hull' is the smallest convex set containing a given set of points."
    },
    {
      "id": "02-003",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "medium",
      "title": "P3 — Inner product induced by A ≻ 0; Cauchy–Schwarz; ellipsoid is convex",
      "statement": "Let \\(A \\in \\mathbb{S}^n_{++}\\) (symmetric positive definite). Define \\(\\langle x,y \\rangle_A := x^T A y\\) for \\(x,y \\in \\mathbb{R}^n\\).\\n\\n1. Show \\(\\langle \\cdot,\\cdot \\rangle_A\\) is an inner product.\\n\\n2. Prove the Cauchy–Schwarz inequality:\\n   $$|\\langle x,y \\rangle_A|^2 \\le \\langle x,x \\rangle_A \\cdot \\langle y,y \\rangle_A \\quad (\\forall x,y)$$\\n\\n3. Prove the set \\(E = \\{x \\in \\mathbb{R}^n \\mid x^T A x \\le 1\\}\\) is convex.",
      "hints": [
        "Part 1: Verify the three inner product axioms—bilinearity, symmetry, and positive definiteness",
        "For bilinearity: Use \\((\\alpha x_1 + \\beta x_2)^T A y = \\alpha x_1^T A y + \\beta x_2^T A y\\)",
        "For symmetry: Use that \\(A = A^T\\), so \\(x^T A y = y^T A x\\)",
        "For positive definiteness: Use that \\(A \\succ 0\\) means \\(x^T A x > 0\\) for all \\(x \\ne 0\\)",
        "Part 2, Method A: Consider the quadratic \\(0 \\le \\langle x-\\alpha y, x-\\alpha y \\rangle_A\\) for any \\(\\alpha \\in \\mathbb{R}\\) and examine its discriminant",
        "Part 2, Method B: Use \\(A^{1/2}\\) and reduce to the standard Euclidean Cauchy–Schwarz",
        "Part 3: Define the A-norm \\(\\|x\\|_A = \\sqrt{x^T A x}\\) and show it's a norm, then use that norm balls are convex"
      ],
      "solution": "## Part 1 — Inner Product Axioms\\n\\nWe verify the three axioms:\\n\\n### (i) Bilinearity\\n\\nFor any scalars \\(\\alpha, \\beta\\) and vectors \\(x_1, x_2, y\\):\\n\\n$$\\langle \\alpha x_1 + \\beta x_2, y \\rangle_A = (\\alpha x_1 + \\beta x_2)^T A y = \\alpha x_1^T A y + \\beta x_2^T A y = \\alpha \\langle x_1,y \\rangle_A + \\beta \\langle x_2,y \\rangle_A$$\\n\\nSimilarly, linearity holds in the second slot:\\n\\n$$\\langle x, \\alpha y_1 + \\beta y_2 \\rangle_A = \\alpha \\langle x,y_1 \\rangle_A + \\beta \\langle x,y_2 \\rangle_A$$\\n\\n### (ii) Symmetry\\n\\nSince \\(A = A^T\\):\\n\\n$$\\langle x,y \\rangle_A = x^T A y = (x^T A y)^T = y^T A^T x = y^T A x = \\langle y,x \\rangle_A$$\\n\\n### (iii) Positive Definiteness\\n\\nFor \\(x \\ne 0\\):\\n\\n$$\\langle x,x \\rangle_A = x^T A x > 0$$\\n\\nbecause \\(A \\succ 0\\) (positive definite). Also, \\(\\langle 0,0 \\rangle_A = 0^T A 0 = 0\\).\\n\\nThus \\(\\langle \\cdot,\\cdot \\rangle_A\\) is an inner product. ∎\\n\\n## Part 2 — Cauchy–Schwarz Inequality\\n\\n### Method A (One-variable quadratic)\\n\\nFor any \\(\\alpha \\in \\mathbb{R}\\):\\n\\n$$0 \\le \\langle x - \\alpha y, x - \\alpha y \\rangle_A = \\langle x,x \\rangle_A - 2\\alpha\\langle x,y \\rangle_A + \\alpha^2\\langle y,y \\rangle_A$$\\n\\nThe right side is a quadratic in \\(\\alpha\\) that is nonnegative for all \\(\\alpha\\). Therefore, its discriminant must be nonpositive:\\n\\n$$(2\\langle x,y \\rangle_A)^2 - 4\\langle x,x \\rangle_A\\langle y,y \\rangle_A \\le 0$$\\n\\nThis gives:\\n\\n$$|\\langle x,y \\rangle_A|^2 \\le \\langle x,x \\rangle_A \\cdot \\langle y,y \\rangle_A ∎$$\\n\\n### Method B (Reduce to Euclidean Cauchy–Schwarz)\\n\\nLet \\(B = A^{1/2} \\succ 0\\) (the unique positive definite square root of \\(A\\)). Then:\\n\\n$$\\langle x,y \\rangle_A = x^T A y = x^T(B^2)y = (Bx)^T(By) = \\langle Bx, By \\rangle_2$$\\n\\nwhere \\(\\langle \\cdot,\\cdot \\rangle_2\\) is the standard Euclidean inner product. By the standard Cauchy–Schwarz inequality in \\(\\mathbb{R}^n\\):\\n\\n$$|\\langle x,y \\rangle_A| = |\\langle Bx, By \\rangle_2| \\le \\|Bx\\|_2 \\cdot \\|By\\|_2$$\\n\\nNow:\\n\\n$$\\|Bx\\|_2 = \\sqrt{(Bx)^T(Bx)} = \\sqrt{x^T B^T B x} = \\sqrt{x^T A x} = \\sqrt{\\langle x,x \\rangle_A}$$\\n\\nSimilarly, \\(\\|By\\|_2 = \\sqrt{\\langle y,y \\rangle_A}\\). Therefore:\\n\\n$$|\\langle x,y \\rangle_A| \\le \\sqrt{\\langle x,x \\rangle_A} \\cdot \\sqrt{\\langle y,y \\rangle_A}$$\\n\\nSquaring both sides gives the result. ∎\\n\\n## Part 3 — Ellipsoid is Convex\\n\\nDefine the **A-norm**:\\n\\n$$\\|x\\|_A := \\sqrt{x^T A x} = \\sqrt{\\langle x,x \\rangle_A}$$\\n\\nSince \\(\\langle \\cdot,\\cdot \\rangle_A\\) is an inner product, \\(\\|\\cdot\\|_A\\) is a norm. The set\\n\\n$$E = \\{x \\mid \\|x\\|_A \\le 1\\}$$\\n\\nis the closed unit ball of the norm \\(\\|\\cdot\\|_A\\), which is convex.\\n\\n### Direct verification\\n\\nFor \\(x, y \\in E\\) and \\(\\theta \\in [0,1]\\), we use the **triangle inequality** for the norm \\(\\|\\cdot\\|_A\\):\\n\\n$$\\|\\theta x + (1-\\theta)y\\|_A \\le \\|\\theta x\\|_A + \\|(1-\\theta)y\\|_A$$\\n\\nBy **positive homogeneity** of norms:\\n\\n$$\\|\\theta x\\|_A = \\theta\\|x\\|_A, \\quad \\|(1-\\theta)y\\|_A = (1-\\theta)\\|y\\|_A$$\\n\\nSince \\(x, y \\in E\\), we have \\(\\|x\\|_A \\le 1\\) and \\(\\|y\\|_A \\le 1\\). Therefore:\\n\\n$$\\|\\theta x + (1-\\theta)y\\|_A \\le \\theta\\|x\\|_A + (1-\\theta)\\|y\\|_A \\le \\theta \\cdot 1 + (1-\\theta) \\cdot 1 = 1$$\\n\\nHence \\(\\theta x + (1-\\theta)y \\in E\\). Therefore \\(E\\) is convex. ∎\\n\\n## Key Insights\\n\\n1. **Induced inner products**: Any positive definite matrix \\(A\\) induces an inner product on \\(\\mathbb{R}^n\\)\\n2. **Generalized norms**: The induced inner product gives rise to a norm \\(\\|\\cdot\\|_A\\), called the A-norm or energy norm\\n3. **Ellipsoids are norm balls**: The set \\(E\\) is an ellipsoid (when \\(A \\succ 0\\)), and it's the unit ball of \\(\\|\\cdot\\|_A\\)\\n4. **Cauchy–Schwarz is universal**: The Cauchy–Schwarz inequality holds for any inner product space\\n5. **Connection to geometry**: The quadratic form \\(x^T A x \\le 1\\) defines an ellipsoid centered at the origin with shape determined by the eigenvectors of \\(A\\) and radii determined by \\(A\\)'s eigenvalues\\n6. **Transformation perspective**: Under the change of variables \\(z = A^{1/2}x\\), the ellipsoid \\(E\\) becomes the standard Euclidean unit ball in the \\(z\\)-coordinates",
      "learningObjectives": [
        "inner-product-axioms",
        "positive-definite-matrices",
        "cauchy-schwarz-inequality",
        "norms-from-inner-products",
        "ellipsoid-convexity",
        "matrix-square-root",
        "quadratic-forms",
        "triangle-inequality"
      ],
      "estimatedTime": 35,
      "relatedProblems": ["02-001", "02-013", "02-010"],
      "notes": "This is a rich problem that connects linear algebra, inner product spaces, and convex geometry. Part 1 reinforces the abstract definition of inner products. Part 2 shows that Cauchy–Schwarz is a property of ALL inner products, not just the Euclidean one. Part 3 demonstrates that ellipsoids are convex by showing they are unit balls of induced norms. The two methods for proving Cauchy–Schwarz (discriminant vs. reduction to Euclidean case) illustrate different proof strategies—one purely algebraic, one geometric via transformation."
    },
    {
      "id": "02-004",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "medium",
      "title": "P4 — Minkowski sum of convex sets; nonconvex summands may still sum to convex",
      "statement": "For \\(X, Y \\subset \\mathbb{R}^n\\), define the **Minkowski sum**\\n\\n$$X + Y = \\{x + y \\mid x \\in X, y \\in Y\\}$$\\n\\n(a) If \\(X\\) and \\(Y\\) are convex, prove \\(X + Y\\) is convex.\\n\\n(b) If \\(X\\) and \\(Y\\) are not convex, can \\(X + Y\\) be convex?",
      "hints": [
        "Part (a): Take arbitrary \\(z_1, z_2 \\in X + Y\\); write them as \\(z_1 = x_1 + y_1\\) and \\(z_2 = x_2 + y_2\\)",
        "Form the convex combination \\(\\theta z_1 + (1-\\theta)z_2\\) and group the \\(x\\) and \\(y\\) terms separately",
        "Use that \\(\\theta x_1 + (1-\\theta)x_2 \\in X\\) and \\(\\theta y_1 + (1-\\theta)y_2 \\in Y\\) by convexity",
        "Part (b): Try constructing explicit nonconvex sets in \\(\\mathbb{R}\\) whose Minkowski sum is convex",
        "Consider disconnected unions of intervals: \\(X = [0,1] \\cup [3,4]\\), \\(Y = [0,3] \\cup [4,7]\\)",
        "Compute all pairwise sums of intervals and check if the union is convex"
      ],
      "solution": "## Part (a) — Convexity of Minkowski Sum\\n\\nTake any \\(z_1, z_2 \\in X + Y\\). By definition, there exist \\(x_1, x_2 \\in X\\) and \\(y_1, y_2 \\in Y\\) such that:\\n\\n$$z_1 = x_1 + y_1, \\quad z_2 = x_2 + y_2$$\\n\\nLet \\(\\theta \\in [0,1]\\). Consider the convex combination:\\n\\n$$\\theta z_1 + (1-\\theta)z_2 = \\theta(x_1 + y_1) + (1-\\theta)(x_2 + y_2) = [\\theta x_1 + (1-\\theta)x_2] + [\\theta y_1 + (1-\\theta)y_2]$$\\n\\n### Apply convexity of \\(X\\) and \\(Y\\)\\n\\nSince \\(X\\) is convex:\\n\\(\\theta x_1 + (1-\\theta)x_2 \\in X\\)\\n\\nSince \\(Y\\) is convex:\\n\\(\\theta y_1 + (1-\\theta)y_2 \\in Y\\)\\n\\n### Conclusion\\n\\nTherefore:\\n$$\\theta z_1 + (1-\\theta)z_2 = [\\theta x_1 + (1-\\theta)x_2] + [\\theta y_1 + (1-\\theta)y_2] \\in X + Y$$\\n\\nSince \\(z_1, z_2\\), and \\(\\theta\\) were arbitrary, \\(X + Y\\) is convex. ∎\\n\\n## Part (b) — Nonconvex Sets with Convex Sum\\n\\n**Answer**: Yes, two nonconvex sets can have a convex Minkowski sum.\\n\\n### Explicit example in \\(\\mathbb{R}\\)\\n\\nLet:\\n$$X = [0,1] \\cup [3,4]$$\\n$$Y = [0,3] \\cup [4,7]$$\\n\\nBoth \\(X\\) and \\(Y\\) are **not convex** (they are disconnected unions of intervals).\\n\\n### Computing \\(X + Y\\)\\n\\nWe compute the Minkowski sum by taking pairwise sums of intervals:\\n\\n**Interval 1**: \\([0,1] + [0,3] = [0,4]\\)\\n(All sums \\(x + y\\) where \\(x \\in [0,1], y \\in [0,3]\\) give \\([0+0, 1+3] = [0,4]\\))\\n\\n**Interval 2**: \\([0,1] + [4,7] = [4,8]\\)\\n(All sums \\(x + y\\) where \\(x \\in [0,1], y \\in [4,7]\\) give \\([0+4, 1+7] = [4,8]\\))\\n\\n**Interval 3**: \\([3,4] + [0,3] = [3,7]\\)\\n(All sums \\(x + y\\) where \\(x \\in [3,4], y \\in [0,3]\\) give \\([3+0, 4+3] = [3,7]\\))\\n\\n**Interval 4**: \\([3,4] + [4,7] = [7,11]\\)\\n(All sums \\(x + y\\) where \\(x \\in [3,4], y \\in [4,7]\\) give \\([3+4, 4+7] = [7,11]\\))\\n\\n### Union of all intervals\\n\\n$$X + Y = [0,4] \\cup [4,8] \\cup [3,7] \\cup [7,11] = [0,11]$$\\n\\nThis is a **single connected interval**, which is convex!\\n\\nThus two nonconvex sets can have a convex Minkowski sum. ∎\\n\\n## Alternative Proof for (a) — Support Functions\\n\\nThe **support function** of a nonempty set \\(S\\) is:\\n\\n$$h_S(u) = \\sup_{x \\in S} u^T x$$\\n\\nA standard fact from convex analysis:\\n\\n$$h_{X+Y}(u) = h_X(u) + h_Y(u)$$\\n\\nfor all \\(u \\in \\mathbb{R}^n\\).\\n\\nIf \\(X\\) and \\(Y\\) are convex, then for each direction \\(u\\), the supporting halfspace\\n\\n$$\\{z \\mid u^T z \\le h_X(u) + h_Y(u)\\}$$\\n\\ncontains \\(X + Y\\). Since a convex set is the intersection of all its supporting halfspaces:\\n\\n$$X + Y = \\bigcap_{u \\in \\mathbb{R}^n} \\{z \\mid u^T z \\le h_X(u) + h_Y(u)\\}$$\\n\\nThis is an intersection of halfspaces, hence convex. ∎\\n\\n## Key Insights\\n\\n1. **Minkowski sum preserves convexity**: The sum of two convex sets is always convex\\n2. **Geometric interpretation**: \\(X + Y\\) is obtained by \"sliding\" \\(X\\) along every point in \\(Y\\) (or vice versa)\\n3. **Counterintuitive result**: Nonconvex sets can sum to convex (the \"gaps\" can fill in)\\n4. **Support function additivity**: \\(h_{X+Y} = h_X + h_Y\\) is a powerful dual characterization\\n5. **Applications**: Minkowski sums appear in robot motion planning (configuration space obstacles), image processing (dilation), and optimization (constraint sets)\\n6. **Connection to duality**: The support function viewpoint connects primal geometry (sets) to dual geometry (halfspaces)",
      "learningObjectives": [
        "minkowski-sum",
        "convexity-preservation",
        "support-functions",
        "counterexamples",
        "interval-arithmetic",
        "geometric-operations",
        "duality-concepts"
      ],
      "estimatedTime": 25,
      "relatedProblems": ["02-005", "02-006", "02-007"],
      "notes": "The Minkowski sum is a fundamental operation in convex geometry with applications across robotics, computer graphics, and optimization. Part (a) shows that convexity is preserved under this operation—a key closure property. Part (b) provides a surprising counterexample showing that nonconvex + nonconvex can yield convex, illustrating that convexity preservation is directional (sufficient but not necessary). The support function proof offers an elegant dual perspective that will be important in later duality theory."
    },
    {
      "id": "02-005",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "medium",
      "title": "P5 — Thickening a convex set is convex",
      "statement": "Let \\(X \\subset \\mathbb{R}^n\\) be **convex and closed**, and let \\(\\epsilon > 0\\). Define\\n\\n$$B_\\epsilon(X) = \\{y \\in \\mathbb{R}^n \\mid \\inf_{x \\in X} \\|y - x\\| \\le \\epsilon\\}$$\\n\\nProve that \\(B_\\epsilon(X)\\) is convex.\\n\\n(Here \\(\\|\\cdot\\|\\) denotes the Euclidean norm.)",
      "hints": [
        "Recognize that \\(B_\\epsilon(X) = \\{y \\mid \\text{dist}(y, X) \\le \\epsilon\\}\\), the \\(\\epsilon\\)-neighborhood of \\(X\\)",
        "Method 1: Use that closedness of \\(X\\) guarantees a nearest point exists for any \\(y\\)",
        "For \\(y_1, y_2 \\in B_\\epsilon(X)\\), find witnesses \\(x_1, x_2 \\in X\\) with \\(\\|y_1 - x_1\\| \\le \\epsilon\\) and \\(\\|y_2 - x_2\\| \\le \\epsilon\\)",
        "Form the convex combination \\(x_\\theta = \\theta x_1 + (1-\\theta)x_2 \\in X\\) and show it's close to \\(\\theta y_1 + (1-\\theta)y_2\\)",
        "Method 2: Recognize \\(B_\\epsilon(X) = X + \\epsilon B\\) where \\(B = \\{u : \\|u\\| \\le 1\\}\\) is the unit ball",
        "Apply the result from P4 that Minkowski sums of convex sets are convex"
      ],
      "solution": "## Solution 1 (Direct, Using Nearest Points)\\n\\n### Why closedness matters\\n\\nBecause \\(X\\) is closed in finite dimensions, for every \\(y\\) the continuous function \\(x \\mapsto \\|y - x\\|\\) attains its minimum on \\(X\\). Hence:\\n\\n$$\\inf_{x \\in X} \\|y - x\\| \\le \\epsilon \\iff \\exists x \\in X : \\|y - x\\| \\le \\epsilon \\quad (\\star)$$\\n\\n### Goal\\n\\nShow: if \\(y_1, y_2 \\in B_\\epsilon(X)\\) and \\(\\theta \\in [0,1]\\), then \\(\\theta y_1 + (1-\\theta)y_2 \\in B_\\epsilon(X)\\).\\n\\n### Step 1 (Pick witnesses in \\(X\\))\\n\\nBy \\((\\star)\\), choose \\(x_1, x_2 \\in X\\) with:\\n\\n$$\\|y_1 - x_1\\| \\le \\epsilon, \\quad \\|y_2 - x_2\\| \\le \\epsilon$$\\n\\n### Step 2 (Use convexity of \\(X\\))\\n\\nSince \\(X\\) is convex, the convex combination:\\n\\n$$x_\\theta := \\theta x_1 + (1-\\theta)x_2 \\in X$$\\n\\n### Step 3 (Triangle inequality)\\n\\nCompute the distance from \\(\\theta y_1 + (1-\\theta)y_2\\) to \\(x_\\theta\\):\\n\\n$$\\|\\theta y_1 + (1-\\theta)y_2 - x_\\theta\\| = \\|\\theta y_1 + (1-\\theta)y_2 - \\theta x_1 - (1-\\theta)x_2\\| = \\|\\theta(y_1 - x_1) + (1-\\theta)(y_2 - x_2)\\| \\le \\theta\\|y_1 - x_1\\| + (1-\\theta)\\|y_2 - x_2\\| \\le \\theta\\epsilon + (1-\\theta)\\epsilon = \\epsilon$$\\n\\nThus there exists \\(x_\\theta \\in X\\) within distance \\(\\epsilon\\) of \\(\\theta y_1 + (1-\\theta)y_2\\), i.e.:\\n\\n$$\\theta y_1 + (1-\\theta)y_2 \\in B_\\epsilon(X)$$\\n\\nSince the choices were arbitrary, \\(B_\\epsilon(X)\\) is convex. ∎\\n\\n## Solution 2 (Minkowski Sum Viewpoint)\\n\\nLet \\(B = \\{u : \\|u\\| \\le 1\\}\\) be the closed unit ball, which is convex. For closed \\(X\\):\\n\\n$$B_\\epsilon(X) = \\{y : \\exists x \\in X, \\|y - x\\| \\le \\epsilon\\} = \\{y : \\exists x \\in X, \\exists u \\in B, y = x + \\epsilon u\\} = X + \\epsilon B$$\\n\\nThis is the **Minkowski sum** of \\(X\\) and \\(\\epsilon B\\) (scaled unit ball).\\n\\n### Apply P4\\n\\nBy Problem P4(a), the Minkowski sum of two convex sets is convex. Since both \\(X\\) and \\(\\epsilon B\\) are convex, \\(B_\\epsilon(X) = X + \\epsilon B\\) is convex. ∎\\n\\n## Remark on Closedness\\n\\n**Is closedness essential?** Not for convexity. Even if \\(X\\) were not closed, we have:\\n\\n$$B_\\epsilon(X) = \\bar{X} + \\epsilon B$$ (closure of \\(X\\) plus ball)\\n\\nSince the closure of a convex set is convex (see P6), and Minkowski sums preserve convexity, \\(B_\\epsilon(X)\\) is still convex.\\n\\nClosedness was only used to ensure that \\(\\inf = \\min\\), i.e., nearest points exist. For convexity alone, it's not strictly necessary.\\n\\n## Key Insights\\n\\n1. **\\(\\epsilon\\)-neighborhood (thickening)**: \\(B_\\epsilon(X)\\) is the set of all points within distance \\(\\epsilon\\) of \\(X\\)\\n2. **Closed convex = distance minimizer**: For closed convex \\(X\\), every external point has a unique nearest point in \\(X\\)\\n3. **Minkowski sum interpretation**: Thickening by \\(\\epsilon\\) is the same as Minkowski sum with \\(\\epsilon B\\)\\n4. **Convexity is stable under thickening**: This is crucial for approximation theory and robust optimization\\n5. **Applications**: Appears in collision detection (safety margins), robust optimization (parameter uncertainty), and image processing (morphological dilation)\\n6. **Triangle inequality is key**: The norm's triangle inequality and positive homogeneity are essential for the direct proof",
      "learningObjectives": [
        "epsilon-neighborhoods",
        "distance-functions",
        "minkowski-sum-applications",
        "convexity-preservation",
        "triangle-inequality",
        "closed-sets",
        "nearest-points"
      ],
      "estimatedTime": 20,
      "relatedProblems": ["02-004", "02-006", "08-003"],
      "notes": "This problem introduces the important operation of 'thickening' or 'fattening' a set, which has practical applications in safety-critical systems (e.g., collision avoidance with safety margins). The two proofs illustrate different approaches: direct computation using witnesses vs. recognizing the operation as a Minkowski sum and applying existing theory. The closedness assumption is interesting—it's needed for the 'nearest point exists' claim but not for convexity itself."
    },
    {
      "id": "02-006",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "easy",
      "title": "P6 — Closure of a convex set is convex",
      "statement": "Let \\(C \\subset \\mathbb{R}^n\\) be convex. Prove that its closure \\(\\bar{C}\\) is convex.",
      "hints": [
        "Use sequences to approximate points in the closure",
        "For \\(u, v \\in \\bar{C}\\), find sequences \\((u_k) \\subset C\\) and \\((v_k) \\subset C\\) with \\(u_k \\to u\\) and \\(v_k \\to v\\)",
        "Form the convex combinations \\(w_k = \\theta u_k + (1-\\theta)v_k\\) for fixed \\(\\theta \\in [0,1]\\)",
        "Show that \\(w_k \\in C\\) by convexity of \\(C\\)",
        "Pass to the limit: \\(w_k \\to \\theta u + (1-\\theta)v\\), and conclude the limit is in \\(\\bar{C}\\)",
        "Alternative: Show \\(\\bar{C}\\) is the intersection of all closed convex supersets of \\(C\\)"
      ],
      "solution": "## Solution 1 (Sequence/Limit Argument)\\n\\n### Goal\\n\\nFor any \\(u, v \\in \\bar{C}\\) and any \\(\\theta \\in [0,1]\\), show \\(\\theta u + (1-\\theta)v \\in \\bar{C}\\).\\n\\n### Step 1 (Approximate by points in \\(C\\))\\n\\nBy definition of closure, there exist sequences \\((u_k) \\subset C\\) and \\((v_k) \\subset C\\) such that:\\n\\(u_k \\to u, \\quad v_k \\to v\\)\\n\\nas \\(k \\to \\infty\\).\\n\\n### Step 2 (Use convexity of \\(C\\))\\n\\nFor each \\(k\\), since \\(C\\) is convex:\\n\\n$$w_k := \\theta u_k + (1-\\theta)v_k \\in C$$\\n\\n### Step 3 (Pass to the limit)\\n\\nThe map \\((a, b) \\mapsto \\theta a + (1-\\theta)b\\) is continuous (it's affine). Therefore:\\n\\n$$w_k = \\theta u_k + (1-\\theta)v_k \\to \\theta u + (1-\\theta)v$$\\n\\nas \\(k \\to \\infty\\).\\n\\n### Step 4 (Closure contains limits)\\n\\nSince each \\(w_k \\in C\\) and \\(w_k \\to \\theta u + (1-\\theta)v\\), the limit belongs to \\(\\bar{C}\\).\\n\\nHence \\(\\theta u + (1-\\theta)v \\in \\bar{C}\\). Since \\(u, v\\), and \\(\\theta\\) were arbitrary, \\(\\bar{C}\\) is convex. ∎\\n\\n## Solution 2 (Closed Convex Hull Perspective)\\n\\nLet:\\n\\n$$\\mathcal{F} = \\{K \\subset \\mathbb{R}^n \\mid K \\text{ is closed and convex, and } C \\subseteq K\\}$$\\n\\nThen:\\n\\n$$\\bar{C} = \\bigcap_{K \\in \\mathcal{F}} K$$\\n\\n### Why this equality holds\\n\\n**Right-to-left (\\(\\bigcap K \\subseteq \\bar{C}\\))**: Any set \\(K \\in \\mathcal{F}\\) is closed and contains \\(C\\), so it contains \\(\\bar{C}\\). Therefore \\(\\bar{C} \\supseteq \\bigcap K\\).\\n\\n**Left-to-right (\\(\\bar{C} \\subseteq \\bigcap K\\))**: Note that \\(\\bar{C}\\) itself is closed and contains \\(C\\). If \\(\\bar{C}\\) were convex (which we're proving), then \\(\\bar{C} \\in \\mathcal{F}\\), so \\(\\bar{C} \\supseteq \\bigcap K\\).\\n\\nBut actually, we can argue directly: \\(\\bar{C}\\) is the **smallest closed set** containing \\(C\\). The intersection \\(\\bigcap_{K \\in \\mathcal{F}} K\\) is closed (arbitrary intersection of closed sets is closed), contains \\(C\\), hence contains \\(\\bar{C}\\). Conversely, \\(\\bar{C}\\) is one such closed set if we ignore convexity momentarily.\\n\\n### Convexity from intersection\\n\\nSince each \\(K \\in \\mathcal{F}\\) is convex, and intersections of convex sets are convex:\\n\\n$$\\bigcap_{K \\in \\mathcal{F}} K \\text{ is convex}$$\\n\\nTherefore \\(\\bar{C}\\) is convex. ∎\\n\\n## Alternative Characterization\\n\\nThe closure \\(\\bar{C}\\) can also be characterized as:\\n\\n$$\\bar{C} = \\{x \\in \\mathbb{R}^n \\mid \\forall \\epsilon > 0, B_\\epsilon(x) \\cap C \\ne \\emptyset\\}$$\\n\\nwhere \\(B_\\epsilon(x) = \\{y : \\|y - x\\| < \\epsilon\\}\\) is the open ball.\\n\\nFor \\(x_1, x_2 \\in \\bar{C}\\) and \\(\\theta \\in [0,1]\\), for any \\(\\epsilon > 0\\), choose \\(y_1 \\in B_{\\epsilon/2}(x_1) \\cap C\\) and \\(y_2 \\in B_{\\epsilon/2}(x_2) \\cap C\\). Then:\\n\\n$$\\theta y_1 + (1-\\theta)y_2 \\in C \\quad \\text{(by convexity)}$$\\n\\nand by triangle inequality:\\n\\n$$\\|(\\theta y_1 + (1-\\theta)y_2) - (\\theta x_1 + (1-\\theta)x_2)\\| \\le \\theta\\|y_1 - x_1\\| + (1-\\theta)\\|y_2 - x_2\\| < \\epsilon$$\\n\\nSo \\(B_\\epsilon(\\theta x_1 + (1-\\theta)x_2) \\cap C \\ne \\emptyset\\). Since \\(\\epsilon\\) was arbitrary, \\(\\theta x_1 + (1-\\theta)x_2 \\in \\bar{C}\\). ∎\\n\\n## Key Insights\\n\\n1. **Closure preserves convexity**: Taking limits of convex combinations stays convex\\n2. **Continuity of convex operations**: The map \\((x, y, \\theta) \\mapsto \\theta x + (1-\\theta)y\\) is continuous\\n3. **Closed convex hull**: \\(\\bar{C}\\) when \\(C\\) is convex is called the **closed convex hull** of \\(C\\)\\n4. **Intersection characterization**: Closed convex sets can be characterized as intersections of closed halfspaces (supporting hyperplane theorem)\\n5. **Finite dimensions vs. infinite**: This proof works in \\(\\mathbb{R}^n\\); in infinite dimensions, closure may not preserve convexity without additional assumptions\\n6. **Stability property**: Convexity is \"robust\" under closure—small perturbations don't destroy it",
      "learningObjectives": [
        "closure-operation",
        "convexity-preservation",
        "limit-arguments",
        "continuity-of-affine-maps",
        "intersection-of-convex-sets",
        "closed-convex-hull"
      ],
      "estimatedTime": 15,
      "relatedProblems": ["02-005", "02-007", "02-008"],
      "notes": "This is a fundamental topological property of convex sets. The sequence-based proof is constructive and uses the basic definition of closure, making it suitable for students first learning topology. The intersection characterization provides a more abstract view that connects to duality theory (supporting hyperplanes). The result is essential for existence theorems in optimization—many important convex sets are naturally defined as closures."
    },
    {
      "id": "02-007",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "medium",
      "title": "P7 — Cartesian products and convexity",
      "statement": "Let \\(X \\subset \\mathbb{R}^m\\) and \\(Y \\subset \\mathbb{R}^n\\).\\n\\n(a) If \\(X\\) and \\(Y\\) are convex, prove that \\(X \\times Y \\subset \\mathbb{R}^{m+n}\\) is convex.\\n\\n(b) If \\(X\\) and \\(Y\\) are **not** convex, can \\(X \\times Y\\) be convex?",
      "hints": [
        "Part (a): Take two arbitrary points \\((x_1, y_1), (x_2, y_2) \\in X \\times Y\\)",
        "Form the convex combination \\(\\theta(x_1, y_1) + (1-\\theta)(x_2, y_2)\\) and separate coordinates",
        "Show the \\(x\\)-component is in \\(X\\) and the \\(y\\)-component is in \\(Y\\) using their convexity",
        "Part (b): Assume \\(X\\) is not convex; find witnesses \\(x_1, x_2 \\in X\\) with \\(\\theta x_1 + (1-\\theta)x_2 \\notin X\\)",
        "Pick any \\(y \\in Y\\) and show the line segment in \\(X \\times Y\\) fails convexity",
        "Conclude that if either factor is nonconvex (and nonempty), the product is nonconvex"
      ],
      "solution": "## Part (a) — Product of Convex Sets is Convex\\n\\nTake any two points \\((x_1, y_1), (x_2, y_2) \\in X \\times Y\\) and \\(\\theta \\in [0,1]\\).\\n\\n### Separate coordinates\\n\\nSince \\((x_1, y_1) \\in X \\times Y\\), we have \\(x_1, x_2 \\in X\\) and \\(y_1, y_2 \\in Y\\).\\n\\n### Form convex combination\\n\\n$$\\theta(x_1, y_1) + (1-\\theta)(x_2, y_2) = (\\theta x_1 + (1-\\theta)x_2, \\theta y_1 + (1-\\theta)y_2)$$\\n\\n### Apply convexity of factors\\n\\nSince \\(X\\) is convex:\\n\\(x_\\theta := \\theta x_1 + (1-\\theta)x_2 \\in X\\)\\n\\nSince \\(Y\\) is convex:\\n\\(y_\\theta := \\theta y_1 + (1-\\theta)y_2 \\in Y\\)\\n\\n### Conclusion\\n\\nTherefore:\\n\\((x_\\theta, y_\\theta) \\in X \\times Y\\)\\n\\nHence \\(X \\times Y\\) is convex. ∎\\n\\n## Part (b) — Nonconvex Factors Cannot Yield Convex Product\\n\\n**Answer**: No (assuming nonempty factors). If either \\(X\\) or \\(Y\\) is not convex, then \\(X \\times Y\\) is not convex.\\n\\n### Proof\\n\\nAssume **both sets are nonempty**. Suppose \\(X\\) is not convex. Then there exist \\(x_1, x_2 \\in X\\) and \\(\\theta \\in (0,1)\\) such that:\\n\\n$$x_\\theta := \\theta x_1 + (1-\\theta)x_2 \\notin X$$\\n\\n### Construct failure in product\\n\\nPick any \\(y \\in Y\\) (possible since \\(Y \\ne \\emptyset\\)). Then:\\n\\n$$(x_1, y), (x_2, y) \\in X \\times Y$$\\n\\nBut:\\n\\n$$\\theta(x_1, y) + (1-\\theta)(x_2, y) = (\\theta x_1 + (1-\\theta)x_2, y) = (x_\\theta, y)$$\\n\\nSince \\(x_\\theta \\notin X\\), we have \\((x_\\theta, y) \\notin X \\times Y\\).\\n\\nThus \\(X \\times Y\\) is **not convex**.\\n\\n### Symmetric argument\\n\\nThe same argument applies if \\(Y\\) is not convex: pick any \\(x \\in X\\) and find a convex combination failure in the \\(Y\\) coordinates.\\n\\n### Conclusion\\n\\nWith nonempty factors, \\(X \\times Y\\) is convex **if and only if** both \\(X\\) and \\(Y\\) are convex.\\n\\n(The only trivial exception: if \\(X = \\emptyset\\) or \\(Y = \\emptyset\\), then \\(X \\times Y = \\emptyset\\) is vacuously convex.)\\n\\n∎\\n\\n## Alternative View (Projections from a Convex Product)\\n\\n### Reverse direction\\n\\nSuppose \\(X \\times Y\\) is convex and \\(Y \\ne \\emptyset\\). Fix some \\(y_0 \\in Y\\). The slice:\\n\\n$$X \\times \\{y_0\\} \\subset X \\times Y$$\\n\\nis convex (as a subset of a convex set? Not automatic). But we can argue differently:\\n\\nFor any \\(x_1, x_2 \\in X\\) and \\(\\theta \\in [0,1]\\), we have:\\n\\n$$(x_1, y_0), (x_2, y_0) \\in X \\times Y$$\\n\\nBy convexity of \\(X \\times Y\\):\\n\\n$$(\\theta x_1 + (1-\\theta)x_2, y_0) = \\theta(x_1, y_0) + (1-\\theta)(x_2, y_0) \\in X \\times Y$$\\n\\nThis means \\(\\theta x_1 + (1-\\theta)x_2 \\in X\\). Hence \\(X\\) is convex.\\n\\nSimilarly, \\(Y\\) is convex. This proves the \"only if\" direction cleanly.\\n\\n## Key Insights\\n\\n1. **Product preserves convexity**: Cartesian products of convex sets are convex\\n2. **Componentwise convexity**: Convexity of \\(X \\times Y\\) is equivalent to convexity of each component\\n3. **Strict necessity**: Both factors must be convex (unlike Minkowski sums, where nonconvex can sum to convex)\\n4. **Geometric interpretation**: \\(X \\times Y\\) is the set of all \"stacked\" pairs \\((x, y)\\) in the product space\\n5. **Applications**: In optimization, product structure allows decomposition of problems into smaller subproblems (separability)\\n6. **Higher products**: The result generalizes to \\(n\\)-fold products: \\(X_1 \\times \\dots \\times X_n\\) is convex iff each \\(X_i\\) is convex\\n7. **Connection to direct sums**: In vector spaces, direct sums preserve convexity in the same way",
      "learningObjectives": [
        "cartesian-products",
        "product-spaces",
        "convexity-preservation",
        "necessary-and-sufficient-conditions",
        "counterexamples",
        "projection-maps",
        "separable-structure"
      ],
      "estimatedTime": 20,
      "relatedProblems": ["02-004", "02-006", "04-005"],
      "notes": "Cartesian products are fundamental in optimization when dealing with multiple independent variables or subsystems. Unlike Minkowski sums (where nonconvex + nonconvex can yield convex), products have a strict equivalence: the product is convex iff each factor is convex. This makes products simpler to analyze but also means there's no 'lucky accident' where nonconvexity disappears. The projection viewpoint in the alternative proof foreshadows important concepts in constraint elimination and variable projection in optimization."
    },
    {
      "id": "02-008",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "medium",
      "title": "BV 2.1 — Two-point convexity implies k-point convexity",
      "statement": "Let \\(C \\subset \\mathbb{R}^n\\) be convex. For any \\(k \\ge 2\\), any \\(x_1, \\dots, x_k \\in C\\), and any weights \\(\\theta_1, \\dots, \\theta_k \\ge 0\\) with \\(\\sum_{i=1}^k \\theta_i = 1\\), prove\\n\\n$$\\theta_1 x_1 + \\dots + \\theta_k x_k \\in C$$\\n\\nRecall: A set \\(C\\) is convex if for all \\(x, y \\in C\\) and \\(\\theta \\in [0,1]\\), \\(\\theta x + (1-\\theta)y \\in C\\).",
      "hints": [
        "Use induction on \\(k\\), the number of points",
        "Base case \\(k=2\\): This is exactly the definition of convexity",
        "Inductive step: Assume the result holds for \\(k-1\\) points",
        "For \\(k\\) points, separate out the last point: write \\(\\sum_{i=1}^k \\theta_i x_i = \\alpha(\\sum_{i=1}^{k-1} (\\theta_i/\\alpha)x_i) + (1-\\alpha)x_k\\) where \\(\\alpha = \\sum_{i=1}^{k-1} \\theta_i\\)",
        "Handle two cases: \\(\\alpha = 0\\) (all weight on \\(x_k\\)) and \\(\\alpha > 0\\) (apply induction hypothesis)",
        "Note that when \\(\\alpha > 0\\), the coefficients \\(\\theta_i/\\alpha\\) are nonnegative and sum to 1"
      ],
      "solution": "## Proof (Induction on \\(k\\))\\n\\n### Base case (\\(k = 2\\))\\n\\nThis is exactly the definition of convexity: for \\(x_1, x_2 \\in C\\) and \\(\\theta_1, \\theta_2 \\ge 0\\) with \\(\\theta_1 + \\theta_2 = 1\\),\\n\\n$$\\theta_1 x_1 + \\theta_2 x_2 \\in C$$\\n\\nThis holds by assumption since \\(C\\) is convex. ✓\\n\\n### Inductive step\\n\\n**Assumption**: The claim holds for \\(k-1 \\ge 2\\) points.\\n\\n**Goal**: Prove it for \\(k\\) points.\\n\\n### Step 1 (Separate the last point)\\n\\nLet \\(x_1, \\dots, x_k \\in C\\) and \\(\\theta_1, \\dots, \\theta_k \\ge 0\\) with \\(\\sum_{i=1}^k \\theta_i = 1\\). Define:\\n\\n$$\\alpha = \\sum_{i=1}^{k-1} \\theta_i, \\quad \\beta = \\theta_k$$\\n\\nNote that \\(\\alpha, \\beta \\ge 0\\) and \\(\\alpha + \\beta = 1\\).\\n\\n### Step 2 (Case analysis)\\n\\n**Case 1: \\(\\alpha = 0\\)**\\n\\nThen \\(\\beta = 1\\), so the point is simply:\\n\\n$$\\sum_{i=1}^k \\theta_i x_i = \\theta_k x_k = x_k \\in C ✓$$\\n\\n**Case 2: \\(\\alpha > 0\\)**\\n\\nDefine normalized weights for the first \\(k-1\\) points:\\n\\n$$\\hat{\\theta}_i = \\theta_i/\\alpha \\quad \\text{for } i = 1, \\dots, k-1$$\\n\\n### Step 3 (Verify normalization)\\n\\nFor each \\(i\\): \\(\\hat{\\theta}_i = \\theta_i/\\alpha \\ge 0\\) (since \\(\\theta_i \\ge 0\\) and \\(\\alpha > 0\\))\\n\\nSum: \\(\\sum_{i=1}^{k-1} \\hat{\\theta}_i = \\sum_{i=1}^{k-1} (\\theta_i/\\alpha) = (1/\\alpha)\\sum_{i=1}^{k-1} \\theta_i = (1/\\alpha) \\cdot \\alpha = 1\\) ✓\\n\\n### Step 4 (Apply induction hypothesis)\\n\\nBy the induction hypothesis, since we have \\(k-1\\) points with weights summing to 1:\\n\\n$$\\hat{x} := \\sum_{i=1}^{k-1} \\hat{\\theta}_i x_i \\in C$$\\n\\n### Step 5 (Combine with the \\(k\\)-th point)\\n\\nNow rewrite the original sum:\\n\\n$$\\sum_{i=1}^k \\theta_i x_i = \\sum_{i=1}^{k-1} \\theta_i x_i + \\theta_k x_k = \\alpha \\left( \\sum_{i=1}^{k-1} \\frac{\\theta_i}{\\alpha} x_i \\right) + \\beta x_k = \\alpha \\hat{x} + \\beta x_k$$\\n\\n### Step 6 (Apply two-point convexity)\\n\\nSince \\(\\hat{x} \\in C\\) and \\(x_k \\in C\\), and \\(\\alpha + \\beta = 1\\) with \\(\\alpha, \\beta \\ge 0\\), by the definition of convexity (base case):\\n\\n$$\\alpha \\hat{x} + \\beta x_k \\in C$$\\n\\nTherefore \\(\\sum_{i=1}^k \\theta_i x_i \\in C\\). ✓\\n\\n### Conclusion\\n\\nBy mathematical induction, the statement holds for all \\(k \\ge 2\\). ∎\\n\\n## Geometric Intuition\\n\\nThis result says that **convexity defined for pairs of points automatically extends to arbitrary finite convex combinations**. You can think of it as:\\n\\n1. Start with \\(k\\) points in \\(C\\)\\n2. Take their weighted average (with weights summing to 1)\\n3. The result is still in \\(C\\)\\n\\nThe proof works by **reducing \\(k\\) points to 2 points**:\\n- Group the first \\(k-1\\) points into one \"meta-point\" (their weighted average)\\n- Combine this meta-point with the \\(k\\)-th point using two-point convexity\\n\\n## Key Insights\\n\\n1. **Definition suffices**: The two-point definition of convexity is complete—it implies all finite convex combinations\\n2. **Inductive structure**: This is a classic example of structural induction in geometry\\n3. **Convex combinations**: Points of the form \\(\\sum \\theta_i x_i\\) with \\(\\theta_i \\ge 0, \\sum \\theta_i = 1\\) are called convex combinations\\n4. **Finite vs. infinite**: This result is for *finite* sums; infinite sums require closure arguments\\n5. **Fundamental property**: This characterizes the convex hull: \\(\\text{conv}(\\{x_1,\\dots,x_k\\}) = \\) all convex combinations of \\(\\{x_1,\\dots,x_k\\}\\)\\n6. **Coefficient preservation**: The key is that convex combinations of convex combinations remain convex combinations",
      "learningObjectives": [
        "proof-by-induction",
        "convexity-definition",
        "convex-combinations",
        "coefficient-normalization",
        "case-analysis",
        "convex-hull"
      ],
      "estimatedTime": 20,
      "relatedProblems": ["02-002", "02-012", "02-009"],
      "notes": "This is a fundamental theorem that shows the two-point definition of convexity is equivalent to the k-point definition. It's one of the first results typically proven in any convex analysis course. The inductive proof technique is important to master—grouping k-1 points and treating them as a single point is a recurring pattern in convex geometry. This result justifies why we can work with finite convex hulls using the simple formula conv({x₁,...,xₖ}) = {∑θᵢxᵢ | θᵢ ≥ 0, ∑θᵢ = 1}."
    },
    {
      "id": "02-009",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "medium",
      "title": "BV 2.2 — A set is convex/affine iff every line slice is",
      "statement": "A set \\(C \\subset \\mathbb{R}^n\\) is convex **if and only if** for every line \\(L\\), the intersection \\(C \\cap L\\) is convex.\\n\\nA set \\(C\\) is affine **if and only if** for every line \\(L\\), the intersection \\(C \\cap L\\) is affine.\\n\\nRecall: \\(C\\) is affine if for all \\(x, y \\in C\\) and \\(\\theta \\in \\mathbb{R}\\), \\(\\theta x + (1-\\theta)y \\in C\\).",
      "hints": [
        "For both cases, prove two directions: (\\(\\implies\\)) and (\\(\\impliedby\\))",
        "Direction (\\(\\implies\\)): If \\(C\\) is convex/affine, show any line slice \\(C \\cap L\\) is convex/affine",
        "Use that lines themselves are convex (and affine)",
        "Direction (\\(\\impliedby\\)): Assume all line slices are convex/affine; take arbitrary \\(x, y \\in C\\)",
        "Consider the line \\(L = \\{x + t(y - x) \\mid t \\in \\mathbb{R}\\}\\) passing through \\(x\\) and \\(y\\)",
        "Show that \\(x, y \\in C \\cap L\\), and use the assumed convexity/affineness of \\(C \\cap L\\)"
      ],
      "solution": "## Proof (Convex Case)\\n\\n### Direction (\\(\\implies\\)): \\(C\\) convex \\(\\implies C \\cap L\\) convex for all lines \\(L\\)\\n\\nLet \\(C\\) be convex and \\(L\\) be any line. Take arbitrary \\(u, v \\in C \\cap L\\) and \\(\\theta \\in [0,1]\\).\\n\\n**Claim**: \\(\\theta u + (1-\\theta)v \\in C \\cap L\\)\\n\\n### Step 1 (Membership in \\(C\\))\\n\\nSince \\(u, v \\in C \\cap L\\), we have \\(u, v \\in C\\). By convexity of \\(C\\):\\n\\n$$\\theta u + (1-\\theta)v \\in C$$\\n\\n### Step 2 (Membership in \\(L\\))\\n\\nSince \\(u, v \\in L\\) and \\(L\\) is a line (which is affine, hence convex), and \\(\\theta \\in [0,1]\\):\\n\\n$$\\theta u + (1-\\theta)v \\in L$$\\n\\n### Step 3 (Membership in intersection)\\n\\nCombining Steps 1 and 2:\\n\\n$$\\theta u + (1-\\theta)v \\in C \\cap L$$\\n\\nHence \\(C \\cap L\\) is convex. ✓\\n\\n### Direction (\\(\\impliedby\\)): \\(C \\cap L\\) convex for all \\(L \\implies C\\) convex\\n\\nAssume \\(C \\cap L\\) is convex for every line \\(L\\). Take arbitrary \\(x, y \\in C\\) and \\(\\theta \\in [0,1]\\).\\n\\n### Step 1 (Define the line through \\(x\\) and \\(y\\))\\n\\nLet:\\n\\n$$L = \\{x + t(y - x) \\mid t \\in \\mathbb{R}\\}$$\\n\\nThis is the line passing through \\(x\\) (at \\(t=0\\)) and \\(y\\) (at \\(t=1\\)).\\n\\n### Step 2 (\\(x\\) and \\(y\\) are in the slice)\\n\\nClearly \\(x, y \\in L\\) (by construction). Since \\(x, y \\in C\\), we have:\\n\\n$$x, y \\in C \\cap L$$\\n\\n### Step 3 (Apply convexity of the slice)\\n\\nBy assumption, \\(C \\cap L\\) is convex. Therefore, for any \\(\\theta \\in [0,1]\\):\\n\\n$$\\theta x + (1-\\theta)y \\in C \\cap L \\subseteq C$$\\n\\nHence \\(C\\) is convex. ✓\\n\\n## Proof (Affine Case)\\n\\nThe proof is **identical** to the convex case, except we replace:\\n- \"\\(\\theta \\in [0,1]\\)\" with \"\\(\\theta \\in \\mathbb{R}\\)\"\\n- \"convex\" with \"affine\" throughout\\n\\n### Direction (\\(\\implies\\)): \\(C\\) affine \\(\\implies C \\cap L\\) affine\\n\\nFor \\(u, v \\in C \\cap L\\) and any \\(\\theta \\in \\mathbb{R}\\):\\n- \\(\\theta u + (1-\\theta)v \\in C\\) (since \\(C\\) is affine)\\n- \\(\\theta u + (1-\\theta)v \\in L\\) (since \\(L\\) is a line, hence affine)\\n- Therefore \\(\\theta u + (1-\\theta)v \\in C \\cap L\\) ✓\\n\\n### Direction (\\(\\impliedby\\)): \\(C \\cap L\\) affine for all \\(L \\implies C\\) affine\\n\\nFor \\(x, y \\in C\\) and any \\(\\theta \\in \\mathbb{R}\\), define \\(L = \\{x + t(y-x) \\mid t \\in \\mathbb{R}\\}\\). Then:\\n- \\(x, y \\in C \\cap L\\)\\n- \\(C \\cap L\\) is affine (by assumption)\\n- Therefore \\(\\theta x + (1-\\theta)y \\in C \\cap L \\subseteq C\\) ✓\\n\\n## Geometric Interpretation\\n\\nThis theorem says:\\n\\n**Convex sets are exactly those sets whose line slices are all convex.**\\n\\nIntuitively:\\n- To check convexity of \\(C\\), you only need to verify that every line intersects \\(C\\) in a convex set (an interval, a point, or empty)\\n- In \\(\\mathbb{R}^2\\), this means checking all 1D slices\\n- In \\(\\mathbb{R}^3\\), this means checking all 1D slices through 3D space\\n\\n**For affine sets**: The same holds, but line slices must be affine (entire lines, points, or empty).\\n\\n## Key Insights\\n\\n1. **Dimension reduction**: Convexity in \\(\\mathbb{R}^n\\) reduces to checking convexity in \\(\\mathbb{R}^1\\) (along all lines)\\n2. **Lines are universal**: Lines are the \"probe\" sets that detect convexity\\n3. **Necessary and sufficient**: This gives a complete characterization—both directions hold\\n4. **Affine vs. convex**: The proof technique is identical; only the coefficient range differs (\\(\\theta \\in \\mathbb{R}\\) vs. \\(\\theta \\in [0,1]\\))\\n5. **Verification strategy**: To check if a set is convex, you can verify convexity along arbitrary lines through the set\\n6. **Connection to line segments**: For convex sets, \\(C \\cap L\\) is either empty, a point, or a line segment (possibly infinite)\\n7. **Extension to cones/subspaces**: Similar characterizations exist for cones (rays) and subspaces (lines through origin)",
      "learningObjectives": [
        "line-slices",
        "convexity-characterization",
        "affine-sets",
        "iff-proofs",
        "dimension-reduction",
        "intersection-properties"
      ],
      "estimatedTime": 20,
      "relatedProblems": ["02-008", "02-010", "02-015"],
      "notes": "This is a powerful characterization theorem that reduces n-dimensional convexity to 1-dimensional convexity. It's particularly useful for verifying convexity of sets defined implicitly—just check that intersections with arbitrary lines are convex. The affine case is included to show the parallel structure. This result is often used in optimization to restrict problems to line searches, and it's fundamental to understanding why line search methods work for convex functions."
    },
    {
      "id": "02-010",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "hard",
      "title": "BV 2.3 — Closed + midpoint-convex implies convex",
      "statement": "A set \\(C \\subset \\mathbb{R}^n\\) is **midpoint-convex** if for all \\(x, y \\in C\\),\\n\\n$$(x + y)/2 \\in C$$\\n\\nShow: if \\(C\\) is midpoint-convex **and closed**, then \\(C\\) is convex.",
      "hints": [
        "First, show that midpoint-convexity implies dyadic rationals \\(\\theta = m/2^r\\) work",
        "Use repeated midpoint operations: \\((x+y)/2\\), then \\(((x+y)/2 + y)/2 = (x+3y)/4\\), etc.",
        "For general \\(\\theta \\in [0,1]\\), approximate \\(\\theta\\) by dyadic rationals \\(\\theta_k = m_k/2^{r_k} \\to \\theta\\)",
        "Show that \\(\\theta_k x + (1-\\theta_k)y \\in C\\) for all \\(k\\) (by Step 1)",
        "Use closedness: take the limit as \\(k \\to \\infty\\) to get \\(\\theta x + (1-\\theta)y \\in \\bar{C} = C\\)",
        "Key: Dyadic rationals are dense in \\([0,1]\\), and affine maps are continuous"
      ],
      "solution": "## Proof\\n\\nFix \\(x, y \\in C\\). We will prove \\(\\theta x + (1-\\theta)y \\in C\\) for every \\(\\theta \\in [0,1]\\).\\n\\n### Step 1 (Dyadic coefficients)\\n\\n**Claim**: For any dyadic rational \\(\\theta = m/2^r\\) with integers \\(m, r \\ge 0\\) and \\(0 \\le m \\le 2^r\\),\\n\\n$$\\theta x + (1-\\theta)y \\in C$$\\n\\n**Proof by induction on \\(r\\)**:\\n\\n**Base case \\(r = 0\\)**: \\(\\theta \\in \\{0, 1\\}\\)\\n- If \\(\\theta = 0\\): \\(\\theta x + (1-\\theta)y = y \\in C\\) ✓\\n- If \\(\\theta = 1\\): \\(\\theta x + (1-\\theta)y = x \\in C\\) ✓\\n\\n**Inductive step**: Assume the claim holds for \\(r-1\\). Consider \\(\\theta = m/2^r\\).\\n\\n**Case 1**: \\(m\\) is even, say \\(m = 2k\\). Then \\(\\theta = k/2^{r-1}\\), which has the form from \\(r-1\\). By induction, \\(\\theta x + (1-\\theta)y \\in C\\). ✓\\n\\n**Case 2**: \\(m\\) is odd, say \\(m = 2k+1\\). Then:\\n\\n$$\\theta = \\frac{2k+1}{2^r} = \\frac{k}{2^{r-1}} \\cdot \\frac{1}{2} + \\frac{k+1}{2^{r-1}} \\cdot \\frac{1}{2}$$\\n\\nLet:\\n- \\(\\theta_1 = k/2^{r-1}\\)\\n- \\(\\theta_2 = (k+1)/2^{r-1}\\)\\n\\nNote that \\(\\theta = (\\theta_1 + \\theta_2)/2\\).\\n\\nBy the induction hypothesis:\\n\\(u_1 := \\theta_1 x + (1-\\theta_1)y \\in C\\)\\n\\(u_2 := \\theta_2 x + (1-\\theta_2)y \\in C\\)\\n\\nBy midpoint-convexity:\\n\\n$$(u_1 + u_2)/2 \\in C$$\\n\\nBut:\\n\\n$$(u_1 + u_2)/2 = \\frac{(\\theta_1 x + (1-\\theta_1)y + \\theta_2 x + (1-\\theta_2)y)}{2} = \\left(\\frac{\\theta_1+\\theta_2}{2}\\right)x + \\left(\\frac{2-(\\theta_1+\\theta_2)}{2}\\right)y = \\theta x + (1-\\theta)y \\in C ✓$$\\n\\nBy induction, dyadic rationals work.\\n\\n### Step 2 (Density of dyadic rationals)\\n\\nDyadic rationals in \\([0,1]\\) are **dense**: for any \\(\\theta \\in [0,1]\\) and any \\(\\epsilon > 0\\), there exists a dyadic rational \\(\\theta'\\) with \\(|\\theta - \\theta'| < \\epsilon\\).\\n\\nEquivalently, there exists a sequence of dyadic rationals \\((\\theta_k)\\) with \\(\\theta_k \\to \\theta\\).\\n\\n### Step 3 (Construct sequence in \\(C\\))\\n\\nChoose a sequence of dyadic rationals \\(\\theta_k \\to \\theta\\). By Step 1, for each \\(k\\):\\n\\n$$w_k := \\theta_k x + (1-\\theta_k)y \\in C$$\\n\\n### Step 4 (Pass to the limit)\\n\\nThe map \\(\\theta \\mapsto \\theta x + (1-\\theta)y\\) is continuous (it's affine in \\(\\theta\\)). Therefore:\\n\\n$$w_k = \\theta_k x + (1-\\theta_k)y \\to \\theta x + (1-\\theta)y$$\\n\\nas \\(k \\to \\infty\\).\\n\\n### Step 5 (Use closedness)\\n\\nSince each \\(w_k \\in C\\) and \\(C\\) is closed:\\n\\n$$\\lim_{k\\to\\infty} w_k \\in \\bar{C} = C$$\\n\\nTherefore:\\n\\n$$\\theta x + (1-\\theta)y \\in C$$\\n\\nSince \\(x, y\\), and \\(\\theta\\) were arbitrary, \\(C\\) is convex. ∎\\n\\n## Why \"Closed\" is Essential\\n\\n**Counterexample without closedness**:\\n\\nLet \\(C = \\mathbb{Q} \\cap [0,1]\\) (rationals in \\([0,1]\\)).\\n\\n**Midpoint-convex**: If \\(x, y \\in \\mathbb{Q}\\), then \\((x+y)/2 \\in \\mathbb{Q}\\). ✓\\n\\n**Not convex**: Take \\(x = 0, y = 1, \\theta = \\sqrt{2}/2 \\approx 0.707...\\)\\n\\nThen \\(\\theta x + (1-\\theta)y = (2-\\sqrt{2})/2 \\notin \\mathbb{Q}\\), so \\(\\theta x + (1-\\theta)y \\notin C\\).\\n\\nHence \\(C\\) is midpoint-convex but **not convex**.\\n\\n**Why it fails**: \\(C\\) is not closed (\\(\\bar{C} = [0,1] \\supset C\\)). The limit argument in Step 5 breaks down because the limit might not be in \\(C\\).\\n\\n## Key Insights\\n\\n1. **Midpoint-convexity is weaker**: Midpoint-convexity (only \\(\\theta = 1/2\\)) is strictly weaker than convexity (all \\(\\theta \\in [0,1]\\))\\n2. **Closure bridges the gap**: Adding closedness makes midpoint-convexity equivalent to convexity\\n3. **Dyadic density**: The dyadic rationals \\(\\{m/2^r\\}\\) are dense in \\([0,1]\\), which is key to the approximation\\n4. **Inductive construction**: The proof builds up from \\(1/2\\) to all dyadic rationals by repeated subdivision\\n5. **Limit arguments**: Passing to limits requires closedness—this is a common pattern in analysis\\n6. **Topological + algebraic**: The result combines topology (closedness, limits) with algebra (dyadic arithmetic)\\n7. **Applications**: This is used in functional analysis where midpoint-convexity is sometimes easier to verify directly",
      "learningObjectives": [
        "midpoint-convexity",
        "closed-sets",
        "dyadic-rationals",
        "density-arguments",
        "limit-points",
        "induction-on-binary-expansions",
        "counterexamples"
      ],
      "estimatedTime": 30,
      "relatedProblems": ["02-006", "02-009", "03-015"],
      "notes": "This is a deep result showing that midpoint-convexity plus closedness implies full convexity. The dyadic construction is clever—it builds all binary fractions by repeated bisection, then uses density to reach all real coefficients. The counterexample (rationals in [0,1]) is instructive: it shows closedness is truly necessary. This theorem appears in functional analysis when studying convex functions on Banach spaces, where sometimes only midpoint-convexity can be verified directly. The proof technique—building dyadic rationals inductively—appears in many contexts (e.g., proving uniqueness of Lebesgue measure)."
    },
    {
      "id": "02-011",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "easy",
      "title": "BV 2.4 — Convex hull equals intersection of all convex supersets",
      "statement": "For \\(S \\subset \\mathbb{R}^n\\), let \\(\\text{conv}(S)\\) be its convex hull (all finite convex combinations of points in \\(S\\)). Show:\\n\\n$$\\text{conv}(S) = \\bigcap \\{C \\subset \\mathbb{R}^n \\mid C \\text{ is convex and } S \\subseteq C\\}$$\\n\\nIn words: the convex hull of \\(S\\) is the intersection of all convex sets containing \\(S\\).",
      "hints": [
        "Let \\(\\mathcal{F} = \\{C \\mid C \\text{ is convex and } S \\subseteq C\\}\\) and \\(K = \\bigcap_{C \\in \\mathcal{F}} C\\)",
        "First show \\(K\\) is convex (intersection of convex sets)",
        "Direction 1 (\\(\\text{conv}(S) \\subseteq K\\)): Show every convex set containing \\(S\\) must contain \\(\\text{conv}(S)\\)",
        "Use that convex sets are closed under convex combinations (BV 2.1)",
        "Direction 2 (\\(K \\subseteq \\text{conv}(S)\\)): Show \\(\\text{conv}(S)\\) itself is in \\(\\mathcal{F}\\), so \\(K \\subseteq \\text{conv}(S)\\)",
        "Verify that \\(\\text{conv}(S)\\) is convex and contains \\(S\\)"
      ],
      "solution": "## Proof\\n\\nLet:\\n\\n$$\\mathcal{F} = \\{C \\subset \\mathbb{R}^n \\mid C \\text{ is convex and } S \\subseteq C\\}$$\\n\\n$$K = \\bigcap_{C \\in \\mathcal{F}} C$$\\n\\n### Preliminary: \\(K\\) is convex and contains \\(S\\)\\n\\n**\\(K\\) is convex**: Arbitrary intersections of convex sets are convex. Since each \\(C \\in \\mathcal{F}\\) is convex, \\(K\\) is convex. ✓\\n\\n**\\(S \\subseteq K\\)**: Every \\(C \\in \\mathcal{F}\\) contains \\(S\\), so their intersection \\(K\\) contains \\(S\\). ✓\\n\\n### Direction 1: \\(\\text{conv}(S) \\subseteq K\\)\\n\\n**Goal**: Show every point in \\(\\text{conv}(S)\\) lies in \\(K\\).\\n\\nTake any \\(C \\in \\mathcal{F}\\). We'll show \\(\\text{conv}(S) \\subseteq C\\).\\n\\n### Step 1.1 (\\(C\\) contains \\(S\\))\\n\\nBy definition of \\(\\mathcal{F}\\): \\(S \\subseteq C\\).\\n\\n### Step 1.2 (\\(C\\) contains all convex combinations)\\n\\nSince \\(C\\) is convex and contains \\(S\\), by BV 2.1 (\\(k\\)-point convexity), \\(C\\) must contain **all finite convex combinations** of points from \\(S\\).\\n\\nThat is: \\(\\text{conv}(S) \\subseteq C\\).\\n\\n### Step 1.3 (Intersect over all \\(C\\))\\n\\nSince \\(\\text{conv}(S) \\subseteq C\\) for every \\(C \\in \\mathcal{F}\\):\\n\\n$$\\text{conv}(S) \\subseteq \\bigcap_{C \\in \\mathcal{F}} C = K ✓$$\\n\\n### Direction 2: \\(K \\subseteq \\text{conv}(S)\\)\\n\\n**Goal**: Show every point in \\(K\\) lies in \\(\\text{conv}(S)\\).\\n\\n### Step 2.1 (Show \\(\\text{conv}(S) \\in \\mathcal{F}\\))\\n\\nWe need to verify that \\(\\text{conv}(S)\\) is convex and contains \\(S\\).\\n\\n**Contains \\(S\\)**: By definition, \\(S \\subseteq \\text{conv}(S)\\) (every point \\(x \\in S\\) equals the trivial convex combination \\(1 \\cdot x\\)). ✓\\n\\n**Is convex**: This was proven in Problem P2 (BV 2.1 also gives this). The convex hull is convex. ✓\\n\\nTherefore: \\(\\text{conv}(S) \\in \\mathcal{F}\\).\\n\\n### Step 2.2 (\\(K\\) is a subset)\\n\\nSince \\(K\\) is the intersection of all sets in \\(\\mathcal{F}\\), and \\(\\text{conv}(S) \\in \\mathcal{F}\\):\\n\\n$$K = \\bigcap_{C \\in \\mathcal{F}} C \\subseteq \\text{conv}(S) ✓$$\\n\\n### Conclusion\\n\\nFrom Directions 1 and 2:\\n\\n$$K = \\text{conv}(S) ∎$$\\n\\n## Alternative Characterization\\n\\nThis result shows:\\n\\n**\\(\\text{conv}(S)\\) is the smallest convex set containing \\(S\\)**\\n\\nSmallest in the sense that:\\n1. \\(\\text{conv}(S)\\) is convex and contains \\(S\\)\\n2. \\(\\text{conv}(S)\\) is contained in every other convex set containing \\(S\\)\\n\\n## Geometric Interpretation\\n\\nThink of \\(\\text{conv}(S)\\) as:\\n\\n- **Explicitly**: All convex combinations \\(\\sum \\theta_i s_i\\) with \\(s_i \\in S, \\theta_i \\ge 0, \\sum \\theta_i = 1\\)\\n- **Implicitly**: The intersection of all convex supersets (halfspaces, polytopes, balls, etc. that contain \\(S\\))\\n\\nThis theorem says these two descriptions are **equivalent**.\\n\\n## Examples\\n\\n### Example 1: Finite set in \\(\\mathbb{R}^2\\)\\n\\n\\(S = \\{(0,0), (1,0), (0,1)\\}\\)\\n\\n**Explicit**: \\(\\text{conv}(S) = \\{\\theta_1(0,0) + \\theta_2(1,0) + \\theta_3(0,1) \\mid \\theta_i \\ge 0, \\theta_1+\\theta_2+\\theta_3=1\\} = \\) triangle with vertices at \\(S\\)\\n\\n**Implicit**: \\(\\text{conv}(S) = \\) intersection of all halfspaces containing \\(S\\) \\(= \\{(x,y) \\mid x \\ge 0, y \\ge 0, x+y \\le 1\\}\\)\\n\\n### Example 2: Unit circle in \\(\\mathbb{R}^2\\)\\n\\n\\(S = \\{(\\cos \\theta, \\sin \\theta) \\mid \\theta \\in [0, 2\\pi)\\}\\)\\n\\n**Explicit**: \\(\\text{conv}(S) = \\) all convex combinations of points on the circle \\(= \\) unit disk \\(\\{(x,y) \\mid x^2+y^2 \\le 1\\}\\)\\n\\n**Implicit**: \\(\\text{conv}(S) = \\bigcap \\{\\text{halfspaces } H \\mid S \\subseteq H\\} = \\) unit disk (intersection of all supporting halfspaces)\\n\\n## Key Insights\\n\\n1. **Two definitions, one object**: Convex hull has both a constructive definition (convex combinations) and a descriptive definition (smallest convex set)\\n2. **Smallest convex set**: \\(\\text{conv}(S)\\) is the unique smallest convex set containing \\(S\\)\\n3. **Intersection characterization**: This gives a way to compute/describe \\(\\text{conv}(S)\\) via supporting hyperplanes\\n4. **Closure under convex combinations**: Any convex set containing \\(S\\) must contain all convex combinations of \\(S\\)\\n5. **Duality**: The implicit characterization connects to the dual description via support functions\\n6. **Carathéodory's theorem**: In \\(\\mathbb{R}^n\\), every point in \\(\\text{conv}(S)\\) is a convex combination of at most \\(n+1\\) points from \\(S\\)\\n7. **Computational geometry**: This characterization is fundamental for computing convex hulls",
      "learningObjectives": [
        "convex-hull",
        "intersection-characterization",
        "smallest-convex-set",
        "implicit-vs-explicit",
        "closure-properties",
        "set-containment-proofs"
      ],
      "estimatedTime": 15,
      "relatedProblems": ["02-002", "02-008", "02-015"],
      "notes": "This elegant characterization theorem shows that the convex hull can be defined either constructively (take all convex combinations) or descriptively (take the smallest convex superset). The proof is straightforward but instructive: one direction uses that convex sets must be closed under convex combinations, the other uses that conv(S) is itself convex. This result is foundational in convex geometry and connects to duality theory—the implicit description via intersections of halfspaces leads to the dual representation via support functions. In computational geometry, this characterization justifies algorithms that compute convex hulls by finding supporting hyperplanes."
    },
    {
      "id": "02-012",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "hard",
      "title": "BV 2.10 — Solution set of a quadratic inequality",
      "statement": "Let\\n\\n$$C = \\{x \\in \\mathbb{R}^n \\mid q(x) \\le 0\\}, \\quad q(x) = x^T A x + b^T x + c$$\\n\\nwith \\(A \\in \\mathbb{S}^n\\) (symmetric), \\(b \\in \\mathbb{R}^n, c \\in \\mathbb{R}\\).\\n\\n(a) Show that \\(C\\) is convex if \\(A \\succeq 0\\) (positive semidefinite).\\n\\n(b) Let \\(H = \\{x \\in \\mathbb{R}^n \\mid g^T x + h = 0\\}\\) with \\(g \\ne 0\\). Show that \\(C \\cap H\\) is convex if there exists \\(\\lambda \\in \\mathbb{R}\\) such that\\n\\n$$A + \\lambda gg^T \\succeq 0$$\\n\\n**Are the converses true?** Does convexity of \\(C\\) imply \\(A \\succeq 0\\)? Does convexity of \\(C \\cap H\\) imply such a \\(\\lambda\\) exists?",
      "hints": [
        "Part (a): Show \\(q\\) is a convex function by computing its Hessian \\(\\nabla^2 q(x) = 2A\\)",
        "A sublevel set of a convex function is convex",
        "Part (b): Parametrize the hyperplane \\(H\\) using an orthonormal basis",
        "Let \\(x_0\\) satisfy \\(g^T x_0 + h = 0\\), and let \\(U\\) have orthonormal columns spanning \\(g^\\perp\\)",
        "Every point in \\(H\\) can be written as \\(x = x_0 + Uy\\) for \\(y \\in \\mathbb{R}^{n-1}\\)",
        "Restrict \\(q\\) to \\(H\\): define \\(\\tilde{q}(y) = q(x_0 + Uy)\\) and show it's convex when \\(U^T A U \\succeq 0\\)",
        "Use that \\(U^T g = 0\\) implies \\(U^T(A + \\lambda gg^T)U = U^T A U\\)",
        "For converses: construct counterexamples"
      ],
      "solution": "## Part (a) — Sublevel Set of Convex Quadratic\\n\\n### Step 1 (\\(q\\) is convex when \\(A \\succeq 0\\))\\n\\nThe function \\(q(x) = x^T A x + b^T x + c\\) has Hessian:\\n\\n$$\\nabla^2 q(x) = 2A$$\\n\\nSince \\(A \\succeq 0\\), we have \\(\\nabla^2 q(x) \\succeq 0\\) for all \\(x\\). Therefore \\(q\\) is convex.\\n\\n### Step 2 (Sublevel sets of convex functions are convex)\\n\\nA fundamental result: if \\(f\\) is convex, then for any \\(\\alpha \\in \\mathbb{R}\\), the sublevel set\\n\\n$$\\{x \\mid f(x) \\le \\alpha\\}$$\\n\\nis convex.\\n\\n### Step 3 (Apply to \\(q\\))\\n\\nSince \\(q\\) is convex, the sublevel set\\n\\n$$C = \\{x \\mid q(x) \\le 0\\}$$\\n\\nis convex. ∎\\n\\n## Part (b) — Restriction to a Hyperplane\\n\\n### Step 1 (Parametrize the hyperplane)\\n\\nChoose any \\(x_0 \\in \\mathbb{R}^n\\) with \\(g^T x_0 + h = 0\\). Let \\(U \\in \\mathbb{R}^{n \\times (n-1)}\\) have orthonormal columns forming a basis of \\(g^\\perp\\) (the orthogonal complement of \\(g\\)), so:\\n\\n$$U^T U = I_{n-1}, \\quad U^T g = 0$$\\n\\nEvery point of \\(H\\) can be written uniquely as:\\n\\n$$x = x_0 + Uy, \\quad y \\in \\mathbb{R}^{n-1}$$\\n\\n### Step 2 (Reduce the quadratic on \\(H\\))\\n\\nDefine the \"restricted\" quadratic:\\n\\n$$\\tilde{q}(y) := q(x_0 + Uy) = (x_0 + Uy)^T A (x_0 + Uy) + b^T(x_0 + Uy) + c$$\\n\\nExpanding:\\n\\n$$\\tilde{q}(y) = y^T(U^T A U)y + (2x_0^T A U + b^T U)y + (x_0^T A x_0 + b^T x_0 + c)$$\\n\\nThis is quadratic in \\(y\\) with Hessian:\\n\\n$$\\nabla^2 \\tilde{q}(y) = 2U^T A U$$\\n\\n### Step 3 (Convexity condition on \\(H\\))\\n\\nConvexity of \\(\\{y \\mid \\tilde{q}(y) \\le 0\\}\\) requires:\\n\\n$$U^T A U \\succeq 0 \\quad (\\star)$$\\n\\n### Step 4 (Use the given condition)\\n\\nAssume there exists \\(\\lambda\\) with \\(A + \\lambda gg^T \\succeq 0\\). Then:\\n\\n$$U^T(A + \\lambda gg^T)U = U^T A U + \\lambda(U^T g)(g^T U) = U^T A U + \\lambda \\cdot 0 = U^T A U$$\\n\\nSince \\(A + \\lambda gg^T \\succeq 0\\), we have:\\n\\n$$U^T A U = U^T(A + \\lambda gg^T)U \\succeq 0$$\\n\\nThis verifies condition \\((\\star)\\). Therefore \\(\\tilde{q}\\) is convex in \\(y\\), and:\\n\\n$$C \\cap H = \\{x_0 + Uy \\mid \\tilde{q}(y) \\le 0\\}$$\\n\\nis convex. ∎\\n\\n## Are the Converses True?\\n\\n### Part (a) Converse: FALSE\\n\\n**Counterexample**: Let \\(n = 1, A = -1, b = 0, c = 0\\). Then:\\n\\n$$q(x) = -x^2 \\le 0 \\iff x^2 \\ge 0 \\iff x \\in \\mathbb{R}$$\\n\\nSo \\(C = \\mathbb{R}\\), which is convex, even though \\(A = -1 \\not\\succeq 0\\).\\n\\n**General principle**: If \\(q(x) \\le 0\\) for all \\(x\\) (i.e., \\(q \\le 0\\) everywhere), then \\(C = \\mathbb{R}^n\\) is convex regardless of whether \\(A \\succeq 0\\).\\n\\n### Part (b) Converse: FALSE\\n\\n**Counterexample**: Let \\(n = 2, g = e_1 = (1,0), h = 0\\), so \\(H = \\{(0, x_2) \\mid x_2 \\in \\mathbb{R}\\}\\) (the vertical axis).\\n\\nLet:\\n\\n$$A = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}, \\quad b = 0, \\quad c = 0$$\\n\\nThen \\(q(x) = 2x_1 x_2\\).\\n\\nOn \\(H\\), write \\(x = (0, t)\\). Then:\\n\\n$$q(x) = 2 \\cdot 0 \\cdot t = 0$$\\n\\nSo \\(C \\cap H = \\{x \\in H \\mid 0 \\le 0\\} = H\\), which is convex.\\n\\n**Check the claimed converse**: We need \\(\\lambda\\) such that\\n\\n$$A + \\lambda gg^T = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} + \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix} = \\begin{bmatrix} \\lambda & 1 \\\\ 1 & 0 \\end{bmatrix}$$\\n\\nis positive semidefinite.\\n\\nFor a \\(2 \\times 2\\) matrix \\(\\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}\\) to be PSD, we need:\\n\\(a \\ge 0, c \\ge 0, ac - b^2 \\ge 0\\)\\n\\nHere: \\(a = \\lambda, b = 1, c = 0\\).\\n\\nFor PSD: \\(\\lambda \\ge 0, 0 \\ge 0, \\lambda \\cdot 0 - 1^2 = -1 \\ge 0\\).\\n\\nThe last condition gives \\(-1 \\ge 0\\), which is **false** for all \\(\\lambda\\).\\n\\nThus no such \\(\\lambda\\) exists, yet \\(C \\cap H\\) is convex. The converse is false. ∎\\n\\n## Geometric Intuition\\n\\n**Part (a)**: The set \\(C\\) is the region where a quadratic form is nonpositive. When \\(A \\succeq 0\\), this is a sublevel set of a convex function (a \"bowl\" or ellipsoid-like region). When \\(A \\not\\succeq 0\\), the quadratic can be nonconvex (saddle-shaped), but \\(C\\) might still be convex if it's \\(\\mathbb{R}^n\\) or empty.\\n\\n**Part (b)**: Restricting to a hyperplane \\(H\\) reduces the dimension by 1. The condition \\(A + \\lambda gg^T \\succeq 0\\) ensures the quadratic form has nonnegative curvature within \\(H\\). The counterexample shows this is sufficient but not necessary—convexity on \\(H\\) only constrains curvature within \\(H\\), not in the normal direction \\(g\\).\\n\\n## Key Insights\\n\\n1. **Convex functions \\(\\to\\) convex sublevel sets**: This is a fundamental connection between convex functions and convex sets\\n2. **Hessian test**: A quadratic is convex iff its Hessian (which is constant for quadratics) is PSD\\n3. **Dimension reduction**: Restricting to a hyperplane reduces the problem to one dimension lower\\n4. **Sufficient vs. necessary**: The given conditions are sufficient for convexity but not necessary (counterexamples show this)\\n5. **Normal vs. tangent curvature**: For \\(C \\cap H\\), only tangent curvature (within \\(H\\)) matters; normal curvature (along \\(g\\)) doesn't affect convexity of the slice\\n6. **S-procedure**: Part (b) is related to the S-procedure in optimization—adding a multiple of one constraint to another\\n7. **Ellipsoids and level sets**: When \\(A \\succ 0\\), \\(C\\) is an ellipsoid (possibly empty); when \\(A \\succeq 0\\), \\(C\\) is cylindrical or the whole space",
      "learningObjectives": [
        "quadratic-forms",
        "positive-semidefinite-matrices",
        "sublevel-sets",
        "convex-functions",
        "hessian-test",
        "hyperplane-restrictions",
        "s-procedure",
        "counterexamples",
        "necessary-vs-sufficient-conditions"
      ],
      "estimatedTime": 35,
      "relatedProblems": ["02-003", "03-008", "04-012"],
      "notes": "This problem connects quadratic forms, matrix analysis, and convexity in a deep way. Part (a) is straightforward—it's the standard sublevel set result for convex quadratics. Part (b) is more sophisticated, requiring parametrization of hyperplanes and understanding how rank-1 updates (λggᵀ) affect positive semidefiniteness. The counterexamples are crucial: they show the conditions are sufficient but not necessary, illustrating that convexity of a set doesn't uniquely determine the properties of defining functions. This is foundational for quadratically constrained quadratic programming (QCQP) and semidefinite programming (SDP)."
    },
    {
      "id": "02-013",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "easy",
      "title": "BV 2.11 — Hyperbolic sets are convex",
      "statement": "Show that the set\\n\\n$$\\{x \\in \\mathbb{R}^2_+ \\mid x_1 x_2 \\ge 1\\}$$\\n\\nis convex. Generalize to\\n\\n$$\\{x \\in \\mathbb{R}^n_+ \\mid \\prod_{i=1}^n x_i \\ge 1\\}$$\\n\\nwhere \\(\\mathbb{R}^n_+ = \\{x \\in \\mathbb{R}^n \\mid x_i \\ge 0 \\text{ for all } i\\}\\).",
      "hints": [
        "Work on the positive orthant where \\(\\log\\) is defined: note that feasible points satisfy \\(x_i > 0\\)",
        "Consider the function \\(\\phi(x) = \\sum_i \\log x_i\\) on \\(\\mathbb{R}^n_{++} = \\{x \\mid x_i > 0 \\forall i\\}\\)",
        "Show that \\(\\phi\\) is concave by checking its Hessian or using that \\(\\log\\) is concave",
        "The set can be written as \\(\\{x \\in \\mathbb{R}^n_{++} \\mid \\phi(x) \\ge 0\\}\\), a superlevel set of a concave function",
        "Superlevel sets of concave functions are convex",
        "Alternative: Use weighted AM-GM inequality directly on convex combinations"
      ],
      "solution": "## Solution (Concavity of log)\\n\\nWork on the positive orthant \\(\\mathbb{R}^n_{++} = \\{x \\in \\mathbb{R}^n \\mid x_i > 0 \\text{ for all } i\\}\\).\\n\\n**Note**: Any \\(x\\) with \\(\\prod_i x_i \\ge 1\\) must satisfy \\(x_i > 0\\) for all \\(i\\), so \\(\\mathbb{R}^n_{++}\\) and \\(\\mathbb{R}^n_+\\) coincide on the feasible set.\\n\\n### Step 1 (Define \\(\\phi\\))\\n\\nDefine:\\n\\n$$\\phi(x) = \\sum_{i=1}^n \\log x_i, \\quad \\text{dom}(\\phi) = \\mathbb{R}^n_{++}$$\\n\\n### Step 2 (\\(\\phi\\) is concave)\\n\\nEach function \\(x \\mapsto \\log x_i\\) is concave on \\((0, \\infty)\\) because:\\n\\n$$\\frac{d^2}{dx_i^2} (\\log x_i) = -1/x_i^2 < 0$$\\n\\nSums of concave functions are concave, so \\(\\phi\\) is concave on \\(\\mathbb{R}^n_{++}\\).\\n\\n### Step 3 (Express the set as a superlevel set)\\n\\nNote that:\\n\\n$$\\prod_{i=1}^n x_i \\ge 1 \\iff \\log(\\prod_i x_i) \\ge 0 \\iff \\sum_{i=1}^n \\log x_i \\ge 0 \\iff \\phi(x) \\ge 0$$\\n\\nTherefore, the set is:\\n\\n$$\\{x \\in \\mathbb{R}^n_{++} \\mid \\phi(x) \\ge 0\\}$$\\n\\n### Step 4 (Superlevel sets of concave functions are convex)\\n\\n**Theorem**: If \\(f\\) is concave, then for any \\(\\alpha\\), the superlevel set \\(\\{x \\mid f(x) \\ge \\alpha\\}\\) is convex.\\n\\n**Proof**: Take \\(x, y\\) with \\(f(x) \\ge \\alpha\\) and \\(f(y) \\ge \\alpha\\). For \\(\\theta \\in [0,1]\\):\\n\\n$$f(\\theta x + (1-\\theta)y) \\ge \\theta f(x) + (1-\\theta)f(y) \\ge \\theta\\alpha + (1-\\theta)\\alpha = \\alpha$$\\n\\nby concavity. ✓\\n\\n### Step 5 (Apply to \\(\\phi\\))\\n\\nSince \\(\\phi\\) is concave, the superlevel set\\n\\n$$\\{x \\in \\mathbb{R}^n_{++} \\mid \\phi(x) \\ge 0\\}$$\\n\\nis convex. ∎\\n\\n## Alternative Solution (Weighted AM-GM)\\n\\nFor \\(x, y \\in \\mathbb{R}^n_{++}\\) with \\(\\prod_i x_i \\ge 1\\) and \\(\\prod_i y_i \\ge 1\\), and \\(\\theta \\in [0,1]\\), we use concavity of \\(\\log\\) directly:\\n\\n$$\\sum_i \\log(\\theta x_i + (1-\\theta)y_i) \\ge \\sum_i [\\theta \\log x_i + (1-\\theta) \\log y_i] = \\theta \\sum_i \\log x_i + (1-\\theta) \\sum_i \\log y_i \\ge \\theta \\cdot 0 + (1-\\theta) \\cdot 0 = 0$$\\n\\nTherefore:\\n\\n$$\\log\\left(\\prod_i(\\theta x_i + (1-\\theta)y_i)\\right) = \\sum_i \\log(\\theta x_i + (1-\\theta)y_i) \\ge 0$$\\n\\nHence:\\n\\n$$\\prod_i(\\theta x_i + (1-\\theta)y_i) \\ge 1 ✓$$\\n\\n## Geometric Interpretation (2D Case)\\n\\nFor \\(n = 2\\), the set \\(\\{(x_1, x_2) \\in \\mathbb{R}^2_+ \\mid x_1 x_2 \\ge 1\\}\\) is the region \"above\" the hyperbola \\(x_1 x_2 = 1\\) in the first quadrant.\\n\\n**Visualization**:\\n- The boundary \\(x_1 x_2 = 1\\) is a hyperbola with asymptotes along the axes\\n- The set includes all points with larger product, i.e., farther from the origin along rays from the origin\\n- Convexity means: any line segment between two points in this region stays in the region\\n\\n## Connection to Geometric Mean\\n\\nThe condition \\(\\prod_i x_i \\ge 1\\) can be rewritten as:\\n\\n$$(\\prod_{i=1}^n x_i)^{1/n} \\ge 1$$\\n\\nThe function \\(x \\mapsto (\\prod_i x_i)^{1/n}\\) is the **geometric mean**. This is a concave function on \\(\\mathbb{R}^n_{++}\\).\\n\\n## Key Insights\\n\\n1. **Logarithm transforms products to sums**: \\(\\log(\\prod x_i) = \\sum \\log x_i\\)\\n2. **Concavity of log**: The function \\(\\log\\) is concave, which makes \\(\\phi = \\sum \\log x_i\\) concave\\n3. **Superlevel sets**: Superlevel sets \\(\\{f \\ge \\alpha\\}\\) of concave \\(f\\) are convex (dual to sublevel sets of convex \\(f\\))\\n4. **Hyperbolic constraint**: The constraint \\(x_1 x_2 \\ge c\\) for \\(c > 0\\) defines a \"rotated second-order cone\"\\n5. **Generalized mean inequalities**: The geometric mean is concave; this connects to AM-GM inequality\\n6. **Standard form**: In conic optimization, this is part of the **power cone** or **exponential cone**\\n7. **Applications**: Appears in geometric programming, portfolio optimization, and resource allocation",
      "learningObjectives": [
        "concave-functions",
        "superlevel-sets",
        "logarithm-properties",
        "hyperbolic-sets",
        "geometric-mean",
        "conic-constraints",
        "am-gm-inequality"
      ],
      "estimatedTime": 20,
      "relatedProblems": ["02-003", "03-010", "04-015"],
      "notes": "This problem introduces hyperbolic constraints, which appear frequently in geometric programming and modern conic optimization. The key insight is that the logarithm transforms the nonlinear product constraint into a linear constraint in log-space, where convexity is obvious. This 'change of coordinates' technique is powerful—many seemingly nonconvex sets become convex after a suitable transformation. The set {x | x₁x₂ ≥ 1, x ≥ 0} is a rotated second-order cone, a standard primitive in conic optimization solvers."
    },
    {
      "id": "02-014",
      "lecture": "02-convex-sets",
      "type": "computation",
      "difficulty": "medium",
      "title": "BV 2.12 — Which sets are convex?",
      "statement": "For each set below, determine whether it is convex. If convex, provide a brief justification. If not convex, give a counterexample.\\n\\nAll distances and norms are **Euclidean** unless stated otherwise.\\n\\n(a) **Slab**: \\(S = \\{x \\in \\mathbb{R}^n \\mid \\alpha \\le a^T x \\le \\beta\\}\\)\\n\\n(b) **Hyperrectangle (box)**: \\(R = \\{x \\in \\mathbb{R}^n \\mid \\alpha_i \\le x_i \\le \\beta_i, i=1,\\dots,n\\}\\)\\n\\n(c) **Wedge**: \\(W = \\{x \\in \\mathbb{R}^n \\mid a_1^T x \\le b_1, a_2^T x \\le b_2\\}\\)\\n\\n(d) **At least as close to \\(a\\) as to \\(S\\)**: \\(D = \\{x \\in \\mathbb{R}^n \\mid \\|x-a\\|_2 \\le \\text{dist}_2(x, S)\\}\\) where \\(\\text{dist}_2(x, S) = \\inf_{y \\in S} \\|x-y\\|_2\\)\\n\\n(e) **Closer to \\(S\\) than to \\(T\\)**: \\(E = \\{x \\in \\mathbb{R}^n \\mid \\text{dist}_2(x, S) \\le \\text{dist}_2(x, T)\\}\\)\\n\\n(f) **Translates of \\(S_2\\) lie in \\(S_1\\)**: \\(F = \\{x \\in \\mathbb{R}^n \\mid x + S_2 \\subseteq S_1\\}\\) where \\(S_1\\) is convex\\n\\n(g) **Closer to \\(a\\) than to \\(b\\) (up to a factor)**: \\(G = \\{x \\in \\mathbb{R}^n \\mid \\|x-a\\|_2 \\le \\theta\\|x-b\\|_2\\}\\) with \\(a \\ne b\\) and \\(0 \\le \\theta \\le 1\\)",
      "hints": [
        "Part (a): Write as intersection of two halfspaces",
        "Part (b): Write as product of intervals, or intersection of halfspaces",
        "Part (c): Intersection of two halfspaces",
        "Part (d): For each fixed \\(y \\in S\\), \\(\\{x \\mid \\|x-a\\| \\le \\|x-y\\|\\}\\) is a halfspace; intersect over all \\(y\\)",
        "Part (e): Look for a simple counterexample with \\(S\\) and \\(T\\) being two points",
        "Part (f): Show \\(F = \\bigcap_{y \\in S_2} (S_1 - y)\\), an intersection of translates of \\(S_1\\)",
        "Part (g): Square both sides and complete the square to get a ball (\\(\\theta < 1\\)) or halfspace (\\(\\theta = 1\\))"
      ],
      "solution": "## (a) Slab — CONVEX ✓\\n\\n### Justification\\n\\n$$S = \\{x \\mid \\alpha \\le a^T x \\le \\beta\\} = \\{x \\mid a^T x \\ge \\alpha\\} \\cap \\{x \\mid a^T x \\le \\beta\\}$$\\n\\nThis is the intersection of two halfspaces, hence convex. ∎\\n\\n## (b) Hyperrectangle (Box) — CONVEX ✓\\n\\n### Justification (Method 1: Product of intervals)\\n\\n$$R = [\\alpha_1, \\beta_1] \\times \\dots \\times [\\alpha_n, \\beta_n]$$\\n\\nEach interval is convex. The Cartesian product of convex sets is convex. ∎\\n\\n## (c) Wedge — CONVEX ✓\\n\\n### Justification\\n\\n$$W = \\{x \\mid a_1^T x \\le b_1\\} \\cap \\{x \\mid a_2^T x \\le b_2\\}$$\\n\\nIntersection of two halfspaces, hence convex. ∎\\n\\n## (d) At least as close to \\(a\\) as to \\(S\\) — CONVEX ✓\\n\\n### Justification\\n\\n\\(\\|x - a\\|_2 \\le \\text{dist}_2(x, S) \\iff \\|x - a\\|_2 \\le \\|x - y\\|_2\\) for all \\(y \\in S\\).\\n\\nFor fixed \\(y\\), the set \\(\\{x \\mid \\|x-a\\|_2 \\le \\|x-y\\|_2\\}\\) is a halfspace. Squaring both sides gives \\(2(y-a)^T x \\le \\|y\\|^2 - \\|a\\|^2\\). \\(D\\) is the intersection of these halfspaces over all \\(y \\in S\\), hence convex. ∎\\n\\n## (e) Closer to \\(S\\) than to \\(T\\) — NOT CONVEX ✗\\n\\n### Counterexample\\n\\nLet \\(S = \\{-1, 1\\}\\), \\(T = \\{0\\}\\) in \\(\\mathbb{R}\\). Then \\(\\text{dist}(x, S) = \\min(\\|x+1\\|, \\|x-1\\|)\\) and \\(\\text{dist}(x, T) = \\|x\\|\\). The set \\(E\\) is \\(\\{x \\mid \\min(\\|x+1\\|, \\|x-1\\|) \\le \\|x\\|\\}\\). This set is not convex. For instance, \\(x = -2\\) and \\(x=2\\) are in \\(E\\), but their midpoint \\(x=0\\) is not. ∎\\n\\n## (f) Translates of \\(S_2\\) lie in \\(S_1\\) — CONVEX ✓ (when \\(S_1\\) is convex)\\n\\n### Justification\\n\\(x + S_2 \\subseteq S_1 \\iff \\forall y \\in S_2, x + y \\in S_1 \\iff \\forall y \\in S_2, x \\in S_1 - y\\). Therefore, \\(F = \\bigcap_{y \\in S_2} (S_1 - y)\\). Each set \\(S_1 - y\\) is a translate of a convex set, hence convex. The intersection of convex sets is convex. ∎\\n\\n## (g) Closer to \\(a\\) than to \\(b\\) (up to a factor \\(\\theta\\)) — CONVEX ✓\\n\\n### Derivation\\n\\nSquaring \\(\\|x - a\\|_2 \\le \\theta\\|x - b\\|_2\\) gives \\((1 - \\theta^2)\\|x\\|^2 - 2(a - \\theta^2 b)^T x + (\\|a\\|^2 - \\theta^2 \\|b\\|^2) \\le 0\\). If \\(\\theta < 1\\), this defines a closed ball (ellipsoid), which is convex. If \\(\\theta = 1\\), it defines a halfspace, which is also convex. ∎"
    },
    {
      "id": "02-015",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "medium",
      "title": "BV 2.16 — Summing dependent coordinates preserves convexity",
      "statement": "Let \\(S_1, S_2 \\subset \\mathbb{R}^{m+n}\\) be convex. Define\\n\\n$$S = \\{(x, y_1 + y_2) \\mid (x, y_1) \\in S_1, (x, y_2) \\in S_2\\} \\subset \\mathbb{R}^{m+n}$$\\n\\nShow that \\(S\\) is convex.",
      "hints": [
        "Take two arbitrary points \\((x^{(1)}, z^{(1)}), (x^{(2)}, z^{(2)}) \\in S\\)",
        "By definition, there exist \\(y_1^{(1)}, y_2^{(1)}, y_1^{(2)}, y_2^{(2)}\\) such that \\((x^{(i)}, y_1^{(i)}) \\in S_1, (x^{(i)}, y_2^{(i)}) \\in S_2\\), and \\(z^{(i)} = y_1^{(i)} + y_2^{(i)}\\)",
        "Form convex combination: \\((\\bar{x}, \\bar{z}) = \\theta(x^{(1)}, z^{(1)}) + (1-\\theta)(x^{(2)}, z^{(2)})\\)",
        "Define \\(\\bar{y}_1 = \\theta y_1^{(1)} + (1-\\theta)y_1^{(2)}\\) and \\(\\bar{y}_2 = \\theta y_2^{(1)} + (1-\\theta)y_2^{(2)}\\)",
        "Show \\((\\bar{x}, \\bar{y}_1) \\in S_1\\) and \\((\\bar{x}, \\bar{y}_2) \\in S_2\\) using convexity of \\(S_1\\) and \\(S_2\\)",
        "Verify that \\(\\bar{z} = \\bar{y}_1 + \\bar{y}_2\\)",
        "Alternative: View \\(S\\) as a linear image of \\(S_1 \\times S_2\\) restricted to \\(x\\)-coordinates matching"
      ],
      "solution": "## Solution (Explicit Witnesses)\\n\\nTake any two points \\((x^{(1)}, z^{(1)}), (x^{(2)}, z^{(2)}) \\in S\\). By definition of \\(S\\), for each \\(i \\in \\{1, 2\\}\\) there exist \\(y_1^{(i)}, y_2^{(i)}\\) such that:\\n\\n$$(x^{(i)}, y_1^{(i)}) \\in S_1, \\quad (x^{(i)}, y_2^{(i)}) \\in S_2, \\quad z^{(i)} = y_1^{(i)} + y_2^{(i)}$$\\n\\n### Step 1 (Form convex combination)\\n\\nFix \\(\\theta \\in [0,1]\\). Consider:\\n\\n$$(\\bar{x}, \\bar{z}) := \\theta(x^{(1)}, z^{(1)}) + (1-\\theta)(x^{(2)}, z^{(2)}) = (\\theta x^{(1)} + (1-\\theta)x^{(2)}, \\theta z^{(1)} + (1-\\theta)z^{(2)})$$\\n\\n### Step 2 (Define witness points)\\n\\nLet:\\n\\n$$\\bar{y}_1 := \\theta y_1^{(1)} + (1-\\theta)y_1^{(2)}$$\\n$$\\bar{y}_2 := \\theta y_2^{(1)} + (1-\\theta)y_2^{(2)}$$\\n\\n### Step 3 (Apply convexity of \\(S_1\\) and \\(S_2\\))\\n\\nSince \\(S_1\\) is convex and \\((x^{(1)}, y_1^{(1)}), (x^{(2)}, y_1^{(2)}) \\in S_1\\):\\n\\n$$(\\bar{x}, \\bar{y}_1) = \\theta(x^{(1)}, y_1^{(1)}) + (1-\\theta)(x^{(2)}, y_1^{(2)}) \\in S_1$$\\n\\nSimilarly, since \\(S_2\\) is convex and \\((x^{(1)}, y_2^{(1)}), (x^{(2)}, y_2^{(2)}) \\in S_2\\):\\n\\n$$(\\bar{x}, \\bar{y}_2) = \\theta(x^{(1)}, y_2^{(1)}) + (1-\\theta)(x^{(2)}, y_2^{(2)}) \\in S_2$$\\n\\n### Step 4 (Verify the sum)\\n\\nCompute:\\n\\n$$\\bar{y}_1 + \\bar{y}_2 = \\theta y_1^{(1)} + (1-\\theta)y_1^{(2)} + \\theta y_2^{(1)} + (1-\\theta)y_2^{(2)} = \\theta(y_1^{(1)} + y_2^{(1)}) + (1-\\theta)(y_1^{(2)} + y_2^{(2)}) = \\theta z^{(1)} + (1-\\theta)z^{(2)} = \\bar{z}$$\\n\\n### Step 5 (Conclusion)\\n\\nWe have \\((\\bar{x}, \\bar{y}_1) \\in S_1, (\\bar{x}, \\bar{y}_2) \\in S_2\\), and \\(\\bar{z} = \\bar{y}_1 + \\bar{y}_2\\). Therefore:\\n\\n$$(\\bar{x}, \\bar{z}) \\in S$$\\n\\nSince the choices were arbitrary, \\(S\\) is convex. ∎\\n\\n## Alternative Solution (Linear Image)\\n\\nLet:\\n\\n$$C := \\{(x, y_1, x', y_2) \\mid (x, y_1) \\in S_1, (x', y_2) \\in S_2, x = x'\\}$$\\n\\n### Step 1 (\\(C\\) is convex)\\n\\n\\(C\\) is the intersection of the Cartesian product \\(S_1 \\times S_2\\) with the affine subspace \\(\\{x = x'\\}\\).\\n\\n- \\(S_1 \\times S_2\\) is convex (product of convex sets, Problem P7)\\n- \\(\\{x = x'\\}\\) is an affine subspace, hence convex\\n- Intersections of convex sets are convex\\n\\nTherefore \\(C\\) is convex.\\n\\n### Step 2 (\\(S\\) is a linear image of \\(C\\))\\n\\nDefine the linear map \\(T: \\mathbb{R}^{m+n+m+n} \\to \\mathbb{R}^{m+n}\\) by:\\n\\n$$T(x, y_1, x', y_2) = (x, y_1 + y_2)$$\\n\\nThen \\(S = T(C)\\).\\n\\n### Step 3 (Linear images preserve convexity)\\n\\nLinear maps preserve convexity. ∎\\n\\n## Geometric Interpretation\\n\\nThe set \\(S\\) consists of points \\((x, z)\\) where \\(x\\) is a \"common coordinate\" that must appear in both \\(S_1\\) and \\(S_2\\), and \\(z\\) is the **sum** of the \"dependent coordinates\" \\(y_1\\) from \\(S_1\\) and \\(y_2\\) from \\(S_2\\), both paired with the same \\(x\\). This operation is called **partial addition** or **fiber-wise summation** in convex analysis.\\n\\n## Example\\n\\nLet \\(m = 1, n = 1\\). Consider \\(S_1 = \\{(x, y_1) \\mid y_1 \\le x^2\\}\\) and \\(S_2 = \\{(x, y_2) \\mid y_2 \\le 2x^2\\}\\). Then \\(S = \\{(x, z) \\mid z \\le 3x^2\\}\\), which is the epigraph of \\(3x^2\\) and is convex. ∎"
    }
  ]
}
