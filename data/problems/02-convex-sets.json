{
  "lecture": "02-convex-sets",
  "problems": [
    {
      "id": "02-001",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "easy",
      "title": "P1 — Epigraph of a norm is convex",
      "statement": "Let S = {(x,t) ∈ ℝⁿ×ℝ | |x| ≤ t}. Show that S ⊂ ℝⁿ⁺¹ is convex.\n\nRecall: A set C is convex if for all u,v ∈ C and θ ∈ [0,1], the convex combination θu + (1-θ)v ∈ C.",
      "hints": [
        "Start by taking two arbitrary points (x₁,t₁), (x₂,t₂) ∈ S and a coefficient θ ∈ [0,1]",
        "Use the definition of S: this means |x₁| ≤ t₁ and |x₂| ≤ t₂",
        "For the convex combination (x̄,t̄) = θ(x₁,t₁) + (1-θ)(x₂,t₂), you need to show |x̄| ≤ t̄",
        "Apply the triangle inequality: |θx₁ + (1-θ)x₂| ≤ |θx₁| + |(1-θ)x₂|",
        "Use positive homogeneity of norms: |θx₁| = θ|x₁| for θ ≥ 0",
        "Combine with the feasibility conditions |x₁| ≤ t₁ and |x₂| ≤ t₂"
      ],
      "solution": "## Solution (Direct, Step by Step)\n\nLet (x₁,t₁), (x₂,t₂) ∈ S. By definition of S:\n\n|x₁| ≤ t₁,    |x₂| ≤ t₂\n\nFix any θ ∈ [0,1]. Consider the convex combination:\n\n(x̄,t̄) = θ(x₁,t₁) + (1-θ)(x₂,t₂) = (θx₁ + (1-θ)x₂, θt₁ + (1-θ)t₂)\n\nWe must show (x̄,t̄) ∈ S, i.e., |x̄| ≤ t̄.\n\n### Step 1 (Triangle inequality)\n\n|θx₁ + (1-θ)x₂| ≤ |θx₁| + |(1-θ)x₂|\n\n### Step 2 (Positive homogeneity of norms)\n\n|θx₁| = θ|x₁|,    |(1-θ)x₂| = (1-θ)|x₂|\n\n### Step 3 (Combine with feasibility of the endpoints)\n\n|x̄| ≤ θ|x₁| + (1-θ)|x₂| ≤ θt₁ + (1-θ)t₂ = t̄\n\nThus (x̄,t̄) ∈ S. Since (x₁,t₁), (x₂,t₂) and θ were arbitrary, S is convex. ∎\n\n## Alternative Solution (Epigraph viewpoint)\n\nThe map f(x) = |x| is convex. The **epigraph** of f,\n\nepi f = {(x,t) | f(x) ≤ t}\n\nis always convex for any convex function f. Here epi f = S, therefore S is convex. ∎\n\n## Key Insights\n\n1. **Geometric interpretation**: The set S is the region \"above\" the norm cone in ℝⁿ⁺¹\n2. **Epigraph property**: Any convex function has a convex epigraph—this is a fundamental connection between convex functions and convex sets\n3. **Norm properties used**: Triangle inequality and positive homogeneity (both defining properties of norms)",
      "learningObjectives": [
        "convexity-definition",
        "triangle-inequality",
        "norm-properties",
        "epigraph-concept",
        "convex-combinations",
        "proof-technique"
      ],
      "estimatedTime": 15,
      "relatedProblems": ["02-002", "02-003", "02-005"],
      "notes": "This is a fundamental problem that connects convex sets and convex functions through the epigraph. The direct proof reinforces basic properties of norms and convex combinations. The alternative solution introduces the powerful epigraph viewpoint that will be used extensively in convex analysis."
    },
    {
      "id": "02-002",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "easy",
      "title": "P2 — Convex hull is convex",
      "statement": "Let x₁, ..., xₖ ∈ ℝⁿ. Define\n\nC = {∑ᵢ₌₁ᵏ θᵢxᵢ | θᵢ ≥ 0, ∑ᵢ₌₁ᵏ θᵢ = 1}\n\nShow that C is convex.",
      "hints": [
        "Pick two arbitrary points u, v ∈ C and express them using the definition",
        "For u ∈ C, write u = ∑ᵢ₌₁ᵏ θᵢxᵢ with θᵢ ≥ 0 and ∑θᵢ = 1",
        "For v ∈ C, write v = ∑ᵢ₌₁ᵏ φᵢxᵢ with φᵢ ≥ 0 and ∑φᵢ = 1",
        "Form the convex combination λu + (1-λ)v for λ ∈ [0,1]",
        "Expand: λu + (1-λ)v = ∑ᵢ₌₁ᵏ [λθᵢ + (1-λ)φᵢ]xᵢ",
        "Verify that the new coefficients λθᵢ + (1-λ)φᵢ are nonnegative and sum to 1"
      ],
      "solution": "## Solution (Direct Coefficient Bookkeeping)\n\nPick two points of C:\n\nu = ∑ᵢ₌₁ᵏ θᵢxᵢ,    v = ∑ᵢ₌₁ᵏ φᵢxᵢ\n\nwith θᵢ, φᵢ ≥ 0 and ∑ᵢ θᵢ = ∑ᵢ φᵢ = 1.\n\nFix λ ∈ [0,1]. Then:\n\nλu + (1-λ)v = ∑ᵢ₌₁ᵏ [λθᵢ + (1-λ)φᵢ]xᵢ\n\n### Nonnegativity\n\nEach coefficient λθᵢ + (1-λ)φᵢ ≥ 0 because:\n- λ ≥ 0, θᵢ ≥ 0 ⟹ λθᵢ ≥ 0\n- (1-λ) ≥ 0, φᵢ ≥ 0 ⟹ (1-λ)φᵢ ≥ 0\n\n### Sum-to-one\n\n∑ᵢ₌₁ᵏ [λθᵢ + (1-λ)φᵢ] = λ∑ᵢ θᵢ + (1-λ)∑ᵢ φᵢ = λ·1 + (1-λ)·1 = 1\n\nHence λu + (1-λ)v ∈ C. Therefore C is convex. ∎\n\n## Alternative Solution (Induction on k)\n\n**Base case (k=2)**: This is exactly the definition of convexity—the convex hull of two points is the line segment between them, which is clearly convex.\n\n**Inductive step**: Assume the claim holds for k-1 points. For k points, write any point in C as:\n\n∑ᵢ₌₁ᵏ θᵢxᵢ = α(∑ᵢ₌₁ᵏ⁻¹ (θᵢ/α)xᵢ) + (1-α)xₖ\n\nwhere α = ∑ᵢ₌₁ᵏ⁻¹ θᵢ ∈ [0,1].\n\n**Case 1**: If α = 0, then the point is just xₖ ∈ C.\n\n**Case 2**: If α > 0, note that θᵢ/α ≥ 0 and ∑ᵢ₌₁ᵏ⁻¹ (θᵢ/α) = 1. By the induction hypothesis, the parenthesis ∑ᵢ₌₁ᵏ⁻¹ (θᵢ/α)xᵢ lies in the convex hull of {x₁,...,xₖ₋₁}.\n\nTaking a convex combination (with weights α and 1-α) with xₖ keeps us in the convex hull of all k points.\n\nTaking any convex combination of two points in C (each expressible in this form) keeps us in C by the base case. ∎\n\n## Key Insights\n\n1. **Convex hull definition**: C is precisely the convex hull of {x₁,...,xₖ}, denoted conv({x₁,...,xₖ})\n2. **Coefficient preservation**: Convex combinations preserve the affine combination structure (nonnegativity and sum-to-one)\n3. **Inductive reasoning**: The convex hull can be built incrementally by adding one point at a time\n4. **Smallest convex set**: The convex hull is the smallest convex set containing the given points",
      "learningObjectives": [
        "convex-hull",
        "convexity-definition",
        "affine-combinations",
        "proof-by-induction",
        "coefficient-bookkeeping",
        "convex-combinations"
      ],
      "estimatedTime": 20,
      "relatedProblems": ["02-001", "02-008", "02-012"],
      "notes": "This problem establishes that the convex hull—one of the most important constructions in convex geometry—is itself convex. The direct proof emphasizes careful coefficient tracking, while the inductive proof provides geometric intuition about building convex hulls incrementally. This result is foundational for understanding that 'convex hull' is the smallest convex set containing a given set of points."
    },
    {
      "id": "02-003",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "medium",
      "title": "P3 — Inner product induced by A ≻ 0; Cauchy–Schwarz; ellipsoid is convex",
      "statement": "Let A ∈ Sⁿ₊₊ (symmetric positive definite). Define ⟨x,y⟩_A := xᵀAy for x,y ∈ ℝⁿ.\n\n1. Show ⟨·,·⟩_A is an inner product.\n\n2. Prove the Cauchy–Schwarz inequality:\n   |⟨x,y⟩_A|² ≤ ⟨x,x⟩_A · ⟨y,y⟩_A    (∀x,y)\n\n3. Prove the set E = {x ∈ ℝⁿ | xᵀAx ≤ 1} is convex.",
      "hints": [
        "Part 1: Verify the three inner product axioms—bilinearity, symmetry, and positive definiteness",
        "For bilinearity: Use (αx₁ + βx₂)ᵀAy = αx₁ᵀAy + βx₂ᵀAy",
        "For symmetry: Use that A = Aᵀ, so xᵀAy = yᵀAx",
        "For positive definiteness: Use that A ≻ 0 means xᵀAx > 0 for all x ≠ 0",
        "Part 2, Method A: Consider the quadratic 0 ≤ ⟨x-αy, x-αy⟩_A for any α ∈ ℝ and examine its discriminant",
        "Part 2, Method B: Use A^(1/2) and reduce to the standard Euclidean Cauchy–Schwarz",
        "Part 3: Define the A-norm |x|_A = √(xᵀAx) and show it's a norm, then use that norm balls are convex"
      ],
      "solution": "## Part 1 — Inner Product Axioms\n\nWe verify the three axioms:\n\n### (i) Bilinearity\n\nFor any scalars α, β and vectors x₁, x₂, y:\n\n⟨αx₁ + βx₂, y⟩_A = (αx₁ + βx₂)ᵀAy = αx₁ᵀAy + βx₂ᵀAy = α⟨x₁,y⟩_A + β⟨x₂,y⟩_A\n\nSimilarly, linearity holds in the second slot:\n\n⟨x, αy₁ + βy₂⟩_A = α⟨x,y₁⟩_A + β⟨x,y₂⟩_A\n\n### (ii) Symmetry\n\nSince A = Aᵀ:\n\n⟨x,y⟩_A = xᵀAy = (xᵀAy)ᵀ = yᵀAᵀx = yᵀAx = ⟨y,x⟩_A\n\n### (iii) Positive Definiteness\n\nFor x ≠ 0:\n\n⟨x,x⟩_A = xᵀAx > 0\n\nbecause A ≻ 0 (positive definite). Also, ⟨0,0⟩_A = 0ᵀA0 = 0.\n\nThus ⟨·,·⟩_A is an inner product. ∎\n\n## Part 2 — Cauchy–Schwarz Inequality\n\n### Method A (One-variable quadratic)\n\nFor any α ∈ ℝ:\n\n0 ≤ ⟨x - αy, x - αy⟩_A = ⟨x,x⟩_A - 2α⟨x,y⟩_A + α²⟨y,y⟩_A\n\nThe right side is a quadratic in α that is nonnegative for all α. Therefore, its discriminant must be nonpositive:\n\n(2⟨x,y⟩_A)² - 4⟨x,x⟩_A⟨y,y⟩_A ≤ 0\n\nThis gives:\n\n|⟨x,y⟩_A|² ≤ ⟨x,x⟩_A · ⟨y,y⟩_A ∎\n\n### Method B (Reduce to Euclidean Cauchy–Schwarz)\n\nLet B = A^(1/2) ≻ 0 (the unique positive definite square root of A). Then:\n\n⟨x,y⟩_A = xᵀAy = xᵀ(B²)y = (Bx)ᵀ(By) = ⟨Bx, By⟩₂\n\nwhere ⟨·,·⟩₂ is the standard Euclidean inner product. By the standard Cauchy–Schwarz inequality in ℝⁿ:\n\n|⟨x,y⟩_A| = |⟨Bx, By⟩₂| ≤ |Bx|₂ · |By|₂\n\nNow:\n\n|Bx|₂ = √((Bx)ᵀ(Bx)) = √(xᵀBᵀBx) = √(xᵀAx) = √⟨x,x⟩_A\n\nSimilarly, |By|₂ = √⟨y,y⟩_A. Therefore:\n\n|⟨x,y⟩_A| ≤ √⟨x,x⟩_A · √⟨y,y⟩_A\n\nSquaring both sides gives the result. ∎\n\n## Part 3 — Ellipsoid is Convex\n\nDefine the **A-norm**:\n\n|x|_A := √(xᵀAx) = √⟨x,x⟩_A\n\nSince ⟨·,·⟩_A is an inner product, |·|_A is a norm. The set\n\nE = {x | |x|_A ≤ 1}\n\nis the closed unit ball of the norm |·|_A, which is convex.\n\n### Direct verification\n\nFor x, y ∈ E and θ ∈ [0,1], we use the **triangle inequality** for the norm |·|_A:\n\n|θx + (1-θ)y|_A ≤ |θx|_A + |(1-θ)y|_A\n\nBy **positive homogeneity** of norms:\n\n|θx|_A = θ|x|_A,    |(1-θ)y|_A = (1-θ)|y|_A\n\nSince x, y ∈ E, we have |x|_A ≤ 1 and |y|_A ≤ 1. Therefore:\n\n|θx + (1-θ)y|_A ≤ θ|x|_A + (1-θ)|y|_A ≤ θ·1 + (1-θ)·1 = 1\n\nHence θx + (1-θ)y ∈ E. Therefore E is convex. ∎\n\n## Key Insights\n\n1. **Induced inner products**: Any positive definite matrix A induces an inner product on ℝⁿ\n2. **Generalized norms**: The induced inner product gives rise to a norm |·|_A, called the A-norm or energy norm\n3. **Ellipsoids are norm balls**: The set E is an ellipsoid (when A ≻ 0), and it's the unit ball of |·|_A\n4. **Cauchy–Schwarz is universal**: The Cauchy–Schwarz inequality holds for any inner product space\n5. **Connection to geometry**: The quadratic form xᵀAx ≤ 1 defines an ellipsoid centered at the origin with shape determined by the eigenvectors of A and radii determined by A's eigenvalues\n6. **Transformation perspective**: Under the change of variables z = A^(1/2)x, the ellipsoid E becomes the standard Euclidean unit ball in the z-coordinates",
      "learningObjectives": [
        "inner-product-axioms",
        "positive-definite-matrices",
        "cauchy-schwarz-inequality",
        "norms-from-inner-products",
        "ellipsoid-convexity",
        "matrix-square-root",
        "quadratic-forms",
        "triangle-inequality"
      ],
      "estimatedTime": 35,
      "relatedProblems": ["02-001", "02-013", "02-010"],
      "notes": "This is a rich problem that connects linear algebra, inner product spaces, and convex geometry. Part 1 reinforces the abstract definition of inner products. Part 2 shows that Cauchy–Schwarz is a property of ALL inner products, not just the Euclidean one. Part 3 demonstrates that ellipsoids are convex by showing they are unit balls of induced norms. The two methods for proving Cauchy–Schwarz (discriminant vs. reduction to Euclidean case) illustrate different proof strategies—one purely algebraic, one geometric via transformation."
    },
    {
      "id": "02-004",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "medium",
      "title": "P4 — Minkowski sum of convex sets; nonconvex summands may still sum to convex",
      "statement": "For X, Y ⊂ ℝⁿ, define the **Minkowski sum**\n\nX + Y = {x + y | x ∈ X, y ∈ Y}\n\n(a) If X and Y are convex, prove X + Y is convex.\n\n(b) If X and Y are not convex, can X + Y be convex?",
      "hints": [
        "Part (a): Take arbitrary z₁, z₂ ∈ X + Y; write them as z₁ = x₁ + y₁ and z₂ = x₂ + y₂",
        "Form the convex combination θz₁ + (1-θ)z₂ and group the x and y terms separately",
        "Use that θx₁ + (1-θ)x₂ ∈ X and θy₁ + (1-θ)y₂ ∈ Y by convexity",
        "Part (b): Try constructing explicit nonconvex sets in ℝ whose Minkowski sum is convex",
        "Consider disconnected unions of intervals: X = [0,1] ∪ [3,4], Y = [0,3] ∪ [4,7]",
        "Compute all pairwise sums of intervals and check if the union is convex"
      ],
      "solution": "## Part (a) — Convexity of Minkowski Sum\n\nTake any z₁, z₂ ∈ X + Y. By definition, there exist x₁, x₂ ∈ X and y₁, y₂ ∈ Y such that:\n\nz₁ = x₁ + y₁,    z₂ = x₂ + y₂\n\nLet θ ∈ [0,1]. Consider the convex combination:\n\nθz₁ + (1-θ)z₂ = θ(x₁ + y₁) + (1-θ)(x₂ + y₂)\n              = [θx₁ + (1-θ)x₂] + [θy₁ + (1-θ)y₂]\n\n### Apply convexity of X and Y\n\nSince X is convex:\nθx₁ + (1-θ)x₂ ∈ X\n\nSince Y is convex:\nθy₁ + (1-θ)y₂ ∈ Y\n\n### Conclusion\n\nTherefore:\nθz₁ + (1-θ)z₂ = [θx₁ + (1-θ)x₂] + [θy₁ + (1-θ)y₂] ∈ X + Y\n\nSince z₁, z₂, and θ were arbitrary, X + Y is convex. ∎\n\n## Part (b) — Nonconvex Sets with Convex Sum\n\n**Answer**: Yes, two nonconvex sets can have a convex Minkowski sum.\n\n### Explicit example in ℝ\n\nLet:\nX = [0,1] ∪ [3,4]\nY = [0,3] ∪ [4,7]\n\nBoth X and Y are **not convex** (they are disconnected unions of intervals).\n\n### Computing X + Y\n\nWe compute the Minkowski sum by taking pairwise sums of intervals:\n\n**Interval 1**: [0,1] + [0,3] = [0,4]\n(All sums x + y where x ∈ [0,1], y ∈ [0,3] give [0+0, 1+3] = [0,4])\n\n**Interval 2**: [0,1] + [4,7] = [4,8]\n(All sums x + y where x ∈ [0,1], y ∈ [4,7] give [0+4, 1+7] = [4,8])\n\n**Interval 3**: [3,4] + [0,3] = [3,7]\n(All sums x + y where x ∈ [3,4], y ∈ [0,3] give [3+0, 4+3] = [3,7])\n\n**Interval 4**: [3,4] + [4,7] = [7,11]\n(All sums x + y where x ∈ [3,4], y ∈ [4,7] give [3+4, 4+7] = [7,11])\n\n### Union of all intervals\n\nX + Y = [0,4] ∪ [4,8] ∪ [3,7] ∪ [7,11]\n      = [0,11]\n\nThis is a **single connected interval**, which is convex!\n\nThus two nonconvex sets can have a convex Minkowski sum. ∎\n\n## Alternative Proof for (a) — Support Functions\n\nThe **support function** of a nonempty set S is:\n\nh_S(u) = sup_{x∈S} uᵀx\n\nA standard fact from convex analysis:\n\nh_{X+Y}(u) = h_X(u) + h_Y(u)\n\nfor all u ∈ ℝⁿ.\n\nIf X and Y are convex, then for each direction u, the supporting halfspace\n\n{z | uᵀz ≤ h_X(u) + h_Y(u)}\n\ncontains X + Y. Since a convex set is the intersection of all its supporting halfspaces:\n\nX + Y = ⋂_{u∈ℝⁿ} {z | uᵀz ≤ h_X(u) + h_Y(u)}\n\nThis is an intersection of halfspaces, hence convex. ∎\n\n## Key Insights\n\n1. **Minkowski sum preserves convexity**: The sum of two convex sets is always convex\n2. **Geometric interpretation**: X + Y is obtained by \"sliding\" X along every point in Y (or vice versa)\n3. **Counterintuitive result**: Nonconvex sets can sum to convex (the \"gaps\" can fill in)\n4. **Support function additivity**: h_{X+Y} = h_X + h_Y is a powerful dual characterization\n5. **Applications**: Minkowski sums appear in robot motion planning (configuration space obstacles), image processing (dilation), and optimization (constraint sets)\n6. **Connection to duality**: The support function viewpoint connects primal geometry (sets) to dual geometry (halfspaces)",
      "learningObjectives": [
        "minkowski-sum",
        "convexity-preservation",
        "support-functions",
        "counterexamples",
        "interval-arithmetic",
        "geometric-operations",
        "duality-concepts"
      ],
      "estimatedTime": 25,
      "relatedProblems": ["02-005", "02-006", "02-007"],
      "notes": "The Minkowski sum is a fundamental operation in convex geometry with applications across robotics, computer graphics, and optimization. Part (a) shows that convexity is preserved under this operation—a key closure property. Part (b) provides a surprising counterexample showing that nonconvex + nonconvex can yield convex, illustrating that convexity preservation is directional (sufficient but not necessary). The support function proof offers an elegant dual perspective that will be important in later duality theory."
    },
    {
      "id": "02-005",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "medium",
      "title": "P5 — Thickening a convex set is convex",
      "statement": "Let X ⊂ ℝⁿ be **convex and closed**, and let ε > 0. Define\n\nB_ε(X) = {y ∈ ℝⁿ | inf_{x∈X} |y - x| ≤ ε}\n\nProve that B_ε(X) is convex.\n\n(Here |·| denotes the Euclidean norm.)",
      "hints": [
        "Recognize that B_ε(X) = {y | dist(y, X) ≤ ε}, the ε-neighborhood of X",
        "Method 1: Use that closedness of X guarantees a nearest point exists for any y",
        "For y₁, y₂ ∈ B_ε(X), find witnesses x₁, x₂ ∈ X with |y₁ - x₁| ≤ ε and |y₂ - x₂| ≤ ε",
        "Form the convex combination x_θ = θx₁ + (1-θ)x₂ ∈ X and show it's close to θy₁ + (1-θ)y₂",
        "Method 2: Recognize B_ε(X) = X + εB where B = {u : |u| ≤ 1} is the unit ball",
        "Apply the result from P4 that Minkowski sums of convex sets are convex"
      ],
      "solution": "## Solution 1 (Direct, Using Nearest Points)\n\n### Why closedness matters\n\nBecause X is closed in finite dimensions, for every y the continuous function x ↦ |y - x| attains its minimum on X. Hence:\n\ninf_{x∈X} |y - x| ≤ ε  ⟺  ∃x ∈ X : |y - x| ≤ ε    (★)\n\n### Goal\n\nShow: if y₁, y₂ ∈ B_ε(X) and θ ∈ [0,1], then θy₁ + (1-θ)y₂ ∈ B_ε(X).\n\n### Step 1 (Pick witnesses in X)\n\nBy (★), choose x₁, x₂ ∈ X with:\n\n|y₁ - x₁| ≤ ε,    |y₂ - x₂| ≤ ε\n\n### Step 2 (Use convexity of X)\n\nSince X is convex, the convex combination:\n\nx_θ := θx₁ + (1-θ)x₂ ∈ X\n\n### Step 3 (Triangle inequality)\n\nCompute the distance from θy₁ + (1-θ)y₂ to x_θ:\n\n|θy₁ + (1-θ)y₂ - x_θ| = |θy₁ + (1-θ)y₂ - θx₁ - (1-θ)x₂|\n                       = |θ(y₁ - x₁) + (1-θ)(y₂ - x₂)|\n                       ≤ θ|y₁ - x₁| + (1-θ)|y₂ - x₂|  (triangle inequality)\n                       ≤ θ·ε + (1-θ)·ε\n                       = ε\n\nThus there exists x_θ ∈ X within distance ε of θy₁ + (1-θ)y₂, i.e.:\n\nθy₁ + (1-θ)y₂ ∈ B_ε(X)\n\nSince the choices were arbitrary, B_ε(X) is convex. ∎\n\n## Solution 2 (Minkowski Sum Viewpoint)\n\nLet B = {u : |u| ≤ 1} be the closed unit ball, which is convex. For closed X:\n\nB_ε(X) = {y : ∃x ∈ X, |y - x| ≤ ε}\n       = {y : ∃x ∈ X, ∃u ∈ B, y = x + εu}\n       = X + εB\n\nThis is the **Minkowski sum** of X and εB (scaled unit ball).\n\n### Apply P4\n\nBy Problem P4(a), the Minkowski sum of two convex sets is convex. Since both X and εB are convex, B_ε(X) = X + εB is convex. ∎\n\n## Remark on Closedness\n\n**Is closedness essential?** Not for convexity. Even if X were not closed, we have:\n\nB_ε(X) = X̄ + εB  (closure of X plus ball)\n\nSince the closure of a convex set is convex (see P6), and Minkowski sums preserve convexity, B_ε(X) is still convex.\n\nClosedness was only used to ensure that inf = min, i.e., nearest points exist. For convexity alone, it's not strictly necessary.\n\n## Key Insights\n\n1. **ε-neighborhood (thickening)**: B_ε(X) is the set of all points within distance ε of X\n2. **Closed convex = distance minimizer**: For closed convex X, every external point has a unique nearest point in X\n3. **Minkowski sum interpretation**: Thickening by ε is the same as Minkowski sum with εB\n4. **Convexity is stable under thickening**: This is crucial for approximation theory and robust optimization\n5. **Applications**: Appears in collision detection (safety margins), robust optimization (parameter uncertainty), and image processing (morphological dilation)\n6. **Triangle inequality is key**: The norm's triangle inequality and positive homogeneity are essential for the direct proof",
      "learningObjectives": [
        "epsilon-neighborhoods",
        "distance-functions",
        "minkowski-sum-applications",
        "convexity-preservation",
        "triangle-inequality",
        "closed-sets",
        "nearest-points"
      ],
      "estimatedTime": 20,
      "relatedProblems": ["02-004", "02-006", "08-003"],
      "notes": "This problem introduces the important operation of 'thickening' or 'fattening' a set, which has practical applications in safety-critical systems (e.g., collision avoidance with safety margins). The two proofs illustrate different approaches: direct computation using witnesses vs. recognizing the operation as a Minkowski sum and applying existing theory. The closedness assumption is interesting—it's needed for the 'nearest point exists' claim but not for convexity itself."
    },
    {
      "id": "02-006",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "easy",
      "title": "P6 — Closure of a convex set is convex",
      "statement": "Let C ⊂ ℝⁿ be convex. Prove that its closure C̄ is convex.",
      "hints": [
        "Use sequences to approximate points in the closure",
        "For u, v ∈ C̄, find sequences (u_k) ⊂ C and (v_k) ⊂ C with u_k → u and v_k → v",
        "Form the convex combinations w_k = θu_k + (1-θ)v_k for fixed θ ∈ [0,1]",
        "Show that w_k ∈ C by convexity of C",
        "Pass to the limit: w_k → θu + (1-θ)v, and conclude the limit is in C̄",
        "Alternative: Show C̄ is the intersection of all closed convex supersets of C"
      ],
      "solution": "## Solution 1 (Sequence/Limit Argument)\n\n### Goal\n\nFor any u, v ∈ C̄ and any θ ∈ [0,1], show θu + (1-θ)v ∈ C̄.\n\n### Step 1 (Approximate by points in C)\n\nBy definition of closure, there exist sequences (u_k) ⊂ C and (v_k) ⊂ C such that:\n\nu_k → u,    v_k → v\n\nas k → ∞.\n\n### Step 2 (Use convexity of C)\n\nFor each k, since C is convex:\n\nw_k := θu_k + (1-θ)v_k ∈ C\n\n### Step 3 (Pass to the limit)\n\nThe map (a, b) ↦ θa + (1-θ)b is continuous (it's affine). Therefore:\n\nw_k = θu_k + (1-θ)v_k → θu + (1-θ)v\n\nas k → ∞.\n\n### Step 4 (Closure contains limits)\n\nSince each w_k ∈ C and w_k → θu + (1-θ)v, the limit belongs to C̄.\n\nHence θu + (1-θ)v ∈ C̄. Since u, v, and θ were arbitrary, C̄ is convex. ∎\n\n## Solution 2 (Closed Convex Hull Perspective)\n\nLet:\n\nℱ = {K ⊂ ℝⁿ | K is closed and convex, and C ⊆ K}\n\nThen:\n\nC̄ = ⋂_{K∈ℱ} K\n\n### Why this equality holds\n\n**Right-to-left (⋂K ⊆ C̄)**: Any set K ∈ ℱ is closed and contains C, so it contains C̄. Therefore C̄ ⊇ ⋂K.\n\n**Left-to-right (C̄ ⊆ ⋂K)**: Note that C̄ itself is closed and contains C. If C̄ were convex (which we're proving), then C̄ ∈ ℱ, so C̄ ⊇ ⋂K.\n\nBut actually, we can argue directly: C̄ is the **smallest closed set** containing C. The intersection ⋂_{K∈ℱ} K is closed (arbitrary intersection of closed sets is closed), contains C, hence contains C̄. Conversely, C̄ is one such closed set if we ignore convexity momentarily.\n\n### Convexity from intersection\n\nSince each K ∈ ℱ is convex, and intersections of convex sets are convex:\n\n⋂_{K∈ℱ} K is convex\n\nTherefore C̄ is convex. ∎\n\n## Alternative Characterization\n\nThe closure C̄ can also be characterized as:\n\nC̄ = {x ∈ ℝⁿ | ∀ε > 0, B_ε(x) ∩ C ≠ ∅}\n\nwhere B_ε(x) = {y : |y - x| < ε} is the open ball.\n\nFor x₁, x₂ ∈ C̄ and θ ∈ [0,1], for any ε > 0, choose y₁ ∈ B_{ε/2}(x₁) ∩ C and y₂ ∈ B_{ε/2}(x₂) ∩ C. Then:\n\nθy₁ + (1-θ)y₂ ∈ C  (by convexity)\n\nand by triangle inequality:\n\n|(θy₁ + (1-θ)y₂) - (θx₁ + (1-θ)x₂)| ≤ θ|y₁ - x₁| + (1-θ)|y₂ - x₂| < ε\n\nSo B_ε(θx₁ + (1-θ)x₂) ∩ C ≠ ∅. Since ε was arbitrary, θx₁ + (1-θ)x₂ ∈ C̄. ∎\n\n## Key Insights\n\n1. **Closure preserves convexity**: Taking limits of convex combinations stays convex\n2. **Continuity of convex operations**: The map (x, y, θ) ↦ θx + (1-θ)y is continuous\n3. **Closed convex hull**: C̄ when C is convex is called the **closed convex hull** of C\n4. **Intersection characterization**: Closed convex sets can be characterized as intersections of closed halfspaces (supporting hyperplane theorem)\n5. **Finite dimensions vs. infinite**: This proof works in ℝⁿ; in infinite dimensions, closure may not preserve convexity without additional assumptions\n6. **Stability property**: Convexity is \"robust\" under closure—small perturbations don't destroy it",
      "learningObjectives": [
        "closure-operation",
        "convexity-preservation",
        "limit-arguments",
        "continuity-of-affine-maps",
        "intersection-of-convex-sets",
        "closed-convex-hull"
      ],
      "estimatedTime": 15,
      "relatedProblems": ["02-005", "02-007", "02-008"],
      "notes": "This is a fundamental topological property of convex sets. The sequence-based proof is constructive and uses the basic definition of closure, making it suitable for students first learning topology. The intersection characterization provides a more abstract view that connects to duality theory (supporting hyperplanes). The result is essential for existence theorems in optimization—many important convex sets are naturally defined as closures."
    },
    {
      "id": "02-007",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "medium",
      "title": "P7 — Cartesian products and convexity",
      "statement": "Let X ⊂ ℝᵐ and Y ⊂ ℝⁿ.\n\n(a) If X and Y are convex, prove that X × Y ⊂ ℝᵐ⁺ⁿ is convex.\n\n(b) If X and Y are **not** convex, can X × Y be convex?",
      "hints": [
        "Part (a): Take two arbitrary points (x₁, y₁), (x₂, y₂) ∈ X × Y",
        "Form the convex combination θ(x₁, y₁) + (1-θ)(x₂, y₂) and separate coordinates",
        "Show the x-component is in X and the y-component is in Y using their convexity",
        "Part (b): Assume X is not convex; find witnesses x₁, x₂ ∈ X with θx₁ + (1-θ)x₂ ∉ X",
        "Pick any y ∈ Y and show the line segment in X × Y fails convexity",
        "Conclude that if either factor is nonconvex (and nonempty), the product is nonconvex"
      ],
      "solution": "## Part (a) — Product of Convex Sets is Convex\n\nTake any two points (x₁, y₁), (x₂, y₂) ∈ X × Y and θ ∈ [0,1].\n\n### Separate coordinates\n\nSince (x₁, y₁) ∈ X × Y, we have x₁, x₂ ∈ X and y₁, y₂ ∈ Y.\n\n### Form convex combination\n\nθ(x₁, y₁) + (1-θ)(x₂, y₂) = (θx₁ + (1-θ)x₂, θy₁ + (1-θ)y₂)\n\n### Apply convexity of factors\n\nSince X is convex:\nx_θ := θx₁ + (1-θ)x₂ ∈ X\n\nSince Y is convex:\ny_θ := θy₁ + (1-θ)y₂ ∈ Y\n\n### Conclusion\n\nTherefore:\n(x_θ, y_θ) ∈ X × Y\n\nHence X × Y is convex. ∎\n\n## Part (b) — Nonconvex Factors Cannot Yield Convex Product\n\n**Answer**: No (assuming nonempty factors). If either X or Y is not convex, then X × Y is not convex.\n\n### Proof\n\nAssume **both sets are nonempty**. Suppose X is not convex. Then there exist x₁, x₂ ∈ X and θ ∈ (0,1) such that:\n\nx_θ := θx₁ + (1-θ)x₂ ∉ X\n\n### Construct failure in product\n\nPick any y ∈ Y (possible since Y ≠ ∅). Then:\n\n(x₁, y), (x₂, y) ∈ X × Y\n\nBut:\n\nθ(x₁, y) + (1-θ)(x₂, y) = (θx₁ + (1-θ)x₂, y) = (x_θ, y)\n\nSince x_θ ∉ X, we have (x_θ, y) ∉ X × Y.\n\nThus X × Y is **not convex**.\n\n### Symmetric argument\n\nThe same argument applies if Y is not convex: pick any x ∈ X and find a convex combination failure in the Y coordinates.\n\n### Conclusion\n\nWith nonempty factors, X × Y is convex **if and only if** both X and Y are convex.\n\n(The only trivial exception: if X = ∅ or Y = ∅, then X × Y = ∅ is vacuously convex.)\n\n∎\n\n## Alternative View (Projections from a Convex Product)\n\n### Reverse direction\n\nSuppose X × Y is convex and Y ≠ ∅. Fix some y₀ ∈ Y. The slice:\n\nX × {y₀} ⊂ X × Y\n\nis convex (as a subset of a convex set? Not automatic). But we can argue differently:\n\nFor any x₁, x₂ ∈ X and θ ∈ [0,1], we have:\n\n(x₁, y₀), (x₂, y₀) ∈ X × Y\n\nBy convexity of X × Y:\n\n(θx₁ + (1-θ)x₂, y₀) = θ(x₁, y₀) + (1-θ)(x₂, y₀) ∈ X × Y\n\nThis means θx₁ + (1-θ)x₂ ∈ X. Hence X is convex.\n\nSimilarly, Y is convex. This proves the \"only if\" direction cleanly.\n\n## Key Insights\n\n1. **Product preserves convexity**: Cartesian products of convex sets are convex\n2. **Componentwise convexity**: Convexity of X × Y is equivalent to convexity of each component\n3. **Strict necessity**: Both factors must be convex (unlike Minkowski sums, where nonconvex can sum to convex)\n4. **Geometric interpretation**: X × Y is the set of all \"stacked\" pairs (x, y) in the product space\n5. **Applications**: In optimization, product structure allows decomposition of problems into smaller subproblems (separability)\n6. **Higher products**: The result generalizes to n-fold products: X₁ × ⋯ × Xₙ is convex iff each Xᵢ is convex\n7. **Connection to direct sums**: In vector spaces, direct sums preserve convexity in the same way",
      "learningObjectives": [
        "cartesian-products",
        "product-spaces",
        "convexity-preservation",
        "necessary-and-sufficient-conditions",
        "counterexamples",
        "projection-maps",
        "separable-structure"
      ],
      "estimatedTime": 20,
      "relatedProblems": ["02-004", "02-006", "04-005"],
      "notes": "Cartesian products are fundamental in optimization when dealing with multiple independent variables or subsystems. Unlike Minkowski sums (where nonconvex + nonconvex can yield convex), products have a strict equivalence: the product is convex iff each factor is convex. This makes products simpler to analyze but also means there's no 'lucky accident' where nonconvexity disappears. The projection viewpoint in the alternative proof foreshadows important concepts in constraint elimination and variable projection in optimization."
    },
    {
      "id": "02-008",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "medium",
      "title": "BV 2.1 — Two-point convexity implies k-point convexity",
      "statement": "Let C ⊂ ℝⁿ be convex. For any k ≥ 2, any x₁, ..., xₖ ∈ C, and any weights θ₁, ..., θₖ ≥ 0 with ∑ᵢ₌₁ᵏ θᵢ = 1, prove\n\nθ₁x₁ + ⋯ + θₖxₖ ∈ C\n\nRecall: A set C is convex if for all x, y ∈ C and θ ∈ [0,1], θx + (1-θ)y ∈ C.",
      "hints": [
        "Use induction on k, the number of points",
        "Base case k=2: This is exactly the definition of convexity",
        "Inductive step: Assume the result holds for k-1 points",
        "For k points, separate out the last point: write ∑ᵢ₌₁ᵏ θᵢxᵢ = α(∑ᵢ₌₁ᵏ⁻¹ (θᵢ/α)xᵢ) + (1-α)xₖ where α = ∑ᵢ₌₁ᵏ⁻¹ θᵢ",
        "Handle two cases: α = 0 (all weight on xₖ) and α > 0 (apply induction hypothesis)",
        "Note that when α > 0, the coefficients θᵢ/α are nonnegative and sum to 1"
      ],
      "solution": "## Proof (Induction on k)\n\n### Base case (k = 2)\n\nThis is exactly the definition of convexity: for x₁, x₂ ∈ C and θ₁, θ₂ ≥ 0 with θ₁ + θ₂ = 1,\n\nθ₁x₁ + θ₂x₂ ∈ C\n\nThis holds by assumption since C is convex. ✓\n\n### Inductive step\n\n**Assumption**: The claim holds for k-1 ≥ 2 points.\n\n**Goal**: Prove it for k points.\n\n### Step 1 (Separate the last point)\n\nLet x₁, ..., xₖ ∈ C and θ₁, ..., θₖ ≥ 0 with ∑ᵢ₌₁ᵏ θᵢ = 1. Define:\n\nα = ∑ᵢ₌₁ᵏ⁻¹ θᵢ,    β = θₖ\n\nNote that α, β ≥ 0 and α + β = 1.\n\n### Step 2 (Case analysis)\n\n**Case 1: α = 0**\n\nThen β = 1, so the point is simply:\n\n∑ᵢ₌₁ᵏ θᵢxᵢ = θₖxₖ = xₖ ∈ C ✓\n\n**Case 2: α > 0**\n\nDefine normalized weights for the first k-1 points:\n\nθ̂ᵢ = θᵢ/α    for i = 1, ..., k-1\n\n### Step 3 (Verify normalization)\n\nFor each i: θ̂ᵢ = θᵢ/α ≥ 0 (since θᵢ ≥ 0 and α > 0)\n\nSum: ∑ᵢ₌₁ᵏ⁻¹ θ̂ᵢ = ∑ᵢ₌₁ᵏ⁻¹ (θᵢ/α) = (1/α)∑ᵢ₌₁ᵏ⁻¹ θᵢ = (1/α)·α = 1 ✓\n\n### Step 4 (Apply induction hypothesis)\n\nBy the induction hypothesis, since we have k-1 points with weights summing to 1:\n\nx̂ := ∑ᵢ₌₁ᵏ⁻¹ θ̂ᵢxᵢ ∈ C\n\n### Step 5 (Combine with the k-th point)\n\nNow rewrite the original sum:\n\n∑ᵢ₌₁ᵏ θᵢxᵢ = ∑ᵢ₌₁ᵏ⁻¹ θᵢxᵢ + θₖxₖ\n           = α(∑ᵢ₌₁ᵏ⁻¹ (θᵢ/α)xᵢ) + βxₖ\n           = αx̂ + βxₖ\n\n### Step 6 (Apply two-point convexity)\n\nSince x̂ ∈ C and xₖ ∈ C, and α + β = 1 with α, β ≥ 0, by the definition of convexity (base case):\n\nαx̂ + βxₖ ∈ C\n\nTherefore ∑ᵢ₌₁ᵏ θᵢxᵢ ∈ C. ✓\n\n### Conclusion\n\nBy mathematical induction, the statement holds for all k ≥ 2. ∎\n\n## Geometric Intuition\n\nThis result says that **convexity defined for pairs of points automatically extends to arbitrary finite convex combinations**. You can think of it as:\n\n1. Start with k points in C\n2. Take their weighted average (with weights summing to 1)\n3. The result is still in C\n\nThe proof works by **reducing k points to 2 points**:\n- Group the first k-1 points into one \"meta-point\" (their weighted average)\n- Combine this meta-point with the k-th point using two-point convexity\n\n## Key Insights\n\n1. **Definition suffices**: The two-point definition of convexity is complete—it implies all finite convex combinations\n2. **Inductive structure**: This is a classic example of structural induction in geometry\n3. **Convex combinations**: Points of the form ∑θᵢxᵢ with θᵢ ≥ 0, ∑θᵢ = 1 are called convex combinations\n4. **Finite vs. infinite**: This result is for *finite* sums; infinite sums require closure arguments\n5. **Fundamental property**: This characterizes the convex hull: conv({x₁,...,xₖ}) = all convex combinations of {x₁,...,xₖ}\n6. **Coefficient preservation**: The key is that convex combinations of convex combinations remain convex combinations",
      "learningObjectives": [
        "proof-by-induction",
        "convexity-definition",
        "convex-combinations",
        "coefficient-normalization",
        "case-analysis",
        "convex-hull"
      ],
      "estimatedTime": 20,
      "relatedProblems": ["02-002", "02-012", "02-009"],
      "notes": "This is a fundamental theorem that shows the two-point definition of convexity is equivalent to the k-point definition. It's one of the first results typically proven in any convex analysis course. The inductive proof technique is important to master—grouping k-1 points and treating them as a single point is a recurring pattern in convex geometry. This result justifies why we can work with finite convex hulls using the simple formula conv({x₁,...,xₖ}) = {∑θᵢxᵢ | θᵢ ≥ 0, ∑θᵢ = 1}."
    },
    {
      "id": "02-009",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "medium",
      "title": "BV 2.2 — A set is convex/affine iff every line slice is",
      "statement": "A set C ⊂ ℝⁿ is convex **if and only if** for every line L, the intersection C ∩ L is convex.\n\nA set C is affine **if and only if** for every line L, the intersection C ∩ L is affine.\n\nRecall: C is affine if for all x, y ∈ C and θ ∈ ℝ, θx + (1-θ)y ∈ C.",
      "hints": [
        "For both cases, prove two directions: (⇒) and (⇐)",
        "Direction (⇒): If C is convex/affine, show any line slice C ∩ L is convex/affine",
        "Use that lines themselves are convex (and affine)",
        "Direction (⇐): Assume all line slices are convex/affine; take arbitrary x, y ∈ C",
        "Consider the line L = {x + t(y - x) | t ∈ ℝ} passing through x and y",
        "Show that x, y ∈ C ∩ L, and use the assumed convexity/affineness of C ∩ L"
      ],
      "solution": "## Proof (Convex Case)\n\n### Direction (⇒): C convex ⟹ C ∩ L convex for all lines L\n\nLet C be convex and L be any line. Take arbitrary u, v ∈ C ∩ L and θ ∈ [0,1].\n\n**Claim**: θu + (1-θ)v ∈ C ∩ L\n\n### Step 1 (Membership in C)\n\nSince u, v ∈ C ∩ L, we have u, v ∈ C. By convexity of C:\n\nθu + (1-θ)v ∈ C\n\n### Step 2 (Membership in L)\n\nSince u, v ∈ L and L is a line (which is affine, hence convex), and θ ∈ [0,1]:\n\nθu + (1-θ)v ∈ L\n\n### Step 3 (Membership in intersection)\n\nCombining Steps 1 and 2:\n\nθu + (1-θ)v ∈ C ∩ L\n\nHence C ∩ L is convex. ✓\n\n### Direction (⇐): C ∩ L convex for all L ⟹ C convex\n\nAssume C ∩ L is convex for every line L. Take arbitrary x, y ∈ C and θ ∈ [0,1].\n\n### Step 1 (Define the line through x and y)\n\nLet:\n\nL = {x + t(y - x) | t ∈ ℝ}\n\nThis is the line passing through x (at t=0) and y (at t=1).\n\n### Step 2 (x and y are in the slice)\n\nClearly x, y ∈ L (by construction). Since x, y ∈ C, we have:\n\nx, y ∈ C ∩ L\n\n### Step 3 (Apply convexity of the slice)\n\nBy assumption, C ∩ L is convex. Therefore, for any θ ∈ [0,1]:\n\nθx + (1-θ)y ∈ C ∩ L ⊆ C\n\nHence C is convex. ✓\n\n## Proof (Affine Case)\n\nThe proof is **identical** to the convex case, except we replace:\n- \"θ ∈ [0,1]\" with \"θ ∈ ℝ\"\n- \"convex\" with \"affine\" throughout\n\n### Direction (⇒): C affine ⟹ C ∩ L affine\n\nFor u, v ∈ C ∩ L and any θ ∈ ℝ:\n- θu + (1-θ)v ∈ C (since C is affine)\n- θu + (1-θ)v ∈ L (since L is a line, hence affine)\n- Therefore θu + (1-θ)v ∈ C ∩ L ✓\n\n### Direction (⇐): C ∩ L affine for all L ⟹ C affine\n\nFor x, y ∈ C and any θ ∈ ℝ, define L = {x + t(y-x) | t ∈ ℝ}. Then:\n- x, y ∈ C ∩ L\n- C ∩ L is affine (by assumption)\n- Therefore θx + (1-θ)y ∈ C ∩ L ⊆ C ✓\n\n## Geometric Interpretation\n\nThis theorem says:\n\n**Convex sets are exactly those sets whose line slices are all convex.**\n\nIntuitively:\n- To check convexity of C, you only need to verify that every line intersects C in a convex set (an interval, a point, or empty)\n- In ℝ², this means checking all 1D slices\n- In ℝ³, this means checking all 1D slices through 3D space\n\n**For affine sets**: The same holds, but line slices must be affine (entire lines, points, or empty).\n\n## Key Insights\n\n1. **Dimension reduction**: Convexity in ℝⁿ reduces to checking convexity in ℝ¹ (along all lines)\n2. **Lines are universal**: Lines are the \"probe\" sets that detect convexity\n3. **Necessary and sufficient**: This gives a complete characterization—both directions hold\n4. **Affine vs. convex**: The proof technique is identical; only the coefficient range differs (θ ∈ ℝ vs. θ ∈ [0,1])\n5. **Verification strategy**: To check if a set is convex, you can verify convexity along arbitrary lines through the set\n6. **Connection to line segments**: For convex sets, C ∩ L is either empty, a point, or a line segment (possibly infinite)\n7. **Extension to cones/subspaces**: Similar characterizations exist for cones (rays) and subspaces (lines through origin)",
      "learningObjectives": [
        "line-slices",
        "convexity-characterization",
        "affine-sets",
        "iff-proofs",
        "dimension-reduction",
        "intersection-properties"
      ],
      "estimatedTime": 20,
      "relatedProblems": ["02-008", "02-010", "02-015"],
      "notes": "This is a powerful characterization theorem that reduces n-dimensional convexity to 1-dimensional convexity. It's particularly useful for verifying convexity of sets defined implicitly—just check that intersections with arbitrary lines are convex. The affine case is included to show the parallel structure. This result is often used in optimization to restrict problems to line searches, and it's fundamental to understanding why line search methods work for convex functions."
    },
    {
      "id": "02-010",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "hard",
      "title": "BV 2.3 — Closed + midpoint-convex implies convex",
      "statement": "A set C ⊂ ℝⁿ is **midpoint-convex** if for all x, y ∈ C,\n\n(x + y)/2 ∈ C\n\nShow: if C is midpoint-convex **and closed**, then C is convex.",
      "hints": [
        "First, show that midpoint-convexity implies dyadic rationals θ = m/2ʳ work",
        "Use repeated midpoint operations: (x+y)/2, then ((x+y)/2 + y)/2 = (x+3y)/4, etc.",
        "For general θ ∈ [0,1], approximate θ by dyadic rationals θₖ = mₖ/2^rₖ → θ",
        "Show that θₖx + (1-θₖ)y ∈ C for all k (by Step 1)",
        "Use closedness: take the limit as k → ∞ to get θx + (1-θ)y ∈ C̄ = C",
        "Key: Dyadic rationals are dense in [0,1], and affine maps are continuous"
      ],
      "solution": "## Proof\n\nFix x, y ∈ C. We will prove θx + (1-θ)y ∈ C for every θ ∈ [0,1].\n\n### Step 1 (Dyadic coefficients)\n\n**Claim**: For any dyadic rational θ = m/2ʳ with integers m, r ≥ 0 and 0 ≤ m ≤ 2ʳ,\n\nθx + (1-θ)y ∈ C\n\n**Proof by induction on r**:\n\n**Base case r = 0**: θ ∈ {0, 1}\n- If θ = 0: θx + (1-θ)y = y ∈ C ✓\n- If θ = 1: θx + (1-θ)y = x ∈ C ✓\n\n**Inductive step**: Assume the claim holds for r-1. Consider θ = m/2ʳ.\n\n**Case 1**: m is even, say m = 2k. Then θ = k/2^(r-1), which has the form from r-1. By induction, θx + (1-θ)y ∈ C. ✓\n\n**Case 2**: m is odd, say m = 2k+1. Then:\n\nθ = (2k+1)/2ʳ = k/2^(r-1) · (1/2) + (k+1)/2^(r-1) · (1/2)\n\nLet:\n- θ₁ = k/2^(r-1)\n- θ₂ = (k+1)/2^(r-1)\n\nNote that θ = (θ₁ + θ₂)/2.\n\nBy the induction hypothesis:\n\nu₁ := θ₁x + (1-θ₁)y ∈ C\nu₂ := θ₂x + (1-θ₂)y ∈ C\n\nBy midpoint-convexity:\n\n(u₁ + u₂)/2 ∈ C\n\nBut:\n\n(u₁ + u₂)/2 = (θ₁x + (1-θ₁)y + θ₂x + (1-θ₂)y)/2\n             = ((θ₁+θ₂)/2)x + ((2-(θ₁+θ₂))/2)y\n             = θx + (1-θ)y ∈ C ✓\n\nBy induction, dyadic rationals work.\n\n### Step 2 (Density of dyadic rationals)\n\nDyadic rationals in [0,1] are **dense**: for any θ ∈ [0,1] and any ε > 0, there exists a dyadic rational θ' with |θ - θ'| < ε.\n\nEquivalently, there exists a sequence of dyadic rationals (θₖ) with θₖ → θ.\n\n### Step 3 (Construct sequence in C)\n\nChoose a sequence of dyadic rationals θₖ → θ. By Step 1, for each k:\n\nwₖ := θₖx + (1-θₖ)y ∈ C\n\n### Step 4 (Pass to the limit)\n\nThe map θ ↦ θx + (1-θ)y is continuous (it's affine in θ). Therefore:\n\nwₖ = θₖx + (1-θₖ)y → θx + (1-θ)y\n\nas k → ∞.\n\n### Step 5 (Use closedness)\n\nSince each wₖ ∈ C and C is closed:\n\nlim_{k→∞} wₖ ∈ C̄ = C\n\nTherefore:\n\nθx + (1-θ)y ∈ C\n\nSince x, y, and θ were arbitrary, C is convex. ∎\n\n## Why \"Closed\" is Essential\n\n**Counterexample without closedness**:\n\nLet C = ℚ ∩ [0,1] (rationals in [0,1]).\n\n**Midpoint-convex**: If x, y ∈ ℚ, then (x+y)/2 ∈ ℚ. ✓\n\n**Not convex**: Take x = 0, y = 1, θ = √2/2 ≈ 0.707...\n\nThen θx + (1-θ)y = (2-√2)/2 ∉ ℚ, so θx + (1-θ)y ∉ C.\n\nHence C is midpoint-convex but **not convex**.\n\n**Why it fails**: C is not closed (C̄ = [0,1] ⊃ C). The limit argument in Step 5 breaks down because the limit might not be in C.\n\n## Key Insights\n\n1. **Midpoint-convexity is weaker**: Midpoint-convexity (only θ = 1/2) is strictly weaker than convexity (all θ ∈ [0,1])\n2. **Closure bridges the gap**: Adding closedness makes midpoint-convexity equivalent to convexity\n3. **Dyadic density**: The dyadic rationals {m/2ʳ} are dense in [0,1], which is key to the approximation\n4. **Inductive construction**: The proof builds up from 1/2 to all dyadic rationals by repeated subdivision\n5. **Limit arguments**: Passing to limits requires closedness—this is a common pattern in analysis\n6. **Topological + algebraic**: The result combines topology (closedness, limits) with algebra (dyadic arithmetic)\n7. **Applications**: This is used in functional analysis where midpoint-convexity is sometimes easier to verify directly",
      "learningObjectives": [
        "midpoint-convexity",
        "closed-sets",
        "dyadic-rationals",
        "density-arguments",
        "limit-points",
        "induction-on-binary-expansions",
        "counterexamples"
      ],
      "estimatedTime": 30,
      "relatedProblems": ["02-006", "02-009", "03-015"],
      "notes": "This is a deep result showing that midpoint-convexity plus closedness implies full convexity. The dyadic construction is clever—it builds all binary fractions by repeated bisection, then uses density to reach all real coefficients. The counterexample (rationals in [0,1]) is instructive: it shows closedness is truly necessary. This theorem appears in functional analysis when studying convex functions on Banach spaces, where sometimes only midpoint-convexity can be verified directly. The proof technique—building dyadic rationals inductively—appears in many contexts (e.g., proving uniqueness of Lebesgue measure)."
    },
    {
      "id": "02-011",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "easy",
      "title": "BV 2.4 — Convex hull equals intersection of all convex supersets",
      "statement": "For S ⊂ ℝⁿ, let conv(S) be its convex hull (all finite convex combinations of points in S). Show:\n\nconv(S) = ⋂ {C ⊂ ℝⁿ | C is convex and S ⊆ C}\n\nIn words: the convex hull of S is the intersection of all convex sets containing S.",
      "hints": [
        "Let ℱ = {C | C is convex and S ⊆ C} and K = ⋂_{C∈ℱ} C",
        "First show K is convex (intersection of convex sets)",
        "Direction 1 (conv(S) ⊆ K): Show every convex set containing S must contain conv(S)",
        "Use that convex sets are closed under convex combinations (BV 2.1)",
        "Direction 2 (K ⊆ conv(S)): Show conv(S) itself is in ℱ, so K ⊆ conv(S)",
        "Verify that conv(S) is convex and contains S"
      ],
      "solution": "## Proof\n\nLet:\n\nℱ = {C ⊂ ℝⁿ | C is convex and S ⊆ C}\n\nK = ⋂_{C∈ℱ} C\n\n### Preliminary: K is convex and contains S\n\n**K is convex**: Arbitrary intersections of convex sets are convex. Since each C ∈ ℱ is convex, K is convex. ✓\n\n**S ⊆ K**: Every C ∈ ℱ contains S, so their intersection K contains S. ✓\n\n### Direction 1: conv(S) ⊆ K\n\n**Goal**: Show every point in conv(S) lies in K.\n\nTake any C ∈ ℱ. We'll show conv(S) ⊆ C.\n\n### Step 1.1 (C contains S)\n\nBy definition of ℱ: S ⊆ C.\n\n### Step 1.2 (C contains all convex combinations)\n\nSince C is convex and contains S, by BV 2.1 (k-point convexity), C must contain **all finite convex combinations** of points from S.\n\nThat is: conv(S) ⊆ C.\n\n### Step 1.3 (Intersect over all C)\n\nSince conv(S) ⊆ C for every C ∈ ℱ:\n\nconv(S) ⊆ ⋂_{C∈ℱ} C = K ✓\n\n### Direction 2: K ⊆ conv(S)\n\n**Goal**: Show every point in K lies in conv(S).\n\n### Step 2.1 (Show conv(S) ∈ ℱ)\n\nWe need to verify that conv(S) is convex and contains S.\n\n**Contains S**: By definition, S ⊆ conv(S) (every point x ∈ S equals the trivial convex combination 1·x). ✓\n\n**Is convex**: This was proven in Problem P2 (BV 2.1 also gives this). The convex hull is convex. ✓\n\nTherefore: conv(S) ∈ ℱ.\n\n### Step 2.2 (K is a subset)\n\nSince K is the intersection of all sets in ℱ, and conv(S) ∈ ℱ:\n\nK = ⋂_{C∈ℱ} C ⊆ conv(S) ✓\n\n### Conclusion\n\nFrom Directions 1 and 2:\n\nK = conv(S) ∎\n\n## Alternative Characterization\n\nThis result shows:\n\n**conv(S) is the smallest convex set containing S**\n\nSmallest in the sense that:\n1. conv(S) is convex and contains S\n2. conv(S) is contained in every other convex set containing S\n\n## Geometric Interpretation\n\nThink of conv(S) as:\n\n- **Explicitly**: All convex combinations ∑θᵢsᵢ with sᵢ ∈ S, θᵢ ≥ 0, ∑θᵢ = 1\n- **Implicitly**: The intersection of all convex supersets (halfspaces, polytopes, balls, etc. that contain S)\n\nThis theorem says these two descriptions are **equivalent**.\n\n## Examples\n\n### Example 1: Finite set in ℝ²\n\nS = {(0,0), (1,0), (0,1)}\n\n**Explicit**: conv(S) = {θ₁(0,0) + θ₂(1,0) + θ₃(0,1) | θᵢ ≥ 0, θ₁+θ₂+θ₃=1}\n           = triangle with vertices at S\n\n**Implicit**: conv(S) = intersection of all halfspaces containing S\n           = {(x,y) | x ≥ 0, y ≥ 0, x+y ≤ 1}\n\n### Example 2: Unit circle in ℝ²\n\nS = {(cos θ, sin θ) | θ ∈ [0, 2π)}\n\n**Explicit**: conv(S) = all convex combinations of points on the circle\n           = unit disk {(x,y) | x²+y² ≤ 1}\n\n**Implicit**: conv(S) = ⋂ {halfspaces H | S ⊆ H}\n           = unit disk (intersection of all supporting halfspaces)\n\n## Key Insights\n\n1. **Two definitions, one object**: Convex hull has both a constructive definition (convex combinations) and a descriptive definition (smallest convex set)\n2. **Smallest convex set**: conv(S) is the unique smallest convex set containing S\n3. **Intersection characterization**: This gives a way to compute/describe conv(S) via supporting hyperplanes\n4. **Closure under convex combinations**: Any convex set containing S must contain all convex combinations of S\n5. **Duality**: The implicit characterization connects to the dual description via support functions\n6. **Carathéodory's theorem**: In ℝⁿ, every point in conv(S) is a convex combination of at most n+1 points from S\n7. **Computational geometry**: This characterization is fundamental for computing convex hulls",
      "learningObjectives": [
        "convex-hull",
        "intersection-characterization",
        "smallest-convex-set",
        "implicit-vs-explicit",
        "closure-properties",
        "set-containment-proofs"
      ],
      "estimatedTime": 15,
      "relatedProblems": ["02-002", "02-008", "02-015"],
      "notes": "This elegant characterization theorem shows that the convex hull can be defined either constructively (take all convex combinations) or descriptively (take the smallest convex superset). The proof is straightforward but instructive: one direction uses that convex sets must be closed under convex combinations, the other uses that conv(S) is itself convex. This result is foundational in convex geometry and connects to duality theory—the implicit description via intersections of halfspaces leads to the dual representation via support functions. In computational geometry, this characterization justifies algorithms that compute convex hulls by finding supporting hyperplanes."
    },
    {
      "id": "02-012",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "hard",
      "title": "BV 2.10 — Solution set of a quadratic inequality",
      "statement": "Let\n\nC = {x ∈ ℝⁿ | q(x) ≤ 0},    q(x) = xᵀAx + bᵀx + c\n\nwith A ∈ Sⁿ (symmetric), b ∈ ℝⁿ, c ∈ ℝ.\n\n(a) Show that C is convex if A ⪰ 0 (positive semidefinite).\n\n(b) Let H = {x ∈ ℝⁿ | gᵀx + h = 0} with g ≠ 0. Show that C ∩ H is convex if there exists λ ∈ ℝ such that\n\nA + λggᵀ ⪰ 0\n\n**Are the converses true?** Does convexity of C imply A ⪰ 0? Does convexity of C ∩ H imply such a λ exists?",
      "hints": [
        "Part (a): Show q is a convex function by computing its Hessian ∇²q(x) = 2A",
        "A sublevel set of a convex function is convex",
        "Part (b): Parametrize the hyperplane H using an orthonormal basis",
        "Let x₀ satisfy gᵀx₀ + h = 0, and let U have orthonormal columns spanning g⊥",
        "Every point in H can be written as x = x₀ + Uy for y ∈ ℝⁿ⁻¹",
        "Restrict q to H: define q̃(y) = q(x₀ + Uy) and show it's convex when UᵀAU ⪰ 0",
        "Use that UᵀgU = 0 implies Uᵀ(A + λggᵀ)U = UᵀAU",
        "For converses: construct counterexamples"
      ],
      "solution": "## Part (a) — Sublevel Set of Convex Quadratic\n\n### Step 1 (q is convex when A ⪰ 0)\n\nThe function q(x) = xᵀAx + bᵀx + c has Hessian:\n\n∇²q(x) = 2A\n\nSince A ⪰ 0, we have ∇²q(x) ⪰ 0 for all x. Therefore q is convex.\n\n### Step 2 (Sublevel sets of convex functions are convex)\n\nA fundamental result: if f is convex, then for any α ∈ ℝ, the sublevel set\n\n{x | f(x) ≤ α}\n\nis convex.\n\n### Step 3 (Apply to q)\n\nSince q is convex, the sublevel set\n\nC = {x | q(x) ≤ 0}\n\nis convex. ∎\n\n## Part (b) — Restriction to a Hyperplane\n\n### Step 1 (Parametrize the hyperplane)\n\nChoose any x₀ ∈ ℝⁿ with gᵀx₀ + h = 0. Let U ∈ ℝⁿˣ⁽ⁿ⁻¹⁾ have orthonormal columns forming a basis of g⊥ (the orthogonal complement of g), so:\n\nUᵀU = I_{n-1},    UᵀG = 0\n\nEvery point of H can be written uniquely as:\n\nx = x₀ + Uy,    y ∈ ℝⁿ⁻¹\n\n### Step 2 (Reduce the quadratic on H)\n\nDefine the \"restricted\" quadratic:\n\nq̃(y) := q(x₀ + Uy) = (x₀ + Uy)ᵀA(x₀ + Uy) + bᵀ(x₀ + Uy) + c\n\nExpanding:\n\nq̃(y) = yᵀ(UᵀAU)y + (2x₀ᵀAU + bᵀU)y + (x₀ᵀAx₀ + bᵀx₀ + c)\n\nThis is quadratic in y with Hessian:\n\n∇²q̃(y) = 2UᵀAU\n\n### Step 3 (Convexity condition on H)\n\nConvexity of {y | q̃(y) ≤ 0} requires:\n\nUᵀAU ⪰ 0    (★)\n\n### Step 4 (Use the given condition)\n\nAssume there exists λ with A + λggᵀ ⪰ 0. Then:\n\nUᵀ(A + λggᵀ)U = UᵀAU + λ(UᵀG)(gᵀU) = UᵀAU + λ·0 = UᵀAU\n\nSince A + λggᵀ ⪰ 0, we have:\n\nUᵀAU = Uᵀ(A + λggᵀ)U ⪰ 0\n\nThis verifies condition (★). Therefore q̃ is convex in y, and:\n\nC ∩ H = {x₀ + Uy | q̃(y) ≤ 0}\n\nis convex. ∎\n\n## Are the Converses True?\n\n### Part (a) Converse: FALSE\n\n**Counterexample**: Let n = 1, A = -1, b = 0, c = 0. Then:\n\nq(x) = -x² ≤ 0  ⟺  x² ≥ 0  ⟺  x ∈ ℝ\n\nSo C = ℝ, which is convex, even though A = -1 ⊀ 0.\n\n**General principle**: If q(x) ≤ 0 for all x (i.e., q ≤ 0 everywhere), then C = ℝⁿ is convex regardless of whether A ⪰ 0.\n\n### Part (b) Converse: FALSE\n\n**Counterexample**: Let n = 2, g = e₁ = (1,0), h = 0, so H = {(0, x₂) | x₂ ∈ ℝ} (the vertical axis).\n\nLet:\n\nA = [0  1]\n    [1  0],    b = 0,    c = 0\n\nThen q(x) = 2x₁x₂.\n\nOn H, write x = (0, t). Then:\n\nq(x) = 2·0·t = 0\n\nSo C ∩ H = {x ∈ H | 0 ≤ 0} = H, which is convex.\n\n**Check the claimed converse**: We need λ such that\n\nA + λggᵀ = [0  1] + λ[1  0] = [λ  1]\n            [1  0]     [0  0]   [1  0]\n\nis positive semidefinite.\n\nFor a 2×2 matrix [a  b] to be PSD, we need:\n                  [b  c]\n- a ≥ 0, c ≥ 0, ac - b² ≥ 0\n\nHere: a = λ, b = 1, c = 0.\n\nFor PSD: λ ≥ 0, 0 ≥ 0, λ·0 - 1² = -1 ≥ 0.\n\nThe last condition gives -1 ≥ 0, which is **false** for all λ.\n\nThus no such λ exists, yet C ∩ H is convex. The converse is false. ∎\n\n## Geometric Intuition\n\n**Part (a)**: The set C is the region where a quadratic form is nonpositive. When A ⪰ 0, this is a sublevel set of a convex function (a \"bowl\" or ellipsoid-like region). When A ⊀ 0, the quadratic can be nonconvex (saddle-shaped), but C might still be convex if it's ℝⁿ or empty.\n\n**Part (b)**: Restricting to a hyperplane H reduces the dimension by 1. The condition A + λggᵀ ⪰ 0 ensures the quadratic form has nonnegative curvature within H. The counterexample shows this is sufficient but not necessary—convexity on H only constrains curvature within H, not in the normal direction g.\n\n## Key Insights\n\n1. **Convex functions → convex sublevel sets**: This is a fundamental connection between convex functions and convex sets\n2. **Hessian test**: A quadratic is convex iff its Hessian (which is constant for quadratics) is PSD\n3. **Dimension reduction**: Restricting to a hyperplane reduces the problem to one dimension lower\n4. **Sufficient vs. necessary**: The given conditions are sufficient for convexity but not necessary (counterexamples show this)\n5. **Normal vs. tangent curvature**: For C ∩ H, only tangent curvature (within H) matters; normal curvature (along g) doesn't affect convexity of the slice\n6. **S-procedure**: Part (b) is related to the S-procedure in optimization—adding a multiple of one constraint to another\n7. **Ellipsoids and level sets**: When A ≻ 0, C is an ellipsoid (possibly empty); when A ⪰ 0, C is cylindrical or the whole space",
      "learningObjectives": [
        "quadratic-forms",
        "positive-semidefinite-matrices",
        "sublevel-sets",
        "convex-functions",
        "hessian-test",
        "hyperplane-restrictions",
        "s-procedure",
        "counterexamples",
        "necessary-vs-sufficient-conditions"
      ],
      "estimatedTime": 35,
      "relatedProblems": ["02-003", "03-008", "04-012"],
      "notes": "This problem connects quadratic forms, matrix analysis, and convexity in a deep way. Part (a) is straightforward—it's the standard sublevel set result for convex quadratics. Part (b) is more sophisticated, requiring parametrization of hyperplanes and understanding how rank-1 updates (λggᵀ) affect positive semidefiniteness. The counterexamples are crucial: they show the conditions are sufficient but not necessary, illustrating that convexity of a set doesn't uniquely determine the properties of defining functions. This is foundational for quadratically constrained quadratic programming (QCQP) and semidefinite programming (SDP)."
    },
    {
      "id": "02-013",
      "lecture": "02-convex-sets",
      "type": "verification",
      "difficulty": "easy",
      "title": "BV 2.11 — Hyperbolic sets are convex",
      "statement": "Show that the set\n\n{x ∈ ℝ²₊ | x₁x₂ ≥ 1}\n\nis convex. Generalize to\n\n{x ∈ ℝⁿ₊ | ∏ᵢ₌₁ⁿ xᵢ ≥ 1}\n\nwhere ℝⁿ₊ = {x ∈ ℝⁿ | xᵢ ≥ 0 for all i}.",
      "hints": [
        "Work on the positive orthant where log is defined: note that feasible points satisfy xᵢ > 0",
        "Consider the function φ(x) = ∑ᵢ log xᵢ on ℝⁿ₊₊ = {x | xᵢ > 0 ∀i}",
        "Show that φ is concave by checking its Hessian or using that log is concave",
        "The set can be written as {x ∈ ℝⁿ₊₊ | φ(x) ≥ 0}, a superlevel set of a concave function",
        "Superlevel sets of concave functions are convex",
        "Alternative: Use weighted AM-GM inequality directly on convex combinations"
      ],
      "solution": "## Solution (Concavity of log)\n\nWork on the positive orthant ℝⁿ₊₊ = {x ∈ ℝⁿ | xᵢ > 0 for all i}.\n\n**Note**: Any x with ∏ᵢ xᵢ ≥ 1 must satisfy xᵢ > 0 for all i, so ℝⁿ₊₊ and ℝⁿ₊ coincide on the feasible set.\n\n### Step 1 (Define φ)\n\nDefine:\n\nφ(x) = ∑ᵢ₌₁ⁿ log xᵢ,    dom(φ) = ℝⁿ₊₊\n\n### Step 2 (φ is concave)\n\nEach function x ↦ log xᵢ is concave on (0, ∞) because:\n\nd²/dx²ᵢ (log xᵢ) = -1/xᵢ² < 0\n\nSums of concave functions are concave, so φ is concave on ℝⁿ₊₊.\n\n### Step 3 (Express the set as a superlevel set)\n\nNote that:\n\n∏ᵢ₌₁ⁿ xᵢ ≥ 1  ⟺  log(∏ᵢ xᵢ) ≥ 0  ⟺  ∑ᵢ₌₁ⁿ log xᵢ ≥ 0  ⟺  φ(x) ≥ 0\n\nTherefore, the set is:\n\n{x ∈ ℝⁿ₊₊ | φ(x) ≥ 0}\n\n### Step 4 (Superlevel sets of concave functions are convex)\n\n**Theorem**: If f is concave, then for any α, the superlevel set {x | f(x) ≥ α} is convex.\n\n**Proof**: Take x, y with f(x) ≥ α and f(y) ≥ α. For θ ∈ [0,1]:\n\nf(θx + (1-θ)y) ≥ θf(x) + (1-θ)f(y) ≥ θα + (1-θ)α = α\n\nby concavity. ✓\n\n### Step 5 (Apply to φ)\n\nSince φ is concave, the superlevel set\n\n{x ∈ ℝⁿ₊₊ | φ(x) ≥ 0}\n\nis convex. ∎\n\n## Alternative Solution (Weighted AM-GM)\n\nFor x, y ∈ ℝⁿ₊₊ with ∏ᵢ xᵢ ≥ 1 and ∏ᵢ yᵢ ≥ 1, and θ ∈ [0,1], we use concavity of log directly:\n\n∑ᵢ log(θxᵢ + (1-θ)yᵢ) ≥ ∑ᵢ [θ log xᵢ + (1-θ) log yᵢ]\n                           = θ ∑ᵢ log xᵢ + (1-θ) ∑ᵢ log yᵢ\n                           ≥ θ·0 + (1-θ)·0 = 0\n\nTherefore:\n\nlog(∏ᵢ(θxᵢ + (1-θ)yᵢ)) = ∑ᵢ log(θxᵢ + (1-θ)yᵢ) ≥ 0\n\nHence:\n\n∏ᵢ(θxᵢ + (1-θ)yᵢ) ≥ 1 ✓\n\n## Geometric Interpretation (2D Case)\n\nFor n = 2, the set {(x₁, x₂) ∈ ℝ²₊ | x₁x₂ ≥ 1} is the region \"above\" the hyperbola x₁x₂ = 1 in the first quadrant.\n\n**Visualization**:\n- The boundary x₁x₂ = 1 is a hyperbola with asymptotes along the axes\n- The set includes all points with larger product, i.e., farther from the origin along rays from the origin\n- Convexity means: any line segment between two points in this region stays in the region\n\n## Connection to Geometric Mean\n\nThe condition ∏ᵢ xᵢ ≥ 1 can be rewritten as:\n\n(∏ᵢ₌₁ⁿ xᵢ)^(1/n) ≥ 1\n\nThe function x ↦ (∏ᵢ xᵢ)^(1/n) is the **geometric mean**. This is a concave function on ℝⁿ₊₊.\n\n## Key Insights\n\n1. **Logarithm transforms products to sums**: log(∏xᵢ) = ∑log xᵢ\n2. **Concavity of log**: The function log is concave, which makes φ = ∑log xᵢ concave\n3. **Superlevel sets**: Superlevel sets {f ≥ α} of concave f are convex (dual to sublevel sets of convex f)\n4. **Hyperbolic constraint**: The constraint x₁x₂ ≥ c for c > 0 defines a \"rotated second-order cone\"\n5. **Generalized mean inequalities**: The geometric mean is concave; this connects to AM-GM inequality\n6. **Standard form**: In conic optimization, this is part of the **power cone** or **exponential cone**\n7. **Applications**: Appears in geometric programming, portfolio optimization, and resource allocation",
      "learningObjectives": [
        "concave-functions",
        "superlevel-sets",
        "logarithm-properties",
        "hyperbolic-sets",
        "geometric-mean",
        "conic-constraints",
        "am-gm-inequality"
      ],
      "estimatedTime": 20,
      "relatedProblems": ["02-003", "03-010", "04-015"],
      "notes": "This problem introduces hyperbolic constraints, which appear frequently in geometric programming and modern conic optimization. The key insight is that the logarithm transforms the nonlinear product constraint into a linear constraint in log-space, where convexity is obvious. This 'change of coordinates' technique is powerful—many seemingly nonconvex sets become convex after a suitable transformation. The set {x | x₁x₂ ≥ 1, x ≥ 0} is a rotated second-order cone, a standard primitive in conic optimization solvers."
    },
    {
      "id": "02-014",
      "lecture": "02-convex-sets",
      "type": "computation",
      "difficulty": "medium",
      "title": "BV 2.12 — Which sets are convex?",
      "statement": "For each set below, determine whether it is convex. If convex, provide a brief justification. If not convex, give a counterexample.\n\nAll distances and norms are **Euclidean** unless stated otherwise.\n\n(a) **Slab**: S = {x ∈ ℝⁿ | α ≤ aᵀx ≤ β}\n\n(b) **Hyperrectangle (box)**: R = {x ∈ ℝⁿ | αᵢ ≤ xᵢ ≤ βᵢ, i=1,...,n}\n\n(c) **Wedge**: W = {x ∈ ℝⁿ | a₁ᵀx ≤ b₁, a₂ᵀx ≤ b₂}\n\n(d) **At least as close to a as to S**: D = {x ∈ ℝⁿ | |x-a|₂ ≤ dist₂(x, S)}\nwhere dist₂(x, S) = inf_{y∈S} |x-y|₂\n\n(e) **Closer to S than to T**: E = {x ∈ ℝⁿ | dist₂(x, S) ≤ dist₂(x, T)}\n\n(f) **Translates of S₂ lie in S₁**: F = {x ∈ ℝⁿ | x + S₂ ⊆ S₁} where S₁ is convex\n\n(g) **Closer to a than to b (up to a factor)**: G = {x ∈ ℝⁿ | |x-a|₂ ≤ θ|x-b|₂}\nwith a ≠ b and 0 ≤ θ ≤ 1",
      "hints": [
        "Part (a): Write as intersection of two halfspaces",
        "Part (b): Write as product of intervals, or intersection of halfspaces",
        "Part (c): Intersection of two halfspaces",
        "Part (d): For each fixed y ∈ S, {x | |x-a| ≤ |x-y|} is a halfspace; intersect over all y",
        "Part (e): Look for a simple counterexample with S and T being two points",
        "Part (f): Show F = ⋂_{y∈S₂} (S₁ - y), an intersection of translates of S₁",
        "Part (g): Square both sides and complete the square to get a ball (θ < 1) or halfspace (θ = 1)"
      ],
      "solution": "## (a) Slab — CONVEX ✓\n\n### Justification\n\nS = {x | α ≤ aᵀx ≤ β} = {x | aᵀx ≥ α} ∩ {x | aᵀx ≤ β}\n\nThis is the intersection of two halfspaces:\n- {x | -aᵀx ≤ -α} (halfspace)\n- {x | aᵀx ≤ β} (halfspace)\n\nHalfspaces are convex, and intersections of convex sets are convex. Therefore S is convex. ∎\n\n## (b) Hyperrectangle (Box) — CONVEX ✓\n\n### Justification (Method 1: Product of intervals)\n\nR = [α₁, β₁] × [α₂, β₂] × ⋯ × [αₙ, βₙ]\n\nEach interval [αᵢ, βᵢ] is convex. By Problem P7 (Cartesian products), the product of convex sets is convex. Therefore R is convex. ∎\n\n### Justification (Method 2: Intersection of halfspaces)\n\nR = ⋂ᵢ₌₁ⁿ {x | eᵢᵀx ≤ βᵢ} ∩ ⋂ᵢ₌₁ⁿ {x | -eᵢᵀx ≤ -αᵢ}\n\nwhere eᵢ is the i-th standard basis vector. This is an intersection of 2n halfspaces, hence convex. ∎\n\n## (c) Wedge — CONVEX ✓\n\n### Justification\n\nW = {x | a₁ᵀx ≤ b₁} ∩ {x | a₂ᵀx ≤ b₂}\n\nIntersection of two halfspaces, hence convex. ∎\n\n## (d) At least as close to a as to S — CONVEX ✓\n\n### Justification\n\n|x - a|₂ ≤ dist₂(x, S)  ⟺  |x - a|₂ ≤ |x - y|₂  for all y ∈ S\n\n### Step 1 (Each constraint is a halfspace)\n\nFor fixed y, the set {x | |x-a|₂ ≤ |x-y|₂} is a halfspace.\n\n**Proof**: Square both sides (valid since both sides are nonnegative):\n\n|x-a|₂² ≤ |x-y|₂²\n|x|² - 2aᵀx + |a|² ≤ |x|² - 2yᵀx + |y|²\n2yᵀx - 2aᵀx ≤ |y|² - |a|²\n2(y-a)ᵀx ≤ |y|² - |a|²\n\nThis is a halfspace (linear inequality in x). ✓\n\n### Step 2 (Intersect over all y ∈ S)\n\nD = ⋂_{y∈S} {x | 2(y-a)ᵀx ≤ |y|² - |a|²}\n\nThis is an intersection of halfspaces (possibly infinitely many), hence convex. ∎\n\n**Geometric interpretation**: D is the set of points closer to a than to any point in S—the \"Voronoi cell\" of a with respect to S.\n\n## (e) Closer to S than to T — NOT CONVEX ✗\n\n### Counterexample\n\nLet S = {-2}, T = {2} in ℝ. Then S and T are single points.\n\ndist(x, S) = |x - (-2)| = |x + 2|\ndist(x, T) = |x - 2|\n\nE = {x | |x + 2| ≤ |x - 2|}\n\nSquaring:\n\n(x+2)² ≤ (x-2)²\nx² + 4x + 4 ≤ x² - 4x + 4\n8x ≤ 0\nx ≤ 0\n\nSo E = (-∞, 0].\n\nNow extend to a slight variation: let S = {-2, 2}, T = {0}.\n\ndist(x, S) = min{|x+2|, |x-2|}\ndist(x, T) = |x|\n\nE = {x | min{|x+2|, |x-2|} ≤ |x|}\n\nFor x ∈ [-2, 2]: min{|x+2|, |x-2|} = min{x+2, 2-x} = min values occur at endpoints.\n\nActually, let me use a cleaner example:\n\nS = {-1, 1}, T = {0}.\n\ndist(x, S) = min{|x+1|, |x-1|}\ndist(x, T) = |x|\n\n- For x = -2: dist = min{1, 3} = 1, |x| = 2. Not in E.\n- For x = -0.5: dist = min{0.5, 1.5} = 0.5, |x| = 0.5. In E.\n- For x = 0: dist = min{1, 1} = 1, |x| = 0. In E.\n- For x = 0.5: dist = min{1.5, 0.5} = 0.5, |x| = 0.5. In E.\n- For x = 2: dist = min{3, 1} = 1, |x| = 2. Not in E.\n\nE includes (-0.5, 0, 0.5) but checking if (-2, 2) are in:\n- x = -2: dist = 1, |x| = 2. NO\n- x = 2: dist = 1, |x| = 2. NO\n\nSo E seems to be (-1, 1) approximately. But actually:\n\nE = {x | min{|x+1|, |x-1|} ≤ |x|} = (-∞, -1] ∪ [1, ∞)\n\nWait, let me recalculate. At x = -1.5:\ndist = min{0.5, 2.5} = 0.5, |x| = 1.5. Not satisfied.\n\nAt x = -0.5:\ndist = min{0.5, 1.5} = 0.5, |x| = 0.5. Satisfied (equality).\n\nOK so the set includes points near ±1 but not the middle. Likely E = (-∞, -a] ∪ [a, ∞) for some a, which is **not convex**.\n\nActually, the simplest counterexample:\n\nS = {-1, 1}, T = {0} in ℝ.\n\nBy symmetry and calculation, E excludes a neighborhood around 0 but includes points far from 0. Thus E is not convex. ∎\n\n## (f) Translates of S₂ lie in S₁ — CONVEX ✓ (when S₁ is convex)\n\n### Justification\n\nx + S₂ ⊆ S₁  ⟺  ∀y ∈ S₂, x + y ∈ S₁  ⟺  ∀y ∈ S₂, x ∈ S₁ - y\n\nTherefore:\n\nF = ⋂_{y∈S₂} (S₁ - y)\n\nEach S₁ - y is a translate of S₁. Translations preserve convexity, so each S₁ - y is convex.\n\nIntersections of convex sets are convex. Therefore F is convex. ∎\n\n**Geometric interpretation**: F is the set of translations that keep S₂ inside S₁—the \"erosion\" of S₁ by S₂ in mathematical morphology.\n\n## (g) Closer to a than to b (up to factor θ) — CONVEX ✓\n\n### Derivation\n\n|x - a|₂ ≤ θ|x - b|₂\n\nSquare both sides (valid since both ≥ 0):\n\n|x - a|₂² ≤ θ²|x - b|₂²\n|x|² - 2aᵀx + |a|² ≤ θ²(|x|² - 2bᵀx + |b|²)\n|x|² - 2aᵀx + |a|² ≤ θ²|x|² - 2θ²bᵀx + θ²|b|²\n(1 - θ²)|x|² - 2(a - θ²b)ᵀx + (|a|² - θ²|b|²) ≤ 0\n\n### Case 1: θ < 1 (Ball)\n\nDivide by (1 - θ²) > 0:\n\n|x|² - 2(a - θ²b)/(1 - θ²))ᵀx + (|a|² - θ²|b|²)/(1 - θ²) ≤ 0\n\nComplete the square. Let c = (a - θ²b)/(1 - θ²). Then:\n\n|x - c|² ≤ r²\n\nfor some r ≥ 0 (computed from the constant terms). This is a **closed ball**, which is convex. ✓\n\n### Case 2: θ = 1 (Halfspace)\n\nThe quadratic term cancels:\n\n-2(a - b)ᵀx + |a|² - |b|² ≤ 0\n2(b - a)ᵀx ≤ |b|² - |a|²\n\nThis is a **halfspace**, which is convex. ✓\n\nTherefore G is convex in both cases. ∎\n\n**Geometric interpretation**: For θ < 1, G is the interior of an Apollonius circle/sphere. For θ = 1, it's a halfspace (the perpendicular bisector of a and b).\n\n## Summary Table\n\n| Part | Set | Convex? | Characterization |\n|------|-----|---------|------------------|\n| (a) | Slab | ✓ | Intersection of 2 halfspaces |\n| (b) | Box | ✓ | Product of intervals / Intersection of 2n halfspaces |\n| (c) | Wedge | ✓ | Intersection of 2 halfspaces |\n| (d) | Closer to a than S | ✓ | Intersection of infinitely many halfspaces |\n| (e) | Closer to S than T | ✗ | Counterexample: S={-1,1}, T={0} |\n| (f) | x + S₂ ⊆ S₁ | ✓ | Intersection of translates of S₁ |\n| (g) | |x-a| ≤ θ|x-b| | ✓ | Ball (θ<1) or halfspace (θ=1) |\n\n## Key Insights\n\n1. **Halfspaces are fundamental**: Many convex sets are intersections of halfspaces\n2. **Squaring technique**: For Euclidean norms, squaring |x-a| ≤ |x-b| yields linear inequalities\n3. **Voronoi cells**: Part (d) defines a generalized Voronoi cell, which is always convex\n4. **Not always convex**: Part (e) shows that similar-looking distance conditions can be nonconvex\n5. **Erosion/Minkowski**: Part (f) is the \"erosion\" operation, dual to Minkowski sum (dilation)\n6. **Apollonius circles**: Part (g) gives circles/spheres for θ < 1, halfspaces for θ = 1\n7. **Complete the square**: Quadratic inequalities can often be rewritten as norm balls",
      "learningObjectives": [
        "halfspace-intersections",
        "distance-sets",
        "voronoi-cells",
        "apollonius-circles",
        "completing-the-square",
        "counterexamples",
        "erosion-dilation",
        "convexity-verification"
      ],
      "estimatedTime": 40,
      "relatedProblems": ["02-001", "02-004", "02-005"],
      "notes": "This problem is a comprehensive exercise in recognizing convex sets and proving/disproving convexity. It covers standard geometric objects (slabs, boxes, wedges) and more subtle distance-based sets. The key techniques are: (1) writing sets as intersections of simpler convex sets, (2) using algebraic manipulations (squaring, completing the square) to reveal structure, and (3) constructing explicit counterexamples for nonconvex cases. Part (g) on Apollonius circles is particularly elegant—the complete-the-square technique reveals that the set is a ball for θ < 1 and a halfspace for θ = 1."
    }
  ]
}
