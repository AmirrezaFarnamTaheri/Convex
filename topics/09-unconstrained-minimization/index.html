<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>09. Algorithms I: Unconstrained Minimization — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/styles.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
</head>
<body>
  <!-- Header with navigation -->
  <header class="site-header">
    <div class="container header-inner">
      <div class="brand">
        <img src="../../static/assets/branding/logo.svg" class="logo" alt="logo" />
        <a href="../../index.html" style="text-decoration:none;color:inherit">
          <strong>Convex Optimization</strong>
        </a>
      </div>
      <nav class="nav">
        <a href="../../index.html">All Lectures</a>
        <a href="#widgets">Interactive</a>
        <a href="#readings">Readings</a>
      </nav>
    </div>
  </header>

  <!-- Main content -->
  <main class="container" style="padding: 32px 0 60px;">
    <!-- Lecture header -->
    <article class="card" style="margin-bottom: 32px;">
      <h1 style="margin-top: 0;">09. Algorithms I: Unconstrained Minimization</h1>
      <div class="meta">
        Date: 2025-12-16 · Duration: 90 min · Tags: algorithms, unconstrained
      </div>

      <!-- Brief introduction -->
      <section style="margin-top: 16px;">
        <p><strong>Overview:</strong> Gradient descent, Newton's method, step size selection, convergence rates.</p>
        <p><strong>Prerequisites:</strong> <a href="../03-convex-functions/index.html">Lecture 03</a></p>
      </section>
    </article>

    <!-- Learning objectives -->
    <section class="card" style="margin-bottom: 32px;">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you should understand:</p>
      <ul style="line-height: 1.8;">
        <li>The gradient descent algorithm and its convergence properties.</li>
        <li>Newton's method and its convergence properties.</li>
        <li>The trade-offs between gradient descent and Newton's method.</li>
      </ul>
    </section>

    <!-- Main lecture content -->
    <section class="card" style="margin-bottom: 32px;">
      <h2>Key Concepts</h2>

      <h3>Concept 1: Gradient Descent</h3>
      <p>The gradient descent algorithm is an iterative method for finding the minimum of a function. The update rule is $x_{k+1} = x_k - t_k \nabla f(x_k)$.</p>
      <p>Visual intuition: We are taking steps in the direction of the negative gradient.</p>

      <!-- Include images as needed -->
      <figure style="margin: 16px 0; text-align: center;">
        <img src="../../static/assets/topics/09-unconstrained-minimization/gd-trajectory.svg" alt="The trajectory of gradient descent" style="max-width: 100%; height: auto;" />
        <figcaption style="font-size: 13px; color: var(--muted); margin-top: 8px;">
          Gradient descent converges to a local minimum.
        </figcaption>
      </figure>
    </section>

    <!-- Interactive widgets -->
    <section class="card" id="widgets" style="margin-bottom: 32px;">
      <h2>Interactive Widgets</h2>
      <p>Below are tools to explore the concepts interactively. Try tweaking parameters to build intuition.</p>

      <!-- Widget 1 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Gradient Descent Visualizer</h3>
        <p>Visualize the trajectory of gradient descent for a 2D quadratic function.</p>
        <div id="widget-1" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 2 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">GD vs. Newton Race</h3>
        <p>Compare the convergence of gradient descent and Newton's method.</p>
        <div id="widget-2" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 3 (if exists) -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Convergence Rate Comparison</h3>
        <p>Plots the convergence rates of different first-order methods.</p>
        <div id="widget-3" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>
    </section>

    <!-- Readings -->
    <section class="card" id="readings" style="margin-bottom: 32px;">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Boyd & Vandenberghe, Convex Optimization:</strong> Chapter 9 — Unconstrained Minimization</li>
        <li><strong>Course slides:</strong> [Link if available]</li>
      </ul>
    </section>

    <!-- Example problems -->
    <section class="card" style="margin-bottom: 32px;">
      <h2>Example Problems</h2>

      <h3>Example 1: Solving a least squares problem with gradient descent</h3>
      <p>[Problem statement and worked solution]</p>

      <h3>Example 2: Solving a logistic regression problem with Newton's method</h3>
      <p>[Problem statement and worked solution]</p>
    </section>

    <!-- Exercises (optional) -->
    <section class="card" style="margin-bottom: 32px;">
      <h2>Exercises</h2>
      <p>Try these problems to deepen your understanding. Solutions are provided at the end of the course.</p>
      <ol style="line-height: 2;">
        <li>What is the convergence rate of gradient descent?</li>
        <li>What are the advantages and disadvantages of Newton's method?</li>
      </ol>
    </section>
  </main></div>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">
        © <span id="year"></span> Convex Optimization Course ·
        <a href="../../README.md" style="color: var(--brand);">About</a>
      </p>
    </div>
  </footer>

  <!-- Load Pyodide for Python widgets (optional) -->
  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>

  <!-- Widget loaders -->
  <script type="module">
    import { initGradientDescentVisualizer } from './widgets/js/gradient-descent-visualizer.js';
    initGradientDescentVisualizer('widget-1');
  </script>
  <script type="module">
    import { initGDvsNewton } from './widgets/js/gd-vs-newton.js';
    initGDvsNewton('widget-2');
  </script>
  <script type="module">
    import { initConvergenceRate } from './widgets/js/convergence-rate.js';
    initConvergenceRate('widget-3');
  </script>

  <!-- Global utilities -->
  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/theme-switcher.js"></script>
<script src="../../static/js/toc.js"></script>
<script src="../../static/js/theme-switcher.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
</body>
</html>
