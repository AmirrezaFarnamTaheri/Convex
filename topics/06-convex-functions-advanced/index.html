<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>06. Convex Functions: Advanced Topics — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../05-convex-functions-basics/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../07-convex-problems-standard/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>06. Convex Functions: Advanced Topics</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: conjugate, quasiconvex, log-concave, advanced</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture explores advanced topics in convex analysis that bridge theory and modern applications. We examine the convex conjugate (Fenchel conjugate) and its central role in duality, define quasi-convex functions and their properties, and study log-concave functions, which are essential for probabilistic modeling and volume approximation.</p>
        <p><strong>Prerequisites:</strong> <a href="../05-convex-functions-basics/index.html">Lecture 05: Convex Functions Basics</a>.</p>
        <p><strong>Forward Connections:</strong> The convex conjugate is the key tool for deriving dual problems in <a href="../09-duality/index.html">Lecture 09</a>. Quasi-convexity appears in fractional programming problems in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Compute Conjugates:</b> Derive the convex conjugate for standard functions like norms, quadratics, and log-sum-exp.</li>
        <li><b>Apply Duality Mappings:</b> Connect primal domain properties to dual domain constraints via conjugation.</li>
        <li><b>Analyze Quasi-Convexity:</b> Identify functions defined by convex sublevel sets and apply operations that preserve this property.</li>
        <li><b>Work with Log-Concavity:</b> Recognize log-concave probability distributions and use their properties (integration, marginalization) in modeling.</li>
      </ul>
    </section>

    <article>
      <section class="section-card" id="section-1">
      <h2>1. The Convex Conjugate (Fenchel Conjugate)</h2>

      <h3>1.1 Definition and Geometric Meaning</h3>
      <p>Start with a function $f : \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$. Allowing $+\infty$ is a standard way of encoding hard constraints ("$x$ not allowed"). The <a href="#" class="definition-link" data-term="convex conjugate">Fenchel conjugate</a> is defined as:</p>
      $$
      \boxed{f^*(y) := \sup_{x\in\mathrm{dom} f} \big( y^\top x - f(x) \big), \quad y\in\mathbb{R}^n}
      $$

      <h4>Interpretation</h4>
      <p>For a fixed slope $y$, consider the <b>affine</b> family of functions in $x$: $\ell_{y,c}(x) = y^\top x - c$. We want to find the largest shift such that the line lies below the function.
      <br>Require $\ell_{y,c}(x) \le f(x)$ for all $x$. Rearranging: $c \ge y^\top x - f(x)$ for all $x$.
      <br>The <b>smallest</b> such $c$ that works is $c_{\min}(y) = \sup_x (y^\top x - f(x)) = f^*(y)$.</p>

      <div class="insight">
        <h4>Geometric Intuition</h4>
        <p>For each slope $y$, $f^*(y)$ is the minimal vertical shift so that the line $x \mapsto y^\top x - f^*(y)$ lies everywhere <b>below</b> the graph of $f$. Equivalently, $f^*(y)$ is the <b>maximal gap</b> between the linear function $y^\top x$ and the graph of $f$.</p>
      </div>

      <h3>1.2 Why $f^*$ is Always Convex</h3>
      <p>A crucial property is that $f^*$ is convex <b>regardless</b> of whether $f$ is convex.</p>
      <div class="proof-box">
        <h4>Proof</h4>
        <p>Take any fixed $x$. As a function of $y$, the expression $\phi_x(y) := y^\top x - f(x)$ is <b>affine</b> in $y$ (linear term $y^\top x$ plus constant $-f(x)$).
        <br>Now $f^*$ is the pointwise supremum over all these affine functions:
        $$ f^*(y) = \sup_{x} \phi_x(y) $$
        <b>Fact:</b> The pointwise supremum of any family of convex (or affine) functions is convex.
        <br>Explicitly, if $g(y) = \sup_i g_i(y)$ where each $g_i$ is convex, then for any $\theta \in [0,1]$:
        $$
        \begin{aligned}
        g(\theta y_1+(1-\theta)y_2) &= \sup_i g_i(\theta y_1+(1-\theta)y_2) \\
        &\le \sup_i \big( \theta g_i(y_1) + (1-\theta)g_i(y_2) \big) \\
        &\le \theta \sup_i g_i(y_1) + (1-\theta)\sup_i g_i(y_2) \\
        &= \theta g(y_1) + (1-\theta)g(y_2)
        \end{aligned}
        $$
        Thus, $f^*$ is convex for <b>any</b> $f$. The conjugate lives in the "convex world" even if $f$ doesn't.</p>
      </div>

      <h3>1.3 1D Examples in Detail</h3>

      <div class="example">
        <h4>(a) Affine function $f(x) = ax + b$</h4>
        <p>Compute $f^*(y) = \sup_{x} (yx - (ax+b)) = \sup_x ((y-a)x - b)$.</p>
        <ul>
            <li><b>Case 1: $y \ne a$.</b> The term $((y-a)x - b)$ is a line with nonzero slope. As $x \to \pm \infty$, this goes to $+\infty$. So $f^*(y) = +\infty$.</li>
            <li><b>Case 2: $y = a$.</b> The term is constant $-b$. Supremum is $-b$.</li>
        </ul>
        <p><b>Result:</b> $f^*(y) = -b$ if $y=a$, else $+\infty$. Domain is the singleton $\{a\}$. (Indicator of a single slope).</p>
      </div>

      <div class="example">
        <h4>(b) Negative Log: $f(x) = -\log x$ on $(0,\infty)$</h4>
        <p>Maximize $g(x) = yx + \log x$. Derivative $g'(x) = y + 1/x$. Set to 0: $x^* = -1/y$.
        <br>Since $x^* > 0$, we must have $y < 0$. $g''(x) = -1/x^2 < 0$ (concave max).
        <br>Value: $y(-1/y) + \log(-1/y) = -1 - \log(-y)$.
        <br>If $y \ge 0$, $yx + \log x \to \infty$ as $x \to \infty$.
        <br><b>Result:</b> $f^*(y) = -1 - \log(-y)$ for $y < 0$. Domain $(-\infty, 0)$.</p>
      </div>

      <div class="example">
        <h4>(c) Exponential: $f(x) = e^x$ on $\mathbb{R}$</h4>
        <p>Maximize $g(x) = yx - e^x$. Derivative $g'(x) = y - e^x = 0 \implies x^* = \log y$.
        <br>Requires $y > 0$. Value: $y \log y - e^{\log y} = y \log y - y$.
        <br>If $y=0$, $\sup (-e^x) = 0$ (as $x \to -\infty$).
        <br>If $y < 0$, let $x \to -\infty$. Then $e^x \to 0$ and $yx \to +\infty$ (negative times negative). Sup is $+\infty$.
        <br><b>Result:</b> $f^*(y) = y \log y - y$ for $y \ge 0$ (with $0 \log 0 = 0$), else $\infty$. Domain $[0, \infty)$.</p>
      </div>

      <div class="example">
        <h4>(d) Inverse: $f(x) = 1/x$ on $(0,\infty)$</h4>
        <p>Maximize $g(x) = yx - 1/x$. Derivative $g'(x) = y + 1/x^2 = 0 \implies x^2 = -1/y$.
        <br>Requires $y < 0$. Solution $x^* = \sqrt{-1/y}$.
        <br>Value: $y\sqrt{-1/y} - \sqrt{-y} = -\sqrt{-y} - \sqrt{-y} = -2\sqrt{-y}$.
        <br>If $y \ge 0$, $yx - 1/x \to \infty$ as $x \to \infty$.
        <br><b>Result:</b> $f^*(y) = -2\sqrt{-y}$ for $y < 0$. Domain $(-\infty, 0)$.</p>
      </div>

      <h3>1.4 Quadratic and Matrix Examples</h3>

      <div class="example">
        <h4>(a) Quadratic: $f(x) = \frac{1}{2} x^\top Q x$ with $Q \succ 0$</h4>
        <p>Maximize $\phi(x) = y^\top x - \frac{1}{2} x^\top Q x$. This is a concave quadratic.
        <br>Gradient $\nabla \phi(x) = y - Qx = 0 \implies x^* = Q^{-1}y$.
        <br>Value: $y^\top (Q^{-1}y) - \frac{1}{2} (Q^{-1}y)^\top Q (Q^{-1}y) = y^\top Q^{-1} y - \frac{1}{2} y^\top Q^{-1} y = \frac{1}{2} y^\top Q^{-1} y$.
        <br><b>Result:</b> Quadratic with matrix $Q$ conjugates to quadratic with $Q^{-1}$.</p>
      </div>

      <div class="example">
        <h4>(b) Log-Determinant: $f(X) = -\log \det X$ on $\mathbb{S}^n_{++}$</h4>
        <p>Maximize $\mathrm{tr}(YX) + \log \det X$ over $X \succ 0$.
        <br>Gradient: $Y + X^{-1} = 0 \implies X = -Y^{-1}$.
        <br>Requires $Y$ invertible and $-Y^{-1} \succ 0 \implies Y \prec 0$ (negative definite).
        <br>Value: $\mathrm{tr}(Y(-Y^{-1})) + \log \det (-Y^{-1}) = -n - \log \det(-Y)$.
        <br><b>Result:</b> $f^*(Y) = -\log \det(-Y) - n$ on domain $\mathbb{S}^n_{--}$.</p>
      </div>

      <div class="example">
        <h4>(c) Indicator $\leftrightarrow$ Support Function</h4>
        <p>Let $I_S(x)$ be the indicator of a set $S$ (0 if $x \in S$, $+\infty$ else).
        <br>$I_S^*(y) = \sup_x (y^\top x - I_S(x)) = \sup_{x \in S} (y^\top x)$.
        <br><b>Result:</b> $I_S^* = \sigma_S$, the support function of $S$.</p>
      </div>

      <div class="example">
        <h4>(d) Norm $\leftrightarrow$ Indicator of Dual Ball</h4>
        <p>Let $f(x) = \|x\|$. Dual norm definition: $\|y\|_* = \sup_{\|x\|\le 1} y^\top x$.
        <br>$f^*(y) = \sup_x (y^\top x - \|x\|)$.
        <br>Case 1: $\|y\|_* \le 1$. Then $y^\top x \le \|y\|_* \|x\| \le \|x\|$. So $y^\top x - \|x\| \le 0$. Max is 0 (at $x=0$).
        <br>Case 2: $\|y\|_* > 1$. There exists $x_0$ with $\|x_0\|=1$ and $y^\top x_0 > 1$. Let $x = t x_0$. Value $t(y^\top x_0 - 1) \to \infty$.
        <br><b>Result:</b> $f^*$ is the indicator of the unit ball of the dual norm $\|\cdot\|_*$.
        $$ f^*(y) = \begin{cases} 0 & \|y\|_* \le 1 \\ +\infty & \|y\|_* > 1 \end{cases} $$</p>
      </div>

      <h3>1.5 Algebra Rules for Conjugates</h3>
      <table class="data-table">
        <tr><th>Operation</th><th>Function $g(x)$</th><th>Conjugate $g^*(y)$</th></tr>
        <tr><td>Scaling</td><td>$a f(x) + b \quad (a>0)$</td><td>$a f^*(y/a) - b$</td></tr>
        <tr><td>Affine Precomp</td><td>$f(Ax+b)$ (invertible A)</td><td>$f^*(A^{-\top} y) - b^\top (A^{-\top} y)$</td></tr>
        <tr><td>Sum (Independent)</td><td>$f_1(u) + f_2(v)$</td><td>$f_1^*(y_1) + f_2^*(y_2)$</td></tr>
        <tr><td>General Composition</td><td>$f(Ax+b)$ (general A)</td><td>$f^*(z) - b^\top z$ such that $A^\top z = y$ (infimum over z)</td></tr>
      </table>

      <h3>1.6 Fenchel Inequality and Biconjugate</h3>
      <p>From the definition $f^*(y) \ge y^\top x - f(x)$, we immediately get <b>Fenchel's Inequality</b>:</p>
      $$ \boxed{ f(x) + f^*(y) \ge x^\top y } $$
      <p>Equality holds exactly when $y \in \partial f(x)$ (for convex $f$).</p>

      <h4>The Biconjugate $f^{**}$</h4>
      <p>The conjugate of the conjugate is $f^{**}(x) = \sup_y (x^\top y - f^*(y))$.
      <br><b>Theorem:</b> $f^{**} \le f$ always. If $f$ is closed and convex, then $f^{**} = f$.
      <br><i>Geometric Intuition:</i> $\mathrm{epi}(f^{**})$ is the closed convex hull of $\mathrm{epi}(f)$. Conjugation encodes all supporting hyperplanes; taking it again reconstructs the convex envelope from these planes.</p>

      <h3>1.7 Legendre Transform</h3>
      <p>If $f$ is convex and differentiable, and $\nabla f$ is injective, we can define the conjugate via the gradient condition.
      <br>Maximize $y^\top x - f(x) \implies y = \nabla f(x)$. Let $x^* = (\nabla f)^{-1}(y)$.
      $$ f^*(y) = y^\top x^* - f(x^*) $$
      If we parametrize by $z$ where $y = \nabla f(z)$:
      $$ f^*(\nabla f(z)) = z^\top \nabla f(z) - f(z) $$
      This is the classical Legendre transform used in mechanics (Lagrangian $\leftrightarrow$ Hamiltonian).</p>
    </section>

    <section class="section-card" id="section-2">
        <h2>2. Quasiconvex Functions</h2>

        <h3>2.1 Definition: Sublevel Sets</h3>
        <p>A function $f: \mathbb{R}^n \to \mathbb{R}$ is <a href="#" class="definition-link">quasiconvex</a> if its domain is convex and all its <b>sublevel sets</b> are convex:</p>
        $$
        S_\alpha := \{x \in \mathrm{dom}\, f \mid f(x) \le \alpha\}
        $$
        <p>is a convex set for every $\alpha \in \mathbb{R}$.
        <br><i>Note:</i> Convex $\implies$ Quasiconvex, but not conversely. Quasiconvexity allows "bendy" graphs, as long as the "valley" shape is maintained.</p>

        <h3>2.2 Simple 1D Examples</h3>
        <div class="example">
            <h4>(a) Logarithm: $f(x) = \log x$</h4>
            <p>Sublevel set $\{x > 0 \mid \log x \le \alpha\} = \{x \mid x \le e^\alpha\} = (0, e^\alpha]$. This is a convex interval.
            <br>So $\log x$ is quasiconvex. (It is also concave, so $-\log x$ is convex).</p>
        </div>
        <div class="example">
            <h4>(b) Ceiling: $f(x) = \lceil x \rceil$</h4>
            <p>Sublevel set $\{x \mid \lceil x \rceil \le k\} = (-\infty, k]$. This is an interval.
            <br>So $\lceil x \rceil$ is quasiconvex. (It is step-like and definitely not convex).</p>
        </div>

        <h3>2.3 Vector Examples</h3>
        <div class="example">
            <h4>1. Length of a Vector</h4>
            <p>Define len$(x) = \max\{i \mid x_i \ne 0\}$ (index of last nonzero element).
            <br>Sublevel set $\{x \mid \text{len}(x) \le k\} = \{x \mid x_{k+1} = \dots = x_n = 0\}$.
            <br>This is a linear subspace, hence convex. So len$(x)$ is quasiconvex.</p>
        </div>
        <div class="example">
            <h4>2. Bilinear: $f(x_1, x_2) = x_1 x_2$ on $\mathbb{R}^2_{++}$</h4>
            <p>Hessian is indefinite (not convex).
            <br>Superlevel set $\{x \in \mathbb{R}^2_{++} \mid x_1 x_2 \ge \alpha\}$ is convex (bounded by hyperbola).
            <br>Thus $f$ is <b>quasiconcave</b> on the positive quadrant.</p>
        </div>
        <div class="example">
            <h4>3. Linear-Fractional: $f(x) = \frac{a^\top x + b}{c^\top x + d}$</h4>
            <p>Defined where $c^\top x + d > 0$.
            <br>Sublevel set: $\frac{a^\top x + b}{c^\top x + d} \le \alpha \iff a^\top x + b \le \alpha(c^\top x + d)$.
            <br>This is a linear inequality (halfspace), hence convex.
            <br>Thus $f$ is quasiconvex (and similarly quasiconcave, so <b>quasilinear</b>).</p>
        </div>
        <div class="example">
            <h4>4. Distance Ratio</h4>
            <p>$f(x) = \frac{\|x-a\|}{\|x-b\|}$. Sublevel set $\|x-a\| \le \alpha \|x-b\|$.
            <br>Squared: $\|x-a\|^2 \le \alpha^2 \|x-b\|^2$.
            <br>For $\alpha \le 1$, this defines a convex set (ball). For $\alpha > 1$, it is the complement of a ball (non-convex).
            <br>So $f$ is quasiconvex on the domain where $f(x) \le 1$ (closer to $a$ than $b$).</p>
        </div>
        <div class="example">
            <h4>5. Internal Rate of Return (IRR)</h4>
            <p>Cashflows $x = (x_0, \dots, x_n)$. Present value $PV(x, r) = \sum x_i (1+r)^{-i}$.
            <br>IRR$(x) = \inf \{r \ge 0 \mid PV(x, r) = 0\}$.
            <br>IRR is a <b>quasiconcave</b> function of the cashflow vector $x$. (Superlevel sets are intersections of halfspaces).</p>
        </div>

        <h3>2.4 Second-Order Condition</h3>
        <p>For a twice-differentiable function $f$, convexity requires $\nabla^2 f(x) \succeq 0$.
        <br>Quasiconvexity only requires positive curvature <b>along the level sets</b>.
        <br><b>Condition:</b> If $y^\top \nabla f(x) = 0$, then $y^\top \nabla^2 f(x) y \ge 0$.
        <br><i>Interpretation:</i> Directions $y$ tangent to the contour ($y \perp \nabla f$) must not see negative curvature (hilltop). Valleys are okay, but you can't have a local maximum or saddle point that curves down along the contour.</p>

        <h3>2.5 Operations Preserving Quasiconvexity</h3>
        <ul>
            <li><b>Max:</b> $f(x) = \max_i f_i(x)$ is quasiconvex (Intersection of convex sublevel sets).</li>
            <li><b>Composition:</b> $g(h(x))$ is quasiconvex if $h$ is quasiconvex and $g$ is <b>non-decreasing</b>.</li>
            <li><b>Warning:</b> Sums of quasiconvex functions are generally <b>not</b> quasiconvex.</li>
        </ul>
    </section>

    <section class="section-card" id="section-3">
        <h2>3. Log-Concave and Log-Convex Functions</h2>

        <h3>3.1 Definitions</h3>
        <p>Let $f: \mathbb{R}^n \to \mathbb{R}_{++}$.
        <ul>
            <li>$f$ is <b>log-concave</b> if $\log f(x)$ is concave.</li>
            <li>$f$ is <b>log-convex</b> if $\log f(x)$ is convex.</li>
        </ul>
        Equivalently, for log-concavity (multiplicative form):
        $$ f(\theta x + (1-\theta)y) \ge f(x)^\theta f(y)^{1-\theta} $$
        Log-concavity is crucial in probability (unimodality, tail bounds).</p>

        <h3>3.2 Examples</h3>
        <div class="example">
            <h4>1. Uniform Distribution on Convex Set</h4>
            <p>Let $C$ be convex. $f(x) = 1/\alpha$ if $x \in C$, else 0.
            <br>$\log f(x) = -\log \alpha$ on $C$, $-\infty$ outside.
            <br>This is a concave function (indicator of convex set). So uniform density is log-concave.</p>
        </div>
        <div class="example">
            <h4>2. Wishart Distribution</h4>
            <p>Density $f(X) \propto (\det X)^{k} e^{-\mathrm{tr}(\Sigma^{-1}X)}$.
            <br>$\log f(X) = c + k \log \det X - \mathrm{tr}(\Sigma^{-1}X)$.
            <br>$\log \det$ is concave; trace is linear. Sum is concave.
            <br>Thus Wishart is log-concave.</p>
        </div>

        <h3>3.3 Integration Rules</h3>
        <p>Log-convexity and log-concavity behave nicely under integration.</p>
        <div class="theorem-box">
            <h4>Preservation of Log-Convexity</h4>
            <p>If $f(x, y)$ is log-convex in $x$ for each $y$, then $g(x) = \int f(x, y) dy$ is log-convex (Artin's Theorem).
            <br>Example: The Gamma function $\Gamma(x) = \int t^{x-1} e^{-t} dt$ is log-convex.</p>
        </div>
        <div class="theorem-box">
            <h4>Preservation of Log-Concavity (Prékopa-Leindler)</h4>
            <p>If $f(x, y)$ is jointly log-concave, then the marginal $g(x) = \int f(x, y) dy$ is log-concave.
            <br><b>Consequence:</b> Convolution of log-concave functions is log-concave ($f*g$).</p>
        </div>

        <h3>3.4 Probability Examples</h3>
        <div class="example">
            <h4>(a) Hitting a Convex Set</h4>
            <p>Let $W$ have log-concave density $p$. Let $C$ be convex.
            <br>$f(x) = \mathbb{P}(x + W \in C) = \int \mathbf{1}_C(u) p(u-x) du$.
            <br>Integrand is log-concave in $(x, u)$ (product of log-concave $\mathbf{1}_C$ and shifted $p$).
            <br>Thus $f(x)$ is log-concave. Probability of hitting a target falls off log-concavely.</p>
        </div>
        <div class="example">
            <h4>(b) Cumulative Distribution Function (CDF)</h4>
            <p>If PDF $f$ is log-concave, then CDF $F(x) = \int_{-\infty}^x f(t) dt$ is log-concave.
            <br>Proof: $F(x) = \int \mathbf{1}_{t \le x} f(t) dt$. The indicator $\mathbf{1}_{t \le x}$ is log-concave in $(x, t)$ (indicator of halfspace).
            <br>Example: Gaussian CDF $\Phi(x)$ is log-concave.</p>
        </div>
    </section>

    <section class="section-card" id="section-4">
      <h2>4. Review & Cheat Sheet</h2>
      <h3>Conjugate Transformations</h3>
      <table class="data-table">
        <tr><th>Primal $f(x)$</th><th>Conjugate $f^*(y)$</th><th>Domain of $f^*$</th></tr>
        <tr><td>$\frac{1}{2}x^\top Q x$ ($Q \succ 0$)</td><td>$\frac{1}{2}y^\top Q^{-1} y$</td><td>$\mathbb{R}^n$</td></tr>
        <tr><td>$-\log x$</td><td>$-1 - \log(-y)$</td><td>$y < 0$</td></tr>
        <tr><td>$e^x$</td><td>$y \log y - y$</td><td>$y \ge 0$</td></tr>
        <tr><td>$1/x$ ($x>0$)</td><td>$-2\sqrt{-y}$</td><td>$y < 0$</td></tr>
        <tr><td>$I_C(x)$</td><td>$\sigma_C(y)$ (Support)</td><td>$\mathbb{R}^n$</td></tr>
        <tr><td>$\|x\|$</td><td>$I_{B_*}(y)$ (Dual Ball)</td><td>$\|y\|_* \le 1$</td></tr>
      </table>

      <h3>Key Concepts</h3>
      <ul>
          <li><b>Conjugate $f^*$:</b> Best linear lower bound (support function of epigraph). Always convex.</li>
          <li><b>Fenchel Inequality:</b> $f(x) + f^*(y) \ge x^\top y$.</li>
          <li><b>Quasiconvex:</b> Convex sublevel sets. $f(\theta x + (1-\theta)y) \le \max(f(x), f(y))$.</li>
          <li><b>Log-Concave:</b> $\log f$ is concave. Closed under product, marginals, convolution.</li>
      </ul>
    </section>

    <section class="section-card" id="section-5">
      <h2><i data-feather="edit-3"></i> 5. Exercises</h2>

<div class="problem">
  <h3>P6.1 — Conjugate of Norm Squared</h3>
  <p>Compute the conjugate of $f(x) = \frac{1}{2}\|x\|^2$ for a general norm $\|\cdot\|$. Show it is $\frac{1}{2}\|y\|_*^2$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Definition.</strong>
    $f^*(y) = \sup_x (y^\top x - \frac{1}{2}\|x\|^2)$.</div>
    <div class="proof-step"><strong>Step 2: Dual Norm Bound.</strong>
    By definition of the dual norm, $y^\top x \le \|y\|_* \|x\|$.
    Let $u = \|x\|$. Then $y^\top x - \frac{1}{2}\|x\|^2 \le \|y\|_* u - \frac{1}{2}u^2$.</div>
    <div class="proof-step"><strong>Step 3: Optimize Scalar $u$.</strong>
    The function $g(u) = \|y\|_* u - \frac{1}{2}u^2$ is a concave quadratic.
    Maximum occurs at $g'(u) = \|y\|_* - u = 0 \implies u^* = \|y\|_*$.
    Max value: $\|y\|_*^2 - \frac{1}{2}\|y\|_*^2 = \frac{1}{2}\|y\|_*^2$.</div>
    <div class="proof-step"><strong>Step 4: Tightness.</strong>
    Can we achieve this bound? By definition of the dual norm, there exists a vector $x_0$ with $\|x_0\|=1$ such that $y^\top x_0 = \|y\|_*$.
    Choose $x = \|y\|_* x_0$. Then $\|x\| = \|y\|_*$.
    $y^\top x = \|y\|_* (y^\top x_0) = \|y\|_*^2$.
    Objective: $\|y\|_*^2 - \frac{1}{2}\|y\|_*^2 = \frac{1}{2}\|y\|_*^2$.
    Thus the supremum is exactly $\frac{1}{2}\|y\|_*^2$.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.2 — Quasi-Convexity of Ceiling</h3>
  <p>Show that $f(x) = \lceil x \rceil$ (on $\mathbb{R}$) is quasi-convex.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Definition of Quasi-Convexity.</strong>
    A function is quasi-convex if its sublevel sets $S_\alpha = \{x \mid f(x) \le \alpha\}$ are convex for all $\alpha \in \mathbb{R}$.</div>
    <div class="proof-step"><strong>Step 2: Analyze Sublevel Sets.</strong>
    Condition: $\lceil x \rceil \le \alpha$.
    Since $\lceil x \rceil$ is an integer, let $k = \lfloor \alpha \rfloor$. The condition is $\lceil x \rceil \le k$.
    This is equivalent to $x \le k$ (since if $x > k$, $\lceil x \rceil \ge k+1$).
    So $S_\alpha = (-\infty, \lfloor \alpha \rfloor]$.</div>
    <div class="proof-step"><strong>Step 3: Conclusion.</strong>
    The set $(-\infty, k]$ is an interval, which is a convex set in $\mathbb{R}$.
    Therefore, $f$ is quasi-convex.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.3 — Softmax Analysis</h3>
  <p>For $f(x) = \log(\sum_{i=1}^n e^{x_i})$: <ol type="a"><li>Show $\max x_i \le f(x) \le \max x_i + \log n$.</li><li>Show $f$ is convex via Hessian analysis.</li></ol></p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Part A: Bounds.</strong>
    Let $x_{\max} = \max_i x_i$.
    Lower bound: $\sum e^{x_i} \ge e^{x_{\max}}$.
    $f(x) \ge \log(e^{x_{\max}}) = x_{\max}$.
    Upper bound: $\sum e^{x_i} = e^{x_{\max}} \sum e^{x_i - x_{\max}}$.
    Since $x_i - x_{\max} \le 0$, each term $e^{x_i - x_{\max}} \le 1$.
    Sum is bounded by $n$.
    $f(x) = x_{\max} + \log(\sum e^{x_i - x_{\max}}) \le x_{\max} + \log n$.</div>
    <div class="proof-step"><strong>Part B: Hessian.</strong>
    Gradient $\nabla f(x) = p$ where $p_i = e^{x_i}/\sum e^{x_k}$. Note $\mathbf{1}^\top p = 1, p > 0$.
    Hessian $\nabla^2 f(x) = \text{diag}(p) - pp^\top$.
    Let $v \in \mathbb{R}^n$.
    $v^\top \nabla^2 f v = \sum p_i v_i^2 - (p^\top v)^2 = \sum p_i v_i^2 - (\sum p_i v_i)^2$.</div>
    <div class="proof-step"><strong>Part C: Variance Interpretation.</strong>
    Consider a discrete random variable $Z$ that takes value $v_i$ with probability $p_i$ (where $p$ is the softmax vector defined above).
    <br>The term $\sum p_i v_i$ is the expected value $\mathbb{E}[Z]$.
    <br>The term $\sum p_i v_i^2$ is the second moment $\mathbb{E}[Z^2]$.
    <br>The expression for the quadratic form is $\mathbb{E}[Z^2] - (\mathbb{E}[Z])^2$, which is exactly the variance $\text{Var}(Z)$.
    <br>Since variance is always non-negative, $v^\top \nabla^2 f v \ge 0$. Thus $f$ is convex.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.4 — Concavity of Geometric Mean</h3>
  <p>Prove $G(x) = (\prod_{i=1}^n x_i)^{1/n}$ is concave on $\mathbb{R}^n_{++}$ using log-concavity properties.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Check Log-Concavity.</strong>
    Consider $\log G(x)$.
    $$ \log \left( \prod x_i^{1/n} \right) = \frac{1}{n} \sum_{i=1}^n \log x_i $$
    The function $\log x_i$ is concave. The sum of concave functions is concave.
    Thus $\log G(x)$ is a concave function.
    This means $G(x)$ is <b>log-concave</b>.</div>
    <div class="proof-step"><strong>Step 2: Homogeneity.</strong>
    $G(\alpha x) = (\prod (\alpha x_i))^{1/n} = (\alpha^n \prod x_i)^{1/n} = \alpha G(x)$.
    $G$ is homogeneous of degree 1.</div>
    <div class="proof-step"><strong>Step 3: Theorem.</strong>
    We show that a function $G$ that is log-concave and homogeneous of degree 1 is concave.
    We want to show $G(\theta x + (1-\theta)y) \ge \theta G(x) + (1-\theta)G(y)$.
    By log-concavity, we have:
    $$ G(\theta x + (1-\theta)y) \ge G(x)^\theta G(y)^{1-\theta} $$
    Let $\alpha = G(x)$ and $\beta = G(y)$. If either is 0, the inequality holds trivially (since $G \ge 0$). Assume $\alpha, \beta > 0$.
    Consider the normalized vectors $\tilde{x} = x/\alpha$ and $\tilde{y} = y/\beta$, so $G(\tilde{x}) = G(\tilde{y}) = 1$.
    We want to relate the weighted geometric mean to the weighted arithmetic mean.
    Actually, a simpler argument uses the AM-GM inequality directly on the log-concavity result.
    $$ G(\theta x + (1-\theta)y) \ge \alpha^\theta \beta^{1-\theta} $$
    This lower bound is the weighted geometric mean of the values. Concavity requires the weighted arithmetic mean.
    However, if we choose specific weights based on the function values, we can prove superadditivity $G(x+y) \ge G(x) + G(y)$, which implies concavity for homogeneous functions.
    <br><b>Formal Superadditivity Proof:</b>
    Let $x, y$ be such that $G(x)=\alpha, G(y)=\beta$.
    Let $\lambda = \frac{\alpha}{\alpha+\beta}$. Then $1-\lambda = \frac{\beta}{\alpha+\beta}$.
    Consider the point $z = \frac{x+y}{\alpha+\beta} = \lambda \frac{x}{\alpha} + (1-\lambda) \frac{y}{\beta}$.
    By log-concavity (and thus quasi-concavity), $G(z) \ge G(x/\alpha)^\lambda G(y/\beta)^{1-\lambda} = 1^\lambda 1^{1-\lambda} = 1$.
    By homogeneity, $G(x+y) = (\alpha+\beta) G(z) \ge \alpha + \beta = G(x) + G(y)$.
    Superadditivity plus homogeneity implies concavity.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.5 — Geometric Mean Cone</h3>
  <p>Show $K = \{x \in \mathbb{R}^n_+ \mid G(x) \ge \alpha A(x)\}$ is a convex cone for $\alpha \in [0,1]$, where $A(x) = \frac{1}{n}\sum x_i$ is the arithmetic mean.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Rewrite condition.</strong>
    $G(x) - \alpha A(x) \ge 0$.
    Let $h(x) = G(x) - \alpha A(x)$.</div>
    <div class="proof-step"><strong>Step 2: Check Concavity.</strong>
    $G(x)$ is concave (from P6.4).
    $A(x)$ is linear, so $-A(x)$ is concave.
    Since $\alpha \ge 0$, $-\alpha A(x)$ is concave.
    Thus $h(x)$ is a sum of concave functions, so $h$ is concave.</div>
    <div class="proof-step"><strong>Step 3: Superlevel Set.</strong>
    The set $K = \{x \mid h(x) \ge 0\}$ is the 0-superlevel set of a concave function.
    Superlevel sets of concave functions are convex sets.</div>
    <div class="proof-step"><strong>Step 4: Cone Property.</strong>
    $G(kx) = kG(x)$ and $A(kx) = kA(x)$ for $k \ge 0$.
    $G(kx) \ge \alpha A(kx) \iff kG(x) \ge k\alpha A(x) \iff G(x) \ge \alpha A(x)$ (for $k>0$).
    Thus $K$ is a cone.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.6 — Matrix Fractional Function</h3>
  <p>Prove convexity of $f(x, Y) = x^\top Y^{-1} x$ (for $Y \succ 0$) using the epigraph characterization.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Epigraph Definition.</strong>
    $(x, Y, t) \in \text{epi } f \iff t \ge x^\top Y^{-1} x$ and $Y \succ 0$.</div>
    <div class="proof-step"><strong>Step 2: Schur Complement.</strong>
    The inequality $t - x^\top Y^{-1} x \ge 0$ is the Schur complement condition for the block matrix:
    $$ M = \begin{bmatrix} Y & x \\ x^\top & t \end{bmatrix} \succeq 0 $$
    (Assuming $Y \succ 0$. If $Y \succeq 0$ singular, we need $x \in \text{range}(Y)$ etc, but usually defined on open domain).</div>
    <div class="proof-step"><strong>Step 3: Convexity of LMI.</strong>
    The set of matrices $\mathcal{S}_+^k$ is a convex cone (PSD cone).
    The map $(x, Y, t) \to M(x, Y, t)$ is linear.
    The inverse image of a convex set under a linear map is convex.
    Thus $\text{epi } f = \{(x, Y, t) \mid M(x, Y, t) \succeq 0\}$ is convex.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.7 — Fenchel's Inequality & Biconjugate</h3>
  <p>For $f(x) = \frac{1}{2}x^\top Q x$ with $Q \succ 0$: 1. Verify Fenchel's inequality. 2. Verify $f^{**} = f$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Part 1: Fenchel's Inequality.</strong>
    We need $f(x) + f^*(y) \ge x^\top y$.
    We found $f^*(y) = \frac{1}{2}y^\top Q^{-1} y$.
    LHS - RHS = $\frac{1}{2}x^\top Q x + \frac{1}{2}y^\top Q^{-1} y - x^\top y$.
    Let $u = Q^{1/2}x$ and $v = Q^{-1/2}y$. Then $x = Q^{-1/2}u, y = Q^{1/2}v$.
    $x^\top y = u^\top v$.
    Expression: $\frac{1}{2}u^\top u + \frac{1}{2}v^\top v - u^\top v = \frac{1}{2}\|u-v\|^2 \ge 0$.
    Verified.</div>
    <div class="proof-step"><strong>Part 2: Biconjugate.</strong>
    $f^{**}(x) = (f^*)^*(x)$. Let $g(y) = f^*(y) = \frac{1}{2}y^\top P y$ with $P = Q^{-1}$.
    The conjugate of a quadratic $\frac{1}{2}y^\top P y$ is $\frac{1}{2}x^\top P^{-1} x$.
    $P^{-1} = (Q^{-1})^{-1} = Q$.
    So $f^{**}(x) = \frac{1}{2}x^\top Q x = f(x)$. Verified.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.8 — Quasiconvexity of Distance Ratio</h3>
  <p>Show $S_\theta = \{x \mid \|x-a\|_2 \le \theta \|x-b\|_2\}$ is convex for $\theta \le 1$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Square the condition.</strong>
    $\|x-a\|^2 \le \theta^2 \|x-b\|^2$.
    $(x-a)^\top (x-a) \le \theta^2 (x-b)^\top (x-b)$.
    $x^\top x - 2a^\top x + a^\top a \le \theta^2 (x^\top x - 2b^\top x + b^\top b)$.</div>
    <div class="proof-step"><strong>Step 2: Group terms.</strong>
    $(1 - \theta^2) x^\top x - 2(a - \theta^2 b)^\top x + (\|a\|^2 - \theta^2 \|b\|^2) \le 0$.</div>
    <div class="proof-step"><strong>Step 3: Analyze coefficient.</strong>
    Let $q(x)$ be this quadratic expression. The quadratic term is $(1-\theta^2)\|x\|^2$.
    If $\theta \le 1$, then $1-\theta^2 \ge 0$.
    The sublevel set of a convex quadratic ($P \succeq 0$) is convex (an ellipsoid or halfspace).
    Thus $S_\theta$ is convex. (If $\theta > 1$, it's the complement of an ellipsoid, which is non-convex).</div>
  </div>
</div>

<div class="problem">
  <h3>P6.9 — Log-Concavity of Probability Measures</h3>
  <p>If $p(x)$ is a log-concave probability density, show that the function $f(x) = \mathbb{P}(x+W \in C)$ is log-concave, where $W \sim p$ and $C$ is a convex set.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Write as Integral.</strong>
    $f(x) = \int_{\mathbb{R}^n} I_C(y) p(y-x) dy$.
    Wait, let's check definition. $x+W \in C \iff W \in C-x$.
    Prob = $\int_{C-x} p(w) dw = \int_{\mathbb{R}^n} I_{C-x}(w) p(w) dw$.
    Let $u = w+x$. Then $w = u-x$. Range $w \in C-x \iff u \in C$.
    $f(x) = \int I_C(u) p(u-x) du$.</div>
    <div class="proof-step"><strong>Step 2: Identify as Convolution.</strong>
    This is the convolution of two functions: $g(x) = I_C(x)$ (indicator 1 if in C, 0 else) and $h(x) = p(-x)$.
    Wait, convolution is $(g * h)(x) = \int g(u) h(x-u) du$.
    The form $\int g(u) p(u-x) du$ is a correlation or convolution with reflected argument.
    Let's use the Prékopa-Leindler inequality or the theorem that <b>convolution of log-concave functions is log-concave</b>.</div>
    <div class="proof-step"><strong>Step 3: Check Log-Concavity of Components.</strong>
    $p(x)$ is log-concave (given).
    $I_C(x)$ is the indicator function of a convex set (1 on C, 0 else).
    Is it log-concave? $\log I_C(x)$ is 0 on C, $-\infty$ else.
    Since $C$ is convex, this is a concave function (extended value).
    So $I_C$ is log-concave.</div>
    <div class="proof-step"><strong>Step 4: Conclusion.</strong>
    Since $f$ is the convolution of two log-concave functions, $f$ is log-concave.
    (Note: The integral is essentially measuring the measure of the set $C$ shifted by $-x$. Prekopa-Leindler directly applies to marginals of log-concave functions).</div>
  </div>
</div>

<div class="problem">
  <h3>P6.10 — Log-Concavity of Gaussian</h3>
  <p>Show that the multivariate Gaussian density function $f(x)$ is log-concave. $$ f(x) = \frac{1}{(2\pi)^{n/2} (\det \Sigma)^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^\top \Sigma^{-1} (x-\mu)\right) $$ where $\Sigma \succ 0$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      Take the logarithm:
      $$ \log f(x) = -\frac{n}{2}\log(2\pi) - \frac{1}{2}\log(\det \Sigma) - \frac{1}{2}(x-\mu)^\top \Sigma^{-1} (x-\mu) $$
    </div>
    <div class="proof-step">
      The first two terms are constants. The third term is a quadratic form.
      Let $g(x) = \log f(x)$. The gradient is $\nabla g(x) = -\Sigma^{-1}(x-\mu)$.
      The Hessian is $\nabla^2 g(x) = -\Sigma^{-1}$.
    </div>
    <div class="proof-step">
      Since $\Sigma \succ 0$, its inverse $\Sigma^{-1} \succ 0$, so $-\Sigma^{-1} \prec 0$ (negative definite).
      Since the Hessian is negative definite everywhere, $g(x)$ is strictly concave.
      Therefore, $f(x)$ is log-concave.
    </div>
  </div>
</div>


</section>
    </article>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></a> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initConvexFunctionInspector } from './widgets/js/convex-function-inspector.js';
    initConvexFunctionInspector('widget-convex-function-inspector');
  </script>
  <script type="module">
    import { initHessianHeatmap } from './widgets/js/hessian-heatmap.js';
    initHessianHeatmap('widget-hessian-heatmap');
  </script>
  <script type="module">
    import { initOperationsPreserving } from './widgets/js/operations-preserving.js';
    initOperationsPreserving('widget-operations-preserving');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
