<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>06. Convex Functions: Advanced — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../05-convex-functions-basics/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../07-convex-problems-standard/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>06. Convex Functions: Advanced Topics</h1>
      <div class="lecture-meta">
        <span>Date: 2025-11-04</span>
        <span>Duration: 90 min</span>
        <span>Tags: functions, theory, advanced, conjugate, quasiconvex</span>
      </div>
      <div class="lecture-summary">
        <p><strong>Overview:</strong> This lecture covers advanced topics in convex function theory: the convex conjugate (Fenchel conjugate), quasiconvex functions, and log-concavity. These concepts are essential for duality theory and advanced optimization techniques.</p>
        <p><strong>Prerequisites:</strong> <a href="../05-convex-functions-basics/index.html">Lecture 05: Convex Functions Basics</a> (definitions, characterizations, operations, Jensen's inequality).</p>
        <p><strong>Forward Connections:</strong> The convex conjugate is central to duality theory (<a href="../09-duality/index.html">Lecture 09</a>). Quasiconvex optimization appears in problem reformulation (<a href="../08-convex-problems-conic/index.html">Lecture 08</a>).</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <ul>
        <li><b>Understand the Convex Conjugate:</b> Define the Fenchel conjugate and understand its role in duality theory.</li>
        <li><b>Apply Quasi-convexity:</b> Distinguish convex from quasi-convex functions and understand when quasi-convexity suffices for optimization.</li>
        <li><b>Work with Log-concavity:</b> Understand log-concave functions and their applications in probability and optimization.</li>
      </ul>
    </section>

    <article>
      <section class="section-card" id="section-1">
      <h2>1. The Convex Conjugate (Fenchel Conjugate)</h2>

      <div class="insight">
        <h4>Core Idea</h4>
        <p>Given a function $f$, the conjugate $f^*$ is <b>"the best linear lower bound"</b> to $f$, written as a function of the slope $y$.</p>
      </div>

      <h3>1.1 Definition and Geometry</h3>
      <p>Let $f : \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$ be an extended-real function. Its <a href="#" class="definition-link" data-term="convex conjugate">Fenchel conjugate</a> is defined as:</p>
      $$
      \boxed{f^*(y) = \sup_{x \in \mathrm{dom}\, f} \left( y^\top x - f(x) \right)}
      $$
      <p>The domain of the conjugate is $\mathrm{dom}\, f^* = \{ y \in \mathbb{R}^n \mid \sup_x (y^\top x - f(x)) < +\infty \}$.</p>

      <h4>Geometric Interpretation: Best Affine Lower Bound</h4>
      <p>Fix a slope vector $y$. Consider the family of affine functions with slope $y$: $\ell_{y,t}(x) = y^\top x - t$. We want to find the smallest vertical shift $t$ such that the affine function lies everywhere below $f$.
      <br>The condition $\ell_{y,t}(x) \le f(x)$ for all $x$ is equivalent to $t \ge y^\top x - f(x)$ for all $x$.
      <br>The smallest valid shift is $t_{\min} = \sup_x (y^\top x - f(x))$, which is exactly $f^*(y)$.
      <br>Thus, the affine function $h(x) = y^\top x - f^*(y)$ is the <b>best affine lower bound</b> to $f$ with slope $y$. It supports the epigraph of $f$.</p>

      <h3>1.2 Basic Properties</h3>
      <ul>
        <li><b>Convexity:</b> $f^*$ is always convex, regardless of whether $f$ is convex. This is because $f^*$ is the pointwise supremum of a family of affine functions (indexed by $x$): $y \mapsto x^\top y - f(x)$.</li>
        <li><b>Fenchel's Inequality:</b> Directly from the definition, $f^*(y) \ge y^\top x - f(x)$, which rearranges to:
        $$ \boxed{f(x) + f^*(y) \ge x^\top y} $$
        This inequality holds for all $x, y$. Equality holds if and only if $y \in \partial f(x)$ (i.e., $y$ is a subgradient of $f$ at $x$).</li>
      </ul>

      <h3>1.3 Differentiable Case: Legendre Transform</h3>
      <p>If $f$ is convex, differentiable, and strictly convex (so $\nabla f$ is one-to-one), we can find the supremum by setting the gradient of $\phi_y(x) = y^\top x - f(x)$ to zero.
      $$ \nabla_x \phi_y(x) = y - \nabla f(x) = 0 \implies \nabla f(x^*) = y $$
      If we can solve for $x^*$ in terms of $y$ (i.e., invert the gradient map), we get:
      $$ f^*(y) = y^\top x^*(y) - f(x^*(y)) $$
      This specific case is known as the <b>Legendre transform</b>. A useful identity is $\nabla f^*(y) = x^*(y) = (\nabla f)^{-1}(y)$.</p>

      <h3>1.4 Derivations of Examples</h3>
      <p>We formally derive the conjugates for fundamental 1D functions, paying close attention to domains and boundary cases.</p>

      <div class="example">
        <h4>1. Affine Function: $f(x) = ax + b$</h4>
        <p>$$ f^*(y) = \sup_{x \in \mathbb{R}} (yx - (ax+b)) = \sup_x ((y-a)x - b) $$</p>
        <ul>
          <li><b>Case $y \ne a$:</b> The term $(y-a)x$ is linear with non-zero slope. As $x \to \pm \infty$, the supremum goes to $+\infty$.</li>
          <li><b>Case $y = a$:</b> The slope vanishes. The expression is constant $-b$. The supremum is $-b$.</li>
        </ul>
        <p>Thus, the domain is the singleton $\{a\}$:
        $$ f^*(y) = \begin{cases} -b & y=a \\ \infty & y \ne a \end{cases} $$
        This corresponds to the indicator function of $\{a\}$ minus $b$: $f^*(y) = -b + I_{\{a\}}(y)$.</p>
      </div>

      <div class="example">
        <h4>2. Negative Log: $f(x) = -\log x$ on $(0, \infty)$</h4>
        <p>$$ f^*(y) = \sup_{x > 0} (yx + \log x) $$
        Let $\phi(x) = yx + \log x$. Then $\phi'(x) = y + 1/x$.</p>
        <ul>
          <li><b>Case $y \ge 0$:</b> Since $x > 0$, $\phi'(x) = y + 1/x > 0$. The function is strictly increasing. As $x \to \infty$, $\phi(x) \to \infty$. So $f^*(y) = \infty$.</li>
          <li><b>Case $y < 0$:</b> $\phi'(x) = 0 \implies x^* = -1/y > 0$. Since $\phi''(x) = -1/x^2 < 0$, this is a unique global maximum.
          <br>Value: $y(-1/y) + \log(-1/y) = -1 - \log(-y)$.</li>
        </ul>
        $$ f^*(y) = \begin{cases} -1 - \log(-y) & y < 0 \\ \infty & y \ge 0 \end{cases} $$
      </div>

      <div class="example">
        <h4>3. Exponential: $f(x) = e^x$</h4>
        <p>$$ f^*(y) = \sup_{x \in \mathbb{R}} (yx - e^x) $$
        Let $\phi(x) = yx - e^x$. Then $\phi'(x) = y - e^x$.</p>
        <ul>
          <li><b>Case $y > 0$:</b> $\phi'(x) = 0 \implies e^x = y \implies x^* = \log y$. Second derivative $\phi''(x) = -e^x < 0$, so strictly concave.
          <br>Value: $y \log y - e^{\log y} = y \log y - y$.</li>
          <li><b>Case $y = 0$:</b> $\phi(x) = -e^x$. Strictly decreasing. As $x \to -\infty$, $-e^x \to 0$. As $x \to \infty$, $-e^x \to -\infty$. The supremum is $0$.</li>
          <li><b>Case $y < 0$:</b> $\phi'(x) = y - e^x < 0$ everywhere. Strictly decreasing. As $x \to -\infty$, $e^x \to 0$ but $yx \to +\infty$ (product of two negatives). Thus $f^*(y) = \infty$.</li>
        </ul>
        <p>Combining $y=0$ (where $0 \log 0 = 0$) and $y>0$:
        $$ f^*(y) = \begin{cases} y \log y - y & y \ge 0 \\ \infty & y < 0 \end{cases} $$
        </p>
      </div>

      <div class="example">
        <h4>4. Reciprocal: $f(x) = 1/x$ on $(0, \infty)$</h4>
        <p>$$ f^*(y) = \sup_{x > 0} (yx - 1/x) $$
        Let $\phi(x) = yx - 1/x$. Then $\phi'(x) = y + 1/x^2$.</p>
        <ul>
          <li><b>Case $y > 0$:</b> $\phi'(x) > 0$. Strictly increasing. As $x \to \infty$, $yx \to \infty$. So $f^*(y) = \infty$.</li>
          <li><b>Case $y = 0$:</b> $\phi(x) = -1/x$. Increasing from $-\infty$ to $0$. The supremum is $0$.</li>
          <li><b>Case $y < 0$:</b> $\phi'(x) = 0 \implies x^2 = -1/y \implies x^* = \sqrt{-1/y}$. Concave since $\phi''(x) = -2/x^3 < 0$.
          <br>Value: $y \sqrt{-1/y} - \sqrt{-1/y}^{-1} = -\sqrt{-y} - \sqrt{-y} = -2\sqrt{-y}$.</li>
        </ul>
        $$ f^*(y) = \begin{cases} -2\sqrt{-y} & y \le 0 \\ \infty & y > 0 \end{cases} $$
      </div>

      <h3>1.5 Matrix and Vector Examples</h3>

      <div class="example">
        <h4>1. Quadratic: $f(x) = \frac{1}{2} x^\top Q x$ ($Q \in \mathbb{S}^n_{++}$)</h4>
        <p>Maximize $\phi(x) = y^\top x - \frac{1}{2} x^\top Q x$.
        <br>Gradient: $\nabla \phi(x) = y - Qx$. Set to 0: $x^* = Q^{-1}y$.
        <br>Hessian: $\nabla^2 \phi(x) = -Q \prec 0$, so strictly concave.
        <br>Value:
        $$ f^*(y) = y^\top (Q^{-1}y) - \frac{1}{2} (Q^{-1}y)^\top Q (Q^{-1}y) = y^\top Q^{-1} y - \frac{1}{2} y^\top Q^{-1} y = \frac{1}{2} y^\top Q^{-1} y $$
        Result: $f^*(y) = \frac{1}{2} y^\top Q^{-1} y$. The conjugate of a quadratic form is a quadratic form with the inverse matrix.</p>
      </div>

      <div class="example">
        <h4>2. Log-Determinant: $f(X) = -\log \det X$ on $\mathbb{S}^n_{++}$</h4>
        <p>We compute the conjugate of the negative log-determinant function.
        $$ f^*(Y) = \sup_{X \succ 0} (\mathrm{tr}(Y^\top X) - f(X)) = \sup_{X \succ 0} (\mathrm{tr}(YX) + \log \det X) $$
        (Note: $Y^\top X$ or $YX$ have same trace since $X$ is symmetric, but usually dual variable is symmetric too).</p>

        <p><b>Step 1: Gradient Condition.</b> Define $\Phi(X) = \mathrm{tr}(YX) + \log \det X$.
        The gradient with respect to $X$ is:
        $$ \nabla_X \Phi(X) = Y + X^{-1} $$
        Setting the gradient to zero for a stationary point:
        $$ Y + X^{-1} = 0 \implies X^{-1} = -Y \implies X = -Y^{-1} $$</p>

        <p><b>Step 2: Domain Feasibility.</b>
        For the solution $X = -Y^{-1}$ to be feasible (i.e., $X \in \mathrm{dom}(f) = \mathbb{S}^n_{++}$), we must have $-Y^{-1} \succ 0$.
        Since the inverse of a positive definite matrix is positive definite, this requires $-Y \succ 0$, or equivalently $Y \prec 0$ (negative definite).
        If $Y$ is not negative definite, the supremum is $+\infty$ (we can choose $X$ to make the trace term dominate).
        </p>

        <p><b>Step 3: Value at Optimum.</b>
        Substitute $X^* = -Y^{-1}$ back into the objective function:
        $$ f^*(Y) = \mathrm{tr}(Y(-Y^{-1})) + \log \det (-Y^{-1}) $$
        $$ = \mathrm{tr}(-I) + \log ((\det(-Y))^{-1}) $$
        $$ = -n - \log \det (-Y) $$
        </p>

        <p><b>Conclusion:</b>
        $$ f^*(Y) = \begin{cases} -n - \log\det(-Y) & Y \prec 0 \\ \infty & \text{otherwise} \end{cases} $$
        </p>
      </div>

      <div class="example">
        <h4>3. Norm Squared: $f(x) = \frac{1}{2}\|x\|^2$</h4>
        <p>We compute the conjugate of one-half the squared norm for an arbitrary norm $\|\cdot\|$.
        $$ f^*(y) = \sup_x \left( y^\top x - \frac{1}{2}\|x\|^2 \right) $$
        <b>Step 1: Dual Norm Bound.</b> By definition of dual norm, $y^\top x \le \|y\|_* \|x\|$.
        $$ y^\top x - \frac{1}{2}\|x\|^2 \le \|y\|_* \|x\| - \frac{1}{2}\|x\|^2 $$
        <b>Step 2: Scalar Young's Inequality.</b> For scalars $a, b$, $ab \le \frac{1}{2}a^2 + \frac{1}{2}b^2$. Let $a=\|x\|, b=\|y\|_*$.
        $$ \|y\|_* \|x\| \le \frac{1}{2}\|x\|^2 + \frac{1}{2}\|y\|_*^2 $$
        Substituting this back:
        $$ \left( y^\top x - \frac{1}{2}\|x\|^2 \right) \le \left(\frac{1}{2}\|x\|^2 + \frac{1}{2}\|y\|_*^2\right) - \frac{1}{2}\|x\|^2 = \frac{1}{2}\|y\|_*^2 $$
        So $f^*(y) \le \frac{1}{2}\|y\|_*^2$.
        <br><b>Step 3: Tightness.</b> We can choose $x$ aligned with $y$ such that $y^\top x = \|y\|_* \|x\|$ and $\|x\| = \|y\|_*$. Then the bound is achieved. Thus:
        $$ \boxed{f^*(y) = \frac{1}{2}\|y\|_*^2} $$
        In the Euclidean case ($\|y\|_* = \|y\|_2$), we recover the quadratic result with $Q=I$.</p>
      </div>

      <div class="example">
        <h4>4. Indicator and Support Functions</h4>
        <p>Let $S \subseteq \mathbb{R}^n$ be a set. The indicator function is $I_S(x) = 0$ if $x \in S$, and $+\infty$ otherwise.
        $$ f^*(y) = \sup_x (y^\top x - I_S(x)) = \sup_{x \in S} (y^\top x) $$
        This is exactly the <b>support function</b> $h_S(y)$ of the set $S$.
        <br><i>Connection:</i> Since $f^{**}$ is the closed convex hull of $f$, the biconjugate of the indicator $I_S$ is the indicator of the closed convex hull of $S$:
        $$ I_S^* = h_S, \quad h_S^* = I_{\operatorname{cl}\operatorname{conv}S} $$
        </p>
      </div>

      <h3>1.6 Transformation Rules</h3>
      <table class="data-table">
        <thead>
          <tr><th>Function $g(x)$</th><th>Conjugate $g^*(y)$</th><th>Notes</th></tr>
        </thead>
        <tbody>
          <tr><td>$a f(x) + b$ ($a>0$)</td><td>$a f^*(y/a) - b$</td><td>Scale and shift</td></tr>
          <tr><td>$f(Ax + b)$ ($A$ invertible)</td><td>$f^*(A^{-\top}y) - b^\top A^{-\top}y$</td><td>Affine change of variables</td></tr>
          <tr><td>$f_1(u) + f_2(v)$</td><td>$f_1^*(y_1) + f_2^*(y_2)$</td><td>Sum of independent functions</td></tr>
        </tbody>
      </table>

      <h3>1.7 Economic Interpretation: Revenue and Profit</h3>
      <p>The conjugate relationship is central to duality in economics.</p>
      <ul>
        <li>Let $r \in \mathbb{R}^n$ be a resource/quantity vector.</li>
        <li>Let $p \in \mathbb{R}^n$ be the price vector.</li>
        <li>Let $S(r)$ be the <b>sales revenue</b> (or negative cost).</li>
      </ul>
      <p>The <b>maximum profit</b> at prices $p$ is:
      $$ M(p) = \sup_r (S(r) - p^\top r) $$
      If we define a "negative revenue" or cost-like function $f(r) = -S(r)$, then:
      $$ M(p) = \sup_r (-p^\top r - f(r)) = f^*(-p) $$
      Thus, <b>profit as a function of prices</b> is the conjugate of <b>(negative) revenue as a function of quantities</b>.
      <br><i>Hotelling's Lemma:</i> The gradient of the profit function with respect to prices gives the optimal supply: $\nabla M(p) = -r^*$.</p>

      <h3>1.8 The Biconjugate Theorem</h3>
      <p>The <b>biconjugate</b> $f^{**} = (f^*)^*$ is the conjugate of the conjugate.
      <br>By Fenchel's inequality on $f^*$, we always have $f^{**}(x) \le f(x)$.
      <br><b>Theorem:</b> If $f$ is proper, convex, and lower semicontinuous (closed), then:
      $$ \boxed{f^{**} = f} $$
      Generally, $f^{**}$ is the <b>closed convex hull</b> (or lower semicontinuous convex envelope) of $f$. This theorem underpins strong duality: $f=f^{**}$ means we can represent $f$ as the supremum of affine functions.</p>
    </section>

    <section class="section-card" id="section-2">
      <h2>2. Quasi-Convex Functions</h2>

      <h3>2.1 Definition via Sublevel Sets</h3>
      <p>A function $f: \mathbb{R}^n \to \mathbb{R}$ is <a href="#" class="definition-link">quasi-convex</a> if its domain is convex and all its <b>sublevel sets</b> are convex:</p>
      $$
      S_\alpha = \{x \in \mathrm{dom}\, f \mid f(x) \le \alpha\}
      $$
      <p>is convex for all $\alpha \in \mathbb{R}$.</p>
      <p><i>Intuition:</i> The function can have flat spots or kinks, and it doesn't need to curve "up" like a bowl, but it must be <b>unimodal</b> (no local minima that aren't global). It describes a "valley" structure.</p>

      <h3>2.2 Modified Jensen's Inequality</h3>
      <p>$f$ is quasi-convex if and only if for all $x, y \in \mathrm{dom}\, f$ and $\theta \in [0, 1]$:</p>
      $$
      f(\theta x + (1-\theta)y) \le \max\{f(x), f(y)\}
      $$
      <p>This condition says that the value on a segment never exceeds the larger of the two endpoints.</p>

      <h3>2.3 First-Order Condition</h3>
      <p>If $f$ is differentiable, it is quasi-convex if and only if its domain is convex and:</p>
      $$ f(y) \le f(x) \implies \nabla f(x)^\top (y - x) \le 0 $$
      <p><b>Geometric Meaning:</b> The gradient $\nabla f(x)$ is the normal vector to the sublevel set $S_{f(x)}$ at $x$. The condition states that this normal vector supports the sublevel set. If $y$ is any point with a lower (or equal) function value, it must lie in the halfspace defined by the negative gradient.</p>
      <p><i>Contrast with Convexity:</i> Convexity requires $f(y) \ge f(x) + \nabla f(x)^\top (y-x)$ (the tangent lies below). Quasiconvexity only requires that if $f(y) \le f(x)$, the tangent plane separates $y$ from the ascent direction.</p>

      <h3>2.4 Second-Order Condition</h3>
      <p>For a twice-differentiable quasi-convex function, the curvature must be non-negative in directions tangent to the level sets.</p>
      <div class="theorem-box">
        <h4>Theorem</h4>
        <p>If $f$ is quasi-convex, then for all $x$ and vectors $v$ such that $v^\top \nabla f(x) = 0$ (tangent to level set), we must have:</p>
        $$ v^\top \nabla^2 f(x) v \ge 0 $$
        <p>If $v^\top \nabla f(x) = 0 \implies v^\top \nabla^2 f(x) v > 0$ for all $v \neq 0$, then $f$ is strictly quasi-convex.</p>
        <p><i>Note:</i> This condition says that the level surfaces must curve "inward" (like a bowl), even if the function profile along the gradient direction is not convex (e.g., a Gaussian bell curve is quasi-concave, not quasi-convex, but $x^3$ is quasi-convex on $\mathbb{R}$).</p>
      </div>

      <h3>2.5 Operations Preserving Quasi-Convexity</h3>
      <p>We can build quasi-convex functions from simpler components using a calculus of operations.</p>

      <div class="theorem-box">
        <h4>1. Pointwise Maximum</h4>
        <p>If $f_1, \ldots, f_m$ are quasi-convex, then $f(x) = \max_i f_i(x)$ is quasi-convex.</p>
        <p><b>Proof:</b> The sublevel set of the maximum is the intersection of the sublevel sets:
        $$ \{x \mid \max_i f_i(x) \le \alpha\} = \bigcap_i \{x \mid f_i(x) \le \alpha\} $$
        Since the intersection of convex sets is convex, the resulting sublevel set is convex.</p>
      </div>

      <div class="theorem-box">
        <h4>2. Composition</h4>
        <p>If $g: \mathbb{R}^n \to \mathbb{R}$ is quasi-convex and $h: \mathbb{R} \to \mathbb{R}$ is <b>non-decreasing</b>, then $f(x) = h(g(x))$ is quasi-convex.</p>
        <p><b>Proof:</b>
        $$ \{x \mid h(g(x)) \le \alpha\} = \{x \mid g(x) \le \sup\{t \mid h(t) \le \alpha\}\} $$
        This is a sublevel set of $g$ (for some level $\beta$), which is convex.</p>
        <p><i>Example:</i> If $g(x)$ is convex (hence quasi-convex), then $\sqrt{g(x)}$ (for $g \ge 0$) is quasi-convex.</p>
      </div>

      <div class="theorem-box">
        <h4>3. Minimization</h4>
        <p>Minimizing a quasi-convex function is <b>not</b> a standard quasi-convexity preserving operation. However, minimizing a quasi-convex function $f(x)$ over a convex set $C$ is a quasi-convex problem, in the sense that the feasible set is convex. The resulting optimal value function is generally not quasi-convex in parameters.</p>
      </div>

      <h3>2.6 Examples</h3>

      <div class="example">
        <h4>1. Simple 1D Examples</h4>
        <ul>
          <li><b>Logarithm:</b> $f(x) = \log x$ on $(0, \infty)$. Sublevel set $S_\alpha = \{x \mid \log x \le \alpha\} = (0, e^\alpha]$. This is a convex set (interval), so $\log x$ is quasi-convex. (Since it is monotonic, it is also quasi-concave).</li>
          <li><b>Ceiling:</b> $f(x) = \lceil x \rceil$. Sublevel set $S_\alpha = \{x \mid \lceil x \rceil \le \alpha\} = (-\infty, \lfloor \alpha \rfloor]$. This is an interval, so the ceiling function is quasi-convex.</li>
        </ul>
      </div>

      <div class="example">
        <h4>2. Length of a Vector (Support Size)</h4>
        <p>Define $f(x) = \max \{i \mid x_i \ne 0\}$ (index of last nonzero element).
        <br>Sublevel set $S_k = \{x \mid x_{k+1} = \dots = x_n = 0\}$. This is a linear subspace, hence convex. Thus $f$ is quasi-convex.</p>
      </div>

      <div class="example">
        <h4>3. Linear-Fractional Function</h4>
        <p>$f(x) = \frac{a^\top x + b}{c^\top x + d}$ on $\{x \mid c^\top x + d > 0\}$.
        <br>Sublevel set $f(x) \le \alpha \iff a^\top x + b \le \alpha(c^\top x + d)$. This is a linear inequality (halfspace), so $f$ is quasi-convex (and quasi-concave).</p>
      </div>

      <div class="example">
        <h4>4. Distance Ratio</h4>
        <p>$f(x) = \frac{\|x-a\|_2}{\|x-b\|_2}$ on $\{x \mid \|x-a\| \le \|x-b\|\}$.
        <br>Condition $f(x) \le \alpha$ (for $\alpha \le 1$):
        $$ \|x-a\|^2 \le \alpha^2 \|x-b\|^2 \iff x^\top x - 2a^\top x + \|a\|^2 \le \alpha^2 (x^\top x - 2b^\top x + \|b\|^2) $$
        $$ (1-\alpha^2) x^\top x + 2(\alpha^2 b - a)^\top x + C \le 0 $$
        For $\alpha \le 1$, the coefficient of $x^\top x$ is non-negative, so this defines a convex quadratic set (ball or halfspace). Thus quasi-convex.</p>
      </div>

      <div class="example">
        <h4>5. Internal Rate of Return (IRR)</h4>
        <p>Let $c = (c_0, \dots, c_n)$ be cash flows ($c_0 < 0$). The IRR is the rate $r$ satisfying $\sum c_i (1+r)^{-i} = 0$.
        <br>We view $\text{IRR}(c)$ as a function of the cash flows $c$.
        <br>Condition $\text{IRR}(c) \ge R \iff \sum c_i (1+R)^{-i} \ge 0$ (assuming standard investment profile).
        <br>This is a linear inequality in $c$. The superlevel sets are halfspaces (convex). Thus, IRR is a <b>quasi-concave</b> function of the cash flows.</p>
      </div>
    </section>

    <section class="section-card" id="section-3">
      <h2>3. Log-Concave Functions</h2>

      <h3>3.1 Definition</h3>
      <p>A function $f: \mathbb{R}^n \to \mathbb{R}$ is <a href="#" class="definition-link">log-concave</a> if $f(x) > 0$ for all $x \in \mathrm{dom}\, f$, and $\log f(x)$ is a concave function.</p>
      <p>Equivalently, for all $x, y \in \mathrm{dom}\, f$ and $\theta \in [0, 1]$:</p>
      $$ f(\theta x + (1-\theta)y) \ge f(x)^\theta f(y)^{1-\theta} $$
      <p>A function is <b>log-convex</b> if $\log f(x)$ is convex. Note that if $f$ is log-convex, it is also convex (since $f = e^{\log f}$ and exponential is increasing convex). However, a log-concave function is not necessarily concave (e.g., the Gaussian density is log-concave but not concave).</p>

      <h3>3.2 Properties</h3>
      <ul>
        <li><b>Concavity of domain:</b> The domain of a log-concave function must be a convex set.</li>
        <li><b>Products:</b> The product of log-concave functions is log-concave. (Proof: $\log(fg) = \log f + \log g$. Sum of concave is concave).</li>
        <li><b>Integration (Marginals):</b> If $f(x, y)$ is log-concave, then the marginal $g(x) = \int f(x, y) dy$ is log-concave. This deep result is a consequence of the Prékopa-Leindler inequality.</li>
        <li><b>Convolution:</b> The convolution of log-concave functions is log-concave. (Follows from integration property).</li>
        <li><b>Integration Preserves Log-Convexity:</b> If $f(x, y)$ is <b>log-convex</b> in $x$ for each fixed $y$, then $g(x) = \int f(x, y) dy$ is log-convex. (Example: Laplace transform).</li>
      </ul>

      <h3>3.3 Examples</h3>

      <div class="example">
        <h4>1. Gaussian Density</h4>
        <p>The multivariate Gaussian density $f(x) \propto \exp(-\frac{1}{2}(x-\mu)^\top \Sigma^{-1} (x-\mu))$ is log-concave.
        <br>$\log f(x) = -\frac{1}{2}(x-\mu)^\top \Sigma^{-1} (x-\mu) + C$, which is a concave quadratic function (since $\Sigma^{-1} \succ 0$).</p>
      </div>

      <div class="example">
        <h4>2. Indicator of a Convex Set</h4>
        <p>Let $C \subseteq \mathbb{R}^n$ be a convex set. The indicator function $I_C(x)$ (1 if $x \in C$, 0 otherwise) is log-concave.
        <br>$\log I_C(x)$ is 0 if $x \in C$ and $-\infty$ otherwise. This extended-value function is concave.</p>
      </div>

      <div class="example">
        <h4>3. Determinant</h4>
        <p>The function $f(X) = \det X$ is log-concave on $\mathbb{S}^n_{++}$ (positive definite matrices).
        <br>Proof: $\log \det X$ is concave (we proved this via the restriction to a line in Lecture 05).</p>
      </div>

      <div class="example">
        <h4>4. Cumulative Distribution Function (CDF)</h4>
        <p>If a probability density $p(x)$ is log-concave, its CDF $F(x) = \int_{-\infty}^x p(t) dt$ is log-concave.
        <br>Example: The Gaussian CDF $\Phi(x)$ is log-concave. This property is crucial in reliability theory and probit regression.</p>
      </div>

      <div class="example">
        <h4>5. Wishart Distribution</h4>
        <p>The Wishart density on positive definite matrices $X \in \mathbb{S}^n_{++}$ is (up to normalization):
        $$ f(X) \propto (\det X)^{(p-n-1)/2} \exp\left(-\frac{1}{2} \mathrm{tr}(\Sigma^{-1}X)\right) $$
        Taking the log:
        $$ \log f(X) = c + \frac{p-n-1}{2} \log \det X - \frac{1}{2} \mathrm{tr}(\Sigma^{-1}X) $$
        <ul>
            <li>The term $\log \det X$ is <b>concave</b>.</li>
            <li>The term $-\mathrm{tr}(\Sigma^{-1}X)$ is <b>linear</b> (hence concave).</li>
        </ul>
        Since the sum of concave functions is concave, $\log f$ is concave, so the Wishart distribution is log-concave.</p>
      </div>
    </section>

    <section class="section-card" id="section-4">
      <h2>4. Review & Cheat Sheet</h2>

      <h3>Common Convex Functions Reference</h3>

      <p>This table provides a quick reference for recognizing convex functions. Memorize these patterns—they appear constantly in optimization.</p>

      <table class="data-table" style="width: 100%; margin-top: 16px;">
        <thead>
          <tr>
            <th>Function</th>
            <th>Domain</th>
            <th>Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>$x^p$</td>
            <td>$\mathbb{R}_+$ if $p \ge 1$ or $p \le 0$</td>
            <td>Convex on $\mathbb{R}_+$ for $p \ge 1$ or $p \le 0$; concave for $0 \le p \le 1$</td>
          </tr>
          <tr>
            <td>$e^{ax}$</td>
            <td>$\mathbb{R}$</td>
            <td>Convex for any $a \in \mathbb{R}$</td>
          </tr>
          <tr>
            <td>$-\log(x)$</td>
            <td>$\mathbb{R}_{++}$</td>
            <td>Convex; $\log(x)$ is concave</td>
          </tr>
          <tr>
            <td>$x \log(x)$</td>
            <td>$\mathbb{R}_{++}$</td>
            <td>Convex (negative entropy term)</td>
          </tr>
          <tr>
            <td>$\|x\|_p$</td>
            <td>$\mathbb{R}^n$</td>
            <td>Convex for $p \ge 1$; not a norm for $p < 1$</td>
          </tr>
          <tr>
            <td>$\|x\|_2^2 = x^\top x$</td>
            <td>$\mathbb{R}^n$</td>
            <td>Convex (quadratic with PSD Hessian $2I$)</td>
          </tr>
          <tr>
            <td>$x^\top A x$</td>
            <td>$\mathbb{R}^n$</td>
            <td>Convex if $A \succeq 0$</td>
          </tr>
          <tr>
            <td>$\log(\sum_{i=1}^n e^{x_i})$</td>
            <td>$\mathbb{R}^n$</td>
            <td>Log-sum-exp: smooth approximation to $\max\{x_1, \ldots, x_n\}$</td>
          </tr>
          <tr>
            <td>$-\log(\det(X))$</td>
            <td>$\mathbb{S}^n_{++}$</td>
            <td>Negative log-determinant: convex on PD matrices</td>
          </tr>
          <tr>
            <td>$\lambda_{\max}(X)$</td>
            <td>$\mathbb{S}^n$</td>
            <td>Maximum eigenvalue: convex (supremum of linear functions)</td>
          </tr>
          <tr>
            <td>$\mathrm{tr}(X^p)$</td>
            <td>$\mathbb{S}^n_+$</td>
            <td>Convex for $p \ge 1$ or $p \le 0$ on PSD matrices</td>
          </tr>
        </tbody>
      </table>

      <h3>4.1 Operations and Their Effects</h3>
      <table class="data-table" style="width: 100%; margin-top: 16px;">
        <thead>
          <tr>
            <th>Operation</th>
            <th>Preserves Convexity?</th>
            <th>Conditions</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Nonnegative weighted sum</td>
            <td>✅ Yes</td>
            <td>Always</td>
          </tr>
          <tr>
            <td>Pointwise maximum/supremum</td>
            <td>✅ Yes</td>
            <td>Always</td>
          </tr>
          <tr>
            <td>Composition $h(g(x))$</td>
            <td>✅ Yes</td>
            <td>If $g$ convex, $h$ convex non-decreasing</td>
          </tr>
          <tr>
            <td>Affine transformation $f(Ax+b)$</td>
            <td>✅ Yes</td>
            <td>Always</td>
          </tr>
          <tr>
            <td>Minimization $\inf_y f(x,y)$</td>
            <td>✅ Yes</td>
            <td>If $f$ convex in $(x,y)$</td>
          </tr>
          <tr>
            <td>Perspective</td>
            <td>✅ Yes</td>
            <td>Always</td>
          </tr>
          <tr>
            <td>Product $f(x) \cdot g(x)$</td>
            <td>❌ No</td>
            <td>Generally not convex</td>
          </tr>
          <tr>
            <td>Pointwise minimum</td>
            <td>❌ No</td>
            <td>Not preserved (but concave functions preserved)</td>
          </tr>
        </tbody>
      </table>
    </section>

    <section class="section-card" id="section-5">
      <h2><i data-feather="edit-3"></i> 5. Exercises</h2>

      <div class="problem">
        <h3>P6.1 — Verify Convexity Using First-Order Conditions</h3>
        <p>Prove that $f(x) = e^x$ is convex on $\mathbb{R}$ using the first-order condition.</p>

        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
          <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
          <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
              <li><b>Tangent Underestimator:</b> The first-order condition $\nabla f(x)$ defines a global linear underestimator: $f(y) \ge f(x) + \nabla f(x)^\top (y-x)$. Geometrically, the tangent plane lies below the graph.</li>
              <li><b>Inequality Source:</b> Many famous inequalities (like $e^x \ge 1+x$) are simply statements that a specific convex function lies above its tangent line at a specific point.</li>
          </ul>
        </div>
      </div>

      <div class="problem">
        <h3>P6.2 — Verify Convexity Using Second-Order Conditions</h3>
        <p>Show that $f(x) = \|x\|_2^2$ is convex on $\mathbb{R}^n$ using the Hessian test.</p>

        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
          <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
          <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
              <li><b>Curvature Test (Hessian):</b> The Hessian matrix $\nabla^2 f(x)$ captures the local curvature. If the curvature is "non-negative" (Positive Semidefinite) in every direction at every point, the function is globally convex ("bowl shape").</li>
          </ul>
        </div>
      </div>

      <div class="problem">
        <h3>P6.3 — Composition of Convex Functions</h3>
        <p>Let $f(x) = -\log(x)$ on $\mathbb{R}_{++}$ and $g(x) = e^x$ on $\mathbb{R}$. Is $h(x) = f(g(x)) = -\log(e^x) = -x$ convex?</p>
        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
          <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
          <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
              <li><b>Composition Rule Limitations:</b> The standard rules (e.g., Convex + Increasing) are sufficient but not necessary. A composition can be convex even if the rules don't apply.</li>
          </ul>
        </div>
      </div>

      <div class="problem">
        <h3>P6.4 — Maximum of Convex Functions</h3>
        <p>Prove that if $f_1, f_2: \mathbb{R}^n \to \mathbb{R}$ are convex, then $f(x) = \max\{f_1(x), f_2(x)\}$ is convex.</p>
        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
          <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
          <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
              <li><b>Epigraph Intersection:</b> Geometrically, the epigraph of the maximum function $f = \max(f_1, f_2)$ is the intersection of the epigraphs of $f_1$ and $f_2$. Since the intersection of convex sets is convex, the result is convex.</li>
          </ul>
        </div>
      </div>

      <div class="problem">
        <h3>P6.5 — Strong Convexity</h3>
        <p>Show that $f(x) = \frac{1}{2}x^\top Q x$ is $\lambda_{\min}(Q)$-strongly convex if $Q \succ 0$.</p>
        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
          <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
          <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
              <li><b>Strong Convexity Definition:</b> Requires a uniform minimum curvature $m > 0$. The function must curve upward at least as fast as a quadratic parabola $m x^2$.</li>
          </ul>
        </div>
      </div>

      <div class="problem">
        <h3>P6.6 — Conjugate Function</h3>
        <p>Compute the conjugate of $f(x) = \frac{1}{2}\|x\|_2^2$.</p>
        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
          <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
          <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
              <li><b>Self-Conjugacy of Quadratics:</b> The quadratic function $\frac{1}{2}\|x\|^2$ is the unique function (up to scaling) that is its own convex conjugate.</li>
          </ul>
        </div>
      </div>

      <div class="problem">
        <h3>P6.7 — Quasi-Convexity</h3>
        <p>Show that $f(x) = \lceil x \rceil$ (ceiling function) is quasi-convex but not convex.</p>
        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
          <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
          <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
              <li><b>Quasi-Convexity Definition:</b> A function is quasi-convex if all its sublevel sets $S_\alpha = \{x \mid f(x) \le \alpha\}$ are convex sets. This is a weaker condition than convexity.</li>
          </ul>
        </div>
      </div>

      <div class="problem">
        <h3>P6.8 — Fenchel's Inequality and Biconjugate</h3>
        <p>Let $f(x) = \frac{1}{2}x^\top Q x$ for $Q \in \mathbb{S}^n_{++}$.
        <ol type="a">
          <li>Verify Fenchel's inequality $f(x) + f^*(y) \ge x^\top y$ for this specific function. Under what condition does equality hold?</li>
          <li>Compute the biconjugate $f^{**}(x)$ directly from $f^*(y)$ and show it equals $f(x)$.</li>
        </ol></p>

        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
          <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
          <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
              <li><b>Equality Condition:</b> Fenchel's inequality becomes an equality ($f(x) + f^*(y) = x^\top y$) if and only if $y$ is the gradient (or subgradient) of $f$ at $x$. For a quadratic, this means $y = Qx$.</li>
          </ul>
        </div>

        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            <strong>(a) Verification:</strong>
            $f(x) = \frac{1}{2}x^\top Q x$, and we know $f^*(y) = \frac{1}{2}y^\top Q^{-1} y$.
            Sum: $LHS = \frac{1}{2}x^\top Q x + \frac{1}{2}y^\top Q^{-1} y$.
            To check $LHS \ge x^\top y$, consider the vector $z = Q^{1/2}x - Q^{-1/2}y$.
            $\|z\|^2 = (Q^{1/2}x - Q^{-1/2}y)^\top (Q^{1/2}x - Q^{-1/2}y)$
            $= x^\top Q x - 2 x^\top Q^{1/2} Q^{-1/2} y + y^\top Q^{-1} y$
            $= 2 f(x) - 2 x^\top y + 2 f^*(y) \ge 0$.
            Thus $f(x) + f^*(y) \ge x^\top y$.
            Equality holds iff $z=0 \iff Q^{1/2}x = Q^{-1/2}y \iff Qx = y$.
          </div>
          <div class="proof-step">
            <strong>(b) Biconjugate:</strong>
            $f^{**}(x) = (f^*)^*(x) = \sup_y (x^\top y - f^*(y)) = \sup_y (x^\top y - \frac{1}{2}y^\top Q^{-1} y)$.
            This is the conjugate of a quadratic with matrix $Q^{-1}$.
            Using the formula for quadratic conjugate (inverse of the matrix):
            matrix is $(Q^{-1})^{-1} = Q$.
            Thus $f^{**}(x) = \frac{1}{2}x^\top Q x = f(x)$.
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P6.9 — Quasiconvexity of the Distance Ratio</h3>
        <p>Consider the function $f(x) = \frac{\|x-a\|_2}{\|x-b\|_2}$ defined for $x \neq b$. Show that for any $\theta \in [0, 1]$, the sublevel set $S_\theta = \{x \mid f(x) \le \theta\}$ is convex. What is the geometric shape of this set?</p>

        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
          <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
          <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
              <li><b>Apollonius Circle:</b> The locus of points with a constant ratio of distances to two fixed points is a circle (in 2D) or a sphere (in nD). The sublevel set (interior) is a ball.</li>
          </ul>
        </div>

        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            <strong>Step 1: Square the condition.</strong>
            $f(x) \le \theta \iff \|x-a\| \le \theta \|x-b\|$. Since both sides are non-negative, we can square:
            $$ \|x-a\|^2 \le \theta^2 \|x-b\|^2 $$
          </div>
          <div class="proof-step">
            <strong>Step 2: Expand.</strong>
            $$ x^\top x - 2a^\top x + a^\top a \le \theta^2 (x^\top x - 2b^\top x + b^\top b) $$
            Rearranging terms to group by $x$:
            $$ (1-\theta^2) x^\top x - 2(a - \theta^2 b)^\top x + (\|a\|^2 - \theta^2 \|b\|^2) \le 0 $$
          </div>
          <div class="proof-step">
            <strong>Step 3: Analyze Convexity.</strong>
            This is a quadratic inequality of the form $x^\top P x + q^\top x + r \le 0$, where $P = (1-\theta^2)I$.
            Since $\theta \in [0, 1]$, we have $1-\theta^2 \ge 0$, so $P \succeq 0$ (positive semidefinite).
            The sublevel set of a convex quadratic function is a convex set.
            <br>Specifically, if $\theta < 1$, $P \succ 0$, so it's a Euclidean ball. If $\theta = 1$, it's a halfspace.
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P6.10 — Log-Concavity of Probability Measures</h3>
        <p>Let $C \subseteq \mathbb{R}^n$ be a convex set and let $w$ be a random variable with a log-concave probability density function $p(w)$. Consider the function $f(x) = \mathbb{P}(x + w \in C)$. Show that $f$ is log-concave.</p>

        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
          <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
          <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
              <li><b>Convolution Property:</b> The probability $\mathbb{P}(x+w \in C)$ can be written as the convolution of the density $p$ with the indicator function of the set $-C$.</li>
              <li><b>Closure under Convolution:</b> The convolution of two log-concave functions is log-concave. Since the indicator of a convex set is log-concave (0 on set, $-\infty$ outside), the result follows. This is a key result for robust optimization.</li>
          </ul>
        </div>

        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            <strong>Step 1: Integral Representation.</strong>
            $$ f(x) = \int_{\mathbb{R}^n} \mathbb{I}_C(x+w) p(w) dw $$
            Let $u = x+w$, so $w = u-x$.
            $$ f(x) = \int_{\mathbb{R}^n} \mathbb{I}_C(u) p(u-x) du $$
          </div>
          <div class="proof-step">
            <strong>Step 2: Log-Concavity of Integrand.</strong>
            The integrand is $g(x, u) = \mathbb{I}_C(u) p(u-x)$.
            <ul>
              <li>$\mathbb{I}_C(u)$ is the indicator function (1 inside, 0 outside). Its log is 0 inside, $-\infty$ outside. This is a concave function (extended value). Thus $\mathbb{I}_C$ is log-concave.</li>
              <li>$p(z)$ is log-concave by assumption. The composition with the affine map $(x, u) \mapsto u-x$ preserves log-concavity.</li>
            </ul>
            The product of log-concave functions is log-concave. Thus $g(x, u)$ is jointly log-concave in $(x, u)$.
          </div>
          <div class="proof-step">
            <strong>Step 3: Marginalization.</strong>
            The function $f(x)$ is the marginal of a jointly log-concave function: $f(x) = \int g(x, u) du$.
            By the Prekopa-Leindler inequality (or the marginalization property of log-concave functions), $f$ is log-concave.
          </div>
        </div>
      </div>
    </section>

    <section class="section-card" id="section-6">
      <h2>6. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required Reading:</strong> Boyd & Vandenberghe, Chapter 3 (Convex Functions).</li>
        <li><strong>Supplementary:</strong> Rockafellar, <em>Convex Analysis</em>, Part II.</li>
        <li><strong>Interactive:</strong> <a href="#widget-convex-function-inspector">Convex Function Inspector</a>.</li>
      </ul>
    </section>
    </article>
  </main></div>

  <footer class="site-footer">
    <div class="container">
      <p>© <span id="year"></span> Convex Optimization Course</p>
    </div>
  </footer>

  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/ui.js"></script>
<script src="../../static/js/toc.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initConvexFunctionInspector } from './widgets/js/convex-function-inspector.js';
    initConvexFunctionInspector('widget-convex-function-inspector');
  </script>
  <script type="module">
    import { initHessianHeatmap } from './widgets/js/hessian-heatmap.js';
    initHessianHeatmap('widget-hessian-heatmap');
  </script>
  <script type="module">
    import { initOperationsPreserving } from './widgets/js/operations-preserving.js';
    initOperationsPreserving('widget-operations-preserving');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>