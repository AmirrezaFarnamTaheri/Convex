<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>06. Convex Functions: Advanced Topics — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../05-convex-functions-basics/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../07-convex-problems-standard/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

    <article>
      <section class="section-card" id="section-1">
      <h2>1. The Convex Conjugate (Fenchel Conjugate)</h2>

      <div class="insight">
        <h4>Core Idea</h4>
        <p>Given a function $f$, the conjugate $f^*$ is <b>"the best linear lower bound"</b> to $f$, encoded as a function of the slope $y$. This transformation is the bridge to duality.</p>
      </div>

      <h3>1.1 Definition and Geometry</h3>
      <p>Let $f : \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$ be an extended-real function. Its <a href="#" class="definition-link" data-term="convex conjugate">Fenchel conjugate</a> is defined as:</p>
      $$
      \boxed{f^*(y) = \sup_{x \in \mathrm{dom}\, f} \left( y^\top x - f(x) \right)}
      $$
      <p>The domain of the conjugate is $\mathrm{dom}\, f^* = \{ y \in \mathbb{R}^n \mid \sup_x (y^\top x - f(x)) < +\infty \}$.</p>

      <h4>Geometric Interpretation</h4>
      <p>Consider the affine function $h(x) = y^\top x - c$. For this to be a lower bound on $f$ (i.e., $h(x) \le f(x)$ everywhere), we need $c \ge y^\top x - f(x)$. The tightest (smallest) such $c$ is $f^*(y)$. Thus, $y^\top x - f^*(y)$ is the best affine underestimator of $f$ with slope $y$.</p>

      <h3>1.2 Basic Properties</h3>
      <ul>
        <li><b>Convexity:</b> $f^*$ is always convex, because it is the pointwise supremum of a family of affine functions of $y$.</li>
        <li><b>Fenchel's Inequality:</b> Directly from the definition:
        $$ \boxed{f(x) + f^*(y) \ge x^\top y} $$
        Equality holds if and only if $y \in \partial f(x)$ (convex case). This generalizes the inequality $xy \le \frac{1}{2}x^2 + \frac{1}{2}y^2$.</li>
      </ul>

      <h3>1.3 Differentiable Case: Legendre Transform</h3>
      <p>If $f$ is strictly convex and differentiable, the maximum in the definition occurs where $\nabla_x (y^\top x - f(x)) = 0$, i.e., $y = \nabla f(x)$. Let $x^* = (\nabla f)^{-1}(y)$. Then:
      $$ f^*(y) = y^\top x^* - f(x^*) $$
      This specific case is the classical Legendre transform.</p>

      <h3>1.4 1D Examples: Detailed Derivations</h3>

      <div class="example">
        <h4>1. Affine Function: $f(x) = ax + b$</h4>
        <p>$$ f^*(y) = \sup_{x} (yx - ax - b) = \sup_x ((y-a)x - b) $$</p>
        <p>If $y \ne a$, the term is linear in $x$ with a non-zero slope. As $x \to \infty$ or $x \to -\infty$, the value goes to $\infty$. Thus the supremum is unbounded. If $y=a$, the expression simplifies to $-b$.
        $$ f^*(y) = \begin{cases} -b & y=a \\ \infty & y \ne a \end{cases} $$</p>
      </div>

      <div class="example">
        <h4>2. Negative Log: $f(x) = -\log x$ on $\mathbb{R}_{++}$</h4>
        <p>Maximize $yx + \log x$. Derivative $y + 1/x = 0 \implies x = -1/y$.
        <br>Requires $y < 0$ for $x > 0$.
        <br>Value: $y(-1/y) + \log(-1/y) = -1 - \log(-y)$.
        $$ f^*(y) = \begin{cases} -1 - \log(-y) & y < 0 \\ \infty & \text{otherwise} \end{cases} $$</p>
      </div>

      <div class="example">
        <h4>3. Exponential: $f(x) = e^x$</h4>
        <p>Maximize $yx - e^x$. Derivative $y - e^x = 0 \implies x = \log y$. Requires $y > 0$.
        <br>Value: $y \log y - e^{\log y} = y \log y - y$.
        <br>Boundary $y=0$: $\sup (-e^x) = 0$. For $y<0$, unbounded.
        $$ f^*(y) = \begin{cases} y \log y - y & y \ge 0 \\ \infty & y < 0 \end{cases} $$</p>
      </div>

      <h3>1.5 Matrix and Vector Examples</h3>

      <div class="example">
        <h4>1. Quadratic: $f(x) = \frac{1}{2} x^\top Q x$ ($Q \succ 0$)</h4>
        <p>Maximize $y^\top x - \frac{1}{2} x^\top Q x$.
        <br>Gradient zero: $y - Qx = 0 \implies x = Q^{-1}y$.
        <br>Value: $y^\top Q^{-1}y - \frac{1}{2} (Q^{-1}y)^\top Q (Q^{-1}y) = \frac{1}{2} y^\top Q^{-1} y$.
        <br><b>Result:</b> The conjugate of a quadratic is a quadratic with the inverse matrix.</p>
      </div>

      <div class="example">
        <h4>2. Norm Squared: $f(x) = \frac{1}{2}\|x\|^2$</h4>
        <p>Using dual norm definition ($y^\top x \le \|x\|\|y\|_*$) and Young's inequality:
        $$ f^*(y) = \sup_x (y^\top x - \frac{1}{2}\|x\|^2) = \frac{1}{2}\|y\|_*^2 $$
        where $\|\cdot\|_*$ is the dual norm.</p>
      </div>

      <div class="example">
        <h4>3. Log-Sum-Exp and Entropy</h4>
        <p>Let $f(x) = \log(\sum e^{x_i})$. This is a key function in ML.
        <br>Maximize $y^\top x - f(x)$. Gradient condition: $y_i = e^{x_i} / (\sum e^{x_k})$.
        <br>This implies $y > 0$ and $\mathbf{1}^\top y = 1$ (probability simplex $\Delta$).
        <br>Inverting: $e^{x_i} = y_i (\sum e^{x_k})$. Taking log: $x_i = \log y_i + C$.
        <br>Substitute back:
        $$ y^\top x - f(x) = \sum y_i (\log y_i + C) - \log(\sum y_i e^C) $$
        $$ = \sum y_i \log y_i + C \sum y_i - (C + \log 1) = \sum y_i \log y_i $$
        <b>Result:</b> The conjugate is the <b>negative entropy</b> on the simplex:
        $$ f^*(y) = \begin{cases} \sum y_i \log y_i & y \in \Delta \\ \infty & \text{otherwise} \end{cases} $$
        </p>
      </div>

      <div class="example">
        <h4>4. Log-Determinant</h4>
        <p>For $f(X) = -\log \det X$ on $\mathbb{S}^n_{++}$:
        $$ f^*(Y) = \begin{cases} -n - \log \det(-Y) & Y \prec 0 \\ \infty & \text{otherwise} \end{cases} $$
        The conjugate relates $\mathbb{S}^n_{++}$ to $\mathbb{S}^n_{--}$ (negative definite).</p>
      </div>

      <h3>1.6 The Biconjugate Theorem</h3>
      <p>The <b>biconjugate</b> $f^{**} = (f^*)^*$ is the closed convex hull of $f$.
      <br><b>Theorem:</b> If $f$ is closed and convex, then $f^{**} = f$.
      <br>This equivalence ($f$ vs supremum of affine functions) is the foundation of dual decomposition.</p>
    </section>

    <section class="section-card" id="section-2">
        <h2>2. Quasi-Convex Functions</h2>

        <h3>2.1 Definition via Sublevel Sets</h3>
        <p>A function $f: \mathbb{R}^n \to \mathbb{R}$ is <a href="#" class="definition-link">quasi-convex</a> if its domain is convex and all its <b>sublevel sets</b> are convex:</p>
        $$
        S_\alpha = \{x \in \mathrm{dom}\, f \mid f(x) \le \alpha\}
        $$
        <p>is convex for all $\alpha \in \mathbb{R}$. Geometrically, the function is "unimodal" or "valley-shaped" but may not have positive curvature everywhere.</p>

        <h3>2.2 Modified Jensen's Inequality</h3>
        <p>$f$ is quasi-convex if and only if:</p>
        $$
        f(\theta x + (1-\theta)y) \le \max\{f(x), f(y)\}
        $$
        <p>for all $x, y \in \mathrm{dom}\, f$ and $\theta \in [0, 1]$. The value on a segment never exceeds the larger endpoint.</p>

        <h3>2.3 Differentiable Conditions</h3>
        <ul>
            <li><b>First-Order:</b> $f(y) \le f(x) \implies \nabla f(x)^\top (y - x) \le 0$. (The gradient defines a supporting hyperplane to the sublevel set).</li>
            <li><b>Second-Order:</b> If $v^\top \nabla f(x) = 0$, then $v^\top \nabla^2 f(x) v \ge 0$. (Curvature is non-negative along contours).</li>
        </ul>

        <h3>2.4 Operations Preserving Quasi-Convexity</h3>
        <ul>
            <li><b>Max:</b> $f(x) = \max_i f_i(x)$ is quasi-convex (Intersection of convex sublevel sets is convex).</li>
            <li><b>Composition:</b> $g(h(x))$ is quasi-convex if $h$ is quasi-convex and $g$ is <b>non-decreasing</b>.</li>
            <li><b>Warning:</b> Sums of quasi-convex functions are generally <b>not</b> quasi-convex.</li>
        </ul>

        <h3>2.5 Examples</h3>
        <div class="example">
          <h4>1. Linear-Fractional Function</h4>
          <p>$f(x) = \frac{a^\top x + b}{c^\top x + d}$ on $\{x \mid c^\top x + d > 0\}$.
          <br>Sublevel set: $a^\top x + b \le \alpha (c^\top x + d)$. This is a linear inequality (halfspace), so $f$ is quasi-convex (and quasi-concave).</p>
        </div>

        <div class="example">
          <h4>2. Distance Ratio</h4>
          <p>$f(x) = \frac{\|x-a\|_2}{\|x-b\|_2}$ on $\{x \mid \|x-a\| \le \|x-b\|\}$.
          <br>Condition $f(x) \le \alpha$ squares to a quadratic inequality defining a ball (Apollonius circle). Thus quasi-convex.</p>
        </div>

        <div class="example">
          <h4>3. Internal Rate of Return (IRR)</h4>
          <p>The IRR is a quasi-concave function of the cash flows (superlevel sets are convex).</p>
        </div>
    </section>

    <section class="section-card" id="section-3">
        <h2>3. Log-Concave Functions</h2>

        <h3>3.1 Definition</h3>
        <p>A function $f: \mathbb{R}^n \to \mathbb{R}_{++}$ is <a href="#" class="definition-link">log-concave</a> if $\log f(x)$ is concave.
        $$ f(\theta x + (1-\theta)y) \ge f(x)^\theta f(y)^{1-\theta} $$
        Many probability densities are log-concave.</p>

        <h3>3.2 Properties</h3>
        <ul>
            <li><b>Hessian Condition:</b> $f(x) \nabla^2 f(x) \preceq \nabla f(x) \nabla f(x)^\top$.</li>
            <li><b>Product:</b> Products of log-concave functions are log-concave.</li>
            <li><b>Marginals:</b> If $f(x, y)$ is log-concave, $\int f(x, y) dy$ is log-concave (Prékopa-Leindler).</li>
            <li><b>Convolution:</b> Convolution of log-concave functions is log-concave.</li>
        </ul>

        <h3>3.3 Examples</h3>
        <div class="example">
            <ul>
                <li><b>Gaussian Density:</b> $\exp(-\frac{1}{2}x^\top \Sigma^{-1} x)$ has a concave quadratic log.</li>
                <li><b>Indicator of Convex Set:</b> Log is 0 or $-\infty$ (concave).</li>
                <li><b>Determinant:</b> $\det X$ is log-concave on $\mathbb{S}^n_{++}$.</li>
                <li><b>CDF:</b> If a PDF is log-concave, its CDF is log-concave.</li>
            </ul>
        </div>
    </section>

    <section class="section-card" id="section-4">
      <h2>4. Review & Cheat Sheet</h2>
      <h3>Conjugate Transformations</h3>
      <table class="data-table">
        <tr><th>Primal $f(x)$</th><th>Conjugate $f^*(y)$</th></tr>
        <tr><td>$\frac{1}{2}x^\top Q x$ ($Q \succ 0$)</td><td>$\frac{1}{2}y^\top Q^{-1} y$</td></tr>
        <tr><td>$-\log x$</td><td>$-1 - \log(-y)$</td></tr>
        <tr><td>$\log(\sum e^{x_i})$</td><td>$\sum y_i \log y_i$ (Neg Entropy) on $\Delta$</td></tr>
        <tr><td>$I_C(x)$</td><td>$S_C(y)$ (Support Function)</td></tr>
        <tr><td>$\|x\|$</td><td>$I_{B_*}(y)$ (Indicator of dual unit ball)</td></tr>
      </table>
    </section>

    <section class="section-card" id="section-5">
      <h2><i data-feather="edit-3"></i> 5. Exercises</h2>

<div class="problem">
  <h3>P6.1 — Conjugate of Norm Squared</h3>
  <p>Compute the conjugate of $f(x) = \frac{1}{2}\|x\|^2$ for a general norm $\|\cdot\|$. Show it is $\frac{1}{2}\|y\|_*^2$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Definition.</strong>
    $f^*(y) = \sup_x (y^\top x - \frac{1}{2}\|x\|^2)$.</div>
    <div class="proof-step"><strong>Step 2: Dual Norm Bound.</strong>
    By definition of the dual norm, $y^\top x \le \|y\|_* \|x\|$.
    Let $u = \|x\|$. Then $y^\top x - \frac{1}{2}\|x\|^2 \le \|y\|_* u - \frac{1}{2}u^2$.</div>
    <div class="proof-step"><strong>Step 3: Optimize Scalar $u$.</strong>
    The function $g(u) = \|y\|_* u - \frac{1}{2}u^2$ is a concave quadratic.
    Maximum occurs at $g'(u) = \|y\|_* - u = 0 \implies u^* = \|y\|_*$.
    Max value: $\|y\|_*^2 - \frac{1}{2}\|y\|_*^2 = \frac{1}{2}\|y\|_*^2$.</div>
    <div class="proof-step"><strong>Step 4: Tightness.</strong>
    Can we achieve this bound? By definition of the dual norm, there exists a vector $x_0$ with $\|x_0\|=1$ such that $y^\top x_0 = \|y\|_*$.
    Choose $x = \|y\|_* x_0$. Then $\|x\| = \|y\|_*$.
    $y^\top x = \|y\|_* (y^\top x_0) = \|y\|_*^2$.
    Objective: $\|y\|_*^2 - \frac{1}{2}\|y\|_*^2 = \frac{1}{2}\|y\|_*^2$.
    Thus the supremum is exactly $\frac{1}{2}\|y\|_*^2$.</div>
  </div>
</div>
\n<div class="problem">
  <h3>P6.2 — Quasi-Convexity of Ceiling</h3>
  <p>Show that $f(x) = \lceil x \rceil$ (on $\mathbb{R}$) is quasi-convex.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Definition of Quasi-Convexity.</strong>
    A function is quasi-convex if its sublevel sets $S_\alpha = \{x \mid f(x) \le \alpha\}$ are convex for all $\alpha \in \mathbb{R}$.</div>
    <div class="proof-step"><strong>Step 2: Analyze Sublevel Sets.</strong>
    Condition: $\lceil x \rceil \le \alpha$.
    Since $\lceil x \rceil$ is an integer, let $k = \lfloor \alpha \rfloor$. The condition is $\lceil x \rceil \le k$.
    This is equivalent to $x \le k$ (since if $x > k$, $\lceil x \rceil \ge k+1$).
    So $S_\alpha = (-\infty, \lfloor \alpha \rfloor]$.</div>
    <div class="proof-step"><strong>Step 3: Conclusion.</strong>
    The set $(-\infty, k]$ is an interval, which is a convex set in $\mathbb{R}$.
    Therefore, $f$ is quasi-convex.</div>
  </div>
</div>
\n<div class="problem">
  <h3>P6.3 — Softmax Analysis</h3>
  <p>For $f(x) = \log(\sum_{i=1}^n e^{x_i})$: <ol type="a"><li>Show $\max x_i \le f(x) \le \max x_i + \log n$.</li><li>Show $f$ is convex via Hessian analysis.</li></ol></p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Part A: Bounds.</strong>
    Let $x_{\max} = \max_i x_i$.
    Lower bound: $\sum e^{x_i} \ge e^{x_{\max}}$.
    $f(x) \ge \log(e^{x_{\max}}) = x_{\max}$.
    Upper bound: $\sum e^{x_i} = e^{x_{\max}} \sum e^{x_i - x_{\max}}$.
    Since $x_i - x_{\max} \le 0$, each term $e^{x_i - x_{\max}} \le 1$.
    Sum is bounded by $n$.
    $f(x) = x_{\max} + \log(\sum e^{x_i - x_{\max}}) \le x_{\max} + \log n$.</div>
    <div class="proof-step"><strong>Part B: Hessian.</strong>
    Gradient $\nabla f(x) = p$ where $p_i = e^{x_i}/\sum e^{x_k}$. Note $\mathbf{1}^\top p = 1, p > 0$.
    Hessian $\nabla^2 f(x) = \text{diag}(p) - pp^\top$.
    Let $v \in \mathbb{R}^n$.
    $v^\top \nabla^2 f v = \sum p_i v_i^2 - (p^\top v)^2 = \sum p_i v_i^2 - (\sum p_i v_i)^2$.</div>
    <div class="proof-step"><strong>Part B: Variance Interpretation.</strong>
    Consider a random variable $Z$ taking value $v_i$ with probability $p_i$.
    $\mathbb{E}[Z] = \sum p_i v_i$. $\mathbb{E}[Z^2] = \sum p_i v_i^2$.
    The expression is $\mathbb{E}[Z^2] - (\mathbb{E}[Z])^2 = \text{Var}(Z)$.
    Since variance is always non-negative, $v^\top \nabla^2 f v \ge 0$. Thus $f$ is convex.</div>
  </div>
</div>
\n<div class="problem">
  <h3>P6.4 — Concavity of Geometric Mean</h3>
  <p>Prove $G(x) = (\prod_{i=1}^n x_i)^{1/n}$ is concave on $\mathbb{R}^n_{++}$ using log-concavity properties.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Check Log-Concavity.</strong>
    Consider $\log G(x)$.
    $$ \log \left( \prod x_i^{1/n} \right) = \frac{1}{n} \sum_{i=1}^n \log x_i $$
    The function $\log x_i$ is concave. The sum of concave functions is concave.
    Thus $\log G(x)$ is a concave function.
    This means $G(x)$ is <b>log-concave</b>.</div>
    <div class="proof-step"><strong>Step 2: Homogeneity.</strong>
    $G(\alpha x) = (\prod (\alpha x_i))^{1/n} = (\alpha^n \prod x_i)^{1/n} = \alpha G(x)$.
    $G$ is homogeneous of degree 1.</div>
    <div class="proof-step"><strong>Step 3: Theorem.</strong>
    We show that a function $G$ that is log-concave and homogeneous of degree 1 is concave.
    We want to show $G(\theta x + (1-\theta)y) \ge \theta G(x) + (1-\theta)G(y)$.
    By log-concavity, we have:
    $$ G(\theta x + (1-\theta)y) \ge G(x)^\theta G(y)^{1-\theta} $$
    Let $\alpha = G(x)$ and $\beta = G(y)$. If either is 0, the inequality holds trivially (since $G \ge 0$). Assume $\alpha, \beta > 0$.
    Consider the normalized vectors $\tilde{x} = x/\alpha$ and $\tilde{y} = y/\beta$, so $G(\tilde{x}) = G(\tilde{y}) = 1$.
    We want to relate the weighted geometric mean to the weighted arithmetic mean.
    Actually, a simpler argument uses the AM-GM inequality directly on the log-concavity result.
    $$ G(\theta x + (1-\theta)y) \ge \alpha^\theta \beta^{1-\theta} $$
    This lower bound is the weighted geometric mean of the values. Concavity requires the weighted arithmetic mean.
    However, if we choose specific weights based on the function values, we can prove superadditivity $G(x+y) \ge G(x) + G(y)$, which implies concavity for homogeneous functions.
    <br><b>Formal Superadditivity Proof:</b>
    Let $x, y$ be such that $G(x)=\alpha, G(y)=\beta$.
    Let $\lambda = \frac{\alpha}{\alpha+\beta}$. Then $1-\lambda = \frac{\beta}{\alpha+\beta}$.
    Consider the point $z = \frac{x+y}{\alpha+\beta} = \lambda \frac{x}{\alpha} + (1-\lambda) \frac{y}{\beta}$.
    By log-concavity (and thus quasi-concavity), $G(z) \ge G(x/\alpha)^\lambda G(y/\beta)^{1-\lambda} = 1^\lambda 1^{1-\lambda} = 1$.
    By homogeneity, $G(x+y) = (\alpha+\beta) G(z) \ge \alpha + \beta = G(x) + G(y)$.
    Superadditivity plus homogeneity implies concavity.</div>
  </div>
</div>
\n<div class="problem">
  <h3>P6.5 — Geometric Mean Cone</h3>
  <p>Show $K = \{x \in \mathbb{R}^n_+ \mid G(x) \ge \alpha A(x)\}$ is a convex cone for $\alpha \in [0,1]$, where $A(x) = \frac{1}{n}\sum x_i$ is the arithmetic mean.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Rewrite condition.</strong>
    $G(x) - \alpha A(x) \ge 0$.
    Let $h(x) = G(x) - \alpha A(x)$.</div>
    <div class="proof-step"><strong>Step 2: Check Concavity.</strong>
    $G(x)$ is concave (from P6.4).
    $A(x)$ is linear, so $-A(x)$ is concave.
    Since $\alpha \ge 0$, $-\alpha A(x)$ is concave.
    Thus $h(x)$ is a sum of concave functions, so $h$ is concave.</div>
    <div class="proof-step"><strong>Step 3: Superlevel Set.</strong>
    The set $K = \{x \mid h(x) \ge 0\}$ is the 0-superlevel set of a concave function.
    Superlevel sets of concave functions are convex sets.</div>
    <div class="proof-step"><strong>Step 4: Cone Property.</strong>
    $G(kx) = kG(x)$ and $A(kx) = kA(x)$ for $k \ge 0$.
    $G(kx) \ge \alpha A(kx) \iff kG(x) \ge k\alpha A(x) \iff G(x) \ge \alpha A(x)$ (for $k>0$).
    Thus $K$ is a cone.</div>
  </div>
</div>
\n<div class="problem">
  <h3>P6.6 — Matrix Fractional Function</h3>
  <p>Prove convexity of $f(x, Y) = x^\top Y^{-1} x$ (for $Y \succ 0$) using the epigraph characterization.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Epigraph Definition.</strong>
    $(x, Y, t) \in \text{epi } f \iff t \ge x^\top Y^{-1} x$ and $Y \succ 0$.</div>
    <div class="proof-step"><strong>Step 2: Schur Complement.</strong>
    The inequality $t - x^\top Y^{-1} x \ge 0$ is the Schur complement condition for the block matrix:
    $$ M = \begin{bmatrix} Y & x \\ x^\top & t \end{bmatrix} \succeq 0 $$
    (Assuming $Y \succ 0$. If $Y \succeq 0$ singular, we need $x \in \text{range}(Y)$ etc, but usually defined on open domain).</div>
    <div class="proof-step"><strong>Step 3: Convexity of LMI.</strong>
    The set of matrices $\mathcal{S}_+^k$ is a convex cone (PSD cone).
    The map $(x, Y, t) \to M(x, Y, t)$ is linear.
    The inverse image of a convex set under a linear map is convex.
    Thus $\text{epi } f = \{(x, Y, t) \mid M(x, Y, t) \succeq 0\}$ is convex.</div>
  </div>
</div>
\n<div class="problem">
  <h3>P6.7 — Fenchel's Inequality & Biconjugate</h3>
  <p>For $f(x) = \frac{1}{2}x^\top Q x$ with $Q \succ 0$: 1. Verify Fenchel's inequality. 2. Verify $f^{**} = f$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Part 1: Fenchel's Inequality.</strong>
    We need $f(x) + f^*(y) \ge x^\top y$.
    We found $f^*(y) = \frac{1}{2}y^\top Q^{-1} y$.
    LHS - RHS = $\frac{1}{2}x^\top Q x + \frac{1}{2}y^\top Q^{-1} y - x^\top y$.
    Let $u = Q^{1/2}x$ and $v = Q^{-1/2}y$. Then $x = Q^{-1/2}u, y = Q^{1/2}v$.
    $x^\top y = u^\top v$.
    Expression: $\frac{1}{2}u^\top u + \frac{1}{2}v^\top v - u^\top v = \frac{1}{2}\|u-v\|^2 \ge 0$.
    Verified.</div>
    <div class="proof-step"><strong>Part 2: Biconjugate.</strong>
    $f^{**}(x) = (f^*)^*(x)$. Let $g(y) = f^*(y) = \frac{1}{2}y^\top P y$ with $P = Q^{-1}$.
    The conjugate of a quadratic $\frac{1}{2}y^\top P y$ is $\frac{1}{2}x^\top P^{-1} x$.
    $P^{-1} = (Q^{-1})^{-1} = Q$.
    So $f^{**}(x) = \frac{1}{2}x^\top Q x = f(x)$. Verified.</div>
  </div>
</div>
\n<div class="problem">
  <h3>P6.8 — Quasiconvexity of Distance Ratio</h3>
  <p>Show $S_\theta = \{x \mid \|x-a\|_2 \le \theta \|x-b\|_2\}$ is convex for $\theta \le 1$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Square the condition.</strong>
    $\|x-a\|^2 \le \theta^2 \|x-b\|^2$.
    $(x-a)^\top (x-a) \le \theta^2 (x-b)^\top (x-b)$.
    $x^\top x - 2a^\top x + a^\top a \le \theta^2 (x^\top x - 2b^\top x + b^\top b)$.</div>
    <div class="proof-step"><strong>Step 2: Group terms.</strong>
    $(1 - \theta^2) x^\top x - 2(a - \theta^2 b)^\top x + (\|a\|^2 - \theta^2 \|b\|^2) \le 0$.</div>
    <div class="proof-step"><strong>Step 3: Analyze coefficient.</strong>
    Let $q(x)$ be this quadratic expression. The quadratic term is $(1-\theta^2)\|x\|^2$.
    If $\theta \le 1$, then $1-\theta^2 \ge 0$.
    The sublevel set of a convex quadratic ($P \succeq 0$) is convex (an ellipsoid or halfspace).
    Thus $S_\theta$ is convex. (If $\theta > 1$, it's the complement of an ellipsoid, which is non-convex).</div>
  </div>
</div>
\n<div class="problem">
  <h3>P6.9 — Log-Concavity of Probability Measures</h3>
  <p>If $p(x)$ is a log-concave probability density, show that the function $f(x) = \mathbb{P}(x+W \in C)$ is log-concave, where $W \sim p$ and $C$ is a convex set.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Write as Integral.</strong>
    $f(x) = \int_{\mathbb{R}^n} I_C(y) p(y-x) dy$.
    Wait, let's check definition. $x+W \in C \iff W \in C-x$.
    Prob = $\int_{C-x} p(w) dw = \int_{\mathbb{R}^n} I_{C-x}(w) p(w) dw$.
    Let $u = w+x$. Then $w = u-x$. Range $w \in C-x \iff u \in C$.
    $f(x) = \int I_C(u) p(u-x) du$.</div>
    <div class="proof-step"><strong>Step 2: Identify as Convolution.</strong>
    This is the convolution of two functions: $g(x) = I_C(x)$ (indicator 1 if in C, 0 else) and $h(x) = p(-x)$.
    Wait, convolution is $(g * h)(x) = \int g(u) h(x-u) du$.
    The form $\int g(u) p(u-x) du$ is a correlation or convolution with reflected argument.
    Let's use the Prékopa-Leindler inequality or the theorem that <b>convolution of log-concave functions is log-concave</b>.</div>
    <div class="proof-step"><strong>Step 3: Check Log-Concavity of Components.</strong>
    $p(x)$ is log-concave (given).
    $I_C(x)$ is the indicator function of a convex set (1 on C, 0 else).
    Is it log-concave? $\log I_C(x)$ is 0 on C, $-\infty$ else.
    Since $C$ is convex, this is a concave function (extended value).
    So $I_C$ is log-concave.</div>
    <div class="proof-step"><strong>Step 4: Conclusion.</strong>
    Since $f$ is the convolution of two log-concave functions, $f$ is log-concave.
    (Note: The integral is essentially measuring the measure of the set $C$ shifted by $-x$. Prekopa-Leindler directly applies to marginals of log-concave functions).</div>
  </div>
</div>
\n<div class="problem">
  <h3>P6.10 — Log-Concavity of Gaussian</h3>
  <p>Show that the multivariate Gaussian density function $f(x)$ is log-concave. $$ f(x) = \frac{1}{(2\pi)^{n/2} (\det \Sigma)^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^\top \Sigma^{-1} (x-\mu)\right) $$ where $\Sigma \succ 0$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      Take the logarithm:
      $$ \log f(x) = -\frac{n}{2}\log(2\pi) - \frac{1}{2}\log(\det \Sigma) - \frac{1}{2}(x-\mu)^\top \Sigma^{-1} (x-\mu) $$
    </div>
    <div class="proof-step">
      The first two terms are constants. The third term is a quadratic form.
      Let $g(x) = \log f(x)$. The gradient is $\nabla g(x) = -\Sigma^{-1}(x-\mu)$.
      The Hessian is $\nabla^2 g(x) = -\Sigma^{-1}$.
    </div>
    <div class="proof-step">
      Since $\Sigma \succ 0$, its inverse $\Sigma^{-1} \succ 0$, so $-\Sigma^{-1} \prec 0$ (negative definite).
      Since the Hessian is negative definite everywhere, $g(x)$ is strictly concave.
      Therefore, $f(x)$ is log-concave.
    </div>
  </div>
</div>


</section>
    </article>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></a> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initConvexFunctionInspector } from './widgets/js/convex-function-inspector.js';
    initConvexFunctionInspector('widget-convex-function-inspector');
  </script>
  <script type="module">
    import { initHessianHeatmap } from './widgets/js/hessian-heatmap.js';
    initHessianHeatmap('widget-hessian-heatmap');
  </script>
  <script type="module">
    import { initOperationsPreserving } from './widgets/js/operations-preserving.js';
    initOperationsPreserving('widget-operations-preserving');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
