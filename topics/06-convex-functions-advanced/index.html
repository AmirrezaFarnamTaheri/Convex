<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>06. Convex Functions: Advanced Topics — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../05-convex-functions-basics/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../07-convex-problems-standard/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>06. Convex Functions: Advanced Topics</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: conjugate, quasiconvex, log-concave, advanced</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture explores advanced topics in convex analysis that bridge theory and modern applications. We examine the convex conjugate (Fenchel conjugate) and its central role in duality, define quasi-convex functions and their properties, and study log-concave functions, which are essential for probabilistic modeling and volume approximation.</p>
        <p><strong>Prerequisites:</strong> <a href="../05-convex-functions-basics/index.html">Lecture 05: Convex Functions Basics</a>.</p>
        <p><strong>Forward Connections:</strong> The convex conjugate is the key tool for deriving dual problems in <a href="../09-duality/index.html">Lecture 09</a>. Quasi-convexity appears in fractional programming problems in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Compute Conjugates:</b> Derive the convex conjugate for standard functions like norms, quadratics, and log-sum-exp.</li>
        <li><b>Apply Duality Mappings:</b> Connect primal domain properties to dual domain constraints via conjugation.</li>
        <li><b>Analyze Quasi-Convexity:</b> Identify functions defined by convex sublevel sets and apply operations that preserve this property.</li>
        <li><b>Work with Log-Concavity:</b> Recognize log-concave probability distributions and use their properties (integration, marginalization) in modeling.</li>
      </ul>
    </section>

    <article>
      <section class="section-card" id="section-1">
      <h2>1. The Convex Conjugate (Fenchel Conjugate)</h2>

      <h3>1.1 Definition and Geometric Meaning</h3>
      <p>Start with a function $f : \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$. Allowing $+\infty$ is a standard way of encoding hard constraints ("$x$ not allowed"). The <a href="#" class="definition-link" data-term="convex conjugate">Fenchel conjugate</a> is defined as:</p>
      $$
      \boxed{f^*(y) := \sup_{x\in\mathrm{dom} f} \big( y^\top x - f(x) \big), \quad y\in\mathbb{R}^n}
      $$

      <h3>Supporting Hyperplane Viewpoint</h3>
      <p>Fix $y \in \mathbb{R}^n$. Consider affine functions of $x$ with slope $y$:
      $$ \ell_y(x) = y^\top x - c, \quad c \in \mathbb{R} $$
      We want $\ell_y$ to lie <b>below</b> $f$ everywhere:
      $$ y^\top x - c \le f(x) \quad \forall x \iff c \ge y^\top x - f(x) \quad \forall x $$
      For this to hold for all $x$, $c$ must be at least the supremum of the RHS. The <b>smallest</b> possible $c$ that maintains the lower bound property is:
      $$ c_{\min}(y) = \sup_x (y^\top x - f(x)) = f^*(y) $$
      Thus, the tightest affine lower bound with slope $y$ is:
      $$ \ell_y(x) = y^\top x - f^*(y) $$</p>

      <div class="insight">
        <h4>Geometric Intuition</h4>
        <ul>
            <li><b>Epigraph View:</b> The epigraph $\mathrm{epi} f$ is supported by the hyperplane $H = \{(x,t) \mid t = y^\top x - f^*(y)\}$. The conjugate $f^*$ encodes all non-vertical supporting hyperplanes of the epigraph.</li>
            <li><b>Gap View (1D):</b> For a fixed slope $y$, the quantity $yx - f(x)$ is the vertical gap between the line through the origin with slope $y$ and the graph of $f$. $f^*(y)$ is the <b>maximum vertical gap</b>.</li>
        </ul>
      </div>

      <h3>1.2 Why $f^*$ is Always Convex</h3>
      <p>A crucial property is that $f^*$ is convex <b>regardless</b> of whether $f$ is convex.</p>
      <div class="proof-box">
        <h4>Proof</h4>
        <p>Take any fixed $x$. As a function of $y$, the expression $\phi_x(y) := y^\top x - f(x)$ is <b>affine</b> in $y$ (linear term $y^\top x$ plus constant $-f(x)$).
        <br>Now $f^*$ is the pointwise supremum over all these affine functions:
        $$ f^*(y) = \sup_{x} \phi_x(y) $$
        <b>Fact:</b> The pointwise supremum of any family of convex (or affine) functions is convex.
        <br>Explicitly, if $g(y) = \sup_i g_i(y)$ where each $g_i$ is convex, then for any $\theta \in [0,1]$:
        $$
        \begin{aligned}
        g(\theta y_1+(1-\theta)y_2) &= \sup_i g_i(\theta y_1+(1-\theta)y_2) \\
        &\le \sup_i \big( \theta g_i(y_1) + (1-\theta)g_i(y_2) \big) \\
        &\le \theta \sup_i g_i(y_1) + (1-\theta)\sup_i g_i(y_2) \\
        &= \theta g(y_1) + (1-\theta)g(y_2)
        \end{aligned}
        $$
        Thus, $f^*$ is convex for <b>any</b> $f$. The conjugate lives in the "convex world" even if $f$ doesn't.</p>
      </div>

      <h3>1.3 1D Examples in Detail</h3>

      <div class="example">
        <h4>(a) Affine function $f(x) = ax + b$</h4>
        <p>Compute $f^*(y) = \sup_{x} (yx - (ax+b)) = \sup_x ((y-a)x - b)$.</p>
        <ul>
            <li><b>Case 1: $y \ne a$.</b> The term $((y-a)x - b)$ is a line with nonzero slope. As $x \to \pm \infty$, this goes to $+\infty$. So $f^*(y) = +\infty$.</li>
            <li><b>Case 2: $y = a$.</b> The term is constant $-b$. Supremum is $-b$.</li>
        </ul>
        <p><b>Result:</b> $f^*(y) = -b$ if $y=a$, else $+\infty$. Domain is the singleton $\{a\}$. (Indicator of a single slope).</p>
      </div>

      <div class="example">
        <h4>(b) Negative Log: $f(x) = -\log x$ on $(0,\infty)$</h4>
        <p>Maximize $g(x) = yx + \log x$. Derivative $g'(x) = y + 1/x$. Set to 0: $x^* = -1/y$.
        <br>Since $x^* > 0$, we must have $y < 0$. $g''(x) = -1/x^2 < 0$ (concave max).
        <br>Value: $y(-1/y) + \log(-1/y) = -1 - \log(-y)$.
        <br>If $y \ge 0$, $yx + \log x \to \infty$ as $x \to \infty$.
        <br><b>Result:</b> $f^*(y) = -1 - \log(-y)$ for $y < 0$. Domain $(-\infty, 0)$.</p>
      </div>

      <div class="example">
        <h4>(c) Exponential: $f(x) = e^x$ on $\mathbb{R}$</h4>
        <p>Maximize $g(x) = yx - e^x$. Derivative $g'(x) = y - e^x = 0 \implies x^* = \log y$.
        <br>Requires $y > 0$. Value: $y \log y - e^{\log y} = y \log y - y$.
        <br>If $y=0$, $\sup (-e^x) = 0$ (as $x \to -\infty$).
        <br>If $y < 0$, let $x \to -\infty$. Then $e^x \to 0$ and $yx \to +\infty$ (negative times negative). Sup is $+\infty$.
        <br><b>Result:</b> $f^*(y) = y \log y - y$ for $y \ge 0$ (with $0 \log 0 = 0$), else $\infty$. Domain $[0, \infty)$.</p>
      </div>

      <div class="example">
        <h4>(d) Inverse: $f(x) = 1/x$ on $(0,\infty)$</h4>
        <p>Maximize $g(x) = yx - 1/x$. Derivative $g'(x) = y + 1/x^2 = 0 \implies x^2 = -1/y$.
        <br>Requires $y < 0$. Solution $x^* = \sqrt{-1/y}$.
        <br>Value: $y\sqrt{-1/y} - \sqrt{-y} = -\sqrt{-y} - \sqrt{-y} = -2\sqrt{-y}$.
        <br>If $y \ge 0$, $yx - 1/x \to \infty$ as $x \to \infty$.
        <br><b>Result:</b> $f^*(y) = -2\sqrt{-y}$ for $y < 0$. Domain $(-\infty, 0)$.</p>
      </div>

      <h3>1.4 Quadratic and Matrix Examples</h3>

      <div class="example">
        <h4>(a) Quadratic: $f(x) = \frac{1}{2} x^\top Q x$ with $Q \succ 0$</h4>
        <p>Maximize $\phi(x) = y^\top x - \frac{1}{2} x^\top Q x$. This is a concave quadratic.
        <br>Gradient $\nabla \phi(x) = y - Qx = 0 \implies x^* = Q^{-1}y$.
        <br>Value: $y^\top (Q^{-1}y) - \frac{1}{2} (Q^{-1}y)^\top Q (Q^{-1}y) = y^\top Q^{-1} y - \frac{1}{2} y^\top Q^{-1} y = \frac{1}{2} y^\top Q^{-1} y$.
        <br><b>Result:</b> Quadratic with matrix $Q$ conjugates to quadratic with $Q^{-1}$.</p>
      </div>

      <div class="example">
        <h4>(b) Log-Determinant: $f(X) = -\log \det X$ on $\mathbb{S}^n_{++}$</h4>
        <p>Maximize $\mathrm{tr}(YX) + \log \det X$ over $X \succ 0$.
        <br>Gradient: $Y + X^{-1} = 0 \implies X = -Y^{-1}$.
        <br>Requires $Y$ invertible and $-Y^{-1} \succ 0 \implies Y \prec 0$ (negative definite).
        <br>Value: $\mathrm{tr}(Y(-Y^{-1})) + \log \det (-Y^{-1}) = -n - \log \det(-Y)$.
        <br><b>Result:</b> $f^*(Y) = -\log \det(-Y) - n$ on domain $\mathbb{S}^n_{--}$.</p>
      </div>

      <div class="example">
        <h4>(c) Indicator $\leftrightarrow$ Support Function</h4>
        <p>Let $I_S(x)$ be the indicator of a set $S$ (0 if $x \in S$, $+\infty$ else).
        <br>$I_S^*(y) = \sup_x (y^\top x - I_S(x)) = \sup_{x \in S} (y^\top x)$.
        <br><b>Result:</b> $I_S^* = \sigma_S$, the support function of $S$.</p>
      </div>

      <div class="example">
        <h4>(d) Norm $\leftrightarrow$ Indicator of Dual Ball</h4>
        <p>Let $f(x) = \|x\|$. Dual norm definition: $\|y\|_* = \sup_{\|x\|\le 1} y^\top x$.
        <br>$f^*(y) = \sup_x (y^\top x - \|x\|)$.
        <br>Case 1: $\|y\|_* \le 1$. Then $y^\top x \le \|y\|_* \|x\| \le \|x\|$. So $y^\top x - \|x\| \le 0$. Max is 0 (at $x=0$).
        <br>Case 2: $\|y\|_* > 1$. There exists $x_0$ with $\|x_0\|=1$ and $y^\top x_0 > 1$. Let $x = t x_0$. Value $t(y^\top x_0 - 1) \to \infty$.
        <br><b>Result:</b> $f^*$ is the indicator of the unit ball of the dual norm $\|\cdot\|_*$.
        $$ f^*(y) = \begin{cases} 0 & \|y\|_* \le 1 \\ +\infty & \|y\|_* > 1 \end{cases} $$</p>
      </div>

      <h3>1.5 Algebra Rules for Conjugates</h3>

      <h4>(a) Scaling and Vertical Shift</h4>
      <p>Let $g(x) = a f(x) + b$ with $a > 0, b \in \mathbb{R}$.
      $$
      \begin{aligned}
      g^*(y) &= \sup_x (y^\top x - a f(x) - b) \\
      &= \sup_x (y^\top x - a f(x)) - b \\
      &= a \sup_x \left( \frac{y}{a}^\top x - f(x) \right) - b \\
      &= a f^*(y/a) - b
      \end{aligned}
      $$
      So $\boxed{(af+b)^*(y) = a f^*(y/a) - b}$.</p>

      <h4>(b) Affine Precomposition</h4>
      <p>Let $g(x) = f(Ax + b)$.
      $$ g^*(y) = \sup_x (y^\top x - f(Ax+b)) $$
      Let $z = Ax+b$. If $A$ is invertible, $x = A^{-1}(z-b)$.
      $$
      \begin{aligned}
      g^*(y) &= \sup_z (y^\top A^{-1}(z-b) - f(z)) \\
      &= \sup_z ((A^{-\top}y)^\top z - f(z)) - y^\top A^{-1}b \\
      &= f^*(A^{-\top}y) - y^\top A^{-1}b
      \end{aligned}
      $$
      If $A$ is not square/invertible, we use the general rule from convex analysis involving an infimum over preimages:
      $$ g^*(y) = \inf \{ f^*(z) - z^\top b \mid A^\top z = y \} $$
      If $A$ is invertible, $z$ is uniquely determined as $z = A^{-\top}y$, and the infimum collapses to the simple substitution:
      $$ \boxed{ g^*(y) = f^*(A^{-\top}y) - y^\top A^{-1}b } \quad (\text{invertible case}) $$</p>

      <h4>(c) Sum of Independent Functions</h4>
      <p>Let $f(x, z) = f_1(x) + f_2(z)$.
      $$
      \begin{aligned}
      f^*(y, w) &= \sup_{x,z} (y^\top x + w^\top z - f_1(x) - f_2(z)) \\
      &= \sup_x (y^\top x - f_1(x)) + \sup_z (w^\top z - f_2(z)) \\
      &= f_1^*(y) + f_2^*(w)
      \end{aligned}
      $$
      So $\boxed{(f_1 \oplus f_2)^* = f_1^* \oplus f_2^*}$. This separation is key in duality.</p>

      <h3>1.6 Fenchel Inequality and Biconjugate</h3>
      <p>From the definition $f^*(y) \ge y^\top x - f(x)$, we immediately get <b>Fenchel's Inequality</b>:</p>
      $$ \boxed{ f(x) + f^*(y) \ge x^\top y } $$
      <p>Equality holds exactly when $y \in \partial f(x)$ (for convex $f$).</p>

      <h4>The Biconjugate $f^{**}$</h4>
      <p>The conjugate of the conjugate is $f^{**}(x) = \sup_y (x^\top y - f^*(y))$.
      <br><b>Theorem:</b> $f^{**} \le f$ always. If $f$ is closed and convex, then $f^{**} = f$.
      <br><i>Geometric Intuition:</i> $\mathrm{epi}(f^{**})$ is the closed convex hull of $\mathrm{epi}(f)$. Conjugation encodes all supporting hyperplanes; taking it again reconstructs the convex envelope from these planes.</p>

      <h3>1.7 Legendre Transform: Smooth Case</h3>
      <p>Assume $f$ is strictly convex and differentiable everywhere. The optimizer $x^*$ in the definition of $f^*(y)$ is characterized by:
      $$ \nabla_x (y^\top x - f(x)) = y - \nabla f(x) = 0 \implies y = \nabla f(x^*) $$
      Thus $x^* = (\nabla f)^{-1}(y)$. We can write:
      $$ f^*(y) = y^\top (\nabla f)^{-1}(y) - f((\nabla f)^{-1}(y)) $$
      Alternatively, parametrizing by $z$ where $y = \nabla f(z)$, we get a cleaner form:
      $$ f^*(\nabla f(z)) = z^\top \nabla f(z) - f(z) $$
      <b>Nice Identities:</b>
      <ul>
          <li>The gradient maps are inverses: $\nabla f^*(y) = x$ where $y = \nabla f(x)$. So $\nabla f^* = (\nabla f)^{-1}$.</li>
          <li>At the optimum, Fenchel's inequality holds with equality: $f(x) + f^*(y) = x^\top y$ where $y = \nabla f(x)$.</li>
      </ul>
      This transformation connects Lagrangian and Hamiltonian mechanics.</p>
    </section>

    <section class="section-card" id="section-2">
        <h2>2. Quasiconvex Functions</h2>

        <h3>2.1 Definition: Sublevel Sets</h3>
        <p>A function $f: \mathbb{R}^n \to \mathbb{R}$ is <a href="#" class="definition-link">quasiconvex</a> if its domain is convex and all its <b>sublevel sets</b> are convex:</p>
        $$
        S_\alpha := \{x \in \mathrm{dom}\, f \mid f(x) \le \alpha\}
        $$
        <p>is a convex set for every $\alpha \in \mathbb{R}$.
        <br><i>Note:</i> Convex $\implies$ Quasiconvex, but not conversely. Quasiconvexity allows "bendy" graphs, as long as the "valley" shape is maintained.</p>

        <h3>2.2 Simple 1D Examples</h3>
        <div class="example">
            <h4>(a) Logarithm: $f(x) = \log x$</h4>
            <p>Sublevel set $\{x > 0 \mid \log x \le \alpha\} = \{x \mid x \le e^\alpha\} = (0, e^\alpha]$. This is a convex interval.
            <br>So $\log x$ is quasiconvex. (It is also concave, so $-\log x$ is convex).</p>
        </div>
        <div class="example">
            <h4>(b) Ceiling: $f(x) = \lceil x \rceil$</h4>
            <p>Sublevel set $\{x \mid \lceil x \rceil \le k\} = (-\infty, k]$. This is an interval.
            <br>So $\lceil x \rceil$ is quasiconvex. (It is step-like and definitely not convex).</p>
        </div>

        <h3>2.3 Vector Examples</h3>
        <div class="example">
            <h4>1. Length of a Vector</h4>
            <p>Define len$(x) = \max\{i \mid x_i \ne 0\}$ (index of last nonzero element).
            <br>Sublevel set $\{x \mid \text{len}(x) \le k\} = \{x \mid x_{k+1} = \dots = x_n = 0\}$.
            <br>This is a linear subspace, hence convex. So len$(x)$ is quasiconvex.</p>
        </div>
        <div class="example">
            <h4>2. Bilinear: $f(x_1, x_2) = x_1 x_2$ on $\mathbb{R}^2_{++}$</h4>
            <p>Hessian is indefinite (not convex).
            <br>Superlevel set $\{x \in \mathbb{R}^2_{++} \mid x_1 x_2 \ge \alpha\}$ is convex (bounded by hyperbola).
            <br>Thus $f$ is <b>quasiconcave</b> on the positive quadrant.</p>
        </div>
        <div class="example">
            <h4>3. Linear-Fractional: $f(x) = \frac{a^\top x + b}{c^\top x + d}$</h4>
            <p>Defined on the halfspace $D = \{x \mid c^\top x + d > 0\}$.
            <br>Consider the sublevel set $S_\alpha = \{x \in D \mid \frac{a^\top x + b}{c^\top x + d} \le \alpha\}$.
            $$ \frac{a^\top x + b}{c^\top x + d} \le \alpha \iff a^\top x + b \le \alpha (c^\top x + d) \iff (a - \alpha c)^\top x + (b - \alpha d) \le 0 $$
            This is a linear inequality, which defines a closed halfspace.
            <br>Thus $S_\alpha$ is the intersection of the domain $D$ (halfspace) and another halfspace. The intersection of convex sets is convex.
            <br>Conclusion: Linear-fractional functions are quasiconvex. (Applying the same logic to $-f$ shows they are also quasiconcave, i.e., <b>quasilinear</b>).</p>
        </div>
        <div class="example">
            <h4>4. Distance Ratio</h4>
            <p>Let $f(x) = \frac{\|x-a\|_2}{\|x-b\|_2}$ for $a \ne b$.
            <br>Sublevel set condition: $\|x-a\|_2 \le \alpha \|x-b\|_2$.
            <br>Square both sides (valid since non-negative):
            $$ \|x-a\|^2 \le \alpha^2 \|x-b\|^2 \iff (x-a)^\top(x-a) \le \alpha^2 (x-b)^\top(x-b) $$
            Expanding terms:
            $$ x^\top x - 2a^\top x + a^\top a \le \alpha^2 (x^\top x - 2b^\top x + b^\top b) $$
            Grouping quadratic and linear terms:
            $$ (1-\alpha^2) x^\top x - 2(a - \alpha^2 b)^\top x + (\|a\|^2 - \alpha^2 \|b\|^2) \le 0 $$
            For $\alpha \le 1$, the coefficient $(1-\alpha^2) \ge 0$. This inequality describes a convex set (a ball or halfspace).
            <br>Thus $f$ is quasiconvex on the set $\{x \mid f(x) \le 1\}$.</p>
        </div>
        <div class="example">
            <h4>5. Internal Rate of Return (IRR)</h4>
            <p>Let $x = (x_0, \dots, x_n)$ be a cash flow stream.
            <br>Present value at rate $r$: $PV(x, r) = \sum_{i=0}^n x_i (1+r)^{-i}$.
            <br>IRR is defined as $\mathrm{IRR}(x) = \inf \{r \ge 0 \mid PV(x, r) = 0\}$.
            <br>Consider the superlevel set $S_R = \{x \mid \mathrm{IRR}(x) \ge R\}$.
            <br>Assuming standard cash flows (initial investment, then returns), $\mathrm{IRR}(x) \ge R$ roughly means the present value at rate $R$ is non-negative:
            $$ PV(x, R) = \sum_{i=0}^n (1+R)^{-i} x_i \ge 0 $$
            For a fixed $R$, this is a <b>linear inequality</b> in $x$.
            <br>A superlevel set formed by a linear inequality is a halfspace (convex).
            <br>So $\mathrm{IRR}(x)$ is a <b>quasiconcave</b> function of the cash flows.</p>
        </div>

        <h3>2.4 Second-Order Condition</h3>
        <p>Assume $f$ is twice differentiable.</p>

        <h4>Necessary Condition</h4>
        <p>If $f$ is quasiconvex, then for any $x \in \mathrm{dom}\, f$ and $y \in \mathbb{R}^n$:
        $$ y^\top \nabla f(x) = 0 \implies y^\top \nabla^2 f(x) y \ge 0 $$
        <i>Interpretation:</i> If we look in a direction $y$ tangent to the level set ($\nabla f \perp y$), the function must curve upwards (positive curvature). The level sets cannot curve "inward" to create a disconnected or non-convex shape.
        <br><b>1D Case:</b> This reduces to $f'(x) = 0 \implies f''(x) \ge 0$. A quasiconvex function on $\mathbb{R}$ cannot have a strict local maximum (which would have $f'=0, f''<0$). It effectively means the function has at most one "valley".</p>

        <h4>Sufficient Condition</h4>
        <p>If $f$ satisfies the stronger condition:
        $$ y^\top \nabla f(x) = 0, \ y \ne 0 \implies y^\top \nabla^2 f(x) y > 0 $$
        then $f$ is <b>strictly quasiconvex</b>. This ensures no "flat" regions along the contours that could hide non-convexity.</p>

        <h3>2.5 Operations Preserving Quasiconvexity</h3>
        <ul>
            <li><b>Max:</b> $f(x) = \max_i f_i(x)$ is quasiconvex (Intersection of convex sublevel sets).</li>
            <li><b>Composition:</b> $g(h(x))$ is quasiconvex if $h$ is quasiconvex and $g$ is <b>non-decreasing</b>.</li>
            <li><b>Warning:</b> Sums of quasiconvex functions are generally <b>not</b> quasiconvex.</li>
        </ul>
    </section>

    <section class="section-card" id="section-3">
        <h2>3. Log-Concave and Log-Convex Functions</h2>

        <h3>3.1 Definitions</h3>
        <p>Let $f: \mathbb{R}^n \to \mathbb{R}_{++}$.
        <ul>
            <li>$f$ is <b>log-concave</b> if $\log f(x)$ is concave.</li>
            <li>$f$ is <b>log-convex</b> if $\log f(x)$ is convex.</li>
        </ul>
        Equivalently, for log-concavity (multiplicative form):
        $$ f(\theta x + (1-\theta)y) \ge f(x)^\theta f(y)^{1-\theta} $$
        Log-concavity is crucial in probability (unimodality, tail bounds).</p>

        <h3>3.2 Examples</h3>
        <div class="example">
            <h4>1. Uniform Distribution on Convex Set</h4>
            <p>Let $C$ be convex. $f(x) = 1/\alpha$ if $x \in C$, else 0.
            <br>$\log f(x) = -\log \alpha$ on $C$, $-\infty$ outside.
            <br>This is a concave function (indicator of convex set). So uniform density is log-concave.</p>
        </div>
        <div class="example">
            <h4>2. Wishart Distribution</h4>
            <p>Density $f(X) \propto (\det X)^{k} e^{-\mathrm{tr}(\Sigma^{-1}X)}$.
            <br>$\log f(X) = c + k \log \det X - \mathrm{tr}(\Sigma^{-1}X)$.
            <br>$\log \det$ is concave; trace is linear. Sum is concave.
            <br>Thus Wishart is log-concave.</p>
        </div>

        <h3>3.3 Integration Rules</h3>
        <p>Log-convexity and log-concavity behave nicely under integration, though the conditions differ.</p>
        <div class="theorem-box">
            <h4>(a) Integrals of Log-Convex Functions</h4>
            <p>If $f(x, y) \ge 0$ is log-convex in $x$ for each fixed $y$, then $g(x) = \int_C f(x, y) dy$ is log-convex.</p>
            <div class="insight">
                <h4>Intuition Sketch</h4>
                <p>We need $g(\theta x + (1-\theta)z) \le g(x)^\theta g(z)^{1-\theta}$.
                <br>By log-convexity of $f$:
                $$ f(\theta x + (1-\theta)z, y) \le f(x, y)^\theta f(z, y)^{1-\theta} $$
                Integrate both sides over $y$. The RHS integral is bounded using <b>Hölder's Inequality</b> for integrals (with $p=1/\theta, q=1/(1-\theta)$):
                $$ \int f(x, y)^\theta f(z, y)^{1-\theta} dy \le \left(\int f(x, y) dy\right)^\theta \left(\int f(z, y) dy\right)^{1-\theta} = g(x)^\theta g(z)^{1-\theta} $$
                This proves log-convexity of $g$.
                <br><b>Examples:</b> Gamma function, Moment Generating Function ($M(z) = \mathbb{E} e^{z^\top X}$), Laplace Transform.</p>
            </div>
        </div>
        <div class="theorem-box">
            <h4>(b) Integrals of Log-Concave Functions (Prékopa-Leindler)</h4>
            <p>If $f(x, y)$ is <b>jointly</b> log-concave in $(x, y)$, then the marginal $g(x) = \int f(x, y) dy$ is log-concave.
            <br><b>Consequence:</b> Convolution of log-concave functions is log-concave. If $f, g$ are log-concave, so is $(f*g)(x) = \int f(x-y)g(y) dy$.
            <br><i>Why?</i> The integrand $f(x-y)g(y)$ is log-concave in $(x, y)$ (product of log-concave functions). Integrating out $y$ preserves log-concavity in $x$.</p>
        </div>

        <h3>3.4 Probability Examples</h3>
        <div class="example">
            <h4>(a) Hitting a Convex Set with Log-Concave Noise</h4>
            <p>Let $w$ be a random vector with log-concave density $p(w)$. Let $C$ be a convex set. Define:
            $$ f(x) = \mathbb{P}(x + w \in C) $$
            We can express this as a convolution:
            $$ f(x) = \int \mathbf{1}_C(x+w) p(w) dw $$
            Change of variables $u = x+w \implies w = u-x$:
            $$ f(x) = \int \mathbf{1}_C(u) p(u-x) du $$
            Consider the integrand $F(x, u) = \mathbf{1}_C(u) p(u-x)$.
            <ul>
                <li>$\mathbf{1}_C(u)$ is log-concave (indicator of convex set).</li>
                <li>$p(u-x)$ is log-concave in $(x, u)$ (composition of log-concave $p$ with affine map).</li>
                <li>The product is log-concave in $(x, u)$.</li>
            </ul>
            By the integration rule, $f(x)$ is log-concave.
            <br><i>Interpretation:</i> The probability of a random point landing in a moving target $C$ varies "smoothly" (unimodally) as we shift the center $x$.</p>
        </div>
        <div class="example">
            <h4>(b) Cumulative Distribution Function (CDF)</h4>
            <p>If a PDF $p(z)$ is log-concave on $\mathbb{R}^n$, then its CDF is log-concave:
            $$ F(x) = \mathbb{P}(w \preceq x) = \int \mathbf{1}_{(-\infty, x]}(z) p(z) dz $$
            We can view the indicator $\mathbf{1}_{(-\infty, x]}(z)$ as the function:
            $$ I(x, z) = \begin{cases} 1 & z_i \le x_i \ \forall i \\ 0 & \text{otherwise} \end{cases} $$
            The set $\{(x, z) \mid z_i \le x_i\}$ is a convex polyhedron (defined by linear inequalities). Thus its indicator $I(x, z)$ is log-concave in $(x, z)$.
            <br>Since $p(z)$ is log-concave, the product $I(x, z)p(z)$ is jointly log-concave.
            <br>Integrating out $z$ implies $F(x)$ is log-concave.
            <br><b>Example:</b> The Gaussian CDF $\Phi(x)$ is log-concave.</p>
        </div>
    </section>

    <section class="section-card" id="section-4">
      <h2>4. Review & Cheat Sheet</h2>
      <h3>Conjugate Transformations</h3>
      <table class="data-table">
        <tr><th>Primal $f(x)$</th><th>Conjugate $f^*(y)$</th><th>Domain of $f^*$</th></tr>
        <tr><td>$\frac{1}{2}x^\top Q x$ ($Q \succ 0$)</td><td>$\frac{1}{2}y^\top Q^{-1} y$</td><td>$\mathbb{R}^n$</td></tr>
        <tr><td>$-\log x$</td><td>$-1 - \log(-y)$</td><td>$y < 0$</td></tr>
        <tr><td>$e^x$</td><td>$y \log y - y$</td><td>$y \ge 0$</td></tr>
        <tr><td>$1/x$ ($x>0$)</td><td>$-2\sqrt{-y}$</td><td>$y < 0$</td></tr>
        <tr><td>$I_C(x)$</td><td>$\sigma_C(y)$ (Support)</td><td>$\mathbb{R}^n$</td></tr>
        <tr><td>$\|x\|$</td><td>$I_{B_*}(y)$ (Dual Ball)</td><td>$\|y\|_* \le 1$</td></tr>
      </table>

      <h3>Key Concepts</h3>
      <ul>
          <li><b>Conjugate $f^*$:</b> Best linear lower bound (support function of epigraph). Always convex.</li>
          <li><b>Fenchel Inequality:</b> $f(x) + f^*(y) \ge x^\top y$.</li>
          <li><b>Quasiconvex:</b> Convex sublevel sets. $f(\theta x + (1-\theta)y) \le \max(f(x), f(y))$.</li>
          <li><b>Log-Concave:</b> $\log f$ is concave. Closed under product, marginals, convolution.</li>
      </ul>
    </section>

    <section class="section-card" id="section-5">
      <h2><i data-feather="edit-3"></i> 5. Exercises</h2>

<div class="problem">
  <h3>P6.1 — Conjugate of Norm Squared</h3>
  <p>Compute the conjugate of $f(x) = \frac{1}{2}\|x\|^2$ for a general norm $\|\cdot\|$. Show it is $\frac{1}{2}\|y\|_*^2$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Definition.</strong>
    $f^*(y) = \sup_x (y^\top x - \frac{1}{2}\|x\|^2)$.</div>
    <div class="proof-step"><strong>Step 2: Dual Norm Bound.</strong>
    By definition of the dual norm, $y^\top x \le \|y\|_* \|x\|$.
    Let $u = \|x\|$. Then $y^\top x - \frac{1}{2}\|x\|^2 \le \|y\|_* u - \frac{1}{2}u^2$.</div>
    <div class="proof-step"><strong>Step 3: Optimize Scalar $u$.</strong>
    The function $g(u) = \|y\|_* u - \frac{1}{2}u^2$ is a concave quadratic.
    Maximum occurs at $g'(u) = \|y\|_* - u = 0 \implies u^* = \|y\|_*$.
    Max value: $\|y\|_*^2 - \frac{1}{2}\|y\|_*^2 = \frac{1}{2}\|y\|_*^2$.</div>
    <div class="proof-step"><strong>Step 4: Tightness.</strong>
    Can we achieve this bound? By definition of the dual norm, there exists a vector $x_0$ with $\|x_0\|=1$ such that $y^\top x_0 = \|y\|_*$.
    Choose $x = \|y\|_* x_0$. Then $\|x\| = \|y\|_*$.
    $y^\top x = \|y\|_* (y^\top x_0) = \|y\|_*^2$.
    Objective: $\|y\|_*^2 - \frac{1}{2}\|y\|_*^2 = \frac{1}{2}\|y\|_*^2$.
    Thus the supremum is exactly $\frac{1}{2}\|y\|_*^2$.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.2 — Quasi-Convexity of Ceiling</h3>
  <p>Show that $f(x) = \lceil x \rceil$ (on $\mathbb{R}$) is quasi-convex.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Definition of Quasi-Convexity.</strong>
    A function is quasi-convex if its sublevel sets $S_\alpha = \{x \mid f(x) \le \alpha\}$ are convex for all $\alpha \in \mathbb{R}$.</div>
    <div class="proof-step"><strong>Step 2: Analyze Sublevel Sets.</strong>
    Condition: $\lceil x \rceil \le \alpha$.
    Since $\lceil x \rceil$ is an integer, let $k = \lfloor \alpha \rfloor$. The condition is $\lceil x \rceil \le k$.
    This is equivalent to $x \le k$ (since if $x > k$, $\lceil x \rceil \ge k+1$).
    So $S_\alpha = (-\infty, \lfloor \alpha \rfloor]$.</div>
    <div class="proof-step"><strong>Step 3: Conclusion.</strong>
    The set $(-\infty, k]$ is an interval, which is a convex set in $\mathbb{R}$.
    Therefore, $f$ is quasi-convex.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.3 — Softmax Analysis</h3>
  <p>For $f(x) = \log(\sum_{i=1}^n e^{x_i})$: <ol type="a"><li>Show $\max x_i \le f(x) \le \max x_i + \log n$.</li><li>Show $f$ is convex via Hessian analysis.</li></ol></p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Part A: Bounds.</strong>
    Let $x_{\max} = \max_i x_i$.
    Lower bound: $\sum e^{x_i} \ge e^{x_{\max}}$.
    $f(x) \ge \log(e^{x_{\max}}) = x_{\max}$.
    Upper bound: $\sum e^{x_i} = e^{x_{\max}} \sum e^{x_i - x_{\max}}$.
    Since $x_i - x_{\max} \le 0$, each term $e^{x_i - x_{\max}} \le 1$.
    Sum is bounded by $n$.
    $f(x) = x_{\max} + \log(\sum e^{x_i - x_{\max}}) \le x_{\max} + \log n$.</div>
    <div class="proof-step"><strong>Part B: Hessian.</strong>
    Gradient $\nabla f(x) = p$ where $p_i = e^{x_i}/\sum e^{x_k}$. Note $\mathbf{1}^\top p = 1, p > 0$.
    Hessian $\nabla^2 f(x) = \text{diag}(p) - pp^\top$.
    Let $v \in \mathbb{R}^n$.
    $v^\top \nabla^2 f v = \sum p_i v_i^2 - (p^\top v)^2 = \sum p_i v_i^2 - (\sum p_i v_i)^2$.</div>
    <div class="proof-step"><strong>Part C: Variance Interpretation.</strong>
    Consider a discrete random variable $Z$ that takes value $v_i$ with probability $p_i$ (where $p$ is the softmax vector defined above).
    <br>The term $\sum p_i v_i$ is the expected value $\mathbb{E}[Z]$.
    <br>The term $\sum p_i v_i^2$ is the second moment $\mathbb{E}[Z^2]$.
    <br>The expression for the quadratic form is $\mathbb{E}[Z^2] - (\mathbb{E}[Z])^2$, which is exactly the variance $\text{Var}(Z)$.
    <br>Since variance is always non-negative, $v^\top \nabla^2 f v \ge 0$. Thus $f$ is convex.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.4 — Concavity of Geometric Mean</h3>
  <p>Prove $G(x) = (\prod_{i=1}^n x_i)^{1/n}$ is concave on $\mathbb{R}^n_{++}$ using log-concavity properties.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Check Log-Concavity.</strong>
    Consider $\log G(x)$.
    $$ \log \left( \prod x_i^{1/n} \right) = \frac{1}{n} \sum_{i=1}^n \log x_i $$
    The function $\log x_i$ is concave. The sum of concave functions is concave.
    Thus $\log G(x)$ is a concave function.
    This means $G(x)$ is <b>log-concave</b>.</div>
    <div class="proof-step"><strong>Step 2: Homogeneity.</strong>
    $G(\alpha x) = (\prod (\alpha x_i))^{1/n} = (\alpha^n \prod x_i)^{1/n} = \alpha G(x)$.
    $G$ is homogeneous of degree 1.</div>
    <div class="proof-step"><strong>Step 3: Theorem.</strong>
    We show that a function $G$ that is log-concave and homogeneous of degree 1 is concave.
    We want to show $G(\theta x + (1-\theta)y) \ge \theta G(x) + (1-\theta)G(y)$.
    By log-concavity, we have:
    $$ G(\theta x + (1-\theta)y) \ge G(x)^\theta G(y)^{1-\theta} $$
    Let $\alpha = G(x)$ and $\beta = G(y)$. If either is 0, the inequality holds trivially (since $G \ge 0$). Assume $\alpha, \beta > 0$.
    Consider the normalized vectors $\tilde{x} = x/\alpha$ and $\tilde{y} = y/\beta$, so $G(\tilde{x}) = G(\tilde{y}) = 1$.
    We want to relate the weighted geometric mean to the weighted arithmetic mean.
    Actually, a simpler argument uses the AM-GM inequality directly on the log-concavity result.
    $$ G(\theta x + (1-\theta)y) \ge \alpha^\theta \beta^{1-\theta} $$
    This lower bound is the weighted geometric mean of the values. Concavity requires the weighted arithmetic mean.
    However, if we choose specific weights based on the function values, we can prove superadditivity $G(x+y) \ge G(x) + G(y)$, which implies concavity for homogeneous functions.
    <br><b>Formal Superadditivity Proof:</b>
    Let $x, y$ be such that $G(x)=\alpha, G(y)=\beta$.
    Let $\lambda = \frac{\alpha}{\alpha+\beta}$. Then $1-\lambda = \frac{\beta}{\alpha+\beta}$.
    Consider the point $z = \frac{x+y}{\alpha+\beta} = \lambda \frac{x}{\alpha} + (1-\lambda) \frac{y}{\beta}$.
    By log-concavity (and thus quasi-concavity), $G(z) \ge G(x/\alpha)^\lambda G(y/\beta)^{1-\lambda} = 1^\lambda 1^{1-\lambda} = 1$.
    By homogeneity, $G(x+y) = (\alpha+\beta) G(z) \ge \alpha + \beta = G(x) + G(y)$.
    Superadditivity plus homogeneity implies concavity.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.5 — Geometric Mean Cone</h3>
  <p>Show $K = \{x \in \mathbb{R}^n_+ \mid G(x) \ge \alpha A(x)\}$ is a convex cone for $\alpha \in [0,1]$, where $A(x) = \frac{1}{n}\sum x_i$ is the arithmetic mean.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Rewrite condition.</strong>
    $G(x) - \alpha A(x) \ge 0$.
    Let $h(x) = G(x) - \alpha A(x)$.</div>
    <div class="proof-step"><strong>Step 2: Check Concavity.</strong>
    $G(x)$ is concave (from P6.4).
    $A(x)$ is linear, so $-A(x)$ is concave.
    Since $\alpha \ge 0$, $-\alpha A(x)$ is concave.
    Thus $h(x)$ is a sum of concave functions, so $h$ is concave.</div>
    <div class="proof-step"><strong>Step 3: Superlevel Set.</strong>
    The set $K = \{x \mid h(x) \ge 0\}$ is the 0-superlevel set of a concave function.
    Superlevel sets of concave functions are convex sets.</div>
    <div class="proof-step"><strong>Step 4: Cone Property.</strong>
    $G(kx) = kG(x)$ and $A(kx) = kA(x)$ for $k \ge 0$.
    $G(kx) \ge \alpha A(kx) \iff kG(x) \ge k\alpha A(x) \iff G(x) \ge \alpha A(x)$ (for $k>0$).
    Thus $K$ is a cone.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.6 — Matrix Fractional Function</h3>
  <p>Prove convexity of $f(x, Y) = x^\top Y^{-1} x$ (for $Y \succ 0$) using the epigraph characterization.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Epigraph Definition.</strong>
    $(x, Y, t) \in \text{epi } f \iff t \ge x^\top Y^{-1} x$ and $Y \succ 0$.</div>
    <div class="proof-step"><strong>Step 2: Schur Complement.</strong>
    The inequality $t - x^\top Y^{-1} x \ge 0$ is the Schur complement condition for the block matrix:
    $$ M = \begin{bmatrix} Y & x \\ x^\top & t \end{bmatrix} \succeq 0 $$
    (Assuming $Y \succ 0$. If $Y \succeq 0$ singular, we need $x \in \text{range}(Y)$ etc, but usually defined on open domain).</div>
    <div class="proof-step"><strong>Step 3: Convexity of LMI.</strong>
    The set of matrices $\mathcal{S}_+^k$ is a convex cone (PSD cone).
    The map $(x, Y, t) \to M(x, Y, t)$ is linear.
    The inverse image of a convex set under a linear map is convex.
    Thus $\text{epi } f = \{(x, Y, t) \mid M(x, Y, t) \succeq 0\}$ is convex.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.7 — Fenchel's Inequality & Biconjugate</h3>
  <p>For $f(x) = \frac{1}{2}x^\top Q x$ with $Q \succ 0$: 1. Verify Fenchel's inequality. 2. Verify $f^{**} = f$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Part 1: Fenchel's Inequality.</strong>
    We need $f(x) + f^*(y) \ge x^\top y$.
    We found $f^*(y) = \frac{1}{2}y^\top Q^{-1} y$.
    LHS - RHS = $\frac{1}{2}x^\top Q x + \frac{1}{2}y^\top Q^{-1} y - x^\top y$.
    Let $u = Q^{1/2}x$ and $v = Q^{-1/2}y$. Then $x = Q^{-1/2}u, y = Q^{1/2}v$.
    $x^\top y = u^\top v$.
    Expression: $\frac{1}{2}u^\top u + \frac{1}{2}v^\top v - u^\top v = \frac{1}{2}\|u-v\|^2 \ge 0$.
    Verified.</div>
    <div class="proof-step"><strong>Part 2: Biconjugate.</strong>
    $f^{**}(x) = (f^*)^*(x)$. Let $g(y) = f^*(y) = \frac{1}{2}y^\top P y$ with $P = Q^{-1}$.
    The conjugate of a quadratic $\frac{1}{2}y^\top P y$ is $\frac{1}{2}x^\top P^{-1} x$.
    $P^{-1} = (Q^{-1})^{-1} = Q$.
    So $f^{**}(x) = \frac{1}{2}x^\top Q x = f(x)$. Verified.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.8 — Quasiconvexity of Distance Ratio</h3>
  <p>Show $S_\theta = \{x \mid \|x-a\|_2 \le \theta \|x-b\|_2\}$ is convex for $\theta \le 1$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Square the condition.</strong>
    $\|x-a\|^2 \le \theta^2 \|x-b\|^2$.
    $(x-a)^\top (x-a) \le \theta^2 (x-b)^\top (x-b)$.
    $x^\top x - 2a^\top x + a^\top a \le \theta^2 (x^\top x - 2b^\top x + b^\top b)$.</div>
    <div class="proof-step"><strong>Step 2: Group terms.</strong>
    $(1 - \theta^2) x^\top x - 2(a - \theta^2 b)^\top x + (\|a\|^2 - \theta^2 \|b\|^2) \le 0$.</div>
    <div class="proof-step"><strong>Step 3: Analyze coefficient.</strong>
    Let $q(x)$ be this quadratic expression. The quadratic term is $(1-\theta^2)\|x\|^2$.
    If $\theta \le 1$, then $1-\theta^2 \ge 0$.
    The sublevel set of a convex quadratic ($P \succeq 0$) is convex (an ellipsoid or halfspace).
    Thus $S_\theta$ is convex. (If $\theta > 1$, it's the complement of an ellipsoid, which is non-convex).</div>
  </div>
</div>

<div class="problem">
  <h3>P6.9 — Log-Concavity of Probability Measures</h3>
  <p>If $p(x)$ is a log-concave probability density, show that the function $f(x) = \mathbb{P}(x+W \in C)$ is log-concave, where $W \sim p$ and $C$ is a convex set.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Write as Integral.</strong>
    $f(x) = \int_{\mathbb{R}^n} I_C(y) p(y-x) dy$.
    Wait, let's check definition. $x+W \in C \iff W \in C-x$.
    Prob = $\int_{C-x} p(w) dw = \int_{\mathbb{R}^n} I_{C-x}(w) p(w) dw$.
    Let $u = w+x$. Then $w = u-x$. Range $w \in C-x \iff u \in C$.
    $f(x) = \int I_C(u) p(u-x) du$.</div>
    <div class="proof-step"><strong>Step 2: Identify as Convolution.</strong>
    This is the convolution of two functions: $g(x) = I_C(x)$ (indicator 1 if in C, 0 else) and $h(x) = p(-x)$.
    Wait, convolution is $(g * h)(x) = \int g(u) h(x-u) du$.
    The form $\int g(u) p(u-x) du$ is a correlation or convolution with reflected argument.
    Let's use the Prékopa-Leindler inequality or the theorem that <b>convolution of log-concave functions is log-concave</b>.</div>
    <div class="proof-step"><strong>Step 3: Check Log-Concavity of Components.</strong>
    $p(x)$ is log-concave (given).
    $I_C(x)$ is the indicator function of a convex set (1 on C, 0 else).
    Is it log-concave? $\log I_C(x)$ is 0 on C, $-\infty$ else.
    Since $C$ is convex, this is a concave function (extended value).
    So $I_C$ is log-concave.</div>
    <div class="proof-step"><strong>Step 4: Conclusion.</strong>
    Since $f$ is the convolution of two log-concave functions, $f$ is log-concave.
    (Note: The integral is essentially measuring the measure of the set $C$ shifted by $-x$. Prekopa-Leindler directly applies to marginals of log-concave functions).</div>
  </div>
</div>

<div class="problem">
  <h3>P6.10 — Log-Concavity of Gaussian</h3>
  <p>Show that the multivariate Gaussian density function $f(x)$ is log-concave. $$ f(x) = \frac{1}{(2\pi)^{n/2} (\det \Sigma)^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^\top \Sigma^{-1} (x-\mu)\right) $$ where $\Sigma \succ 0$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      Take the logarithm:
      $$ \log f(x) = -\frac{n}{2}\log(2\pi) - \frac{1}{2}\log(\det \Sigma) - \frac{1}{2}(x-\mu)^\top \Sigma^{-1} (x-\mu) $$
    </div>
    <div class="proof-step">
      The first two terms are constants. The third term is a quadratic form.
      Let $g(x) = \log f(x)$. The gradient is $\nabla g(x) = -\Sigma^{-1}(x-\mu)$.
      The Hessian is $\nabla^2 g(x) = -\Sigma^{-1}$.
    </div>
    <div class="proof-step">
      Since $\Sigma \succ 0$, its inverse $\Sigma^{-1} \succ 0$, so $-\Sigma^{-1} \prec 0$ (negative definite).
      Since the Hessian is negative definite everywhere, $g(x)$ is strictly concave.
      Therefore, $f(x)$ is log-concave.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.11 — Log-Convexity of the Gamma Function</h3>
  <p>The Gamma function is defined as $\Gamma(x) = \int_0^\infty t^{x-1} e^{-t} dt$ for $x > 0$. Show that $\Gamma(x)$ is log-convex using the integration rule.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Identify the Integrand.</strong>
      We can rewrite the integral as:
      $$ \Gamma(x) = \int_0^\infty f(x, t) dt $$
      where $f(x, t) = t^{x-1} e^{-t}$. Note that the domain of integration $(0, \infty)$ does not depend on $x$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Check Log-Convexity of Integrand.</strong>
      Consider $g(x) = \log f(x, t)$ for a fixed $t > 0$.
      $$ g(x) = \log(t^{x-1} e^{-t}) = (x-1)\log t - t $$
      This function is linear (affine) in $x$ (slope $\log t$, intercept $-\log t - t$).
      Since affine functions are convex, $g(x)$ is convex in $x$.
      Thus, $f(x, t)$ is log-convex in $x$ for every $t$.
    </div>
    <div class="proof-step">
      <strong>Step 3: Apply Integration Rule.</strong>
      The theorem states that if $f(x, t)$ is log-convex in $x$ for each $t$, then $\int f(x, t) dt$ is log-convex (assuming convergence).
      Therefore, $\Gamma(x)$ is log-convex.
    </div>
  </div>
</div>

</section>
    </article>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></a> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initConvexFunctionInspector } from './widgets/js/convex-function-inspector.js';
    initConvexFunctionInspector('widget-convex-function-inspector');
  </script>
  <script type="module">
    import { initHessianHeatmap } from './widgets/js/hessian-heatmap.js';
    initHessianHeatmap('widget-hessian-heatmap');
  </script>
  <script type="module">
    import { initOperationsPreserving } from './widgets/js/operations-preserving.js';
    initOperationsPreserving('widget-operations-preserving');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
