<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>00. A Primer on Linear Algebra — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <link rel="stylesheet" href="../../static/css/modern-widgets.css" />
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.128/build/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">
          <img src="../../static/assets/branding/logo.svg" alt="Logo" />
          <span>Convex Optimization</span>
        </a>
      </div>
      <nav class="nav">
        <a href="../../index.html">All Lectures</a>
        <a href="../01-introduction/index.html">Next →</a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2>Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header card-v2">
      <h1>00. A Primer on Linear Algebra</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-14</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, review, linear-algebra, foundational</span>
      </div>
      <div class="lecture-summary">
        <p><strong>Overview:</strong> This lecture establishes the linear algebra foundation required for convex optimization. Beginning with vectors and matrices, we progress through inner products, norms, orthogonality, and projections, building the mathematical toolkit necessary for optimization theory.</p>
        <p><strong>Prerequisites:</strong> None. This lecture is the starting point.</p>
        <p><strong>Forward Connections:</strong> Projection techniques introduced here appear in least squares methods (<a href="../01-introduction/index.html">Lecture 01</a>). Positive Semidefinite (PSD) matrices form the basis for convex quadratic programs (<a href="../01-introduction/index.html">Lecture 01</a>). The four fundamental subspaces provide geometric understanding of feasible sets (<a href="../02-convex-sets/index.html">Lecture 02</a>).</p>
      </div>
    </header>

    <section class="card-v2">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you will be able to work with:</p>
      <ul>
        <li><b>Fundamental Objects and Subspaces:</b> Vectors, matrices, the four fundamental subspaces, and the Rank-Nullity Theorem with proofs.</li>
        <li><b>Inner Products and Norms:</b> Standard and generalized inner products, the three canonical vector norms, dual norms, Hölder's inequality, and the Cauchy-Schwarz inequality with proofs.</li>
        <li><b>Orthogonality and Projections:</b> Orthonormal bases, Gram-Schmidt process, QR decomposition, projection onto subspaces and affine sets.</li>
        <li><b>Positive Semidefinite Matrices:</b> Definitions, eigenvalue characterization, quadratic forms, ellipsoids, and the Loewner order.</li>
        <li><b>Robust Numerical Methods:</b> Least squares via normal equations, QR, SVD, pseudoinverse, condition numbers, and when to use each method.</li>
        <li><b>Matrix Structures:</b> Trace inner product, Frobenius norm, componentwise order—all prerequisites for later convex set theory.</li>
      </ul>
    </section>


    <!-- SECTION 0: NOTATION -->
    <section class="card-v2" id="section-0">
      <h2>0. Notation and Basic Objects</h2>

      <h3>Scalars, Vectors, Matrices</h3>
      <ul>
        <li><b>Scalars:</b> Real numbers $a \in \mathbb{R}$.</li>
        <li><b>Column vectors:</b> $x \in \mathbb{R}^n$ are $n \times 1$ matrices with entries $x_i$.</li>
        <li><b>Matrices:</b> $A \in \mathbb{R}^{m \times n}$; entry $a_{ij}$ is row $i$, column $j$.</li>
        <li><b>Transpose:</b> $A^\top \in \mathbb{R}^{n \times m}$ satisfies $(A^\top)_{ij} = a_{ji}$.</li>
        <li><b>Identity:</b> $I_n$ has ones on the diagonal; $I_n x = x$.</li>
        <li><b>Standard basis:</b> $e_1, \dots, e_n$ where $e_i$ has a 1 in position $i$, zeros elsewhere. Every $x \in \mathbb{R}^n$ can be written $x = \sum_{i=1}^n x_i e_i$.</li>
      </ul>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/matrix-multiplication.svg"
             alt="Matrix multiplication diagram showing how rows and columns combine"
             style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 0.1:</i> Matrix multiplication visualized—how rows of the first matrix combine with columns of the second.</figcaption>
      </figure>

      <h3>Matrix–Vector and Matrix–Matrix Multiplication</h3>
      <ul>
        <li>$Ax$ is the <b>linear combination of columns of $A$</b> with coefficients from $x$.</li>
        <li>$(AB)_{ij} = \sum_k a_{ik} b_{kj}$.</li>
      </ul>

      <h3>Linear Maps</h3>
      <p>A function $T: \mathbb{R}^n \to \mathbb{R}^m$ is <b>linear</b> if and only if $T(\alpha x + \beta y) = \alpha T(x) + \beta T(y)$. Every linear map can be represented by a matrix $A$ with $T(x) = Ax$.</p>

      <h3>Componentwise Order Notation</h3>
      <p>A notation used throughout optimization is the componentwise order. For vectors $x, y \in \mathbb{R}^n$:
      </p>
      <ul>
        <li>$x \ge 0$ means $x_i \ge 0$ for all $i=1, \dots, n$.</li>
        <li>$x \le y$ means $x_i \le y_i$ for all $i=1, \dots, n$.</li>
      </ul>
      <p>This provides a compact way to describe constraints, such as non-negativity. The vector $\mathbf{1}$ denotes the vector of all ones. This notation is essential for <a href="../02-convex-sets/index.html">Lecture 02</a>'s treatment of polyhedra and cones.</p>
    </section>

    <!-- SECTION 1: FOUR FUNDAMENTAL SUBSPACES -->
    <section class="card-v2" id="section-1">
      <h2>1. Subspaces and the Four Fundamental Spaces</h2>

      <p>A <b>linear subspace</b> is a set of vectors that is closed under addition and scalar multiplication. Given a matrix $A \in \mathbb{R}^{m \times n}$, we can define four fundamental subspaces that characterize the behavior of the linear map $T(x) = Ax$.</p>
      <ul>
        <li><b>Column Space (Range):</b> $\mathcal{R}(A) = \{Ax \mid x \in \mathbb{R}^n\} \subseteq \mathbb{R}^m$. This is the set of all possible outputs of the linear map, i.e., the span of the columns of $A$.</li>
        <li><b>Nullspace (Kernel):</b> $\mathcal{N}(A) = \{x \in \mathbb{R}^n \mid Ax = 0\}$. This is the set of all input vectors that are mapped to the zero vector.</li>
        <li><b>Row Space:</b> $\mathcal{R}(A^\top) \subseteq \mathbb{R}^n$. This is the span of the rows of $A$.</li>
        <li><b>Left Nullspace:</b> $\mathcal{N}(A^\top) = \{y \in \mathbb{R}^m \mid A^\top y = 0\}$. This is the nullspace of the transpose of $A$.</li>
      </ul>

      <p>These subspaces are linked by two crucial orthogonality relationships:</p>
      <ul>
        <li>$\mathcal{R}(A) \perp \mathcal{N}(A^\top)$ in $\mathbb{R}^m$</li>
        <li>$\mathcal{R}(A^\top) \perp \mathcal{N}(A)$ in $\mathbb{R}^n$</li>
      </ul>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/linear-subspaces.svg"
             alt="Visual representation of intersecting planes showing linear subspaces"
             style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 1.1:</i> Three planes in 3D space—the intersection represents solution sets of linear equations.</figcaption>
      </figure>

      <div class="proof-enhanced">
        <h4>Proof: $\mathcal{R}(A) \perp \mathcal{N}(A^\top)$</h4>

        <div class="proof-step">
          <strong>Setup:</strong> Take any $y \in \mathcal{N}(A^\top)$ and any $x \in \mathbb{R}^n$. By definition, $A^\top y = 0$ and $Ax \in \mathcal{R}(A)$.
        </div>

        <div class="proof-step">
          <strong>Compute inner product:</strong> The inner product between $Ax$ and $y$ is:
          $$ y^\top (Ax) = (A^\top y)^\top x = 0^\top x = 0 $$
        </div>

        <div class="proof-step">
          <strong>Conclusion:</strong> Since this holds for all $x \in \mathbb{R}^n$, every vector in $\mathcal{R}(A)$ is orthogonal to every vector in $\mathcal{N}(A^\top)$.
        </div>
      </div>

      <h3>Rank and Rank–Nullity Theorem</h3>
      <p>$\mathrm{rank}(A) = \dim \mathcal{R}(A)$. For $A \in \mathbb{R}^{m \times n}$:
      $$ \dim \mathcal{N}(A) + \mathrm{rank}(A) = n $$
      This fundamental dimension-counting result is a cornerstone of linear algebra.</p>

      <div class="proof-enhanced">
        <h4>Proof of the Rank-Nullity Theorem</h4>

        <div class="proof-step">
          <strong>Step 1: Construct a basis.</strong> Let $\dim(\mathcal{N}(A)) = k$ and let $\{v_1, \dots, v_k\}$ be a basis for $\mathcal{N}(A)$. Extend this to a basis for $\mathbb{R}^n$: $\{v_1, \dots, v_k, w_1, \dots, w_{n-k}\}$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Express arbitrary vectors.</strong> Any $x \in \mathbb{R}^n$ can be written as $x = \sum_{i=1}^k c_i v_i + \sum_{j=1}^{n-k} d_j w_j$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Apply the matrix.</strong> Then
          $$ Ax = \sum_{i=1}^k c_i A v_i + \sum_{j=1}^{n-k} d_j A w_j = \sum_{j=1}^{n-k} d_j A w_j $$
          since $v_i \in \mathcal{N}(A)$ implies $Av_i = 0$.
        </div>

        <div class="proof-step">
          <strong>Step 4: Show spanning property.</strong> The calculation above shows that the vectors $\{Aw_1, \dots, Aw_{n-k}\}$ span $\mathcal{R}(A)$.
        </div>

        <div class="proof-step">
          <strong>Step 5: Verify linear independence.</strong> Suppose $\sum_{j=1}^{n-k} d_j A w_j = 0$. Then $A(\sum_{j=1}^{n-k} d_j w_j) = 0$, so $\sum_{j=1}^{n-k} d_j w_j \in \mathcal{N}(A)$. This means it can be written as a linear combination of $\{v_i\}$, which contradicts the linear independence of our basis for $\mathbb{R}^n$ unless all $d_j=0$.
        </div>

        <div class="proof-step">
          <strong>Step 6: Conclude.</strong> The set $\{Aw_1, \dots, Aw_{n-k}\}$ is a basis for $\mathcal{R}(A)$, so $\mathrm{rank}(A) = \dim(\mathcal{R}(A)) = n-k = n - \dim(\mathcal{N}(A))$.
        </div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Rank & Nullspace</h3>
        <p><b>Explore the Four Fundamental Subspaces:</b> Define a 2×3 or 3×2 matrix and visualize how its column space, row space, nullspace, and left nullspace relate to each other. Try creating rank-deficient matrices (e.g., with linearly dependent columns) to see the Rank-Nullity Theorem in action—observe how the dimensions of the nullspace and column space always sum to the number of columns.</p>
        <p><i>Key insight:</i> The orthogonality relationships between these subspaces are fundamental to understanding linear transformations and will appear throughout convex optimization.</p>
        <div id="widget-rank-nullspace" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 2: INNER PRODUCTS AND NORMS -->
    <section class="card-v2" id="section-2">
      <h2>2. Inner Products, Norms, and Angles</h2>

      <h3>Inner Product</h3>
      <p>An inner product on $\mathbb{R}^n$ is a mapping $\langle x, y \rangle$ that is bilinear, symmetric, and positive definite:</p>
      <ul>
        <li>$\langle ax + by, z \rangle = a\langle x, z \rangle + b\langle y, z \rangle$</li>
        <li>$\langle x, y \rangle = \langle y, x \rangle$</li>
        <li>$\langle x, x \rangle > 0$ for $x \neq 0$</li>
      </ul>
      <p>The <b>standard (Euclidean) inner product</b> is the most common example: $\langle x, y \rangle = x^\top y = \sum_{i=1}^n x_i y_i$.</p>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/vector-addition-parallelogram.png"
             alt="Parallelogram law for vector addition"
             style="max-width: 70%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 12px; background: white;" />
        <figcaption><i>Figure 2.1:</i> The parallelogram law of vector addition, a geometric reflection of the algebraic properties of inner products.</figcaption>
      </figure>

      <h3>Norms</h3>
      <p>A norm is a function that assigns a strictly positive length or size to each vector in a vector space, except for the zero vector. A norm $\|\cdot\|$ must satisfy:</p>
      <ol>
        <li><b>Non-negativity:</b> $\|x\| \ge 0$, and $\|x\| = 0$ if and only if $x=0$.</li>
        <li><b>Absolute homogeneity:</b> $\|\alpha x\| = |\alpha| \|x\|$ for any scalar $\alpha$.</li>
        <li><b>Triangle inequality:</b> $\|x+y\| \le \|x\| + \|y\|$.</li>
      </ol>
      <p>Three of the most widely used norms are:</p>
      $$
      \|x\|_2 = \sqrt{\sum_i x_i^2} \quad (\text{Euclidean norm}), \quad \|x\|_1 = \sum_i |x_i| \quad (\text{L1 norm}), \quad \|x\|_\infty = \max_i |x_i| \quad (\text{Infinity norm})
      $$

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/vector-norms.svg"
             alt="Comparison of different norm unit balls"
             style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 2.2:</i> The "unit ball" (the set of all vectors with norm less than or equal to 1) for the L1, L2, and infinity norms. The different shapes illustrate how these norms measure distance differently.</figcaption>
      </figure>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: The Geometry of Norms</h3>
        <p><b>Visualize Norm Unit Balls:</b> Explore the unit balls (sets of vectors with norm ≤ 1) for the $\ell_1$, $\ell_2$, and $\ell_\infty$ norms in 2D. Each norm defines a different notion of "distance" and produces a distinctly shaped unit ball:</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>$\ell_2$ (Euclidean):</b> Forms a perfect circle—the most "natural" geometric distance</li>
          <li><b>$\ell_1$ (Manhattan):</b> Forms a diamond with "pointy" corners at the axes—this geometry is why $\ell_1$ regularization promotes sparse solutions in machine learning</li>
          <li><b>$\ell_\infty$ (Chebyshev):</b> Forms a square—measures the maximum component</li>
        </ul>
        <p><i>Connection to optimization:</i> The shape of these unit balls directly affects the solutions to optimization problems. The corners of the $\ell_1$ ball are what make LASSO regression find sparse solutions!</p>
        <div id="widget-norm-geometry" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>Cauchy–Schwarz and Triangle Inequality</h3>

      <div class="proof-enhanced">
        <h4>Proof of Cauchy–Schwarz: $|x^\top y| \le \|x\|_2 \|y\|_2$</h4>

        <div class="proof-step">
          <strong>Step 1: Consider a norm.</strong> For any scalar $t$, the expression $\|x - ty\|_2^2$ is non-negative:
          $$ \|x - ty\|_2^2 = (x - ty)^\top(x - ty) = \|x\|_2^2 - 2t(x^\top y) + t^2 \|y\|_2^2 \ge 0 $$
        </div>

        <div class="proof-step">
          <strong>Step 2: Apply the discriminant condition.</strong> This quadratic in $t$ is non-negative for all $t$, so its discriminant must be non-positive:
          $$ ( -2(x^\top y) )^2 - 4(\|y\|_2^2)(\|x\|_2^2) \le 0 $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Simplify.</strong> Rearranging gives
          $$ 4(x^\top y)^2 \le 4\|x\|_2^2 \|y\|_2^2 $$
          Taking the square root of both sides yields the Cauchy-Schwarz inequality.
        </div>
      </div>

      <div class="proof-enhanced">
        <h4>Proof of the Triangle Inequality: $\|x+y\|_2 \le \|x\|_2 + \|y\|_2$</h4>

        <div class="proof-step">
          <strong>Step 1: Expand the squared norm.</strong> Starting with the left side:
          $$ \|x+y\|_2^2 = (x+y)^\top(x+y) = \|x\|_2^2 + 2x^\top y + \|y\|_2^2 $$
        </div>

        <div class="proof-step">
          <strong>Step 2: Apply Cauchy-Schwarz.</strong> By the Cauchy-Schwarz inequality, $x^\top y \le |x^\top y| \le \|x\|_2 \|y\|_2$. Substituting:
          $$ \|x+y\|_2^2 \le \|x\|_2^2 + 2\|x\|_2 \|y\|_2 + \|y\|_2^2 = (\|x\|_2 + \|y\|_2)^2 $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Take square roots.</strong> Taking the square root of both sides yields the triangle inequality.
        </div>
      </div>

      <h3>Angles and Orthogonality</h3>
      <p>The Cauchy-Schwarz inequality allows us to define the angle between two vectors:
      $$ \cos \angle(x, y) = \frac{x^\top y}{\|x\|_2 \|y\|_2} $$
      Since the magnitude of the right-hand side is guaranteed to be between -1 and 1. Two vectors are <b>orthogonal</b> if their inner product is zero, $x^\top y = 0$, which corresponds to a 90-degree angle.</p>

      <h3>Dual Norms and Hölder's Inequality</h3>
      <p>The concept of duality is central to optimization. For any norm $\|\cdot\|$, we can define a corresponding <b>dual norm</b>, denoted $\|\cdot\|_*$, as:
      $$ \|y\|_* = \sup_{\|x\| \le 1} x^\top y $$
      The dual norm measures the maximum "stretch" of a vector $y$ when applied to unit-norm vectors $x$. This definition leads to some important pairings:</p>
      <ul>
        <li>The dual of the L2 norm is the L2 norm itself (it is self-dual).</li>
        <li>The dual of the L1 norm is the L-infinity norm.</li>
        <li>The dual of the L-infinity norm is the L1 norm.</li>
      </ul>

      <div class="proof-enhanced">
        <h4>Hölder's Inequality: A Generalization of Cauchy-Schwarz</h4>
        <p>Hölder's inequality follows directly from the definition of the dual norm and bounds the inner product of two vectors:
        $$ |x^\top y| \le \|x\| \cdot \|y\|_* $$
        When the norm is the L2 norm, the dual norm is also the L2 norm, and we recover the Cauchy-Schwarz inequality. This inequality is used extensively in the study of dual cones in <a href="../02-convex-sets/index.html">Lecture 02</a> and throughout duality theory.</p>
      </div>

      <h3>Generalized (Weighted) Inner Product</h3>
      <p>If $Q \in \mathbb{R}^{n \times n}$ is symmetric positive definite (SPD), then $\langle x, y \rangle_Q := x^\top Q y$ is an inner product with induced norm $\|x\|_Q = \sqrt{x^\top Q x}$. This yields <b>quadratic forms</b> and ellipsoidal distance.</p>

      <h3>Matrix Inner Product and Frobenius Norm</h3>
      <p>For matrices $X, Y \in \mathbb{R}^{m \times n}$, the <b>trace inner product</b> is:
      $$ \langle X, Y \rangle = \mathrm{tr}(X^\top Y) = \sum_{ij} X_{ij} Y_{ij} $$
      The <b>Frobenius norm</b> is $\|X\|_F = \sqrt{\langle X, X \rangle} = \sqrt{\sum_{ij} X_{ij}^2}$.</p>

      <p>The <b>Loewner order</b> for symmetric matrices: $X \succeq Y$ means $X - Y \succeq 0$ (i.e., $v^\top(X - Y)v \ge 0$ for all $v$). This is the backdrop for <a href="../02-convex-sets/index.html">Lecture 02</a>'s PSD cone and its dual.</p>
    </section>

    <!-- SECTION 3: ORTHOGONALITY AND QR -->
    <section class="card-v2" id="section-3">
      <h2>3. Orthogonality and Orthonormal Bases</h2>

      <h3>Orthonormal Sets</h3>
      <p>A set of vectors $q_1, \dots, q_k$ is <b>orthonormal</b> if its elements are mutually orthogonal and each has a norm of 1. Formally, $q_i^\top q_j = \delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta (1 if $i=j$, 0 otherwise). A square matrix $Q$ with orthonormal columns is an <b>orthogonal matrix</b>, satisfying the important property $Q^\top Q = I$, which means $Q^{-1} = Q^\top$. Orthonormal bases are computationally desirable because they are numerically stable and simplify many calculations, such as projections.</p>

      <h3>The Gram–Schmidt Process</h3>
      <p>The Gram-Schmidt process is an algorithm for constructing an orthonormal basis from a set of linearly independent vectors. Starting with a vector, it iteratively subtracts the components that lie in the direction of the previously processed vectors, leaving a new, orthogonal vector that is then normalized.
      $$
      \tilde{q}_k = a_k - \sum_{i=1}^{k-1} (q_i^\top a_k) q_i, \quad q_k = \frac{\tilde{q}_k}{\|\tilde{q}_k\|_2}
      $$
      </p>

      <h3>The QR Decomposition</h3>
      <p>The QR decomposition expresses a matrix $A$ as the product of an orthonormal matrix $Q$ and an upper triangular matrix $R$. This decomposition is a direct outcome of the Gram-Schmidt process and is a cornerstone of numerical linear algebra. For a matrix $A \in \mathbb{R}^{m \times n}$ with full column rank, we can write $A = QR$, where $Q \in \mathbb{R}^{m \times n}$ has orthonormal columns and $R \in \mathbb{R}^{n \times n}$ is upper-triangular.
      <br><b>Applications:</b></p>
      <ul>
        <li><b>Solving Linear Systems:</b> The system $Ax=b$ becomes $QRx=b$, which simplifies to $Rx = Q^\top b$. This is easily solved using back substitution and is far more numerically stable than forming the normal equations.</li>
        <li><b>Projections:</b> The projection onto the column space of $A$ is given by $P = QQ^\top$.</li>
      </ul>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Explorer: Orthogonality & Projections</h3>
        <p><b>Visualize Vector Relationships:</b> Drag two vectors in the 2D plane and observe how their geometric relationships change in real-time. The tool displays:</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Dot product ($x^\top y$):</b> Becomes zero when vectors are orthogonal (perpendicular)</li>
          <li><b>Angle:</b> Computed using $\cos \theta = \frac{x^\top y}{\|x\|_2 \|y\|_2}$ via the Cauchy-Schwarz inequality</li>
          <li><b>Orthogonal projection:</b> The shadow of one vector onto another—this is the foundation of least squares!</li>
        </ul>
        <p><i>Key concept:</i> Projection is everywhere in optimization—from computing least squares solutions to understanding constraint gradients. The projection of $b$ onto the column space of $A$ gives us the best least-squares fit.</p>
        <div id="widget-orthogonality" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 4: PSD MATRICES -->
    <section class="card-v2" id="section-4">
      <h2>4. Positive Semidefinite Matrices</h2>

      <h3>Definitions</h3>
      <p>A symmetric matrix $Q \in \mathbb{S}^n$ is:</p>
      <ul>
        <li><b>Positive Semidefinite (PSD)</b>, written $Q \succeq 0$, if the quadratic form $x^\top Q x \ge 0$ for all vectors $x$.</li>
        <li><b>Positive Definite (PD)</b>, written $Q \succ 0$, if $x^\top Q x > 0$ for all non-zero vectors $x$.</li>
      </ul>

      <p>These matrices are fundamental to convex optimization because they define convex quadratic functions. An equivalent and often more practical characterization is based on their eigenvalues:</p>
      <ul>
        <li>A matrix is PSD if and only if all of its eigenvalues are non-negative.</li>
        <li>A matrix is PD if and only if all of its eigenvalues are strictly positive.</li>
      </ul>

      <p>A positive definite matrix $Q$ can be used to define a generalized norm, $\|x\|_Q = \sqrt{x^\top Q x}$. The unit ball for this norm, $\{x \mid x^\top Q x \le 1\}$, is an ellipsoid, whose geometry is determined by the eigenvalues and eigenvectors of $Q$.</p>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/eigenvalues-psd.png"
             alt="Eigenvalue visualization for PSD matrices"
             style="max-width: 80%; height: auto; border: 1px solid var(--border); border-radius: 8px;" />
        <figcaption><i>Figure 4.1:</i> Eigenvalues and positive semidefiniteness—the signs determine the matrix's curvature properties.</figcaption>
      </figure>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/eigenvectors.gif"
             alt="Animated eigenvector visualization"
             style="max-width: 70%; height: auto; border: 1px solid var(--border); border-radius: 8px;" />
        <figcaption><i>Figure 4.2:</i> Eigenvectors in action—how matrices transform space along principal directions.</figcaption>
      </figure>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Explorer: Matrix Geometry & Definiteness</h3>
        <p><b>Explore Linear Maps and Quadratic Forms:</b> This comprehensive tool connects the linear transformation $Ax$ with the quadratic form $x^\top Ax$. Toggle "Force Symmetric" to explore the specific properties relevant to optimization (Hessians).</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Linear Map:</b> See how the unit circle transforms into an ellipse. The axes of the ellipse correspond to eigenvectors (for symmetric matrices).</li>
          <li><b>Quadratic Form:</b> Visualize the level sets of $z = x^\top Ax$.
            <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
              <li><b>Positive Definite:</b> Ellipses (Convex Bowl)</li>
              <li><b>Indefinite:</b> Hyperbolas (Saddle Point)</li>
            </ul>
          </li>
        </ul>
        <div id="widget-matrix-geometry" style="width: 100%; position: relative;"></div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Hessian Landscape Visualizer</h3>
        <p><b>Connect Eigenvalues to Function Curvature:</b> This 3D visualizer renders the surface of a quadratic function $f(x) = \frac{1}{2}x^\top Q x$ and displays its Hessian matrix $Q$. The eigenvalues of the Hessian directly control the curvature:</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Large positive eigenvalues:</b> Steep curvature (fast convergence in optimization)</li>
          <li><b>Small positive eigenvalues:</b> Flat directions (slow convergence)</li>
          <li><b>Condition number ($\kappa = \lambda_{\max}/\lambda_{\min}$):</b> When this ratio is large, gradient descent converges slowly</li>
        </ul>
        <p><i>Practical insight:</i> This visualization explains why preconditioning (transforming to balance eigenvalues) dramatically speeds up iterative solvers!</p>
        <div id="widget-hessian-landscape" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

    </section>

    <!-- SECTION 5: PROJECTIONS -->
    <section class="card-v2" id="section-5">
      <h2>5. Projections onto Subspaces and Affine Sets</h2>

      <h3>Orthogonal Projection onto a Subspace</h3>
      <p>Let $\mathcal{S} \subseteq \mathbb{R}^m$ be a subspace and $b \in \mathbb{R}^m$. The <b>orthogonal projection</b> $p \in \mathcal{S}$ of $b$ is the unique vector in $\mathcal{S}$ minimizing $\|b - p\|_2$. It is characterized by the <b>orthogonality condition</b>:
      $$ b - p \perp \mathcal{S} \quad \iff \quad v^\top(b - p) = 0 \ \ \forall v \in \mathcal{S} $$</p>

      <h3>Projection via a Basis</h3>
      <p>If $Q \in \mathbb{R}^{m \times k}$ has orthonormal columns spanning $\mathcal{S}$, the projector is:
      $$ P = QQ^\top \quad \text{and} \quad p = Pb = QQ^\top b $$
      Check: $P^2 = P$ (idempotent) and $P^\top = P$ (symmetric)—that's what makes it an <b>orthogonal</b> projector.</p>

      <h3>Projection onto $\mathcal{R}(A)$ using $A$</h3>
      <p>If $A \in \mathbb{R}^{m \times n}$ has full column rank:
      $$ P = A(A^\top A)^{-1} A^\top \quad \text{and} \quad p = Pb $$
      </p>

      <h3>Projection onto an Affine Set</h3>
      <p>Projecting onto $\{x \mid Fx = g\}$ reduces to subspace projection by translating:</p>
      <ol>
        <li>Pick any $x_0$ with $Fx_0 = g$</li>
        <li>Write the affine set as $x_0 + \mathcal{N}(F)$</li>
        <li>Project $b - x_0$ onto $\mathcal{N}(F)$ using a basis $Z$ for $\mathcal{N}(F)$</li>
        <li>Translate back</li>
      </ol>

    </section>

    <!-- SECTION 6: LEAST SQUARES -->
    <section class="card-v2" id="section-6">
      <h2>6. The Method of Least Squares</h2>

      <h3>The Problem: Overdetermined Systems</h3>
      <p>Often in practice, we encounter a system of linear equations $Ax=b$ where there is no exact solution because the vector $b$ does not lie in the column space of $A$. This is common when $m > n$ (more equations than unknowns). The goal of least squares is to find the "best" approximate solution by minimizing the squared Euclidean norm of the residual vector $r = Ax-b$:
      $$ \min_{x \in \mathbb{R}^n} \|Ax - b\|_2^2 $$</p>

      <h3>Geometric Interpretation: Projection</h3>
      <p>The solution to the least squares problem has a clean geometric interpretation. The vector $Ax$ is always in the column space of $A$, $\mathcal{R}(A)$. The problem is therefore equivalent to finding the point in $\mathcal{R}(A)$ that is closest to $b$. This closest point is the <b>orthogonal projection</b> of $b$ onto $\mathcal{R}(A)$. Let this projection be $p = Ax^\star$. The residual vector $r^\star = b - Ax^\star$ must be orthogonal to the entire column space.</p>

      <h3>The Normal Equations</h3>
      <p>The orthogonality condition, $r^\star \perp \mathcal{R}(A)$, means that the residual must be orthogonal to every column of $A$. This can be expressed compactly as $A^\top r^\star = 0$. Substituting the definition of the residual, we get:
      $$ A^\top(b - Ax^\star) = 0 $$
      Rearranging this gives the <b>normal equations</b>:
      $$ A^\top A x^\star = A^\top b $$
      If the matrix $A$ has linearly independent columns (full column rank), then $A^\top A$ is invertible, and we can write the unique solution as $x^\star = (A^\top A)^{-1} A^\top b$.</p>

      <h3>Uniqueness of the Solution</h3>
      <ul>
        <li>If $A$ has full column rank ($\mathrm{rank}(A) = n$), the solution $x^\star$ is unique.</li>
        <li>If $A$ is rank-deficient ($\mathrm{rank}(A) < n$), there are infinitely many solutions. In this case, the pseudoinverse is used to find the solution with the minimum norm.</li>
      </ul>

    </section>

    <!-- SECTION 7: QR, SVD, PSEUDOINVERSE (COMPREHENSIVE) -->
    <section class="card-v2" id="section-7">
      <h2>7. Advanced Factorizations: QR, SVD, Pseudoinverse, and Applications</h2>

      <p><strong>Motivation:</strong> While Gaussian elimination and normal equations work in theory, numerical stability and efficiency demand better methods. The QR decomposition and Singular Value Decomposition (SVD) are the workhorses of modern numerical linear algebra—providing robust, stable solutions to least squares, revealing matrix structure, and enabling dimensionality reduction and data compression.</p>

      <!-- 7.1 QR Decomposition -->
      <h3>7.1 QR Decomposition (Orthogonal-Triangular Factorization)</h3>

      <h4>Theorem: Existence of QR Decomposition</h4>
      <div class="proof-enhanced">
        <p><strong>Statement:</strong> Every matrix $A \in \mathbb{R}^{m \times n}$ with $m \ge n$ can be factored as:
        $$ A = QR $$
        where $Q \in \mathbb{R}^{m \times n}$ has orthonormal columns ($Q^\top Q = I_n$) and $R \in \mathbb{R}^{n \times n}$ is upper triangular.</p>

        <p><strong>Proof Sketch (Gram-Schmidt):</strong> Apply Gram-Schmidt orthogonalization to the columns of $A$:
        $$
        \begin{aligned}
        q_1 &= a_1 / \|a_1\|_2, \quad r_{11} = \|a_1\|_2 \\
        \tilde{q}_2 &= a_2 - (q_1^\top a_2) q_1, \quad q_2 = \tilde{q}_2 / \|\tilde{q}_2\|_2 \\
        &\vdots
        \end{aligned}
        $$
        The matrix $R$ stores the coefficients: $r_{ij} = q_i^\top a_j$ for $i \le j$, and $r_{ij} = 0$ for $i > j$. The process is numerically stabilized using <strong>Householder reflections</strong> or <strong>Givens rotations</strong> in practice.</p>
      </div>

      <h4>Solving Least Squares via QR</h4>
      <p>Given $A = QR$, the least-squares problem $\min \|Ax - b\|_2$ becomes:
      $$
      \|Ax - b\|_2 = \|QRx - b\|_2 = \|Rx - Q^\top b\|_2
      $$
      (using $\|Qy\|_2 = \|y\|_2$). Thus, we solve:
      $$ Rx^\star = Q^\top b $$
      by back substitution. This is <strong>numerically stable</strong> and avoids forming $A^\top A$, which squares the condition number.</p>

      <p><strong>Complexity:</strong> $O(mn^2)$ for computing QR, $O(n^2)$ for back substitution. For $m \gg n$, this is far more efficient than forming $A^\top A$ ($O(mn^2)$) and solving $A^\top A x = A^\top b$ ($O(n^3)$).</p>

      <!-- 7.2 Singular Value Decomposition (COMPREHENSIVE) -->
      <h3>7.2 Singular Value Decomposition (SVD): The Crown Jewel</h3>

      <h4>7.2.1 Definition and Existence</h4>
      <div class="proof-enhanced">
        <p><strong>Theorem (SVD):</strong> Every matrix $A \in \mathbb{R}^{m \times n}$ admits a factorization:
        $$ A = U \Sigma V^\top $$
        where:</p>
        <ul>
          <li>$U \in \mathbb{R}^{m \times m}$ is orthogonal ($U^\top U = UU^\top = I_m$), with columns $u_1, \dots, u_m$ called <strong>left singular vectors</strong></li>
          <li>$V \in \mathbb{R}^{n \times n}$ is orthogonal ($V^\top V = VV^\top = I_n$), with columns $v_1, \dots, v_n$ called <strong>right singular vectors</strong></li>
          <li>$\Sigma \in \mathbb{R}^{m \times n}$ is a "diagonal" matrix with nonnegative entries $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_r > 0$ (the <strong>singular values</strong>), where $r = \text{rank}(A)$, and $\sigma_{r+1} = \cdots = 0$</li>
        </ul>

        <p><strong>Proof Sketch:</strong> Consider $A^\top A \in \mathbb{S}_+^n$. By the spectral theorem, $A^\top A = V \Lambda V^\top$ where $\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)$ with $\lambda_i \ge 0$. Define $\sigma_i = \sqrt{\lambda_i}$ and $\Sigma = \text{diag}(\sigma_1, \dots, \sigma_r, 0, \dots, 0)$. For $i \le r$, set $u_i = Av_i / \sigma_i$. Extend $\{u_1, \dots, u_r\}$ to an orthonormal basis of $\mathbb{R}^m$ to form $U$. Then $A = U \Sigma V^\top$ follows from construction.</p>
      </div>

      <h4>7.2.2 Compact and Truncated SVD</h4>
      <p>When $m \gg n$ or $A$ has low rank $r \ll \min(m,n)$, we can use more efficient representations:</p>
      <ul>
        <li><strong>Full SVD:</strong> $A = U_{m \times m} \Sigma_{m \times n} V^\top_{n \times n}$ (standard form, $U$ and $V$ square)</li>
        <li><strong>Compact (Reduced) SVD:</strong> $A = U_r \Sigma_r V_r^\top$, where $U_r \in \mathbb{R}^{m \times r}$, $\Sigma_r = \text{diag}(\sigma_1, \dots, \sigma_r) \in \mathbb{R}^{r \times r}$, $V_r \in \mathbb{R}^{n \times r}$. Eliminates zero singular values.</li>
        <li><strong>Truncated SVD (rank-$k$ approximation):</strong> $A_k = U_k \Sigma_k V_k^\top = \sum_{i=1}^k \sigma_i u_i v_i^\top$, using only the top $k$ singular values ($k < r$). This is the <strong>best rank-$k$ approximation</strong> to $A$ in both Frobenius and spectral norms (Eckart-Young theorem).</li>
      </ul>

      <h4>7.2.3 Geometric Interpretation</h4>
      <p>The SVD reveals how $A$ acts as a linear transformation:</p>
      <ol>
        <li><strong>$V^\top$:</strong> Rotates (and possibly reflects) input space, aligning with principal axes</li>
        <li><strong>$\Sigma$:</strong> Scales along these axes by $\sigma_1, \dots, \sigma_r$</li>
        <li><strong>$U$:</strong> Rotates (and possibly reflects) the scaled space to output space</li>
      </ol>
      <p>Thus, $A$ transforms the unit sphere in $\mathbb{R}^n$ into an ellipsoid in $\mathbb{R}^m$ with semi-axes of lengths $\sigma_1, \dots, \sigma_r$.</p>

      <p><strong>Visualizing SVD:</strong> For $A \in \mathbb{R}^{2 \times 2}$, consider $A = U \Sigma V^\top$ acting on the unit circle. $V^\top$ rotates to align with $v_1, v_2$; $\Sigma$ stretches by $\sigma_1, \sigma_2$; $U$ rotates to align with $u_1, u_2$. The result is an ellipse with semi-major axis $\sigma_1 u_1$ and semi-minor axis $\sigma_2 u_2$.</p>

      <h4>7.2.4 SVD and Matrix Norms</h4>
      <p>Singular values determine all major matrix norms:</p>
      <ul>
        <li><strong>Spectral norm (2-norm):</strong> $\|A\|_2 = \sigma_1 = \sigma_{\max}(A)$ (largest singular value)</li>
        <li><strong>Frobenius norm:</strong> $\|A\|_F = \sqrt{\sum_{i,j} a_{ij}^2} = \sqrt{\sum_{i=1}^r \sigma_i^2}$ (square root of sum of squared singular values)</li>
        <li><strong>Nuclear norm (trace norm):</strong> $\|A\|_* = \sum_{i=1}^r \sigma_i$ (sum of singular values, convex envelope of rank)</li>
      </ul>

      <h4>7.2.5 The Eckart-Young-Mirsky Theorem</h4>
      <div class="proof-enhanced">
        <p><strong>Theorem (Optimal Low-Rank Approximation):</strong> Let $A = U \Sigma V^\top$ be the SVD of $A \in \mathbb{R}^{m \times n}$ with rank $r$. For any $k < r$, the best rank-$k$ approximation to $A$ in the Frobenius norm is:
        $$ A_k = \sum_{i=1}^k \sigma_i u_i v_i^\top = U_k \Sigma_k V_k^\top $$
        and the approximation error is:
        $$ \min_{\text{rank}(B) \le k} \|A - B\|_F = \|A - A_k\|_F = \sqrt{\sum_{i=k+1}^r \sigma_i^2} $$
        The same result holds for the spectral norm:
        $$ \min_{\text{rank}(B) \le k} \|A - B\|_2 = \|A - A_k\|_2 = \sigma_{k+1} $$</p>

        <p><strong>Proof Sketch:</strong> By the SVD, $A = \sum_{i=1}^r \sigma_i u_i v_i^\top$. Any rank-$k$ matrix $B$ lies in a $k$-dimensional subspace. The projection theorem and properties of orthogonal bases yield that the optimal choice is to keep the top $k$ terms. The error is orthogonal to the retained subspace, giving the stated formulas.</p>
      </div>

      <p><strong>Applications:</strong> This theorem is the foundation for:</p>
      <ul>
        <li><strong>Data compression:</strong> Store only top-$k$ singular vectors and values (image/video compression)</li>
        <li><strong>Denoising:</strong> Remove noise by truncating small singular values</li>
        <li><strong>Dimensionality reduction:</strong> Project data onto top-$k$ principal components (PCA)</li>
        <li><strong>Recommender systems:</strong> Matrix completion via low-rank factorization (Netflix Prize)</li>
      </ul>

      <h4>7.2.6 Connection to Eigendecomposition</h4>
      <p>For <strong>symmetric</strong> $A = A^\top$, the SVD reduces to the spectral theorem:</p>
      <ul>
        <li>Eigenvalues $\lambda_i$ and singular values $\sigma_i$ are related: $\sigma_i = |\lambda_i|$</li>
        <li>If $A \succeq 0$ (PSD), then $\lambda_i \ge 0$ and $\sigma_i = \lambda_i$, so $A = Q \Lambda Q^\top = U \Sigma V^\top$ with $U = V = Q$</li>
      </ul>
      <p>For <strong>general</strong> (non-symmetric) $A$:</p>
      <ul>
        <li>Eigenvalues of $A$ may be complex; singular values are always real and nonnegative</li>
        <li>$A^\top A$ has eigenvalues $\lambda_i = \sigma_i^2$ and eigenvectors $v_i$ (right singular vectors of $A$)</li>
        <li>$A A^\top$ has eigenvalues $\lambda_i = \sigma_i^2$ and eigenvectors $u_i$ (left singular vectors of $A$)</li>
      </ul>

      <!-- 7.3 Pseudoinverse (Moore-Penrose Inverse) -->
      <h3>7.3 The Pseudoinverse (Moore-Penrose Inverse)</h3>

      <h4>7.3.1 Definition via SVD</h4>
      <p>For $A = U \Sigma V^\top$ with singular values $\sigma_1, \dots, \sigma_r > 0$, the <strong>pseudoinverse</strong> is:
      $$ A^+ = V \Sigma^+ U^\top $$
      where $\Sigma^+ \in \mathbb{R}^{n \times m}$ is formed by taking the reciprocal of nonzero singular values and transposing:
      $$ \Sigma^+ = \begin{bmatrix} \text{diag}(1/\sigma_1, \dots, 1/\sigma_r) & 0 \\ 0 & 0 \end{bmatrix}^\top $$</p>

      <h4>7.3.2 Moore-Penrose Axioms</h4>
      <p>The pseudoinverse $A^+$ is the <strong>unique</strong> matrix satisfying:</p>
      <ol>
        <li>$A A^+ A = A$</li>
        <li>$A^+ A A^+ = A^+$</li>
        <li>$(A A^+)^\top = A A^+$ (symmetric)</li>
        <li>$(A^+ A)^\top = A^+ A$ (symmetric)</li>
      </ol>
      <p>These axioms uniquely define $A^+$ without reference to SVD.</p>

      <h4>7.3.3 Properties and Applications</h4>
      <ul>
        <li><strong>Least squares:</strong> $x^+ = A^+ b$ is the <strong>minimum-norm least-squares solution</strong>: it minimizes $\|Ax - b\|_2$ over all $x$, and among all minimizers, it has the smallest $\|x\|_2$.</li>
        <li><strong>Inverse when invertible:</strong> If $A$ is square and invertible, $A^+ = A^{-1}$.</li>
        <li><strong>Projectors:</strong> $P_{\mathcal{R}(A)} = A A^+$ projects onto $\mathcal{R}(A)$; $P_{\mathcal{R}(A^\top)} = A^+ A$ projects onto $\mathcal{R}(A^\top)$.</li>
        <li><strong>Solution to $Ax = b$:</strong> The general solution is $x = A^+ b + (I - A^+ A) z$ for any $z \in \mathbb{R}^n$ (parametrizes the nullspace component).</li>
      </ul>

      <!-- 7.4 Principal Component Analysis (PCA) -->
      <h3>7.4 Principal Component Analysis (PCA) via SVD</h3>

      <p><strong>Problem:</strong> Given data $X \in \mathbb{R}^{n \times d}$ ($n$ samples, $d$ features), find a $k$-dimensional subspace that captures maximum variance.</p>

      <h4>PCA Algorithm:</h4>
      <ol>
        <li><strong>Center the data:</strong> $\tilde{X} = X - \mathbf{1} \mu^\top$ where $\mu = \frac{1}{n} X^\top \mathbf{1}$ (column means)</li>
        <li><strong>Compute SVD:</strong> $\tilde{X} = U \Sigma V^\top$</li>
        <li><strong>Principal components:</strong> The columns of $V$ are the <strong>principal directions</strong> (eigenvectors of covariance matrix $C = \frac{1}{n} \tilde{X}^\top \tilde{X}$)</li>
        <li><strong>Variance explained:</strong> The $i$-th principal component explains variance $\sigma_i^2 / n$</li>
        <li><strong>Projection:</strong> Project onto top-$k$ components: $Z = \tilde{X} V_k \in \mathbb{R}^{n \times k}$</li>
      </ol>

      <p><strong>Why SVD?</strong> The covariance matrix $C = \frac{1}{n} \tilde{X}^\top \tilde{X} = \frac{1}{n} V \Sigma^2 V^\top$ (from SVD). Eigenvalues of $C$ are $\sigma_i^2 / n$, eigenvectors are columns of $V$. SVD directly provides the principal components without forming $C$ explicitly—more stable and efficient.</p>

      <p><strong>Applications:</strong></p>
      <ul>
        <li>Dimensionality reduction for visualization (e.g., projecting $\mathbb{R}^{1000}$ to $\mathbb{R}^2$)</li>
        <li>Feature extraction in machine learning (reducing input dimension)</li>
        <li>Data compression (JPEG, video codecs)</li>
        <li>Noise reduction (truncate components with low variance)</li>
      </ul>

      <!-- 7.5 Condition Numbers and Perturbation Theory -->
      <h3>7.5 Condition Numbers and Numerical Stability</h3>

      <h4>7.5.1 Definition</h4>
      <p>The <strong>condition number</strong> of a matrix $A \in \mathbb{R}^{m \times n}$ with $\text{rank}(A) = n$ is:
      $$ \kappa(A) = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} = \frac{\sigma_1}{\sigma_n} $$
      For square nonsingular $A$, this is equivalent to $\kappa(A) = \|A\|_2 \|A^{-1}\|_2$.</p>

      <h4>7.5.2 Interpretation</h4>
      <p>The condition number measures how <strong>sensitive</strong> the solution of $Ax = b$ is to perturbations in $A$ or $b$:</p>
      <ul>
        <li><strong>Well-conditioned ($\kappa \approx 1$):</strong> $\sigma_1 \approx \sigma_n$; matrix is "balanced"; small changes in input → small changes in output</li>
        <li><strong>Ill-conditioned ($\kappa \gg 1$):</strong> $\sigma_1 \gg \sigma_n$; matrix is nearly singular; small changes in input → large changes in output</li>
      </ul>

      <h4>7.5.3 Perturbation Bound</h4>
      <div class="proof-enhanced">
        <p><strong>Theorem (Relative Error Bound):</strong> Let $Ax = b$ and $A(x + \Delta x) = b + \Delta b$. If $\|\Delta b\|_2 / \|b\|_2$ is small, then:
        $$ \frac{\|\Delta x\|_2}{\|x\|_2} \le \kappa(A) \frac{\|\Delta b\|_2}{\|b\|_2} $$</p>

        <p><strong>Implication:</strong> A perturbation of size $\epsilon$ in $b$ can cause a perturbation of size $\kappa(A) \cdot \epsilon$ in $x$. If $\kappa(A) = 10^{10}$ and we have machine precision $\epsilon_{\text{mach}} \approx 10^{-16}$, we lose $\log_{10}(\kappa) = 10$ digits of precision!</p>
      </div>

      <h4>7.5.4 Why Normal Equations Fail</h4>
      <p>The normal equations $A^\top A x = A^\top b$ square the condition number:
      $$ \kappa(A^\top A) = \frac{\sigma_1^2}{\sigma_n^2} = \kappa(A)^2 $$
      If $A$ is already ill-conditioned, forming $A^\top A$ makes it catastrophically worse. <strong>This is why QR and SVD are preferred.</strong></p>

      <!-- 7.6 Numerical Algorithms -->
      <h3>7.6 Numerical Algorithms for SVD</h3>

      <h4>7.6.1 Golub-Kahan-Reinsch Algorithm</h4>
      <p>The standard algorithm for computing SVD:</p>
      <ol>
        <li><strong>Bidiagonalization:</strong> Reduce $A$ to bidiagonal form $B$ using Householder reflections: $A = U_1 B V_1^\top$ (cost: $O(mn^2)$)</li>
        <li><strong>Iterative diagonalization:</strong> Apply QR iterations to $B$ to diagonalize it: $B = U_2 \Sigma V_2^\top$ (cost: $O(n^2)$ iterations)</li>
        <li><strong>Combine:</strong> $A = (U_1 U_2) \Sigma (V_1 V_2)^\top$</li>
      </ol>
      <p><strong>Total complexity:</strong> $O(mn^2)$ for $m \ge n$. For large-scale problems, randomized SVD algorithms achieve $O(mn \log k)$ for rank-$k$ approximation.</p>

      <h4>7.6.2 Randomized SVD (for Large Matrices)</h4>
      <p>For massive matrices where exact SVD is prohibitive, <strong>randomized algorithms</strong> provide fast, accurate rank-$k$ approximations:</p>
      <ol>
        <li>Sample $k + p$ random vectors and form $Q$ via QR of $A \Omega$ ($\Omega$ random)</li>
        <li>Compute SVD of small matrix $Q^\top A$</li>
        <li>Reconstruct approximate SVD of $A$</li>
      </ol>
      <p>Complexity: $O(mn \log k)$ vs $O(mn^2)$ for full SVD. Used in big data applications (Netflix, Google).</p>

      <!-- Interactive Widgets -->
      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: SVD for Image Compression</h3>
        <p><b>The Power of Low-Rank Approximation:</b> The Singular Value Decomposition (SVD) reveals that many real-world matrices (like images) can be accurately approximated using only their largest singular values. This demo lets you:</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Load an image:</b> Any image is just a matrix of pixel values</li>
          <li><b>Compute SVD:</b> $A = U\Sigma V^\top$ where $\Sigma$ contains singular values in decreasing order</li>
          <li><b>Reconstruct with k values:</b> Use only the top $k$ singular values and see the quality vs. compression tradeoff</li>
          <li><b>Observe compression ratio:</b> Often 90%+ of the "energy" is captured by just 10-20% of singular values!</li>
        </ul>
        <p><i>Applications:</i> This principle underpins data compression, dimensionality reduction (PCA), recommender systems, and denoising. In optimization, low-rank structure enables efficient large-scale algorithms.</p>
        <div id="widget-svd-approximator" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Condition Number & Numerical Stability</h3>
        <p><b>Why Some Systems Are Harder to Solve:</b> This tool demonstrates how the condition number $\kappa(A) = \sigma_{\max}/\sigma_{\min}$ affects the convergence of iterative solvers. Compare two linear systems $Ax = b$ side by side:</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Well-conditioned ($\kappa \approx 1$):</b> Balanced eigenvalues → fast, stable convergence</li>
          <li><b>Ill-conditioned ($\kappa \gg 1$):</b> Widely varying eigenvalues → slow convergence, numerical instability</li>
        </ul>
        <p><i>Key takeaway:</i> This is why we avoid forming $A^\top A$ in least squares (it squares the condition number!) and prefer QR or SVD methods. In optimization, ill-conditioning makes gradient descent crawl—motivating preconditioning and second-order methods.</p>
        <div id="widget-condition-number" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <!-- 7.7 Projection Revisited -->
      <h3>7.7 Projection Revisited (with SVD)</h3>
      <p>Using the SVD $A = U \Sigma V^\top$ with rank $r$, the projector onto $\mathcal{R}(A)$ is:
      $$ P_{\mathcal{R}(A)} = U_r U_r^\top = A A^+ $$
      where $U_r$ consists of the first $r$ columns of $U$ (corresponding to nonzero singular values). Similarly:
      $$ P_{\mathcal{R}(A^\top)} = V_r V_r^\top = A^+ A $$
      Both are symmetric ($P^\top = P$) and idempotent ($P^2 = P$), and they send any vector to its closest point in the respective subspace.</p>

      <h3>7.8 Summary: When to Use What</h3>
      <table style="width: 100%; border-collapse: collapse; margin: 16px 0;">
        <thead>
          <tr style="background: var(--panel); border-bottom: 2px solid var(--border);">
            <th style="padding: 8px; text-align: left;">Method</th>
            <th style="padding: 8px; text-align: left;">When to Use</th>
            <th style="padding: 8px; text-align: left;">Complexity</th>
          </tr>
        </thead>
        <tbody>
          <tr style="border-bottom: 1px solid var(--border);">
            <td style="padding: 8px;"><strong>Normal Equations</strong></td>
            <td style="padding: 8px;">Small, well-conditioned problems; theoretical derivations</td>
            <td style="padding: 8px;">$O(mn^2 + n^3)$</td>
          </tr>
          <tr style="border-bottom: 1px solid var(--border);">
            <td style="padding: 8px;"><strong>QR Decomposition</strong></td>
            <td style="padding: 8px;">Standard least squares; numerically stable; full rank</td>
            <td style="padding: 8px;">$O(mn^2)$</td>
          </tr>
          <tr style="border-bottom: 1px solid var(--border);">
            <td style="padding: 8px;"><strong>SVD</strong></td>
            <td style="padding: 8px;">Rank-deficient, ill-conditioned; minimum-norm solution; analysis</td>
            <td style="padding: 8px;">$O(mn^2)$</td>
          </tr>
          <tr>
            <td style="padding: 8px;"><strong>Randomized SVD</strong></td>
            <td style="padding: 8px;">Large-scale, low-rank approximation; big data</td>
            <td style="padding: 8px;">$O(mn \log k)$</td>
          </tr>
        </tbody>
      </table>

      <p><strong>Key Takeaway:</strong> <em>Prefer QR or SVD over normal equations for numerical stability. Use SVD when rank-deficiency, condition number analysis, or low-rank approximation is needed.</em></p>

      <!-- 7.9 Comprehensive Worked Examples -->
      <h3>7.9 Comprehensive Worked Examples</h3>

      <h4>Example 7.1: Computing SVD by Hand (2×2 Matrix)</h4>
      <div class="proof-enhanced">
        <p><strong>Problem:</strong> Compute the SVD of $A = \begin{bmatrix} 3 & 0 \\ 4 & 5 \end{bmatrix}$.</p>

        <div class="proof-step">
          <strong>Step 1: Compute $A^\top A$.</strong>
          $$
          A^\top A = \begin{bmatrix} 3 & 4 \\ 0 & 5 \end{bmatrix} \begin{bmatrix} 3 & 0 \\ 4 & 5 \end{bmatrix} = \begin{bmatrix} 25 & 20 \\ 20 & 25 \end{bmatrix}
          $$
        </div>

        <div class="proof-step">
          <strong>Step 2: Find eigenvalues of $A^\top A$.</strong> The characteristic polynomial is:
          $$
          \det(A^\top A - \lambda I) = \det\begin{bmatrix} 25-\lambda & 20 \\ 20 & 25-\lambda \end{bmatrix} = (25-\lambda)^2 - 400 = \lambda^2 - 50\lambda + 225
          $$
          Solving: $\lambda_1 = 45$, $\lambda_2 = 5$. Thus $\sigma_1 = \sqrt{45} = 3\sqrt{5} \approx 6.708$ and $\sigma_2 = \sqrt{5} \approx 2.236$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Find eigenvectors (right singular vectors $V$).</strong>
          <ul>
            <li>For $\lambda_1 = 45$: $(A^\top A - 45I)v = 0$ gives $v_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$</li>
            <li>For $\lambda_2 = 5$: $(A^\top A - 5I)v = 0$ gives $v_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}$</li>
          </ul>
          Thus $V = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}$.
        </div>

        <div class="proof-step">
          <strong>Step 4: Compute left singular vectors $U$.</strong> Use $u_i = Av_i / \sigma_i$:
          $$
          u_1 = \frac{1}{\sigma_1} A v_1 = \frac{1}{3\sqrt{5}} \begin{bmatrix} 3 & 0 \\ 4 & 5 \end{bmatrix} \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{1}{3\sqrt{10}} \begin{bmatrix} 3 \\ 9 \end{bmatrix} = \frac{1}{\sqrt{10}} \begin{bmatrix} 1 \\ 3 \end{bmatrix}
          $$
          $$
          u_2 = \frac{1}{\sigma_2} A v_2 = \frac{1}{\sqrt{5}} \begin{bmatrix} 3 & 0 \\ 4 & 5 \end{bmatrix} \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1 \end{bmatrix} = \frac{1}{\sqrt{10}} \begin{bmatrix} 3 \\ -1 \end{bmatrix}
          $$
          Thus $U = \frac{1}{\sqrt{10}} \begin{bmatrix} 1 & 3 \\ 3 & -1 \end{bmatrix}$.
        </div>

        <div class="proof-step">
          <strong>Step 5: Assemble the SVD.</strong>
          $$
          A = U \Sigma V^\top = \frac{1}{\sqrt{10}} \begin{bmatrix} 1 & 3 \\ 3 & -1 \end{bmatrix} \begin{bmatrix} 3\sqrt{5} & 0 \\ 0 & \sqrt{5} \end{bmatrix} \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}
          $$
        </div>

        <div class="proof-step">
          <strong>Verification:</strong> Check that $U^\top U = I$, $V^\top V = I$, and $U \Sigma V^\top = A$.
        </div>
      </div>

      <h4>Example 7.2: Low-Rank Approximation for Data Compression</h4>
      <div class="proof-enhanced">
        <p><strong>Problem:</strong> Given data matrix $X \in \mathbb{R}^{100 \times 50}$ (100 samples, 50 features) with $\text{rank}(X) = 50$, compute a rank-10 approximation that retains 90% of the "energy" (Frobenius norm squared).</p>

        <div class="proof-step">
          <strong>Step 1: Compute SVD.</strong> $X = U \Sigma V^\top$ where $\Sigma = \text{diag}(\sigma_1, \dots, \sigma_{50})$ with $\sigma_1 \ge \cdots \ge \sigma_{50} > 0$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Energy in each component.</strong> The total energy is:
          $$
          \|X\|_F^2 = \sum_{i=1}^{50} \sigma_i^2
          $$
          The energy retained by the rank-$k$ approximation is:
          $$
          \|X_k\|_F^2 = \sum_{i=1}^k \sigma_i^2
          $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Choose $k$ to retain 90% energy.</strong> Find the smallest $k$ such that:
          $$
          \frac{\sum_{i=1}^k \sigma_i^2}{\sum_{i=1}^{50} \sigma_i^2} \ge 0.90
          $$
          Suppose this gives $k = 10$ (typical for real data with decaying singular values).
        </div>

        <div class="proof-step">
          <strong>Step 4: Form approximation.</strong>
          $$
          X_{10} = U_{:,1:10} \Sigma_{1:10,1:10} V_{:,1:10}^\top = \sum_{i=1}^{10} \sigma_i u_i v_i^\top
          $$
        </div>

        <div class="proof-step">
          <strong>Storage savings:</strong>
          <ul>
            <li><strong>Original:</strong> $100 \times 50 = 5000$ numbers</li>
            <li><strong>Rank-10 approximation:</strong> $10(100 + 1 + 50) = 1510$ numbers (storing $U_{:,1:10}$, $\Sigma_{1:10,1:10}$, $V_{:,1:10}$)</li>
            <li><strong>Compression ratio:</strong> $5000/1510 \approx 3.3\times$ with 90% energy retained</li>
          </ul>
        </div>
      </div>

      <h4>Example 7.3: Solving Rank-Deficient Least Squares</h4>
      <div class="proof-enhanced">
        <p><strong>Problem:</strong> Solve $\min \|Ax - b\|_2$ where $A = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \end{bmatrix}$ (rank 1) and $b = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$.</p>

        <div class="proof-step">
          <strong>Step 1: Recognize rank deficiency.</strong> The second row is $2 \times$ the first row, so $\text{rank}(A) = 1$. Normal equations $A^\top A x = A^\top b$ will be singular.
        </div>

        <div class="proof-step">
          <strong>Step 2: Compute (compact) SVD.</strong> For this simple case:
          $$
          A = \sigma_1 u_1 v_1^\top \quad \text{where } \sigma_1 = \sqrt{1^2 + 2^2 + 3^2 + 2^2 + 4^2 + 6^2} = \sqrt{70}
          $$
          $$
          u_1 = \frac{1}{\sqrt{5}} \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \quad v_1 = \frac{1}{\sqrt{14}} \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
          $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Compute pseudoinverse.</strong>
          $$
          A^+ = V \Sigma^+ U^\top = \frac{1}{\sigma_1} v_1 u_1^\top = \frac{1}{70} \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} \begin{bmatrix} 1 & 2 \end{bmatrix} = \frac{1}{70} \begin{bmatrix} 1 & 2 \\ 2 & 4 \\ 3 & 6 \end{bmatrix}
          $$
        </div>

        <div class="proof-step">
          <strong>Step 4: Minimum-norm solution.</strong>
          $$
          x^+ = A^+ b = \frac{1}{70} \begin{bmatrix} 1 & 2 \\ 2 & 4 \\ 3 & 6 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \frac{1}{70} \begin{bmatrix} 5 \\ 10 \\ 15 \end{bmatrix} = \frac{1}{14} \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
          $$
        </div>

        <div class="proof-step">
          <strong>Verification:</strong>
          <ul>
            <li>$Ax^+ = A \cdot \frac{1}{14}v_1 = \frac{\sigma_1}{14} u_1 v_1^\top v_1 = \frac{\sqrt{70}}{14} \cdot \sqrt{14} u_1 = \frac{1}{2}\sqrt{5} \cdot \frac{1}{\sqrt{5}}\begin{bmatrix} 1 \\ 2 \end{bmatrix} = \frac{1}{2}\begin{bmatrix} 1 \\ 2 \end{bmatrix}$</li>
            <li>Residual: $\|Ax^+ - b\|_2 = \left\|\frac{1}{2}\begin{bmatrix} 1 \\ 2 \end{bmatrix} - \begin{bmatrix} 1 \\ 2 \end{bmatrix}\right\|_2 = \frac{1}{2}\sqrt{5}$</li>
            <li>This is the minimum possible (since $b \notin \mathcal{R}(A)$)</li>
            <li>Among all minimizers, $x^+$ has the smallest norm: $\|x^+\|_2 = \frac{1}{14}\sqrt{14} = \frac{1}{\sqrt{14}}$</li>
          </ul>
        </div>
      </div>

      <h4>Example 7.4: Condition Number Analysis</h4>
      <div class="proof-enhanced">
        <p><strong>Problem:</strong> Compare the condition numbers of $A$ and $A^\top A$ for $A = \begin{bmatrix} 1 & 0 \\ 0 & 0.01 \\ 0 & 0 \end{bmatrix}$.</p>

        <div class="proof-step">
          <strong>Step 1: Compute singular values of $A$.</strong> For this matrix, $\sigma_1 = 1$ and $\sigma_2 = 0.01$ (diagonal structure makes this immediate). Thus:
          $$
          \kappa(A) = \frac{\sigma_1}{\sigma_2} = \frac{1}{0.01} = 100
          $$
        </div>

        <div class="proof-step">
          <strong>Step 2: Compute $A^\top A$.</strong>
          $$
          A^\top A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0.01 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 0.01 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 0.0001 \end{bmatrix}
          $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Condition number of $A^\top A$.</strong> Eigenvalues are 1 and 0.0001, so:
          $$
          \kappa(A^\top A) = \frac{1}{0.0001} = 10000 = \kappa(A)^2
          $$
        </div>

        <div class="proof-step">
          <strong>Implication:</strong> If $A$ already has $\kappa(A) = 100$, forming $A^\top A$ makes it $\kappa = 10^4$—catastrophically worse. With machine precision $\epsilon_{\text{mach}} \approx 10^{-16}$, we lose $\log_{10}(10^4) = 4$ digits of precision!
        </div>

        <div class="proof-step">
          <strong>Solution:</strong> Use QR or SVD to avoid forming $A^\top A$ explicitly.
        </div>
      </div>

      <!-- 7.10 Applications to Optimization -->
      <h3>7.10 SVD Applications in Convex Optimization</h3>

      <h4>7.10.1 Preconditioning Gradient Descent</h4>
      <p>Consider minimizing a quadratic $f(x) = \frac{1}{2}x^\top A x - b^\top x$ where $A = U \Sigma^2 U^\top \succ 0$. Standard gradient descent has convergence rate dependent on $\kappa(A)$:</p>
      $$
      \left\|x^{(k)} - x^*\right\|_A \le \left(\frac{\kappa(A) - 1}{\kappa(A) + 1}\right)^k \|x^{(0)} - x^*\|_A
      $$
      <p>For $\kappa(A) = 10^4$, convergence is painfully slow. <strong>Preconditioning</strong> via SVD:</p>
      <ol>
        <li>Change variables: $y = \Sigma U^\top x$ (whitening transformation)</li>
        <li>In $y$-space, the problem becomes $\min \frac{1}{2}\|y\|_2^2 - \tilde{b}^\top y$ where $\tilde{b} = \Sigma^{-1} U^\top b$</li>
        <li>Hessian in $y$-space is $I$ (perfectly conditioned!), so gradient descent converges in one step</li>
        <li>Transform back: $x^* = U \Sigma^{-1} y^*$</li>
      </ol>
      <p>This is the essence of <strong>Newton's method</strong>: use second-order information (Hessian = $A$) to precondition.</p>

      <h4>7.10.2 Total Least Squares (TLS)</h4>
      <p>Standard least squares minimizes $\|Ax - b\|_2$ assuming $A$ is exact and $b$ is noisy. <strong>Total Least Squares</strong> accounts for errors in both:</p>
      $$
      \min_{\Delta A, \Delta b, x} \left\|\begin{bmatrix} \Delta A & \Delta b \end{bmatrix}\right\|_F \quad \text{s.t.} \quad (A + \Delta A)x = b + \Delta b
      $$
      <p><strong>Solution via SVD:</strong> Form augmented matrix $[A \mid b]$ and compute SVD. The TLS solution is given by the right singular vector corresponding to the smallest singular value. This is used in errors-in-variables regression.</p>

      <h4>7.10.3 Matrix Completion and Recommender Systems</h4>
      <p>The <strong>Netflix Prize</strong> problem: Given a sparse ratings matrix $R \in \mathbb{R}^{m \times n}$ (users × movies) with only $\sim$1% entries observed, recover the full matrix. Assumption: $R$ is low-rank (users have a few latent preferences).</p>
      <p><strong>Formulation:</strong></p>
      $$
      \min_{X \in \mathbb{R}^{m \times n}} \text{rank}(X) \quad \text{s.t.} \quad X_{ij} = R_{ij} \text{ for observed } (i,j)
      $$
      <p>Since $\text{rank}(\cdot)$ is non-convex, relax to the <strong>nuclear norm</strong> (convex envelope of rank):</p>
      $$
      \min_{X \in \mathbb{R}^{m \times n}} \|X\|_* = \sum_i \sigma_i(X) \quad \text{s.t.} \quad X_{ij} = R_{ij} \text{ for observed } (i,j)
      $$
      <p>This is a <strong>semidefinite program</strong> (SDP), solvable via convex optimization. The nuclear norm promotes low-rank solutions.</p>

      <h4>7.10.4 Robust PCA: Separating Low-Rank and Sparse Components</h4>
      <p>Given data $M = L + S$ where $L$ is low-rank (signal) and $S$ is sparse (outliers/noise), recover both:</p>
      $$
      \min_{L, S} \|L\|_* + \lambda \|S\|_1 \quad \text{s.t.} \quad L + S = M
      $$
      <p>This convex formulation (Principal Component Pursuit) provably recovers $L$ and $S$ under mild conditions. Applications: video surveillance (background = low-rank, foreground = sparse), data cleaning, anomaly detection.</p>

      <h4>7.10.5 Dimensionality Reduction for Large-Scale Optimization</h4>
      <p>For problems with $n \gg 1$ variables but low effective dimension (e.g., $x$ lies near a $k$-dimensional subspace), use SVD to reduce dimension:</p>
      <ol>
        <li>Collect samples $x^{(1)}, \dots, x^{(m)}$ from the feasible set or initialization</li>
        <li>Form data matrix $X = [x^{(1)} \cdots x^{(m)}]$ and compute SVD: $X = U \Sigma V^\top$</li>
        <li>Identify dominant subspace: keep top $k$ left singular vectors $U_k$</li>
        <li>Parametrize $x = U_k y$ where $y \in \mathbb{R}^k$ (dimension reduction from $n$ to $k$)</li>
        <li>Solve optimization problem in reduced space (much faster)</li>
      </ol>
      <p>This is used in <strong>active subspace methods</strong> for high-dimensional inverse problems and PDE-constrained optimization.</p>

      <!-- 7.11 Computational Considerations -->
      <h3>7.11 Computational Considerations and Software</h3>

      <h4>7.11.1 When NOT to Compute Full SVD</h4>
      <p>For a matrix $A \in \mathbb{R}^{m \times n}$ with $m, n \approx 10^6$, full SVD costs $O(mn^2) \approx 10^{18}$ operations—infeasible! Alternatives:</p>
      <ul>
        <li><strong>Truncated SVD:</strong> Use iterative methods (Lanczos, Arnoldi) to compute only top $k$ singular values/vectors. Cost: $O(mnk)$ with $k \ll n$. Libraries: ARPACK, PROPACK.</li>
        <li><strong>Randomized SVD:</strong> Cost $O(mn \log k)$. Libraries: scikit-learn (<code>TruncatedSVD</code>), Facebook's <code>fbpca</code>, <code>redsvd</code>.</li>
        <li><strong>Streaming SVD:</strong> For data that doesn't fit in memory, update SVD incrementally as new rows arrive.</li>
      </ul>

      <h4>7.11.2 Numerical Stability: Avoiding $A^\top A$</h4>
      <p><strong>Bad practice (loses precision):</strong></p>
      <pre style="background: var(--panel); padding: 12px; border-radius: 4px; overflow-x: auto;">
# Python / NumPy example (DON'T DO THIS for ill-conditioned A!)
import numpy as np
AtA = A.T @ A
eigenvalues, V = np.linalg.eigh(AtA)
sigma = np.sqrt(eigenvalues)  # numerical errors amplified!</pre>

      <p><strong>Good practice (numerically stable):</strong></p>
      <pre style="background: var(--panel); padding: 12px; border-radius: 4px; overflow-x: auto;">
# Python / NumPy example
import numpy as np
U, sigma, Vt = np.linalg.svd(A, full_matrices=False)  # uses bidiagonalization</pre>

      <h4>7.11.3 Software Recommendations</h4>
      <table style="width: 100%; border-collapse: collapse; margin: 16px 0;">
        <thead>
          <tr style="background: var(--panel); border-bottom: 2px solid var(--border);">
            <th style="padding: 8px; text-align: left;">Language</th>
            <th style="padding: 8px; text-align: left;">Library</th>
            <th style="padding: 8px; text-align: left;">Function</th>
            <th style="padding: 8px; text-align: left;">Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr style="border-bottom: 1px solid var(--border);">
            <td style="padding: 8px;">Python</td>
            <td style="padding: 8px;"><code>numpy.linalg</code></td>
            <td style="padding: 8px;"><code>svd(A)</code></td>
            <td style="padding: 8px;">Full SVD via LAPACK (dgesdd)</td>
          </tr>
          <tr style="border-bottom: 1px solid var(--border);">
            <td style="padding: 8px;">Python</td>
            <td style="padding: 8px;"><code>scipy.sparse.linalg</code></td>
            <td style="padding: 8px;"><code>svds(A, k)</code></td>
            <td style="padding: 8px;">Sparse truncated SVD (top k)</td>
          </tr>
          <tr style="border-bottom: 1px solid var(--border);">
            <td style="padding: 8px;">MATLAB</td>
            <td style="padding: 8px;">Built-in</td>
            <td style="padding: 8px;"><code>[U,S,V] = svd(A)</code></td>
            <td style="padding: 8px;">Full or economy SVD</td>
          </tr>
          <tr style="border-bottom: 1px solid var(--border);">
            <td style="padding: 8px;">Julia</td>
            <td style="padding: 8px;"><code>LinearAlgebra</code></td>
            <td style="padding: 8px;"><code>svd(A)</code></td>
            <td style="padding: 8px;">Fast, via OpenBLAS/MKL</td>
          </tr>
          <tr>
            <td style="padding: 8px;">C/C++</td>
            <td style="padding: 8px;">LAPACK</td>
            <td style="padding: 8px;"><code>dgesvd</code> / <code>dgesdd</code></td>
            <td style="padding: 8px;">Production-grade, highly optimized</td>
          </tr>
        </tbody>
      </table>

      <h4>7.11.4 Sparse vs Dense SVD</h4>
      <p><strong>Sparse matrices:</strong> If $A$ has mostly zeros (e.g., graph Laplacians, finite element matrices), use specialized sparse SVD algorithms that exploit sparsity. Never convert to dense!</p>
      <p><strong>Structured matrices:</strong> For Toeplitz, circulant, or FFT-structured matrices, fast $O(n \log n)$ multiplication enables iterative SVD via Krylov methods.</p>

      <p><strong>Key Takeaway:</strong> <em>Prefer QR or SVD over normal equations for numerical stability. Use SVD when rank-deficiency, condition number analysis, or low-rank approximation is needed.</em></p>
    </section>

    <!-- SECTION 8: VARIANTS -->
    <section class="card-v2" id="section-8">
      <h2>8. Variants You Will Need</h2>

      <h3>(a) Weighted Least Squares (WLS)</h3>
      <p>Given SPD weight $W \in \mathbb{R}^{m \times m}$:
      $$ \min_x \|Ax - b\|_W^2 := (Ax - b)^\top W(Ax - b) $$
      Let $C$ satisfy $W = C^\top C$ (e.g., Cholesky). Then the problem is ordinary least squares in the <b>whitened</b> system $(CA)x \approx Cb$. Normal equations: $A^\top W A x = A^\top W b$.</p>

      <h3>(b) Constrained to an Affine Set</h3>
      <p>Solve:
      $$ \min_x \|Ax - b\|_2^2 \quad \text{s.t.} \quad Fx = g $$
      One method: parametrize $x = x_0 + Zy$, where $Fx_0 = g$ and columns of $Z$ form a basis for $\mathcal{N}(F)$. Then minimize $\|A(x_0 + Zy) - b\|_2^2$ over $y$ (an unconstrained LS). QR on $AZ$ is typically best.</p>

    </section>

    <!-- SECTION 9: WORKED EXAMPLES -->
    <section class="card-v2" id="section-9">
      <h2>9. Worked Examples (Fully Written Out)</h2>

      <div class="proof-enhanced">
        <h3>Example 1: Projection onto a Line</h3>
        <p>Let's find the projection of the vector $b = (2, 3)^\top$ onto the line spanned by the vector $u = (1, 1)^\top$. The projection $p$ is given by the formula:
        $$ p = \frac{u^\top b}{u^\top u} u = \frac{(1)(2) + (1)(3)}{(1)(1) + (1)(1)} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \frac{5}{2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2.5 \\ 2.5 \end{pmatrix} $$
        The residual vector is $r = b - p = (2 - 2.5, 3 - 2.5)^\top = (-0.5, 0.5)^\top$. We can verify that the residual is orthogonal to the line: $u^\top r = (1)(-0.5) + (1)(0.5) = 0$.
        </p>
      </div>

      <div class="proof-enhanced">
        <h3>Example 2: Solving Least Squares with Normal Equations</h3>
        <p>Let's solve the least squares problem for the system $Ax=b$ with:
        $$ A = \begin{pmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 1 \end{pmatrix}, \quad b = \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix} $$
        First, we form the normal equations $A^\top A x = A^\top b$:
        $$ A^\top A = \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & 1 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 1 \end{pmatrix} = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} $$
        $$ A^\top b = \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & 1 \end{pmatrix} \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 3 \\ 3 \end{pmatrix} $$
        We solve the system $\begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} x = \begin{pmatrix} 3 \\ 3 \end{pmatrix}$, which yields the solution $x^\star = (3/4, 3/4)^\top$.
        The projection is $p = Ax^\star = (3/2, 0, 3/2)^\top$, and the residual is $r = b - p = (1/2, 0, -1/2)^\top$. We can verify the orthogonality condition: $A^\top r = (0, 0)^\top$.
        </p>
      </div>

      <div class="proof-enhanced">
        <h3>Example 3: Rank-Deficient Case</h3>
        <p>Consider the system with:
        $$ A = \begin{pmatrix} 1 & 1 \\ 2 & 2 \end{pmatrix}, \quad b = \begin{pmatrix} 3 \\ 6 \end{pmatrix} $$
        The columns of $A$ are linearly dependent, so the system is rank-deficient. The vector $b$ is in the column space of $A$, so there are infinitely many solutions. The normal equations are $A^\top A x = \begin{pmatrix} 5 & 5 \\ 5 & 5 \end{pmatrix} x = \begin{pmatrix} 15 \\ 15 \end{pmatrix}$. This system has infinitely many solutions of the form $x_1 + x_2 = 3$. The pseudoinverse would provide the minimum-norm solution, $x_1 = x_2 = 1.5$.
        </p>
      </div>
    </section>

    <!-- SECTION 10: IMPLEMENTATION GUIDE -->
    <section class="card-v2" id="section-10">
      <h2>10. Implementation Mini-Guide (What to Actually Do in Code)</h2>

      <ul>
        <li>For standard, well-conditioned problems, <b>QR decomposition</b> is the recommended method. It is numerically stable and computationally efficient.</li>
        <li>For problems that are ill-conditioned or rank-deficient, the <b>SVD method</b> is the most robust choice. It is also necessary when the minimum-norm solution is required.</li>
        <li>Avoid explicitly forming the product $A^\top A$ and solving the normal equations, as this squares the condition number and can lead to a loss of numerical precision.</li>
        <li>Before solving, it is often a good practice to <b>preprocess data</b> by centering and scaling features. This can significantly improve the condition number.</li>
        <li>Always <b>verify the solution</b> by checking that the residual is orthogonal to the column space: $A^\top(b - Ax^*) \approx 0$.</li>
      </ul>
    </section>

    <!-- SECTION 11: EXERCISES -->
    <section class="card-v2" id="section-11">
      <h2>11. Exercises</h2>

      <h3>A. Fundamentals</h3>

      <div class="proof-enhanced">
        <h4>A1. Prove that the set of all linear combinations of a fixed set of vectors is a subspace.</h4>
        <p><b>Solution:</b> Let $S = \mathrm{span}\{v_1, \dots, v_k\}$.
        <br>1. <b>Closure under addition:</b> Let $u = \sum c_i v_i$ and $w = \sum d_i v_i$ be in $S$. Then $u+w = \sum (c_i+d_i)v_i$, which is also a linear combination, so $u+w \in S$.
        <br>2. <b>Closure under scalar multiplication:</b> Let $u = \sum c_i v_i \in S$ and $\alpha \in \mathbb{R}$. Then $\alpha u = \sum (\alpha c_i)v_i$, which is also a linear combination, so $\alpha u \in S$.
        <br>3. <b>Contains zero vector:</b> The zero vector is in $S$ because $0 = \sum 0 \cdot v_i$.
        <br>Since $S$ satisfies these three properties, it is a subspace.</p>
      </div>

      <div class="proof-enhanced">
        <h4>A2. Show that $\mathcal{R}(A) \perp \mathcal{N}(A^\top)$.</h4>
        <p><b>Solution:</b> Let $v \in \mathcal{R}(A)$ and $y \in \mathcal{N}(A^\top)$. By definition, $v = Ax$ for some vector $x$, and $A^\top y = 0$. We want to show their inner product is zero:
        $$ v^\top y = (Ax)^\top y = x^\top A^\top y = x^\top (A^\top y) = x^\top 0 = 0 $$
        Since this holds for any $v \in \mathcal{R}(A)$ and $y \in \mathcal{N}(A^\top)$, the subspaces are orthogonal.</p>
      </div>

      <div class="proof-enhanced">
        <h4>A3. Show that $Q^\top Q = I$ implies $\|Qx\|_2 = \|x\|_2$.</h4>
        <p><b>Solution:</b> We start with the squared norm:
        $$ \|Qx\|_2^2 = (Qx)^\top(Qx) $$
        Using the property $(AB)^\top = B^\top A^\top$, this becomes:
        $$ \|Qx\|_2^2 = x^\top Q^\top Q x $$
        Since we are given that $Q^\top Q = I$, we substitute this in:
        $$ \|Qx\|_2^2 = x^\top I x = x^\top x = \|x\|_2^2 $$
        Taking the square root of both sides (and since norms are non-negative), we get $\|Qx\|_2 = \|x\|_2$. This shows that orthogonal transformations preserve the Euclidean norm (length) of vectors.</p>
      </div>

      <div class="proof-enhanced">
        <h4>A4. (New) Prove that if a set of vectors $\{v_1, \dots, v_k\}$ is orthogonal, it must be linearly independent.</h4>
        <p><b>Solution:</b> Consider the equation $c_1 v_1 + \dots + c_k v_k = 0$. To show linear independence, we must show that all coefficients $c_i$ must be zero.
        Take the inner product of both sides with any vector $v_j$ from the set:
        $$ \langle v_j, c_1 v_1 + \dots + c_k v_k \rangle = \langle v_j, 0 \rangle = 0 $$
        By linearity of the inner product:
        $$ c_1 \langle v_j, v_1 \rangle + \dots + c_j \langle v_j, v_j \rangle + \dots + c_k \langle v_j, v_k \rangle = 0 $$
        Since the set is orthogonal, $\langle v_j, v_i \rangle = 0$ for $i \neq j$. The equation simplifies to:
        $$ c_j \langle v_j, v_j \rangle = c_j \|v_j\|_2^2 = 0 $$
        Since the vectors are non-zero, $\|v_j\|_2^2 > 0$, which implies $c_j = 0$. Since this holds for any $j=1, \dots, k$, all coefficients are zero, and the set is linearly independent.
        </p>
      </div>

      <h3>B. Projections</h3>

      <div class="proof-enhanced">
        <h4>B1. Let $S = \mathrm{span}\{u, v\}$ with $u, v$ independent. Derive the projector $P$ onto $S$.</h4>
        <p><b>Solution:</b> Form $A = [u \ v]$. If $A$ has full column rank, $P = A(A^\top A)^{-1}A^\top$.</p>
      </div>

      <div class="proof-enhanced">
        <h4>B2. Show that if $P$ is symmetric and idempotent, then it is an orthogonal projector.</h4>
        <p><b>Solution:</b> For any $b$, set $p = Pb$ and $r = b - Pb$. Then $Pr = 0$ (because $P^2 = P$); thus $r \in \mathcal{N}(P)$. For any $y = Pw \in \mathcal{R}(P)$, $y^\top r = w^\top P^\top(b - Pb) = w^\top P(b - Pb) = w^\top(Pb - P^2b) = 0$, so $r \perp \mathcal{R}(P)$. Hence $p$ is the orthogonal projection.</p>
      </div>

      <div class="proof-enhanced">
        <h4>B3. Project $b = (1, 2, 3)^\top$ onto the affine set $\{x \mid [1 \ 1 \ 1]x = 3\}$.</h4>
        <p><b>Solution:</b> One solution: find $x_0 = (1, 1, 1)^\top$ (satisfies the constraint). Compute $b - x_0 = (0, 1, 2)^\top$ and project onto $\mathcal{N}([1 \ 1 \ 1])$, i.e., vectors summing to zero. Using basis $z_1 = (1, -1, 0)$, $z_2 = (1, 0, -1)$, solve $\min_y \|Zy - (b - x_0)\|_2^2$, with $Z = [z_1 \ z_2]$. Recover $x^\star = x_0 + Zy^\star$.</p>
      </div>

      <h3>C. Least Squares</h3>

      <div class="proof-enhanced">
        <h4>C1. Derive normal equations and prove uniqueness iff $\mathrm{rank}(A) = n$.</h4>
        <p><b>Solution:</b> The objective function is $f(x) = \|Ax-b\|_2^2 = x^\top A^\top A x - 2b^\top A x + b^\top b$. This is a quadratic function of $x$. To find the minimum, we take the gradient with respect to $x$ and set it to zero:
        $$ \nabla_x f(x) = 2A^\top A x - 2A^\top b = 0 \implies A^\top A x = A^\top b $$
        The solution is unique if and only if the matrix $A^\top A$ is invertible. This is true if and only if $A$ has full column rank, i.e., $\mathrm{rank}(A) = n$.
        </p>
      </div>

      <div class="proof-enhanced">
        <h4>C2. Show that the residual at the LS solution is orthogonal to each column of $A$.</h4>
        <p><b>Solution:</b> Columns of $A$ span $\mathcal{R}(A)$. Orthogonality condition $A^\top(b - Ax^\star) = 0$ means $r^\star \perp \mathcal{R}(A)$.</p>
      </div>

      <div class="proof-enhanced">
        <h4>C3. Solve a small overdetermined system by (i) normal equations, (ii) QR, (iii) SVD, and compare answers.</h4>
        <p><b>Solution:</b> Pick any $3 \times 2$ with independent columns; show all three methods agree to numerical precision. Discuss time and conditioning.</p>
      </div>

      <div class="proof-enhanced">
        <h4>C4. (New) Show that the projection matrix $P = A(A^\top A)^{-1}A^\top$ is idempotent ($P^2=P$) and symmetric ($P^\top = P$).</h4>
        <p><b>Solution:</b>
        <br><b>Symmetry:</b>
        $$ P^\top = (A(A^\top A)^{-1}A^\top)^\top = (A^\top)^\top ((A^\top A)^{-1})^\top A^\top = A ((A^\top A)^\top)^{-1} A^\top = A (A^\top (A^\top)^\top)^{-1} A^\top = A(A^\top A)^{-1}A^\top = P $$
        <br><b>Idempotency:</b>
        $$ P^2 = (A(A^\top A)^{-1}A^\top)(A(A^\top A)^{-1}A^\top) = A(A^\top A)^{-1}(A^\top A)(A^\top A)^{-1}A^\top = A(A^\top A)^{-1} I A^\top = A(A^\top A)^{-1}A^\top = P $$
        </p>
      </div>

      <h3>D. Pseudoinverse & Rank Deficiency</h3>

      <div class="proof-enhanced">
        <h4>D1. Prove that every least-squares solution $x^\star$ satisfies $x^\star = A^+b + (I - A^+A)z$ for some $z$.</h4>
        <p><b>Solution:</b> From SVD, the set of minimizers is $A^+b + \mathcal{N}(A)$; note $(I - A^+A)$ projects onto $\mathcal{N}(A)$.</p>
      </div>

      <div class="proof-enhanced">
        <h4>D2. Show $P = AA^+$ is the projector onto $\mathcal{R}(A)$ and $P^\perp = I - AA^+$ projects onto $\mathcal{N}(A^\top)$.</h4>
        <p><b>Solution:</b> Use SVD or basic projector algebra: $P^2 = P$, $P^\top = P$, range/nullspace relations.</p>
      </div>
    </section>

    <!-- SECTION 12: SANITY CHECKLIST -->
    <section class="card-v2" id="section-12">
      <h2>12. Sanity Checklist (What to Remember in Practice)</h2>

      <ul>
        <li>Always check the <b>dimensions</b> of your matrices and vectors.</li>
        <li>For least squares, check if your matrix has <b>full column rank</b> to determine if the solution is unique.</li>
        <li>Prefer <b>QR</b> or <b>SVD</b> over the normal equations for better numerical stability.</li>
        <li>If your results are unstable, check the <b>condition number</b> of your matrix and consider preprocessing your data.</li>
        <li>Remember the geometric interpretation: the least squares solution is a <b>projection</b>.</li>
      </ul>

    </section>

    <!-- SECTION 13: QUICK REFERENCE -->
    <section class="card-v2" id="section-13">
      <h2>13. Key Formulas</h2>

      <ul>
        <li><b>Normal Equations:</b> $A^\top A x^\star = A^\top b$</li>
        <li><b>Projection onto $\mathcal{R}(A)$:</b> $P = A(A^\top A)^{-1}A^\top$</li>
        <li><b>QR Solution:</b> $Rx^\star = Q^\top b$</li>
        <li><b>Pseudoinverse:</b> $A^+ = V\Sigma^+ U^\top$</li>
        <li><b>Condition Number:</b> $\kappa(A) = \sigma_{\max} / \sigma_{\min}$</li>
      </ul>
    </section>

    <!-- SECTION 14: DUAL NORMS AND HÖLDER'S INEQUALITY (EXPANDED) -->
    <section class="card-v2" id="section-14">
      <h2>14. Dual Norms and Hölder's Inequality</h2>

      <h3>Definition: Dual Norm</h3>
      <p>For any norm $\|\cdot\|$ on $\mathbb{R}^n$, its <b>dual norm</b> is defined as:</p>
      $$
      \|y\|_* := \sup\{ x^\top y \mid \|x\| \le 1 \}
      $$
      <p>The dual norm measures the maximum linear functional $y$ can induce over the unit ball of the primal norm.</p>

      <h3>Examples to Memorize</h3>
      <ul>
        <li><b>$\ell_2$ is self-dual:</b> $(\|\cdot\|_2)^* = \|\cdot\|_2$</li>
        <li><b>$\ell_1$ and $\ell_\infty$ are duals:</b> $(\|\cdot\|_1)^* = \|\cdot\|_\infty$ and $(\|\cdot\|_\infty)^* = \|\cdot\|_1$</li>
        <li><b>$\ell_p$ and $\ell_q$ are conjugate:</b> For $1/p + 1/q = 1$, we have $(\|\cdot\|_p)^* = \|\cdot\|_q$</li>
      </ul>

      <div class="proof-enhanced">
        <h4>Proof: $(\|\cdot\|_2)^* = \|\cdot\|_2$ (Self-Duality of Euclidean Norm)</h4>

        <div class="proof-step">
          <strong>Step 1: Upper bound.</strong> By Cauchy-Schwarz, for $\|x\|_2 \le 1$:
          $$ x^\top y \le |x^\top y| \le \|x\|_2 \|y\|_2 \le \|y\|_2 $$
          Thus $\|y\|_* \le \|y\|_2$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Achieve equality.</strong> Choose $x = y/\|y\|_2$ (assuming $y \neq 0$). Then $\|x\|_2 = 1$ and:
          $$ x^\top y = \frac{y^\top y}{\|y\|_2} = \frac{\|y\|_2^2}{\|y\|_2} = \|y\|_2 $$
          Hence $\|y\|_* \ge \|y\|_2$.
        </div>

        <div class="proof-step">
          <strong>Conclusion:</strong> Therefore $\|y\|_* = \|y\|_2$.
        </div>
      </div>

      <div class="proof-enhanced">
        <h4>Proof: $(\|\cdot\|_1)^* = \|\cdot\|_\infty$</h4>

        <div class="proof-step">
          <strong>Step 1: Upper bound.</strong> For $\|x\|_1 \le 1$:
          $$ x^\top y = \sum_i x_i y_i \le \sum_i |x_i| |y_i| \le \max_i |y_i| \sum_i |x_i| \le \|y\|_\infty $$
        </div>

        <div class="proof-step">
          <strong>Step 2: Achieve equality.</strong> Let $j = \arg\max_i |y_i|$. Choose $x = \mathrm{sign}(y_j) e_j$. Then $\|x\|_1 = 1$ and:
          $$ x^\top y = \mathrm{sign}(y_j) y_j = |y_j| = \|y\|_\infty $$
        </div>

        <div class="proof-step">
          <strong>Conclusion:</strong> $\|y\|_* = \|y\|_\infty$.
        </div>
      </div>

      <h3>Hölder's Inequality (Dual-Norm Cauchy–Schwarz)</h3>
      <p>For all $x, y \in \mathbb{R}^n$ and any norm $\|\cdot\|$:</p>
      $$
      x^\top y \le \|x\| \cdot \|y\|_*
      $$

      <div class="proof-enhanced">
        <h4>Proof of Hölder's Inequality</h4>

        <div class="proof-step">
          <strong>Case 1: $x = 0$.</strong> The inequality is trivially true: $0 \le 0$.
        </div>

        <div class="proof-step">
          <strong>Case 2: $x \neq 0$.</strong> Write:
          $$ x^\top y = \|x\| \cdot \frac{x^\top y}{\|x\|} = \|x\| \cdot \left(\frac{x}{\|x\|}\right)^\top y $$
          Note that $\left\|\frac{x}{\|x\|}\right\| = 1$, so by the definition of dual norm:
          $$ \left(\frac{x}{\|x\|}\right)^\top y \le \sup_{\|z\| \le 1} z^\top y = \|y\|_* $$
          Combining: $x^\top y \le \|x\| \cdot \|y\|_*$.
        </div>
      </div>

      <h3>Why We Need This Later</h3>
      <p>In <a href="../02-convex-sets/index.html">Lecture 02</a>, when computing <b>dual cones</b>, you will repeatedly use "$y^\top x \le \|y\|_* \|x\|$" with appropriate norms. This inequality is the workhorse of duality theory.</p>
    </section>

    <!-- SECTION 15: MATRIX STRUCTURES FOR OPTIMIZATION -->
    <section class="card-v2" id="section-15">
      <h2>15. Matrix Inner Product, Frobenius Norm, and Loewner Order</h2>

      <h3>Trace and Frobenius Inner Product</h3>
      <p>For matrices $X, Y \in \mathbb{R}^{m \times n}$, define:</p>
      $$
      \langle X, Y \rangle := \mathrm{tr}(X^\top Y) = \sum_{i,j} X_{ij} Y_{ij}
      $$
      <p>This is an <b>inner product</b> on the space of $m \times n$ matrices (bilinear, symmetric, positive definite).</p>

      <h3>Frobenius Norm</h3>
      <p>The induced norm is the <b>Frobenius norm</b>:</p>
      $$
      \|X\|_F := \sqrt{\langle X, X \rangle} = \sqrt{\sum_{i,j} X_{ij}^2} = \sqrt{\mathrm{tr}(X^\top X)}
      $$

      <h3>Symmetric Matrices</h3>
      <p>Denote the space of $n \times n$ symmetric matrices by:</p>
      $$
      \mathbb{S}^n = \{ X \in \mathbb{R}^{n \times n} \mid X = X^\top \}
      $$

      <h3>Loewner Order (Semidefinite Order)</h3>
      <p>For $X, Y \in \mathbb{S}^n$, we write <b>$X \succeq Y$</b> if and only if $X - Y$ is <b>positive semidefinite (PSD)</b>, meaning:</p>
      $$
      v^\top (X - Y) v \ge 0 \quad \text{for all } v \in \mathbb{R}^n
      $$

      <div class="proof-enhanced">
        <h4>Microfacts About the Loewner Order</h4>

        <ul>
          <li><b>Eigenvalue characterization:</b> $X \succeq 0$ if and only if all eigenvalues of $X$ are nonnegative.</li>
          <li><b>Inner product property:</b> If $X \succeq 0$ and $Y \succeq 0$, then:
            $$ \langle X, Y \rangle = \mathrm{tr}(XY) \ge 0 $$
            (This follows because $XY$ has nonnegative trace when both are PSD.)
          </li>
          <li><b>Partial order:</b> The relation $\succeq$ is reflexive, transitive, and antisymmetric (hence a partial order).</li>
        </ul>
      </div>

      <h3>Connection to Optimization</h3>
      <p>The Loewner order is the language of:</p>
      <ul>
        <li><b>Ellipsoids</b> (defined by PSD matrices in quadratic forms)</li>
        <li><b>PSD cone</b> $\mathbb{S}^n_+ = \{X \in \mathbb{S}^n \mid X \succeq 0\}$ in <a href="../02-convex-sets/index.html">Lecture 02</a></li>
        <li><b>Semidefinite programs (SDPs)</b> in <a href="../04-convex-opt-problems/index.html">Lecture 04</a></li>
      </ul>
    </section>

    <!-- SECTION 16: COMPONENTWISE ORDER AND PROJECTION -->
    <section class="card-v2" id="section-16">
      <h2>16. Componentwise Order and Projection onto Affine Sets</h2>

      <h3>Componentwise Order</h3>
      <p>For vectors $x, y \in \mathbb{R}^n$, we write:</p>
      <ul>
        <li><b>$x \ge 0$</b> means $x_i \ge 0$ for all $i = 1, \dots, n$</li>
        <li><b>$x \le y$</b> means $x_i \le y_i$ for all $i = 1, \dots, n$</li>
        <li><b>$x > 0$</b> means $x_i > 0$ for all $i$ (strictly positive)</li>
      </ul>
      <p>This is the <b>standard partial order</b> on $\mathbb{R}^n$ induced by the nonnegative orthant $\mathbb{R}^n_+ = \{x \mid x \ge 0\}$.</p>

      <h3>Projection onto an Affine Set</h3>
      <p>Consider an affine set defined by:</p>
      $$
      \mathcal{A} = \{ x \in \mathbb{R}^n \mid Fx = g \}
      $$
      <p>where $F \in \mathbb{R}^{p \times n}$ has full row rank and $g \in \mathbb{R}^p$.</p>

      <h4>Parametric Representation</h4>
      <p>Every point in $\mathcal{A}$ can be written as:</p>
      $$
      x = x_0 + Zt
      $$
      <p>where:</p>
      <ul>
        <li>$x_0$ is any particular solution satisfying $Fx_0 = g$</li>
        <li>$Z$ is a matrix whose columns form a basis for $\mathcal{N}(F)$ (the nullspace of $F$)</li>
        <li>$t \in \mathbb{R}^k$ where $k = n - p$ (dimension of nullspace)</li>
      </ul>

      <h4>Euclidean Projection Formula</h4>
      <p>The <b>Euclidean projection</b> of any point $y \in \mathbb{R}^n$ onto $\mathcal{A}$ is:</p>
      $$
      \Pi_{\mathcal{A}}(y) = x_0 + Z(Z^\top Z)^{-1} Z^\top (y - x_0)
      $$

      <div class="proof-enhanced">
        <h4>Geometric Interpretation</h4>
        <p>This formula says: "Project $(y - x_0)$ onto the nullspace $\mathcal{N}(F)$, then shift back by $x_0$."</p>

        <div class="proof-step">
          <strong>Step 1: Translate to subspace problem.</strong> Shifting by $x_0$ converts the affine set to a subspace (the nullspace of $F$).
        </div>

        <div class="proof-step">
          <strong>Step 2: Project onto nullspace.</strong> The projection of $w = y - x_0$ onto $\mathrm{span}(Z)$ is:
          $$ \Pi_{\mathcal{N}(F)}(w) = Z(Z^\top Z)^{-1} Z^\top w $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Shift back.</strong> Add $x_0$ to obtain the projection onto the affine set.
        </div>
      </div>

      <h3>Why This Matters</h3>
      <p>Projection onto affine sets is crucial for:</p>
      <ul>
        <li><b>Equality-constrained optimization</b> (nullspace methods)</li>
        <li><b>Feasible point computation</b> in constrained problems</li>
        <li><b>Active-set methods</b> in quadratic programming</li>
      </ul>
    </section>

    <!-- SECTION 17: PRACTICE PROBLEMS -->
    <section class="card-v2" id="section-17">
      <h2>17. Additional Practice Problems</h2>

      <h3>Problem 1: Dual of $\ell_p$ Norm</h3>
      <p><b>Problem:</b> Show that for $1 < p < \infty$, the dual norm of $\|\cdot\|_p$ is $\|\cdot\|_q$ where $\frac{1}{p} + \frac{1}{q} = 1$. Handle the boundary cases $p = 1 \Rightarrow q = \infty$ and $p = \infty \Rightarrow q = 1$.</p>

      <div class="proof-enhanced">
        <h4>Solution Sketch</h4>

        <div class="proof-step">
          <strong>General case ($1 < p < \infty$):</strong>
          <p>By Hölder's inequality for $\ell_p$ spaces:</p>
          $$ \left| \sum_i x_i y_i \right| \le \left(\sum_i |x_i|^p\right)^{1/p} \left(\sum_i |y_i|^q\right)^{1/q} = \|x\|_p \|y\|_q $$
          <p>Equality is achieved when $|y_i|^q \propto |x_i|^p$. Specifically, choose:</p>
          $$ x_i = \frac{\mathrm{sign}(y_i) |y_i|^{q-1}}{\|y\|_q^{q-1}} $$
          <p>Then $\|x\|_p = 1$ and $x^\top y = \|y\|_q$, proving $(\|\cdot\|_p)^* = \|\cdot\|_q$.</p>
        </div>

        <div class="proof-step">
          <strong>Boundary cases:</strong> Already proven above: $(\|\cdot\|_1)^* = \|\cdot\|_\infty$ and $(\|\cdot\|_\infty)^* = \|\cdot\|_1$.
        </div>
      </div>

      <h3>Problem 2: Frobenius Cauchy–Schwarz</h3>
      <p><b>Problem:</b> Prove that for matrices $X, Y \in \mathbb{R}^{m \times n}$:</p>
      $$
      |\langle X, Y \rangle| \le \|X\|_F \|Y\|_F
      $$

      <div class="proof-enhanced">
        <h4>Solution</h4>
        <p>View matrices as vectors in $\mathbb{R}^{mn}$ by stacking columns. The Frobenius inner product corresponds to the standard Euclidean inner product on $\mathbb{R}^{mn}$, and the Frobenius norm corresponds to the Euclidean norm. Therefore, the standard Cauchy-Schwarz inequality applies directly.</p>
      </div>

      <h3>Problem 3: Loewner Order Transitivity</h3>
      <p><b>Problem:</b> If $X \succeq Y$ and $Y \succeq Z$ (all in $\mathbb{S}^n$), prove that $X \succeq Z$.</p>

      <div class="proof-enhanced">
        <h4>Solution</h4>
        <p>We have $X - Y \succeq 0$ and $Y - Z \succeq 0$. Therefore:</p>
        $$ X - Z = (X - Y) + (Y - Z) $$
        <p>Since the sum of two PSD matrices is PSD (each has nonnegative eigenvalues in their respective quadratic forms), we conclude $X - Z \succeq 0$, i.e., $X \succeq Z$.</p>
      </div>

      <h3>Problem 4: Projection onto Affine Set Example</h3>
      <p><b>Problem:</b> Find the Euclidean projection of $y = (1, 2, 3)^\top$ onto the affine set:</p>
      $$
      \mathcal{A} = \{ x \in \mathbb{R}^3 \mid x_1 + x_2 + x_3 = 1 \}
      $$

      <div class="proof-enhanced">
        <h4>Solution</h4>

        <div class="proof-step">
          <strong>Step 1: Particular solution.</strong> Choose $x_0 = (1, 0, 0)^\top$ (satisfies $x_1 + x_2 + x_3 = 1$).
        </div>

        <div class="proof-step">
          <strong>Step 2: Nullspace basis.</strong> $F = [1\ 1\ 1]$. A basis for the nullspace is given by any two linearly independent vectors whose components sum to zero. For example:
          $$ Z = \begin{bmatrix} 1 & 1 \\ -1 & 0 \\ 0 & -1 \end{bmatrix} $$
          (These are two linearly independent vectors orthogonal to $[1, 1, 1]^\top$.)
        </div>

        <div class="proof-step">
          <strong>Step 3: Alternative approach (direct formula).</strong> The projection onto the hyperplane $\{x \mid a^\top x = b\}$ is:
          $$ \Pi(y) = y - \frac{a^\top y - b}{\|a\|_2^2} a $$
          Here $a = (1, 1, 1)^\top$, $b = 1$, $a^\top y = 6$, $\|a\|_2^2 = 3$:
          $$ \Pi(y) = (1, 2, 3)^\top - \frac{6 - 1}{3}(1, 1, 1)^\top = (1, 2, 3)^\top - \frac{5}{3}(1, 1, 1)^\top $$
          $$ = \left(-\frac{2}{3}, \frac{1}{3}, \frac{4}{3}\right)^\top $$
        </div>

        <div class="proof-step">
          <strong>Verification:</strong> $-\frac{2}{3} + \frac{1}{3} + \frac{4}{3} = 1$ ✓
        </div>
      </div>
    </section>

    <!-- READINGS -->
    <section class="card-v2" id="readings">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required Reading:</strong> Boyd & Vandenberghe, <em>Convex Optimization</em>, Appendix A.</li>
        <li><strong>Video Resource:</strong> Gilbert Strang's "The Fundamental Theorem of Linear Algebra" (MIT OpenCourseWare)</li>
        <li><strong>Additional Reference:</strong> Trefethen & Bau, <em>Numerical Linear Algebra</em> (for QR, SVD, and conditioning)</li>
      </ul>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/theme-switcher.js"></script>
<script src="../../static/js/toc.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initNormGeometryVisualizer } from './widgets/js/norm-geometry-visualizer.js';
    initNormGeometryVisualizer('widget-norm-geometry');
  </script>
  <script type="module">
    import { initOrthogonality } from './widgets/js/orthogonality.js';
    initOrthogonality('widget-orthogonality');
  </script>
  <script type="module">
    import { initRankNullspace } from './widgets/js/rank-nullspace.js';
    initRankNullspace('widget-rank-nullspace');
  </script>
  <script type="module">
    import { initMatrixGeometry } from './widgets/js/matrix-geometry.js';
    initMatrixGeometry('widget-matrix-geometry');
  </script>
  <script type="module">
    import { initSvdApproximator } from './widgets/js/svd-approximator.js';
    initSvdApproximator('widget-svd-approximator');
  </script>
  <script type="module">
    import { initHessianLandscapeVisualizer } from './widgets/js/hessian-landscape-visualizer.js';
    initHessianLandscapeVisualizer('widget-hessian-landscape');
  </script>
  <script type="module">
    import { initConditionNumber } from './widgets/js/condition-number.js';
    initConditionNumber('widget-condition-number');
  </script>
</body>
</html>
