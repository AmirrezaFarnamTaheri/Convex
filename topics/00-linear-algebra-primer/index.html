<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>00. Linear Algebra Primer — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/styles.css" />
  <link rel="stylesheet" href="/static/css/modern-widgets.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
</head>
<body>
  <header class="site-header">
    <div class="container header-inner">
      <div class="brand">
        <img src="../../static/assets/branding/logo.svg" class="logo" alt="logo" />
        <a href="../../index.html" style="text-decoration:none;color:inherit">
          <strong>Convex Optimization</strong>
        </a>
      </div>
      <nav class="nav">
        <a href="../../index.html#sessions">All Lectures</a>
        <a href="#readings">Readings</a>
      </nav>
    </div>
  </header>

  <main class="container" style="padding: 32px 0 60px;">
    <article class="card" style="margin-bottom: 32px;">
      <h1 style="margin-top: 0;">00. A Zero-to-Hero Primer on Linear Algebra</h1>
      <div class="meta">
        Date: 2025-10-14 · Duration: 90 min · Tags: prerequisites, review, linear-algebra, foundational
      </div>
      <section style="margin-top: 16px;">
        <p><strong>Overview:</strong> Welcome to the starting point of your journey into convex optimization. This lecture provides a rigorous, from-scratch review of the essential linear algebra concepts that form the bedrock of the entire field. We will build the theory step-by-step, starting from fundamental objects like vectors and matrices, and moving through the crucial concepts of inner products, norms, orthogonality, projections, and the geometry of linear systems. We will pay special attention to the ideas that are most critical for optimization, including the structure of the four fundamental subspaces, the properties of positive semidefinite matrices, and robust methods for solving linear systems like the QR and Singular Value Decomposition (SVD). This module is designed to be a comprehensive, self-contained resource for newcomers, but it is also thorough enough to serve as a valuable reference for those with prior experience.</p>
        <p><strong>Prerequisites:</strong> None. This is where the journey begins.</p>
      </section>
    </article>

    <section class="card" style="margin-bottom: 32px;">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you will have a deep, practical, and theoretical understanding of the linear algebra required for this course. You will be able to:</p>
      <ul style="line-height: 1.8;">
        <li><b>Master Fundamental Objects and Subspaces:</b> Understand vectors, matrices, and the four fundamental subspaces (column space, nullspace, row space, left nullspace), and be able to prove and apply the Rank-Nullity Theorem.</li>
        <li><b>Work with Inner Products and Norms:</b> Define and use the inner product, understand its geometric interpretation in terms of angles, and work with the three canonical vector norms ($\ell_1, \ell_2, \ell_\infty$). You will be able to prove the Cauchy-Schwarz inequality from first principles.</li>
        <li><b>Explain Orthogonality and Projections:</b> Understand the concepts of orthogonality, orthonormal bases, and orthogonal projections. You will be able to apply the Gram-Schmidt process to construct an orthonormal basis.</li>
        <li><b>Recognize and Analyze Positive Semidefinite (PSD) Matrices:</b> Define PSD matrices, connect them to quadratic forms and ellipsoids, and prove the eigenvalue characterization of definiteness.</li>
        <li><b>Select Appropriate Numerical Methods for Linear Systems:</b> Understand the geometric and numerical properties of the least squares problem and its solution via the normal equations. You will also understand the QR and Singular Value Decompositions and know when to use them for robust solutions.</li>
      </ul>
    </section>

    <section class="card" style="margin-bottom: 32px;">
      <h2>1. The Four Fundamental Subspaces</h2>
      <p>For any matrix $A \in \mathbb{R}^{m \times n}$, we have four fundamental subspaces. Understanding their relationships is key to unlocking the geometry of linear algebra.</p>
      <ul>
        <li><b>Column Space (Range):</b> $\mathcal{R}(A) = \{Ax \mid x \in \mathbb{R}^n\} \subseteq \mathbb{R}^m$. This is the set of all possible outputs of the linear transformation represented by $A$. It is the span of the columns of $A$.</li>
        <li><b>Nullspace (Kernel):</b> $\mathcal{N}(A) = \{x \in \mathbb{R}^n \mid Ax=0\}$. This is the set of all input vectors that are mapped to the zero vector.</li>
        <li><b>Row Space:</b> $\mathcal{R}(A^\top) \subseteq \mathbb{R}^n$. This is the span of the rows of $A$.</li>
        <li><b>Left Nullspace:</b> $\mathcal{N}(A^\top) = \{y \in \mathbb{R}^m \mid A^\top y=0\}$.</li>
      </ul>
      <p>These spaces satisfy two crucial orthogonality conditions: $\mathcal{R}(A) \perp \mathcal{N}(A^\top)$ and $\mathcal{R}(A^\top) \perp \mathcal{N}(A)$. The dimensions of these spaces are related by one of the most important theorems in linear algebra.</p>

      <h4>The Rank-Nullity Theorem</h4>
      <p>The <b>rank</b> of a matrix $A$, denoted $\mathrm{rank}(A)$, is the dimension of its column space. The <b>nullity</b> of $A$ is the dimension of its nullspace. The Rank-Nullity Theorem states that for any $m \times n$ matrix $A$:
      $$ \mathrm{rank}(A) + \dim(\mathcal{N}(A)) = n $$
      This means that the number of dimensions "lost" by the transformation (the nullity) plus the number of dimensions "preserved" (the rank) must equal the total number of dimensions of the input space.</p>

      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Rank & Nullspace</h3>
        <p>This widget allows you to define a 2x3 or 3x2 matrix and visualizes the four fundamental subspaces. Experiment with different matrices (e.g., rank-deficient ones) to see how the dimensions of the subspaces change and to confirm the Rank-Nullity Theorem in action. This provides a direct geometric interpretation of the abstract theorem.</p>
        <div id="widget-rank-nullspace" class="widget-container" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <section class="card" style="margin-bottom: 32px;">
      <h2>2. Inner Products, Norms, and Angles</h2>
      <p>The <b>standard inner product</b> (or dot product) on $\mathbb{R}^n$ is defined as $\langle x, y \rangle = x^\top y = \sum_{i=1}^n x_i y_i$. It is a bilinear, symmetric, and positive-definite function. A <b>norm</b> $\|\cdot\|$ is a function that assigns a strictly positive length or size to each vector. The three most common vector norms are:</p>
      <ul>
        <li><b>$\ell_2$ (Euclidean):</b> $\|x\|_2 = \sqrt{\sum_i x_i^2} = \sqrt{x^\top x}$.</li>
        <li><b>$\ell_1$ (Manhattan):</b> $\|x\|_1 = \sum_i |x_i|$.</li>
        <li><b>$\ell_\infty$ (Chebyshev):</b> $\|x\|_\infty = \max_i |x_i|$.</li>
      </ul>

      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Interactive Visualizer: The Geometry of Norms</h3>
        <p>The choice of norm has a significant geometric consequence. This widget displays the "unit ball"—the set of all vectors $\{x \mid \|x\| \le 1\}$—for the $\ell_1$, $\ell_2$, and $\ell_\infty$ norms. Notice the "pointy" corners of the $\ell_1$ ball; this geometric feature is the reason $\ell_1$ regularization promotes sparse solutions in machine learning, a topic we will explore later.</p>
        <div id="widget-norm-geometry" class="widget-container" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h4>The Cauchy-Schwarz Inequality</h4>
      <p>One of the most fundamental inequalities in mathematics, it states that for any vectors $x, y \in \mathbb{R}^n$:
      $ |\langle x, y \rangle| \le \|x\|_2 \|y\|_2 $.
      Equality holds if and only if $x$ and $y$ are linearly dependent. We can use this to define the angle between two vectors: $\cos\theta = \frac{\langle x, y \rangle}{\|x\|_2\|y\|_2}$.</p>
    </section>

    <section class="card" style="margin-bottom: 32px;">
      <h2>3. Orthogonality, Projections, and QR Decomposition</h2>
      <p>A set of vectors $\{q_1, \dots, q_k\}$ is <b>orthonormal</b> if $q_i^\top q_j = \delta_{ij}$. The <b>Gram-Schmidt</b> process is an algorithm that takes a set of linearly independent vectors and produces an orthonormal set that spans the same subspace. The <b>QR decomposition</b> factors a matrix $A$ into $A=QR$, where $Q$ has orthonormal columns and $R$ is upper triangular. This is a key tool for solving linear systems robustly.</p>

      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Interactive Explorer: Orthogonality & Projections</h3>
        <p>This tool allows you to drag two vectors in a 2D plane. It updates in real-time to show their dot product, the angle between them, and the orthogonal projection of one onto the other. This provides a direct, hands-on feel for these core concepts.</p>
        <div id="widget-orthogonality" class="widget-container" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h4>The Least Squares Problem</h4>
      <p>The least squares problem seeks to find an $x$ that minimizes $\|Ax-b\|_2^2$. This is one of the most common optimization problems. The solution $x^\star$ must satisfy the <b>normal equations</b>: $A^\top A x^\star = A^\top b$. Geometrically, the solution $Ax^\star$ is the orthogonal projection of the vector $b$ onto the column space of $A$. While the normal equations give a direct analytical solution, forming the matrix $A^\top A$ can be numerically unstable if the columns of $A$ are nearly linearly dependent. In such cases, solving the system via QR decomposition is a much more robust method.</p>
    </section>

    <section class="card" style="margin-bottom: 32px;">
      <h2>4. Positive Semidefinite Matrices and Quadratic Forms</h2>
      <p>A symmetric matrix $Q \in \mathbb{S}^n$ is:</p>
      <ul>
        <li><b>Positive Semidefinite (PSD), $Q \succeq 0$</b>, if the quadratic form $x^\top Q x \ge 0$ for all $x \in \mathbb{R}^n$.</li>
        <li><b>Positive Definite (PD), $Q \succ 0$</b>, if $x^\top Q x > 0$ for all non-zero $x \in \mathbb{R}^n$.</li>
      </ul>
      <p>The expression $x^\top Q x$ is a <b>quadratic form</b>, and its properties are central to convex optimization. A function $f(x) = \frac{1}{2}x^\top Q x + c^\top x + d$ is convex if and only if its Hessian, $Q$, is PSD.</p>

      <h4>Eigenvalue Characterization of Definiteness</h4>
      <p><b>Theorem:</b> A symmetric matrix $Q$ is PSD if and only if all its eigenvalues are non-negative. It is PD if and only if all its eigenvalues are positive.</p>

      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Interactive Explorer: Eigenvalues and PSD Matrices</h3>
        <p>This tool for 2x2 matrices shows the geometric interpretation of eigenvalues/eigenvectors and visualizes the quadratic form $x^\top Q x$. Modify the matrix entries and observe how the eigenvalues change. The color of the 3D surface plot indicates whether the matrix is positive definite (a convex bowl), negative definite (a concave bowl), or indefinite (a saddle), directly linking the signs of the eigenvalues to the curvature of the function.</p>
        <div id="widget-eigen-psd" class="widget-container" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <section class="card" style="margin-bottom: 32px;">
      <h2>5. The Singular Value Decomposition (SVD)</h2>
      <p>The <b>Singular Value Decomposition (SVD)</b> is one of the most important matrix factorizations. It asserts that any matrix $A \in \mathbb{R}^{m \times n}$ can be decomposed as $A = U\Sigma V^\top$, where $U$ and $V$ are orthogonal matrices and $\Sigma$ is a diagonal matrix with non-negative singular values. Geometrically, the SVD states that any linear transformation can be decomposed into a rotation ($V^\top$), a scaling along coordinate axes ($\Sigma$), and another rotation ($U$).</p>

      <p>The SVD is a powerful tool for analyzing matrices, solving least squares problems robustly, and performing low-rank approximation. The <b>pseudoinverse</b> $A^+ = V\Sigma^+ U^\top$, where $\Sigma^+$ is formed by taking the reciprocal of the non-zero singular values, gives the minimum-norm solution to the least squares problem: $x^\star = A^+ b$.</p>

      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Interactive Demo: SVD for Image Compression</h3>
        <p>See the power of low-rank approximation in action. This widget loads an image and allows you to reconstruct it using a varying number of singular values. You will see that a surprisingly accurate approximation can be achieved with only a small fraction of the singular values, demonstrating how the SVD captures the most important "energy" of the matrix.</p>
        <div id="widget-svd-approximator" class="widget-container" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <section class="card" id="readings" style="margin-bottom: 32px;">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required Reading:</strong> Boyd & Vandenberghe, <em>Convex Optimization</em>, Appendix A.</li>
        <li><strong>Video Resource:</strong> Gilbert Strang's "The Fundamental Theorem of Linear Algebra" (MIT OpenCourseWare) provides a deep and intuitive explanation of the four subspaces.</li>
      </ul>
    </section>

    <section class="card" style="margin-bottom: 32px;">
      <h2>Exercises</h2>
      <p>Test your understanding of the core concepts from this lecture.</p>
      <ol style="line-height: 2;">
        <li><b>Eigenvalues of Symmetric Matrices:</b> Prove that the eigenvalues of a real symmetric matrix are always real. (Hint: Consider the expression $\bar{x}^\top A x$ for a complex eigenvector $x$).</li>
        <li><b>PSD Property of $A^\top A$:</b> Show that the matrix $A^\top A$ is always positive semidefinite for any matrix $A$. Under what condition is it positive definite?</li>
        <li><b>Gram-Schmidt by Hand:</b> Apply the Gram-Schmidt process to the vectors $a_1=(1,1,0)$, $a_2=(1,0,1)$, and $a_3=(0,1,1)$. Verify that your resulting vectors are orthonormal.</li>
        <li><b>Projection Matrix:</b> Derive the projection matrix $P$ that projects any vector onto the one-dimensional subspace spanned by a single non-zero vector $u$. Show that it is symmetric ($P^\top=P$) and idempotent ($P^2=P$).</li>
      </ol>
      <hr style="margin: 24px 0;">
      <h3>Solutions</h3>
      <ol>
        <li><b>Solution (Eigenvalues):</b>
            <p>Let $A$ be a real symmetric matrix ($A=A^\top$). Let $\lambda$ be an eigenvalue with corresponding eigenvector $x \in \mathbb{C}^n$. Then $Ax = \lambda x$. Taking the conjugate transpose of both sides gives $\bar{x}^\top A^\top = \bar{\lambda} \bar{x}^\top$. Since $A$ is real and symmetric, $A^\top = A$. So, $\bar{x}^\top A = \bar{\lambda} \bar{x}^\top$. Right-multiplying by $x$ gives $\bar{x}^\top A x = \bar{\lambda} \bar{x}^\top x$.
            Now, let's go back to the original equation $Ax = \lambda x$ and left-multiply by $\bar{x}^\top$: $\bar{x}^\top A x = \lambda \bar{x}^\top x$.
            We now have two expressions for $\bar{x}^\top A x$:
            $$ \bar{\lambda} \bar{x}^\top x = \lambda \bar{x}^\top x \implies (\lambda - \bar{\lambda}) \bar{x}^\top x = 0 $$
            Since $x$ is an eigenvector, it is non-zero. The term $\bar{x}^\top x = \sum |x_i|^2 = \|x\|_2^2$ is therefore strictly positive. This forces $(\lambda - \bar{\lambda}) = 0$, which means $\lambda = \bar{\lambda}$, so the eigenvalue $\lambda$ must be real.
            </p>
        </li>
        <li><b>Solution ($A^\top A$):</b>
            <p>To show $A^\top A$ is PSD, we must show that $x^\top (A^\top A) x \ge 0$ for all vectors $x$. We can rewrite this quadratic form as:
            $$ x^\top (A^\top A) x = (Ax)^\top (Ax) = \|Ax\|_2^2 $$
            The squared Euclidean norm of any vector is always non-negative, so $\|Ax\|_2^2 \ge 0$. Thus, $A^\top A$ is positive semidefinite.
            For $A^\top A$ to be positive definite, we need $\|Ax\|_2^2 > 0$ for all $x \neq 0$. This condition, $\|Ax\|_2 > 0$, means that $Ax \neq 0$ for any non-zero $x$. This is true if and only if the nullspace of $A$ contains only the zero vector, which is the definition of $A$ having linearly independent columns.
            </p>
        </li>
        <li><b>Solution (Gram-Schmidt):</b>
            <p>This is a standard calculation.
            1. $q_1 = a_1/\|a_1\|_2 = (1/\sqrt{2}, 1/\sqrt{2}, 0)$.
            2. $\tilde{q}_2 = a_2 - (a_2^\top q_1)q_1 = (1,0,1) - (1/\sqrt{2})q_1 = (1/2, -1/2, 1)$. Normalize to get $q_2 = (1/\sqrt{6}, -1/\sqrt{6}, 2/\sqrt{6})$.
            3. $\tilde{q}_3 = a_3 - (a_3^\top q_1)q_1 - (a_3^\top q_2)q_2 = (0,1,1) - (1/\sqrt{2})q_1 - (1/\sqrt{6})q_2 = (-2/3, 2/3, 2/3)$. Normalize to get $q_3 = (-1/\sqrt{3}, 1/\sqrt{3}, 1/\sqrt{3})$.
            You can verify that $q_i^\top q_j = 0$ for $i \neq j$ and $\|q_i\|_2=1$.
            </p>
        </li>
        <li><b>Solution (Projection Matrix):</b>
            <p>The subspace is one-dimensional, spanned by the single vector $u$. To use the formula $P = A(A^\top A)^{-1}A^\top$, we can let $A$ be the matrix whose single column is $u$. Then $A^\top A = u^\top u$, which is a scalar (the squared norm of $u$). Its inverse is simply $1/(u^\top u)$. Plugging this in gives:
            $$ P = u (u^\top u)^{-1} u^\top = \frac{u u^\top}{u^\top u} $$
            <b>Symmetry:</b> $P^\top = \left(\frac{u u^\top}{u^\top u}\right)^\top = \frac{(u^\top)^\top u^\top}{u^\top u} = \frac{u u^\top}{u^\top u} = P$.
            <b>Idempotence:</b> $P^2 = \left(\frac{u u^\top}{u^\top u}\right) \left(\frac{u u^\top}{u^\top u}\right) = \frac{u (u^\top u) u^\top}{(u^\top u)^2} = \frac{(u^\top u) u u^\top}{(u^\top u)^2} = \frac{u u^\top}{u^\top u} = P$.
            </p>
        </li>
      </ol>
    </section>

  </main>

  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">© <span id="year"></span> Convex Optimization Course · <a href="../../README.md" style="color: var(--brand);">About</a></p>
    </div>
  </footer>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>

  <script src="../../static/js/math-renderer.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initNormGeometryVisualizer } from './widgets/js/norm-geometry-visualizer.js';
    initNormGeometryVisualizer('widget-norm-geometry');
  </script>
  <script type="module">
    import { initOrthogonality } from './widgets/js/orthogonality.js';
    initOrthogonality('widget-orthogonality');
  </script>
  <script type="module">
    import { initRankNullspace } from './widgets/js/rank-nullspace.js';
    initRankNullspace('widget-rank-nullspace');
  </script>
  <script type="module">
    import { initEigenPsd } from './widgets/js/eigen-psd.js';
    initEigenPsd('widget-eigen-psd');
  </script>
  <script type="module">
    import { initSvdApproximator } from './widgets/js/svd-approximator.js';
    initSvdApproximator('widget-svd-approximator');
  </script>
</body>
</html>
