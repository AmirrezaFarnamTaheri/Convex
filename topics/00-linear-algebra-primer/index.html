<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>00. A Primer on Linear Algebra — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/modern-widgets.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
</head>
<body class="dark-theme">
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">
          <img src="../../static/assets/branding/logo.svg" alt="Logo" class="logo"/>
          <span>Convex Optimization</span>
        </a>
      </div>
      <nav class="nav">
        <a href="../../index.html">All Lectures</a>
        <a href="../01-introduction/index.html">Next →</a>
      </nav>
    </div>
  </header>

  <div class="lecture-container">
    <aside class="sidebar">
      <div id="toc-container">
        <h2>Table of Contents</h2>
        <nav id="toc"></nav>
      </div>
    </aside>
    <main class="lecture-content">
      <header class="lecture-header card">
        <h1>00. A Primer on Linear Algebra</h1>
        <div class="lecture-meta">
          <span>Date: 2025-10-14</span>
          <span>Duration: 90 min</span>
          <span>Tags: prerequisites, review, linear-algebra, foundational</span>
        </div>
        <div class="lecture-summary">
          <p><strong>Overview:</strong> This lecture establishes the linear algebra foundation required for convex optimization. Beginning with vectors and matrices, we progress through inner products, norms, orthogonality, and projections, building the mathematical toolkit necessary for optimization theory.</p>
          <p><strong>Prerequisites:</strong> None. This lecture is the starting point.</p>
          <p><strong>Forward Connections:</strong> Projection techniques introduced here appear in least squares methods (<a href="../01-introduction/index.html">Lecture 01</a>). Positive Semidefinite (PSD) matrices form the basis for convex quadratic programs (<a href="../01-introduction/index.html">Lecture 01</a>). The four fundamental subspaces provide geometric understanding of feasible sets (<a href="../02-convex-sets/index.html">Lecture 02</a>).</p>
        </div>
      </header>

      <section class="card">
        <h2>Learning Objectives</h2>
        <p>After this lecture, you will be able to work with:</p>
        <ul>
          <li><b>Fundamental Objects and Subspaces:</b> Vectors, matrices, the four fundamental subspaces, and the Rank-Nullity Theorem with proofs.</li>
          <li><b>Inner Products and Norms:</b> Standard and generalized inner products, the three canonical vector norms, dual norms, Hölder's inequality, and the Cauchy-Schwarz inequality with proofs.</li>
          <li><b>Orthogonality and Projections:</b> Orthonormal bases, Gram-Schmidt process, QR decomposition, projection onto subspaces and affine sets.</li>
          <li><b>Positive Semidefinite Matrices:</b> Definitions, eigenvalue characterization, quadratic forms, ellipsoids, and the Loewner order.</li>
          <li><b>Robust Numerical Methods:</b> Least squares via normal equations, QR, SVD, pseudoinverse, condition numbers, and when to use each method.</li>
          <li><b>Matrix Structures:</b> Trace inner product, Frobenius norm, componentwise order—all prerequisites for later convex set theory.</li>
        </ul>
      </section>

      <article>
        <section class="card" id="section-0">
          <h2>0. Notation and Basic Objects</h2>
          <h3>Scalars, Vectors, Matrices</h3>
          <ul>
            <li><b>Scalars:</b> Real numbers $a \in \mathbb{R}$.</li>
            <li><b>Column vectors:</b> $x \in \mathbb{R}^n$ are $n \times 1$ matrices with entries $x_i$.</li>
            <li><b>Matrices:</b> $A \in \mathbb{R}^{m \times n}$; entry $a_{ij}$ is row $i$, column $j$.</li>
            <li><b>Transpose:</b> $A^\top \in \mathbb{R}^{n \times m}$ satisfies $(A^\top)_{ij} = a_{ji}$.</li>
            <li><b>Identity:</b> $I_n$ has ones on the diagonal; $I_n x = x$.</li>
            <li><b>Standard basis:</b> $e_1, \dots, e_n$ where $e_i$ has a 1 in position $i$, zeros elsewhere. Every $x \in \mathbb{R}^n$ can be written $x = \sum_{i=1}^n x_i e_i$.</li>
          </ul>
          <figure style="text-align: center;">
            <img src="../../static/assets/topics/00-linear-algebra-primer/matrix-multiplication.svg"
                 alt="Matrix multiplication diagram showing how rows and columns combine"
                 style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
            <figcaption><i>Figure 0.1:</i> Matrix multiplication visualized—how rows of the first matrix combine with columns of the second.</figcaption>
          </figure>
          <h3>Matrix–Vector and Matrix–Matrix Multiplication</h3>
          <ul>
            <li>$Ax$ is the <b>linear combination of columns of $A$</b> with coefficients from $x$.</li>
            <li>$(AB)_{ij} = \sum_k a_{ik} b_{kj}$.</li>
          </ul>
          <h3>Linear Maps</h3>
          <p>A function $T: \mathbb{R}^n \to \mathbb{R}^m$ is <b>linear</b> if and only if $T(\alpha x + \beta y) = \alpha T(x) + \beta T(y)$. Every linear map can be represented by a matrix $A$ with $T(x) = Ax$.</p>
          <h3>Componentwise Order Notation</h3>
          <p>A notation used throughout optimization is the componentwise order. For vectors $x, y \in \mathbb{R}^n$:</p>
          <ul>
            <li>$x \ge 0$ means $x_i \ge 0$ for all $i=1, \dots, n$.</li>
            <li>$x \le y$ means $x_i \le y_i$ for all $i=1, \dots, n$.</li>
          </ul>
          <p>This provides a compact way to describe constraints, such as non-negativity. The vector $\mathbf{1}$ denotes the vector of all ones. This notation is essential for <a href="../02-convex-sets/index.html">Lecture 02</a>'s treatment of polyhedra and cones.</p>
        </section>

        <section class="card" id="section-1">
          <h2>1. Subspaces and the Four Fundamental Spaces</h2>
          <p>A <b>linear subspace</b> is a set of vectors that is closed under addition and scalar multiplication. Given a matrix $A \in \mathbb{R}^{m \times n}$, we can define four fundamental subspaces that characterize the behavior of the linear map $T(x) = Ax$.</p>
          <ul>
            <li><b>Column Space (Range):</b> $\mathcal{R}(A) = \{Ax \mid x \in \mathbb{R}^n\} \subseteq \mathbb{R}^m$. This is the set of all possible outputs of the linear map, i.e., the span of the columns of $A$.</li>
            <li><b>Nullspace (Kernel):</b> $\mathcal{N}(A) = \{x \in \mathbb{R}^n \mid Ax = 0\}$. This is the set of all input vectors that are mapped to the zero vector.</li>
            <li><b>Row Space:</b> $\mathcal{R}(A^\top) \subseteq \mathbb{R}^n$. This is the span of the rows of $A$.</li>
            <li><b>Left Nullspace:</b> $\mathcal{N}(A^\top) = \{y \in \mathbb{R}^m \mid A^\top y = 0\}$. This is the nullspace of the transpose of $A$.</li>
          </ul>
          <p>These subspaces are linked by two crucial orthogonality relationships:</p>
          <ul>
            <li>$\mathcal{R}(A) \perp \mathcal{N}(A^\top)$ in $\mathbb{R}^m$</li>
            <li>$\mathcal{R}(A^\top) \perp \mathcal{N}(A)$ in $\mathbb{R}^n$</li>
          </ul>
          <figure style="text-align: center;">
            <img src="../../static/assets/topics/00-linear-algebra-primer/linear-subspaces.svg"
                 alt="Visual representation of intersecting planes showing linear subspaces"
                 style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
            <figcaption><i>Figure 1.1:</i> Three planes in 3D space—the intersection represents solution sets of linear equations.</figcaption>
          </figure>
        </section>

        <section class="card" id="section-2">
          <h2>2. Inner Products, Norms, and Angles</h2>
          <h3>Inner Product</h3>
          <p>An inner product on $\mathbb{R}^n$ is a mapping $\langle x, y \rangle$ that is bilinear, symmetric, and positive definite:</p>
          <ul>
            <li>$\langle ax + by, z \rangle = a\langle x, z \rangle + b\langle y, z \rangle$</li>
            <li>$\langle x, y \rangle = \langle y, x \rangle$</li>
            <li>$\langle x, x \rangle > 0$ for $x \neq 0$</li>
          </ul>
          <p>The <b>standard (Euclidean) inner product</b> is the most common example: $\langle x, y \rangle = x^\top y = \sum_{i=1}^n x_i y_i$.</p>
          <figure style="text-align: center;">
            <img src="../../static/assets/topics/00-linear-algebra-primer/vector-addition-parallelogram.png"
                 alt="Parallelogram law for vector addition"
                 style="max-width: 70%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 12px; background: white;" />
            <figcaption><i>Figure 2.1:</i> The parallelogram law of vector addition, a geometric reflection of the algebraic properties of inner products.</figcaption>
          </figure>
          <h3>Norms</h3>
          <p>A norm is a function that assigns a strictly positive length or size to each vector in a vector space, except for the zero vector. A norm $\|\cdot\|$ must satisfy:</p>
          <ol>
            <li><b>Non-negativity:</b> $\|x\| \ge 0$, and $\|x\| = 0$ if and only if $x=0$.</li>
            <li><b>Absolute homogeneity:</b> $\|\alpha x\| = |\alpha| \|x\|$ for any scalar $\alpha$.</li>
            <li><b>Triangle inequality:</b> $\|x+y\| \le \|x\| + \|y\|$.</li>
          </ol>
          <p>Three of the most widely used norms are:</p>
          $$
          \|x\|_2 = \sqrt{\sum_i x_i^2} \quad (\text{Euclidean norm}), \quad \|x\|_1 = \sum_i |x_i| \quad (\text{L1 norm}), \quad \|x\|_\infty = \max_i |x_i| \quad (\text{Infinity norm})
          $$
          <figure style="text-align: center;">
            <img src="../../static/assets/topics/00-linear-algebra-primer/vector-norms.svg"
                 alt="Comparison of different norm unit balls"
                 style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
            <figcaption><i>Figure 2.2:</i> The "unit ball" (the set of all vectors with norm less than or equal to 1) for the L1, L2, and infinity norms. The different shapes illustrate how these norms measure distance differently.</figcaption>
          </figure>
        </section>

        <section class="card" id="section-3">
          <h2>3. Orthogonality and Orthonormal Bases</h2>
          <h3>Orthonormal Sets</h3>
          <p>A set of vectors $q_1, \dots, q_k$ is <b>orthonormal</b> if its elements are mutually orthogonal and each has a norm of 1. Formally, $q_i^\top q_j = \delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta (1 if $i=j$, 0 otherwise). A square matrix $Q$ with orthonormal columns is an <b>orthogonal matrix</b>, satisfying the important property $Q^\top Q = I$, which means $Q^{-1} = Q^\top$. Orthonormal bases are computationally desirable because they are numerically stable and simplify many calculations, such as projections.</p>
          <h3>The Gram–Schmidt Process</h3>
          <p>The Gram-Schmidt process is an algorithm for constructing an orthonormal basis from a set of linearly independent vectors. Starting with a vector, it iteratively subtracts the components that lie in the direction of the previously processed vectors, leaving a new, orthogonal vector that is then normalized.
          $$
          \tilde{q}_k = a_k - \sum_{i=1}^{k-1} (q_i^\top a_k) q_i, \quad q_k = \frac{\tilde{q}_k}{\|\tilde{q}_k\|_2}
          $$
          </p>
          <h3>The QR Decomposition</h3>
          <p>The QR decomposition expresses a matrix $A$ as the product of an orthonormal matrix $Q$ and an upper triangular matrix $R$. This decomposition is a direct outcome of the Gram-Schmidt process and is a cornerstone of numerical linear algebra. For a matrix $A \in \mathbb{R}^{m \times n}$ with full column rank, we can write $A = QR$, where $Q \in \mathbb{R}^{m \times n}$ has orthonormal columns and $R \in \mathbb{R}^{n \times n}$ is upper-triangular.
          <br><b>Applications:</b></p>
          <ul>
            <li><b>Solving Linear Systems:</b> The system $Ax=b$ becomes $QRx=b$, which simplifies to $Rx = Q^\top b$. This is easily solved using back substitution and is far more numerically stable than forming the normal equations.</li>
            <li><b>Projections:</b> The projection onto the column space of $A$ is given by $P = QQ^\top$.</li>
          </ul>
        </section>

        <section class="card" id="section-4">
          <h2>4. Positive Semidefinite Matrices</h2>
          <h3>Definitions</h3>
          <p>A symmetric matrix $Q \in \mathbb{S}^n$ is:</p>
          <ul>
            <li><b>Positive Semidefinite (PSD)</b>, written $Q \succeq 0$, if the quadratic form $x^\top Q x \ge 0$ for all vectors $x$.</li>
            <li><b>Positive Definite (PD)</b>, written $Q \succ 0$, if $x^\top Q x > 0$ for all non-zero vectors $x$.</li>
          </ul>
          <p>These matrices are fundamental to convex optimization because they define convex quadratic functions. An equivalent and often more practical characterization is based on their eigenvalues:</p>
          <ul>
            <li>A matrix is PSD if and only if all of its eigenvalues are non-negative.</li>
            <li>A matrix is PD if and only if all of its eigenvalues are strictly positive.</li>
          </ul>
          <p>A positive definite matrix $Q$ can be used to define a generalized norm, $\|x\|_Q = \sqrt{x^\top Q x}$. The unit ball for this norm, $\{x \mid x^\top Q x \le 1\}$, is an ellipsoid, whose geometry is determined by the eigenvalues and eigenvectors of $Q$.</p>
          <figure style="text-align: center;">
            <img src="../../static/assets/topics/00-linear-algebra-primer/eigenvalues-psd.png"
                 alt="Eigenvalue visualization for PSD matrices"
                 style="max-width: 80%; height: auto; border: 1px solid var(--border); border-radius: 8px;" />
            <figcaption><i>Figure 4.1:</i> Eigenvalues and positive semidefiniteness—the signs determine the matrix's curvature properties.</figcaption>
          </figure>
        </section>

        <section class="card" id="section-5">
          <h2>5. Projections onto Subspaces and Affine Sets</h2>
          <h3>Orthogonal Projection onto a Subspace</h3>
          <p>Let $\mathcal{S} \subseteq \mathbb{R}^m$ be a subspace and $b \in \mathbb{R}^m$. The <b>orthogonal projection</b> $p \in \mathcal{S}$ of $b$ is the unique vector in $\mathcal{S}$ minimizing $\|b - p\|_2$. It is characterized by the <b>orthogonality condition</b>:
          $$ b - p \perp \mathcal{S} \quad \iff \quad v^\top(b - p) = 0 \ \ \forall v \in \mathcal{S} $$</p>
          <h3>Projection via a Basis</h3>
          <p>If $Q \in \mathbb{R}^{m \times k}$ has orthonormal columns spanning $\mathcal{S}$, the projector is:
          $$ P = QQ^\top \quad \text{and} \quad p = Pb = QQ^\top b $$
          Check: $P^2 = P$ (idempotent) and $P^\top = P$ (symmetric)—that's what makes it an <b>orthogonal</b> projector.</p>
        </section>

        <section class="card" id="section-6">
          <h2>6. The Method of Least Squares</h2>
          <h3>The Problem: Overdetermined Systems</h3>
          <p>Often in practice, we encounter a system of linear equations $Ax=b$ where there is no exact solution because the vector $b$ does not lie in the column space of $A$. This is common when $m > n$ (more equations than unknowns). The goal of least squares is to find the "best" approximate solution by minimizing the squared Euclidean norm of the residual vector $r = Ax-b$:
          $$ \min_{x \in \mathbb{R}^n} \|Ax - b\|_2^2 $$</p>
          <h3>Geometric Interpretation: Projection</h3>
          <p>The solution to the least squares problem has a clean geometric interpretation. The vector $Ax$ is always in the column space of $A$, $\mathcal{R}(A)$. The problem is therefore equivalent to finding the point in $\mathcal{R}(A)$ that is closest to $b$. This closest point is the <b>orthogonal projection</b> of $b$ onto $\mathcal{R}(A)$. Let this projection be $p = Ax^\star$. The residual vector $r^\star = b - Ax^\star$ must be orthogonal to the entire column space.</p>
        </section>

        <section class="card" id="section-7">
          <h2>7. Solving Least Squares Robustly: QR, SVD, Pseudoinverse</h2>
          <h3>QR Method (Recommended in Practice)</h3>
          <p>If $A = QR$ with $Q \in \mathbb{R}^{m \times n}$ ($Q^\top Q = I$) and $R \in \mathbb{R}^{n \times n}$ upper triangular, the least-squares solution solves:
          $$ Rx^\star = Q^\top b \quad \text{(back substitution)} $$
          This avoids forming $A^\top A$, which squares the condition number and amplifies round-off errors.</p>
          <h3>Singular Value Decomposition (SVD)</h3>
          <p>Every $A \in \mathbb{R}^{m \times n}$ admits $A = U\Sigma V^\top$, where:</p>
          <ul>
            <li>$U \in \mathbb{R}^{m \times m}$ and $V \in \mathbb{R}^{n \times n}$ are orthogonal</li>
            <li>$\Sigma \in \mathbb{R}^{m \times n}$ is diagonal with nonnegative entries $\sigma_1 \ge \cdots \ge \sigma_r > 0$, $r = \mathrm{rank}(A)$</li>
          </ul>
          <p>The SVD provides the most robust method for computing the <b>pseudoinverse</b> of a matrix, denoted $A^+$:
          $$ A^+ = V\Sigma^+ U^\top $$
          where $\Sigma^+$ is formed by taking the reciprocal of the non-zero singular values and transposing the resulting matrix. The pseudoinverse provides the minimum-norm solution to the least squares problem:
          $$ x^\star = A^+ b $$
          This solution is numerically stable even when $A$ is rank-deficient or ill-conditioned.</p>
        </section>
      </article>

      <section class="card" id="section-8">
        <h2>8. Exercises</h2>
        <div class="problem">
          <h3>Problem 1: Orthogonality of Subspaces</h3>
          <p>Prove that the row space of a matrix $A$ is orthogonal to its nullspace. That is, show $\mathcal{R}(A^\top) \perp \mathcal{N}(A)$.</p>
          <div class="solution">
            <h4>Solution</h4>
            <p>Let $x \in \mathcal{N}(A)$ and $y \in \mathcal{R}(A^\top)$. By definition, $Ax = 0$ and $y = A^\top z$ for some vector $z$. We need to show that $x^\top y = 0$.
            $$ x^\top y = x^\top (A^\top z) = (Ax)^\top z = 0^\top z = 0 $$
            Thus, the two subspaces are orthogonal.</p>
          </div>
        </div>
        <div class="problem">
          <h3>Problem 2: Projection Matrix Properties</h3>
          <p>Show that the projection matrix $P = A(A^\top A)^{-1}A^\top$ is idempotent ($P^2=P$) and symmetric ($P^\top = P$).</p>
          <div class="solution">
            <h4>Solution</h4>
            <p><b>Symmetry:</b>
            $$ P^\top = (A(A^\top A)^{-1}A^\top)^\top = (A^\top)^\top ((A^\top A)^{-1})^\top A^\top = A ((A^\top A)^\top)^{-1} A^\top = A (A^\top (A^\top)^\top)^{-1} A^\top = A(A^\top A)^{-1}A^\top = P $$
            <b>Idempotency:</b>
            $$ P^2 = (A(A^\top A)^{-1}A^\top)(A(A^\top A)^{-1}A^\top) = A(A^\top A)^{-1}(A^\top A)(A^\top A)^{-1}A^\top = A(A^\top A)^{-1} I A^\top = A(A^\top A)^{-1}A^\top = P $$
            </p>
          </div>
        </div>
      </section>

      <footer class="site-footer">
        <div class="container">
          <p>© <span id="year"></span> Convex Optimization Course</p>
        </div>
      </footer>
    </main>
  </div>

  <div class="theme-switcher">
    <button class="theme-button" id="theme-button">
      <svg class="palette-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2.25c-5.376 0-9.75 4.374-9.75 9.75s4.374 9.75 9.75 9.75 9.75-4.374 9.75-9.75S17.376 2.25 12 2.25zm0 1.5c4.549 0 8.25 3.701 8.25 8.25s-3.701 8.25-8.25 8.25-8.25-3.701-8.25-8.25S7.451 3.75 12 3.75zm0 1.953c-.328 0-.651.031-.966.091 2.893.63 5.021 3.228 5.021 6.206 0 3.52-2.859 6.379-6.379 6.379-.236 0-.469-.013-.695-.038a8.217 8.217 0 007.039-7.04c.025-.226.038-.459.038-.695 0-2.978-2.128-5.576-5.02-6.206.315-.06.638-.091.966-.091z"/></svg>
    </button>
    <div class="theme-options" id="theme-options">
      <button data-theme="dark-theme">Dark</button>
      <button data-theme="light-theme">Light</button>
      <button data-theme="solarized-theme">Solarized</button>
    </div>
  </div>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initNormGeometryVisualizer } from './widgets/js/norm-geometry-visualizer.js';
    initNormGeometryVisualizer('widget-norm-geometry');
  </script>
  <script type="module">
    import { initOrthogonality } from './widgets/js/orthogonality.js';
    initOrthogonality('widget-orthogonality');
  </script>
  <script type="module">
    import { initRankNullspace } from './widgets/js/rank-nullspace.js';
    initRankNullspace('widget-rank-nullspace');
  </script>
  <script type="module">
    import { initEigenPsd } from './widgets/js/eigen-psd.js';
    initEigenPsd('widget-eigen-psd');
  </script>
  <script type="module">
    import { initSvdApproximator } from './widgets/js/svd-approximator.js';
    initSvdApproximator('widget-svd-approximator');
  </script>
  <script type="module">
    import { initMatrixExplorer } from './widgets/js/matrix-explorer.js';
    initMatrixExplorer('widget-matrix-explorer');
  </script>
  <script type="module">
    import { initHessianLandscapeVisualizer } from './widgets/js/hessian-landscape-visualizer.js';
    initHessianLandscapeVisualizer('widget-hessian-landscape');
  </script>
  <script type="module">
    import { initConditionNumber } from './widgets/js/condition-number.js';
    initConditionNumber('widget-condition-number');
  </script>
</body>
</html>
