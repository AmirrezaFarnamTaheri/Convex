<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>00. Linear Algebra Primer — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/styles.css" />
  <link rel="stylesheet" href="/static/css/modern-widgets.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
</head>
<body>
  <header class="site-header">
    <div class="container header-inner">
      <div class="brand">
        <img src="../../static/assets/branding/logo.svg" class="logo" alt="logo" />
        <a href="../../index.html" style="text-decoration:none;color:inherit">
          <strong>Convex Optimization</strong>
        </a>
      </div>
      <nav class="nav">
        <a href="../../index.html#sessions">All Lectures</a>
        <a href="../01-introduction/index.html">Next: Introduction →</a>
        <a href="#readings">Readings</a>
      </nav>
    </div>
  </header>

  <main class="container" style="padding: 32px 0 60px;">
    <article class="card" style="margin-bottom: 32px;">
      <h1 style="margin-top: 0;">00. A Primer on Linear Algebra</h1>
      <div class="meta">
        Date: 2025-10-14 · Duration: 90 min · Tags: prerequisites, review, linear-algebra, foundational
      </div>
      <section style="margin-top: 16px;">
        <p><strong>Overview:</strong> This lecture reviews the essential linear algebra concepts that form the bedrock of convex optimization. We build the theory step-by-step, starting from fundamental objects like vectors and matrices, moving through inner products, norms, orthogonality, projections, and the geometry of linear systems. This lecture focuses exclusively on the linear-algebraic machinery you need, deliberately avoiding topics from future lectures like convex sets or duality.</p>
        <p><strong>Prerequisites:</strong> None. This lecture is the starting point.</p>
        <p><strong>Forward Connections:</strong> The projection techniques developed here are central to <a href="../01-introduction/index.html">Lecture 01</a>'s least squares models. Positive Semidefinite (PSD) matrices are the foundation for convex quadratic programs in <a href="../01-introduction/index.html">Lecture 01</a>. The four fundamental subspaces provide geometric intuition for feasible sets in <a href="../02-convex-sets/index.html">Lecture 02</a>.</p>
      </section>
    </article>

    <section class="card" style="margin-bottom: 32px;">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you will be able to work with:</p>
      <ul style="line-height: 1.8;">
        <li><b>Fundamental Objects and Subspaces:</b> Vectors, matrices, the four fundamental subspaces, and the Rank-Nullity Theorem with proof sketches.</li>
        <li><b>Inner Products and Norms:</b> Standard and generalized inner products, the three canonical vector norms, dual norms, Hölder's inequality, and the Cauchy-Schwarz inequality with proofs.</li>
        <li><b>Orthogonality and Projections:</b> Orthonormal bases, Gram-Schmidt process, QR decomposition, projection onto subspaces and affine sets.</li>
        <li><b>Positive Semidefinite Matrices:</b> Definitions, eigenvalue characterization, quadratic forms, ellipsoids, and the Loewner order.</li>
        <li><b>Robust Numerical Methods:</b> Least squares via normal equations, QR, SVD, pseudoinverse, condition numbers, and when to use each method.</li>
        <li><b>Matrix Structures:</b> Trace inner product, Frobenius norm, componentwise order—all prerequisites for later convex set theory.</li>
      </ul>
    </section>

    <!-- TABLE OF CONTENTS -->
    <section class="card" style="margin-bottom: 32px; background: var(--panel);">
      <h2>Table of Contents</h2>
      <ol style="line-height: 1.8;">
        <li><a href="#section-0">Notation and Basic Objects</a></li>
        <li><a href="#section-1">Subspaces and the Four Fundamental Spaces</a></li>
        <li><a href="#section-2">Inner Products, Norms, and Angles</a></li>
        <li><a href="#section-3">Orthogonality, Orthonormal Bases, and QR</a></li>
        <li><a href="#section-4">Positive Semidefiniteness and Quadratic Forms</a></li>
        <li><a href="#section-5">Projections onto Subspaces and Affine Sets</a></li>
        <li><a href="#section-6">Least Squares: Normal Equations and Geometry</a></li>
        <li><a href="#section-7">Solving Least Squares Robustly: QR, SVD, Pseudoinverse</a></li>
        <li><a href="#section-8">Variants You Will Need</a></li>
        <li><a href="#section-9">Worked Examples (Fully Written Out)</a></li>
        <li><a href="#section-10">Implementation Mini-Guide</a></li>
        <li><a href="#section-11">Exercises with Detailed Solutions</a></li>
        <li><a href="#section-12">Sanity Checklist</a></li>
        <li><a href="#section-13">Quick Reference Formulas</a></li>
      </ol>
    </section>

    <!-- SECTION 0: NOTATION -->
    <section class="card" id="section-0" style="margin-bottom: 32px;">
      <h2>0. Notation and Basic Objects</h2>

      <h3>Scalars, Vectors, Matrices</h3>
      <ul>
        <li><b>Scalars:</b> Real numbers $a \in \mathbb{R}$.</li>
        <li><b>Column vectors:</b> $x \in \mathbb{R}^n$ are $n \times 1$ matrices with entries $x_i$.</li>
        <li><b>Matrices:</b> $A \in \mathbb{R}^{m \times n}$; entry $a_{ij}$ is row $i$, column $j$.</li>
        <li><b>Transpose:</b> $A^\top \in \mathbb{R}^{n \times m}$ satisfies $(A^\top)_{ij} = a_{ji}$.</li>
        <li><b>Identity:</b> $I_n$ has ones on the diagonal; $I_n x = x$.</li>
        <li><b>Standard basis:</b> $e_1, \dots, e_n$ where $e_i$ has a 1 in position $i$, zeros elsewhere. Every $x \in \mathbb{R}^n$ can be written $x = \sum_{i=1}^n x_i e_i$.</li>
      </ul>

      <div style="margin: 24px 0; text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/matrix-multiplication-diagram.svg"
             alt="Matrix multiplication diagram showing how rows and columns combine"
             style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <p style="margin-top: 8px; font-size: 0.9em; color: var(--text-muted);"><i>Figure 0.1:</i> Matrix multiplication visualized—how rows of the first matrix combine with columns of the second.</p>
      </div>

      <h3>Matrix–Vector and Matrix–Matrix Multiplication</h3>
      <ul>
        <li>$Ax$ is the <b>linear combination of columns of $A$</b> with coefficients from $x$.</li>
        <li>$(AB)_{ij} = \sum_k a_{ik} b_{kj}$.</li>
      </ul>

      <div style="margin: 24px 0; text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/matrix-multiplication-animation.svg"
             alt="Animated visualization of matrix multiplication process"
             style="max-width: 80%; height: auto; border: 1px solid var(--border); border-radius: 8px;" />
        <p style="margin-top: 8px; font-size: 0.9em; color: var(--text-muted);"><i>Figure 0.2:</i> A step-by-step animation of matrix multiplication. The highlighted row and column are combined to produce the highlighted entry in the result.</p>
      </div>

      <h3>Linear Maps</h3>
      <p>A function $T: \mathbb{R}^n \to \mathbb{R}^m$ is <b>linear</b> if and only if $T(\alpha x + \beta y) = \alpha T(x) + \beta T(y)$. Every linear map can be represented by a matrix $A$ with $T(x) = Ax$.</p>

      <h3>Componentwise Order Notation</h3>
      <p>A notation used throughout optimization is the componentwise order. For vectors $x, y \in \mathbb{R}^n$:
      <ul>
        <li>$x \ge 0$ means $x_i \ge 0$ for all $i=1, \dots, n$.</li>
        <li>$x \le y$ means $x_i \le y_i$ for all $i=1, \dots, n$.</li>
      </ul>
      This provides a compact way to describe constraints, such as non-negativity. The vector $\mathbf{1}$ denotes the vector of all ones. This notation is essential for <a href="../02-convex-sets/index.html">Lecture 02</a>'s treatment of polyhedra and cones.</p>
    </section>

    <!-- SECTION 1: FOUR FUNDAMENTAL SUBSPACES -->
    <section class="card" id="section-1" style="margin-bottom: 32px;">
      <h2>1. Subspaces and the Four Fundamental Spaces</h2>

      <p>A <b>linear subspace</b> is a set of vectors that is closed under addition and scalar multiplication. Given a matrix $A \in \mathbb{R}^{m \times n}$, we can define four fundamental subspaces that provide deep insights into the behavior of the linear map $T(x) = Ax$.</p>
      <ul>
        <li><b>Column Space (Range):</b> $\mathcal{R}(A) = \{Ax \mid x \in \mathbb{R}^n\} \subseteq \mathbb{R}^m$. This is the set of all possible outputs of the linear map, i.e., the span of the columns of $A$.</li>
        <li><b>Nullspace (Kernel):</b> $\mathcal{N}(A) = \{x \in \mathbb{R}^n \mid Ax = 0\}$. This is the set of all input vectors that are mapped to the zero vector.</li>
        <li><b>Row Space:</b> $\mathcal{R}(A^\top) \subseteq \mathbb{R}^n$. This is the span of the rows of $A$.</li>
        <li><b>Left Nullspace:</b> $\mathcal{N}(A^\top) = \{y \in \mathbb{R}^m \mid A^\top y = 0\}$. This is the nullspace of the transpose of $A$.</li>
      </ul>

      <p>These subspaces are linked by two crucial orthogonality relationships:</p>
      <ul>
        <li>$\mathcal{R}(A) \perp \mathcal{N}(A^\top)$ in $\mathbb{R}^m$</li>
        <li>$\mathcal{R}(A^\top) \perp \mathcal{N}(A)$ in $\mathbb{R}^n$</li>
      </ul>

      <div style="margin: 24px 0; text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/linear-subspaces.svg"
             alt="Visual representation of intersecting planes showing linear subspaces"
             style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <p style="margin-top: 8px; font-size: 0.9em; color: var(--text-muted);"><i>Figure 1.1:</i> Three planes in 3D space—the intersection represents solution sets of linear equations.</p>
      </div>

      <div class="proof" style="margin: 24px 0; padding: 16px; background: #f8f9fa; border-left: 4px solid var(--brand); border-radius: 4px;">
        <h4>Proof: $\mathcal{R}(A) \perp \mathcal{N}(A^\top)$</h4>
        <p>If $y \in \mathcal{N}(A^\top)$, then $A^\top y = 0$. For any $x \in \mathbb{R}^n$, we have $Ax \in \mathcal{R}(A)$. The inner product is:
        $$ y^\top (Ax) = (A^\top y)^\top x = 0^\top x = 0 $$
        Thus every vector in $\mathcal{R}(A)$ is orthogonal to every vector in $\mathcal{N}(A^\top)$.</p>
      </div>

      <h3>Rank and Rank–Nullity Theorem</h3>
      <p>$\mathrm{rank}(A) = \dim \mathcal{R}(A)$. For $A \in \mathbb{R}^{m \times n}$:
      $$ \dim \mathcal{N}(A) + \mathrm{rank}(A) = n $$
      This fundamental dimension-counting result is the algebraic spine of linear algebra.</p>

      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Rank & Nullspace</h3>
        <p>Define a 2×3 or 3×2 matrix and visualize all four fundamental subspaces. Experiment with rank-deficient matrices to see the Rank-Nullity Theorem in action.</p>
        <div id="widget-rank-nullspace" class="widget-container" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Additional Visualizer: Matrix Explorer</h3>
        <p>An interactive tool to explore matrix properties including rank, determinant, and fundamental subspaces.</p>
        <div id="widget-matrix-explorer" class="widget-container" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 2: INNER PRODUCTS AND NORMS -->
    <section class="card" id="section-2" style="margin-bottom: 32px;">
      <h2>2. Inner Products, Norms, and Angles</h2>

      <h3>Inner Product</h3>
      <p>An inner product on $\mathbb{R}^n$ is a mapping $\langle x, y \rangle$ that is bilinear, symmetric, and positive definite:</p>
      <ul>
        <li>$\langle ax + by, z \rangle = a\langle x, z \rangle + b\langle y, z \rangle$</li>
        <li>$\langle x, y \rangle = \langle y, x \rangle$</li>
        <li>$\langle x, x \rangle > 0$ for $x \neq 0$</li>
      </ul>
      <p>The <b>standard (Euclidean) inner product</b> is the most common example: $\langle x, y \rangle = x^\top y = \sum_{i=1}^n x_i y_i$.</p>

      <div style="margin: 24px 0; text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/vector-addition-parallelogram.png"
             alt="Parallelogram law for vector addition"
             style="max-width: 70%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 12px; background: white;" />
        <p style="margin-top: 8px; font-size: 0.9em; color: var(--text-muted);"><i>Figure 2.1:</i> The parallelogram law of vector addition, a geometric reflection of the algebraic properties of inner products.</p>
      </div>

      <h3>Norms</h3>
      <p>A norm is a function that assigns a strictly positive length or size to each vector in a vector space, except for the zero vector. A norm $\|\cdot\|$ must satisfy:</p>
      <ol>
        <li><b>Non-negativity:</b> $\|x\| \ge 0$, and $\|x\| = 0$ if and only if $x=0$.</li>
        <li><b>Absolute homogeneity:</b> $\|\alpha x\| = |\alpha| \|x\|$ for any scalar $\alpha$.</li>
        <li><b>Triangle inequality:</b> $\|x+y\| \le \|x\| + \|y\|$.</li>
      </ol>
      <p>Three of the most widely used norms are:</p>
      $$
      \|x\|_2 = \sqrt{\sum_i x_i^2} \quad (\text{Euclidean norm}), \quad \|x\|_1 = \sum_i |x_i| \quad (\text{L1 norm}), \quad \|x\|_\infty = \max_i |x_i| \quad (\text{Infinity norm})
      $$

      <div style="margin: 24px 0; text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/vector-norms.svg"
             alt="Comparison of different norm unit balls"
             style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <p style="margin-top: 8px; font-size: 0.9em; color: var(--text-muted);"><i>Figure 2.2:</i> The "unit ball" (the set of all vectors with norm less than or equal to 1) for the L1, L2, and infinity norms. The different shapes illustrate how these norms measure distance differently.</p>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Interactive Visualizer: The Geometry of Norms</h3>
        <p>Explore unit balls for $\ell_1$, $\ell_2$, and $\ell_\infty$ norms. Notice the "pointy" corners of the $\ell_1$ ball—this geometric feature promotes sparsity in optimization.</p>
        <div id="widget-norm-geometry" class="widget-container" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>Cauchy–Schwarz and Triangle Inequality</h3>

      <div class="proof" style="margin: 24px 0; padding: 16px; background: #f8f9fa; border-left: 4px solid var(--brand); border-radius: 4px;">
        <h4>Proof of Cauchy–Schwarz: $|x^\top y| \le \|x\|_2 \|y\|_2$</h4>
        <p>Let's consider the expression $\|x - ty\|_2^2$ for any scalar $t$. Since the norm is always non-negative, we have:
        $$ \|x - ty\|_2^2 = (x - ty)^\top(x - ty) = \|x\|_2^2 - 2t(x^\top y) + t^2 \|y\|_2^2 \ge 0 $$
        This is a quadratic polynomial in $t$ that is always non-negative. For this to be true, the discriminant of the quadratic must be non-positive:
        $$ ( -2(x^\top y) )^2 - 4(\|y\|_2^2)(\|x\|_2^2) \le 0 $$
        $$ 4(x^\top y)^2 \le 4\|x\|_2^2 \|y\|_2^2 $$
        Taking the square root of both sides gives the Cauchy-Schwarz inequality.</p>
      </div>

      <p><b>Triangle Inequality:</b> The triangle inequality, $\|x + y\|_2 \le \|x\|_2 + \|y\|_2$, can be proven by squaring both sides and applying the Cauchy-Schwarz inequality to the inner product term.</p>

      <h3>Angles and Orthogonality</h3>
      <p>The Cauchy-Schwarz inequality allows us to define the angle between two vectors:
      $$ \cos \angle(x, y) = \frac{x^\top y}{\|x\|_2 \|y\|_2} $$
      Since the magnitude of the right-hand side is guaranteed to be between -1 and 1. Two vectors are <b>orthogonal</b> if their inner product is zero, $x^\top y = 0$, which corresponds to a 90-degree angle.</p>

      <h3>Dual Norms and Hölder's Inequality</h3>
      <p>The concept of duality is central to optimization. For any norm $\|\cdot\|$, we can define a corresponding <b>dual norm</b>, denoted $\|\cdot\|_*$, as:
      $$ \|y\|_* = \sup_{\|x\| \le 1} x^\top y $$
      The dual norm measures the maximum "stretch" of a vector $y$ when applied to unit-norm vectors $x$. This definition leads to some important pairings:</p>
      <ul>
        <li>The dual of the L2 norm is the L2 norm itself (it is self-dual).</li>
        <li>The dual of the L1 norm is the L-infinity norm.</li>
        <li>The dual of the L-infinity norm is the L1 norm.</li>
      </ul>

      <div style="margin: 24px 0; padding: 16px; background: #fff8e1; border-left: 4px solid #ffa000; border-radius: 4px;">
        <h4>Hölder's Inequality: A Generalization of Cauchy-Schwarz</h4>
        <p>Hölder's inequality is a direct consequence of the definition of the dual norm and provides a powerful bound on the inner product of two vectors:
        $$ |x^\top y| \le \|x\| \cdot \|y\|_* $$
        When the norm is the L2 norm, the dual norm is also the L2 norm, and we recover the Cauchy-Schwarz inequality. This inequality is essential for the study of dual cones in <a href="../02-convex-sets/index.html">Lecture 02</a> and for duality theory in general.</p>
      </div>

      <h3>Generalized (Weighted) Inner Product</h3>
      <p>If $Q \in \mathbb{R}^{n \times n}$ is symmetric positive definite (SPD), then $\langle x, y \rangle_Q := x^\top Q y$ is an inner product with induced norm $\|x\|_Q = \sqrt{x^\top Q x}$. This yields <b>quadratic forms</b> and ellipsoidal distance, but we avoid convex-geometry discussions here.</p>

      <h3>Matrix Inner Product and Frobenius Norm</h3>
      <p>For matrices $X, Y \in \mathbb{R}^{m \times n}$, the <b>trace inner product</b> is:
      $$ \langle X, Y \rangle = \mathrm{tr}(X^\top Y) = \sum_{ij} X_{ij} Y_{ij} $$
      The <b>Frobenius norm</b> is $\|X\|_F = \sqrt{\langle X, X \rangle} = \sqrt{\sum_{ij} X_{ij}^2}$.</p>

      <p>The <b>Loewner order</b> for symmetric matrices: $X \succeq Y$ means $X - Y \succeq 0$ (i.e., $v^\top(X - Y)v \ge 0$ for all $v$). This is the backdrop for <a href="../02-convex-sets/index.html">Lecture 02</a>'s PSD cone and its dual.</p>
    </section>

    <!-- SECTION 3: ORTHOGONALITY AND QR -->
    <section class="card" id="section-3" style="margin-bottom: 32px;">
      <h2>3. Orthogonality and Orthonormal Bases</h2>

      <h3>Orthonormal Sets</h3>
      <p>A set of vectors $q_1, \dots, q_k$ is <b>orthonormal</b> if its elements are mutually orthogonal and each has a norm of 1. Formally, $q_i^\top q_j = \delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta (1 if $i=j$, 0 otherwise). A square matrix $Q$ with orthonormal columns is an <b>orthogonal matrix</b>, satisfying the important property $Q^\top Q = I$, which means $Q^{-1} = Q^\top$. Orthonormal bases are computationally desirable because they are numerically stable and simplify many calculations, such as projections.</p>

      <h3>The Gram–Schmidt Process</h3>
      <p>The Gram-Schmidt process is an algorithm for constructing an orthonormal basis from a set of linearly independent vectors. Starting with a vector, it iteratively subtracts the components that lie in the direction of the previously processed vectors, leaving a new, orthogonal vector that is then normalized.
      $$
      \tilde{q}_k = a_k - \sum_{i=1}^{k-1} (q_i^\top a_k) q_i, \quad q_k = \frac{\tilde{q}_k}{\|\tilde{q}_k\|_2}
      $$
      </p>

      <h3>The QR Decomposition</h3>
      <p>The QR decomposition expresses a matrix $A$ as the product of an orthonormal matrix $Q$ and an upper triangular matrix $R$. This decomposition is a direct outcome of the Gram-Schmidt process and is a cornerstone of numerical linear algebra. For a matrix $A \in \mathbb{R}^{m \times n}$ with full column rank, we can write $A = QR$, where $Q \in \mathbb{R}^{m \times n}$ has orthonormal columns and $R \in \mathbb{R}^{n \times n}$ is upper-triangular.
      <br><b>Applications:</b></p>
      <ul>
        <li><b>Solving Linear Systems:</b> The system $Ax=b$ becomes $QRx=b$, which simplifies to $Rx = Q^\top b$. This is easily solved using back substitution and is far more numerically stable than forming the normal equations.</li>
        <li><b>Projections:</b> The projection onto the column space of $A$ is given by $P = QQ^\top$.</li>
      </ul>

      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Interactive Explorer: Orthogonality & Projections</h3>
        <p>Drag two vectors in the 2D plane and see their dot product, angle, and orthogonal projection update in real-time.</p>
        <div id="widget-orthogonality" class="widget-container" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 4: PSD MATRICES -->
    <section class="card" id="section-4" style="margin-bottom: 32px;">
      <h2>4. Positive Semidefinite Matrices</h2>

      <h3>Definitions</h3>
      <p>A symmetric matrix $Q \in \mathbb{S}^n$ is:</p>
      <ul>
        <li><b>Positive Semidefinite (PSD)</b>, written $Q \succeq 0$, if the quadratic form $x^\top Q x \ge 0$ for all vectors $x$.</li>
        <li><b>Positive Definite (PD)</b>, written $Q \succ 0$, if $x^\top Q x > 0$ for all non-zero vectors $x$.</li>
      </ul>

      <p>These matrices are fundamental to convex optimization because they define convex quadratic functions. An equivalent and often more practical characterization is based on their eigenvalues:</p>
      <ul>
        <li>A matrix is PSD if and only if all of its eigenvalues are non-negative.</li>
        <li>A matrix is PD if and only if all of its eigenvalues are strictly positive.</li>
      </ul>

      <p>A positive definite matrix $Q$ can be used to define a generalized norm, $\|x\|_Q = \sqrt{x^\top Q x}$. The unit ball for this norm, $\{x \mid x^\top Q x \le 1\}$, is an ellipsoid, whose geometry is determined by the eigenvalues and eigenvectors of $Q$.</p>

      <div style="margin: 24px 0; text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/eigenvalues-psd.png"
             alt="Eigenvalue visualization for PSD matrices"
             style="max-width: 80%; height: auto; border: 1px solid var(--border); border-radius: 8px;" />
        <p style="margin-top: 8px; font-size: 0.9em; color: var(--text-muted);"><i>Figure 4.1:</i> Eigenvalues and positive semidefiniteness—the signs determine the matrix's curvature properties.</p>
      </div>

      <div style="margin: 24px 0; text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/eigenvectors.gif"
             alt="Animated eigenvector visualization"
             style="max-width: 70%; height: auto; border: 1px solid var(--border); border-radius: 8px;" />
        <p style="margin-top: 8px; font-size: 0.9em; color: var(--text-muted);"><i>Figure 4.2:</i> Eigenvectors in action—how matrices transform space along principal directions.</p>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Interactive Explorer: Eigenvalues and PSD Matrices</h3>
        <p>For 2×2 matrices, see geometric interpretation of eigenvalues/eigenvectors and visualize the quadratic form $x^\top Q x$. Modify entries and observe how eigenvalues change. The 3D surface plot shows whether the matrix is PD (convex bowl), ND (concave), or indefinite (saddle).</p>
        <div id="widget-eigen-psd" class="widget-container" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Advanced Tool: Hessian Landscape Visualizer</h3>
        <p>Render the 3D surface of a quadratic function and its Hessian matrix, linking eigenvalues to curvature.</p>
        <div id="widget-hessian-landscape" class="widget-container" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: #e3f2fd; border-left: 4px solid #1976d2; border-radius: 4px;">
        <h4>🔗 Forward Connection to Lecture 01</h4>
        <p>PSD matrices are the key to convex quadratic programs (QPs) in <a href="../01-introduction/index.html">Lecture 01</a>. A QP with $Q \succeq 0$ guarantees a convex objective function, enabling efficient global optimization.</p>
      </div>
    </section>

    <!-- SECTION 5: PROJECTIONS -->
    <section class="card" id="section-5" style="margin-bottom: 32px;">
      <h2>5. Projections onto Subspaces and Affine Sets</h2>

      <h3>Orthogonal Projection onto a Subspace</h3>
      <p>Let $\mathcal{S} \subseteq \mathbb{R}^m$ be a subspace and $b \in \mathbb{R}^m$. The <b>orthogonal projection</b> $p \in \mathcal{S}$ of $b$ is the unique vector in $\mathcal{S}$ minimizing $\|b - p\|_2$. It is characterized by the <b>orthogonality condition</b>:
      $$ b - p \perp \mathcal{S} \quad \iff \quad v^\top(b - p) = 0 \ \ \forall v \in \mathcal{S} $$</p>

      <h3>Projection via a Basis</h3>
      <p>If $Q \in \mathbb{R}^{m \times k}$ has orthonormal columns spanning $\mathcal{S}$, the projector is:
      $$ P = QQ^\top \quad \text{and} \quad p = Pb = QQ^\top b $$
      Check: $P^2 = P$ (idempotent) and $P^\top = P$ (symmetric)—that's what makes it an <b>orthogonal</b> projector.</p>

      <h3>Projection onto $\mathcal{R}(A)$ using $A$</h3>
      <p>If $A \in \mathbb{R}^{m \times n}$ has full column rank:
      $$ P = A(A^\top A)^{-1} A^\top \quad \text{and} \quad p = Pb $$
      <b>Proof:</b> Columns of $Q = A(A^\top A)^{-1/2}$ are orthonormal; substitute $QQ^\top$.</p>

      <h3>Projection onto an Affine Set</h3>
      <p>Projecting onto $\{x \mid Fx = g\}$ reduces to subspace projection by translating:</p>
      <ol>
        <li>Pick any $x_0$ with $Fx_0 = g$</li>
        <li>Write the affine set as $x_0 + \mathcal{N}(F)$</li>
        <li>Project $b - x_0$ onto $\mathcal{N}(F)$ using a basis $Z$ for $\mathcal{N}(F)$</li>
        <li>Translate back</li>
      </ol>

      <div style="margin: 24px 0; padding: 16px; background: #e8f5e9; border-left: 4px solid #4caf50; border-radius: 4px;">
        <h4>💡 Key Takeaway</h4>
        <p>Projection is the geometric heart of least squares. Every solution to a linear system "in the least squares sense" is fundamentally a projection of the data vector onto the column space of the design matrix.</p>
      </div>
    </section>

    <!-- SECTION 6: LEAST SQUARES -->
    <section class="card" id="section-6" style="margin-bottom: 32px;">
      <h2>6. The Method of Least Squares</h2>

      <h3>The Problem: Overdetermined Systems</h3>
      <p>Often in practice, we encounter a system of linear equations $Ax=b$ where there is no exact solution because the vector $b$ does not lie in the column space of $A$. This is common when $m > n$ (more equations than unknowns). The goal of least squares is to find the "best" approximate solution by minimizing the squared Euclidean norm of the residual vector $r = Ax-b$:
      $$ \min_{x \in \mathbb{R}^n} \|Ax - b\|_2^2 $$</p>

      <h3>Geometric Interpretation: Projection</h3>
      <p>The solution to the least squares problem has a clean geometric interpretation. The vector $Ax$ is always in the column space of $A$, $\mathcal{R}(A)$. The problem is therefore equivalent to finding the point in $\mathcal{R}(A)$ that is closest to $b$. This closest point is the <b>orthogonal projection</b> of $b$ onto $\mathcal{R}(A)$. Let this projection be $p = Ax^\star$. The residual vector $r^\star = b - Ax^\star$ must be orthogonal to the entire column space.</p>

      <h3>The Normal Equations</h3>
      <p>The orthogonality condition, $r^\star \perp \mathcal{R}(A)$, means that the residual must be orthogonal to every column of $A$. This can be expressed compactly as $A^\top r^\star = 0$. Substituting the definition of the residual, we get:
      $$ A^\top(b - Ax^\star) = 0 $$
      Rearranging this gives the celebrated <b>normal equations</b>:
      $$ A^\top A x^\star = A^\top b $$
      If the matrix $A$ has linearly independent columns (full column rank), then $A^\top A$ is invertible, and we can write the unique solution as $x^\star = (A^\top A)^{-1} A^\top b$.</p>

      <h3>Uniqueness of the Solution</h3>
      <ul>
        <li>If $A$ has full column rank ($\mathrm{rank}(A) = n$), the solution $x^\star$ is unique.</li>
        <li>If $A$ is rank-deficient ($\mathrm{rank}(A) < n$), there are infinitely many solutions. In this case, the pseudoinverse is used to find the solution with the minimum norm.</li>
      </ul>

      <div style="margin: 24px 0; padding: 16px; background: #e3f2fd; border-left: 4px solid #1976d2; border-radius: 4px;">
        <h4>🔗 Forward Connection to Lecture 01</h4>
        <p>Least squares is the canonical "loss function" in <a href="../01-introduction/index.html">Lecture 01</a>. It appears in ridge regression, LASSO, and countless machine learning models. The normal equations are the foundation for understanding convex quadratic programs.</p>
      </div>
    </section>

    <!-- SECTION 7: QR, SVD, PSEUDOINVERSE -->
    <section class="card" id="section-7" style="margin-bottom: 32px;">
      <h2>7. Solving Least Squares Robustly: QR, SVD, Pseudoinverse</h2>

      <h3>QR Method (Recommended in Practice)</h3>
      <p>If $A = QR$ with $Q \in \mathbb{R}^{m \times n}$ ($Q^\top Q = I$) and $R \in \mathbb{R}^{n \times n}$ upper triangular, the least-squares solution solves:
      $$ Rx^\star = Q^\top b \quad \text{(back substitution)} $$
      This avoids forming $A^\top A$, which squares the condition number and amplifies round-off errors.</p>

      <h3>Singular Value Decomposition (SVD)</h3>
      <p>Every $A \in \mathbb{R}^{m \times n}$ admits $A = U\Sigma V^\top$, where:</p>
      <ul>
        <li>$U \in \mathbb{R}^{m \times m}$ and $V \in \mathbb{R}^{n \times n}$ are orthogonal</li>
        <li>$\Sigma \in \mathbb{R}^{m \times n}$ is diagonal with nonnegative entries $\sigma_1 \ge \cdots \ge \sigma_r > 0$, $r = \mathrm{rank}(A)$</li>
      </ul>

      <p>The SVD provides the most robust method for computing the <b>pseudoinverse</b> of a matrix, denoted $A^+$:
      $$ A^+ = V\Sigma^+ U^\top $$
      where $\Sigma^+$ is formed by taking the reciprocal of the non-zero singular values and transposing the resulting matrix. The pseudoinverse provides the minimum-norm solution to the least squares problem:
      $$ x^\star = A^+ b $$
      This solution is numerically stable even when $A$ is rank-deficient or ill-conditioned.</p>

      <h3>The Condition Number</h3>
      <p>The <b>condition number</b> of a matrix, $\kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}}$, measures how sensitive the solution of a linear system is to perturbations in the input data. A large condition number indicates that even small numerical errors can be greatly amplified. The normal equations are particularly susceptible to this, as $\kappa(A^\top A) = \kappa(A)^2$. This is why direct methods like QR and SVD are preferred in practice.</p>

      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style-top: 0;">Interactive Demo: SVD for Image Compression</h3>
        <p>See low-rank approximation in action. Load an image and reconstruct it using varying numbers of singular values. A surprisingly accurate approximation can be achieved with only a small fraction of the singular values.</p>
        <div id="widget-svd-approximator" class="widget-container" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Advanced Tool: Condition Number Race</h3>
        <p>Demonstrates how high condition numbers slow down iterative solvers by comparing two systems of linear equations.</p>
        <div id="widget-condition-number" class="widget-container" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>Projection Revisited (with SVD)</h3>
      <p>The projector onto $\mathcal{R}(A)$ is:
      $$ P = UU^\top \quad \text{(using only the first $r$ columns of $U$)} \quad \text{or} \quad P = AA^+ $$
      Both are symmetric idempotent; both send any $b$ to its closest point in $\mathcal{R}(A)$.</p>
    </section>

    <!-- SECTION 8: VARIANTS -->
    <section class="card" id="section-8" style="margin-bottom: 32px;">
      <h2>8. Variants You Will Actually Need</h2>

      <h3>(a) Weighted Least Squares (WLS)</h3>
      <p>Given SPD weight $W \in \mathbb{R}^{m \times m}$:
      $$ \min_x \|Ax - b\|_W^2 := (Ax - b)^\top W(Ax - b) $$
      Let $C$ satisfy $W = C^\top C$ (e.g., Cholesky). Then the problem is ordinary least squares in the <b>whitened</b> system $(CA)x \approx Cb$. Normal equations: $A^\top W A x = A^\top W b$.</p>

      <h3>(b) Constrained to an Affine Set</h3>
      <p>Solve:
      $$ \min_x \|Ax - b\|_2^2 \quad \text{s.t.} \quad Fx = g $$
      One method: parametrize $x = x_0 + Zy$, where $Fx_0 = g$ and columns of $Z$ form a basis for $\mathcal{N}(F)$. Then minimize $\|A(x_0 + Zy) - b\|_2^2$ over $y$ (an unconstrained LS). QR on $AZ$ is typically best.</p>
      <p><i>We save general constrained formulations and KKT systems for later lectures.</i></p>

      <h3>(c) Data Preprocessing That Prevents Pain</h3>
      <ul>
        <li>Center columns of $A$ and vector $b$ to reduce round-off and improve interpretability</li>
        <li>Scale features to comparable magnitudes before solving; $\kappa_2$ often improves dramatically</li>
      </ul>
    </section>

    <!-- SECTION 9: WORKED EXAMPLES -->
    <section class="card" id="section-9" style="margin-bottom: 32px;">
      <h2>9. Worked Examples (Fully Written Out)</h2>

      <div style="margin: 24px 0; padding: 16px; background: #f5f5f5; border: 1px solid #ddd; border-radius: 8px;">
        <h3>Example 1: Projection onto a Line</h3>
        <p>Let's find the projection of the vector $b = (2, 3)^\top$ onto the line spanned by the vector $u = (1, 1)^\top$. The projection $p$ is given by the formula:
        $$ p = \frac{u^\top b}{u^\top u} u = \frac{(1)(2) + (1)(3)}{(1)(1) + (1)(1)} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \frac{5}{2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2.5 \\ 2.5 \end{pmatrix} $$
        The residual vector is $r = b - p = (2 - 2.5, 3 - 2.5)^\top = (-0.5, 0.5)^\top$. We can verify that the residual is orthogonal to the line: $u^\top r = (1)(-0.5) + (1)(0.5) = 0$.
        </p>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: #f5f5f5; border: 1px solid #ddd; border-radius: 8px;">
        <h3>Example 2: Solving Least Squares with Normal Equations</h3>
        <p>Let's solve the least squares problem for the system $Ax=b$ with:
        $$ A = \begin{pmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 1 \end{pmatrix}, \quad b = \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix} $$
        First, we form the normal equations $A^\top A x = A^\top b$:
        $$ A^\top A = \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & 1 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 1 \end{pmatrix} = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} $$
        $$ A^\top b = \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & 1 \end{pmatrix} \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 3 \\ 3 \end{pmatrix} $$
        We solve the system $\begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} x = \begin{pmatrix} 3 \\ 3 \end{pmatrix}$, which yields the solution $x^\star = (3/4, 3/4)^\top$.
        The projection is $p = Ax^\star = (3/2, 0, 3/2)^\top$, and the residual is $r = b - p = (1/2, 0, -1/2)^\top$. We can verify the orthogonality condition: $A^\top r = (0, 0)^\top$.
        </p>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: #f5f5f5; border: 1px solid #ddd; border-radius: 8px;">
        <h3>Example 3: Rank-Deficient Case</h3>
        <p>Consider the system with:
        $$ A = \begin{pmatrix} 1 & 1 \\ 2 & 2 \end{pmatrix}, \quad b = \begin{pmatrix} 1 \\ 1 \end{pmatrix} $$
        The columns of $A$ are linearly dependent, so the system is rank-deficient. The normal equations are $A^\top A x = \begin{pmatrix} 5 & 5 \\ 5 & 5 \end{pmatrix} x = \begin{pmatrix} 3 \\ 3 \end{pmatrix}$, which has no solution. This highlights the instability of the normal equations in the rank-deficient case. The pseudoinverse, however, would provide the minimum-norm solution.
        </p>
      </div>
    </section>

    <!-- SECTION 10: IMPLEMENTATION GUIDE -->
    <section class="card" id="section-10" style="margin-bottom: 32px;">
      <h2>10. Implementation Mini-Guide (What to Actually Do in Code)</h2>

      <ul>
        <li>For standard, well-conditioned problems, <b>QR decomposition</b> is the recommended method. It is numerically stable and computationally efficient.</li>
        <li>For problems that are ill-conditioned or rank-deficient, the <b>SVD method</b> is the most robust choice. It is also necessary when the minimum-norm solution is required.</li>
        <li>Avoid explicitly forming the product $A^\^\top A$ and solving the normal equations, as this squares the condition number and can lead to a loss of numerical precision.</li>
        <li>Before solving, it is often a good practice to <b>preprocess data</b> by centering and scaling features. This can significantly improve the condition number.</li>
        <li>Always <b>verify the solution</b> by checking that the residual is orthogonal to the column space: $A^\^\top(b - Ax^\*) \approx 0$.</li>
      </ul>
    </section>

    <!-- SECTION 11: EXERCISES -->
    <section class="card" id="section-11" style="margin-bottom: 32px;">
      <h2>11. Exercises</h2>

      <h3>A. Fundamentals</h3>

      <div style="margin: 24px 0; padding: 16px; background: #fafafa; border: 1px solid #e0e0e0; border-radius: 8px;">
        <h4>A1. Prove that the nullspace of a matrix $A$, $\mathcal{N}(A)$, is a subspace.</h4>
        <p><b>Solution:</b>
        <br>1. <b>Closure under addition:</b> Let $x, y \in \mathcal{N}(A)$. Then $A(x+y) = Ax + Ay = 0 + 0 = 0$. So $x+y \in \mathcal{N}(A)$.
        <br>2. <b>Closure under scalar multiplication:</b> Let $x \in \mathcal{N}(A)$ and $\alpha$ be a scalar. Then $A(\alpha x) = \alpha(Ax) = \alpha 0 = 0$. So $\alpha x \in \mathcal{N}(A)$.
        <br>Since the nullspace is closed under addition and scalar multiplication, it is a subspace.
        </p>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: #fafafa; border: 1px solid #e0e0e0; border-radius: 8px;">
        <h4>A2. Show that for an orthogonal matrix $Q$, $\|Qx\|_2 = \|x\|_2$.</h4>
        <p><b>Solution:</b> The squared norm is $\|Qx\|_2^2 = (Qx)^\^\top(Qx) = x^\^\top Q^\^\top Q x$. Since $Q$ is orthogonal, $Q^\^\top Q = I$. Therefore, $\|Qx\|_2^2 = x^\^\top I x = x^\^\top x = \|x\|_2^2$. Taking the square root gives the result.</p>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: #fafafa; border: 1px solid #e0e0e0; border-radius: 8px;">
        <h4>A3. Apply the Gram-Schmidt process to find an orthonormal basis for the space spanned by $a_1 = (1, 1, 0)^\^\top$ and $a_2 = (1, 0, 1)^\^\top$.</h4>
        <p><b>Solution:</b>
        <br>1. <b>First vector:</b> $q_1 = a_1 / \|a_1\|_2 = (1/\sqrt{2}, 1/\sqrt{2}, 0)^\^\top$.
        <br>2. <b>Second vector:</b> $\tilde{q}_2 = a_2 - (q_1^\^\top a_2)q_1 = (1, 0, 1)^\^\top - (1/\sqrt{2})q_1 = (1/2, -1/2, 1)^\^\top$.
        <br>3. <b>Normalize:</b> $q_2 = \tilde{q}_2 / \|\tilde{q}_2\|_2 = (1/\sqrt{6}, -1/\sqrt{6}, 2/\sqrt{6})^\^\top$.
        <br>The orthonormal basis is $\{q_1, q_2\}$.
        </p>
      </div>

      <h3>B. Projections</h3>

      <div style="margin: 24px 0; padding: 16px; background: #fafafa; border: 1px solid #e0e0e0; border-radius: 8px;">
        <h4>B1. Let $S = \mathrm{span}\{u, v\}$ with $u, v$ independent. Derive the projector $P$ onto $S$.</h4>
        <p><b>Solution:</b> Form $A = [u \ v]$. If $A$ has full column rank, $P = A(A^\top A)^{-1}A^\top$.</p>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: #fafafa; border: 1px solid #e0e0e0; border-radius: 8px;">
        <h4>B2. Show that if $P$ is symmetric and idempotent, then it is an orthogonal projector.</h4>
        <p><b>Solution:</b> For any $b$, set $p = Pb$ and $r = b - Pb$. Then $Pr = 0$ (because $P^2 = P$); thus $r \in \mathcal{N}(P)$. For any $y = Pw \in \mathcal{R}(P)$, $y^\top r = w^\top P^\top(b - Pb) = w^\top P(b - Pb) = w^\top(Pb - P^2b) = 0$, so $r \perp \mathcal{R}(P)$. Hence $p$ is the orthogonal projection.</p>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: #fafafa; border: 1px solid #e0e0e0; border-radius: 8px;">
        <h4>B3. Project $b = (1, 2, 3)^\top$ onto the affine set $\{x \mid [1 \ 1 \ 1]x = 3\}$.</h4>
        <p><b>Solution:</b> One solution: find $x_0 = (1, 1, 1)^\top$ (satisfies the constraint). Compute $b - x_0 = (0, 1, 2)^\top$ and project onto $\mathcal{N}([1 \ 1 \ 1])$, i.e., vectors summing to zero. Using basis $z_1 = (1, -1, 0)$, $z_2 = (1, 0, -1)$, solve $\min_y \|Zy - (b - x_0)\|_2^2$, with $Z = [z_1 \ z_2]$. Recover $x^\star = x_0 + Zy^\star$.</p>
      </div>

      <h3>C. Least Squares</h3>

      <div style="margin: 24px 0; padding: 16px; background: #fafafa; border: 1px solid #e0e0e0; border-radius: 8px;">
        <h4>C1. Derive normal equations and prove uniqueness iff $\mathrm{rank}(A) = n$.</h4>
        <p><b>Solution:</b> Already shown; uniqueness follows from strict convexity of $x \mapsto \|Ax - b\|_2^2$ when $A^\top A \succ 0$.</p>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: #fafafa; border: 1px solid #e0e0e0; border-radius: 8px;">
        <h4>C2. Show that the residual at the LS solution is orthogonal to each column of $A$.</h4>
        <p><b>Solution:</b> Columns of $A$ span $\mathcal{R}(A)$. Orthogonality condition $A^\top(b - Ax^\star) = 0$ means $r^\star \perp \mathcal{R}(A)$.</p>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: #fafafa; border: 1px solid #e0e0e0; border-radius: 8px;">
        <h4>C3. Solve a small overdetermined system by (i) normal equations, (ii) QR, (iii) SVD, and compare answers.</h4>
        <p><b>Solution:</b> Pick any $3 \times 2$ with independent columns; show all three methods agree to numerical precision. Discuss time and conditioning.</p>
      </div>

      <div style="margin: 24px 0; padding: 16px; background: #fafafa; border: 1px solid #e0e0e0; border-radius: 8px;">
        <h4>C4. Show that weighted least squares with a diagonal weight matrix $W = \mathrm{diag}(w_i)$ is equivalent to scaling the rows of the system by $\sqrt{w_i}$.</h4>
        <p><b>Solution:</b> The objective is $(Ax-b)^\top W (Ax-b)$. Let $C = \mathrm{diag}(\sqrt{w_i})$. Then $W=C^\top C$, and the objective becomes $\|C(Ax-b)\|_2^2 = \|(CA)x - (Cb)\|_2^2$. This is a standard least squares problem on a modified system where the rows have been scaled.</p>
      </div>
    </section>

    <!-- SECTION 12: SANITY CHECKLIST -->
    <section class="card" id="section-12" style="margin-bottom: 32px;">
      <h2>12. Practical Checklist</h2>

      <ul>
        <li>Always check the <b>dimensions</b> of your matrices and vectors.</li>
        <li>For least squares, check if your matrix has <b>full column rank</b> to determine if the solution is unique.</li>
        <li>Prefer <b>QR</b> or <b>SVD</b> over the normal equations for better numerical stability.</li>
        <li>If your results are unstable, check the <b>condition number</b> of your matrix and consider preprocessing your data.</li>
        <li>Remember the geometric interpretation: the least squares solution is a <b>projection</b>.</li>
      </ul>

      <div style="margin: 24px 0; padding: 16px; background: #ffebee; border-left: 4px solid #c62828; border-radius: 4px;">
        <h4>Scope of this Lecture</h4>
        <p>This lecture is focused exclusively on linear algebra. We have not covered concepts from optimization, such as convex sets, duality, or constrained optimization algorithms. These topics will be covered in subsequent lectures.</p>
      </div>
    </section>

    <!-- SECTION 13: QUICK REFERENCE -->
    <section class="card" id="section-13" style="margin-bottom: 32px;">
      <h2>13. Key Formulas</h2>

      <div style="margin: 24px 0; padding: 16px; background: #f5f5f5; border: 2px solid var(--brand); border-radius: 8px;">
        <ul>
          <li><b>Normal Equations:</b> $A^\top A x^\star = A^\top b$</li>
          <li><b>Projection onto $\mathcal{R}(A)$:</b> $P = A(A^\top A)^{-1}A^\top$</li>
          <li><b>QR Solution:</b> $Rx^\star = Q^\top b$</li>
          <li><b>Pseudoinverse:</b> $A^+ = V\Sigma^+ U^\top$</li>
          <li><b>Condition Number:</b> $\kappa(A) = \sigma_{\max} / \sigma_{\min}$</li>
        </ul>
      </div>
    </section>

    <!-- READINGS -->
    <section class="card" id="readings" style="margin-bottom: 32px;">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required Reading:</strong> Boyd & Vandenberghe, <em>Convex Optimization</em>, Appendix A.</li>
        <li><strong>Video Resource:</strong> Gilbert Strang's "The Fundamental Theorem of Linear Algebra" (MIT OpenCourseWare)</li>
        <li><strong>Additional Reference:</strong> Trefethen & Bau, <em>Numerical Linear Algebra</em> (for deep dive into QR, SVD, conditioning)</li>
      </ul>
    </section>

  </main>

  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">© <span id="year"></span> Convex Optimization Course · <a href="../../README.md" style="color: var(--brand);">About</a> · <a href="../01-introduction/index.html" style="color: var(--brand);">Next: Introduction →</a></p>
    </div>
  </footer>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initNormGeometryVisualizer } from './widgets/js/norm-geometry-visualizer.js';
    initNormGeometryVisualizer('widget-norm-geometry');
  </script>
  <script type="module">
    import { initOrthogonality } from './widgets/js/orthogonality.js';
    initOrthogonality('widget-orthogonality');
  </script>
  <script type="module">
    import { initRankNullspace } from './widgets/js/rank-nullspace.js';
    initRankNullspace('widget-rank-nullspace');
  </script>
  <script type="module">
    import { initEigenPsd } from './widgets/js/eigen-psd.js';
    initEigenPsd('widget-eigen-psd');
  </script>
  <script type="module">
    import { initSvdApproximator } from './widgets/js/svd-approximator.js';
    initSvdApproximator('widget-svd-approximator');
  </script>
  <script type="module">
    import { initMatrixExplorer } from './widgets/js/matrix-explorer.js';
    initMatrixExplorer('widget-matrix-explorer');
  </script>
  <script type="module">
    import { initHessianLandscapeVisualizer } from './widgets/js/hessian-landscape-visualizer.js';
    initHessianLandscapeVisualizer('widget-hessian-landscape');
  </script>
  <script type="module">
    import { initConditionNumber } from './widgets/js/condition-number.js';
    initConditionNumber('widget-condition-number');
  </script>
</body>
</html>
