<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>00. A Primer on Linear Algebra — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.128/build/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">
          <img src="../../static/assets/branding/logo.svg" alt="Logo" />
          <span>Convex Optimization</span>
        </a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../01-introduction/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header card-v2">
      <h1>00. A Primer on Linear Algebra</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-14</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, review, linear-algebra, foundational</span>
      </div>
      <div class="lecture-summary">
        <p><strong>Overview:</strong> This lecture establishes the rigorous linear algebra foundation required for convex optimization. We move beyond basic matrix operations to understand the geometry of linear maps, spectral theory, matrix norms, and matrix calculus. This "zero to hero" guide covers everything from the fundamental subspaces to the derivatives of log-determinants.</p>
      </div>
    </header>

    <!-- SECTION 0: NOTATION AND PRIMITIVES -->
    <section class="card-v2" id="section-0">
      <h2>0. Notation and Primitives</h2>

      <p>We work over the real numbers $\mathbb{R}$ unless otherwise stated. Vectors are column vectors.</p>

      <h3>Inner Product and Norm</h3>
      <p>The standard <b>Euclidean inner product</b> (dot product) on $\mathbb{R}^n$ is:</p>
      $$ \langle x, y \rangle = x^\top y = \sum_{i=1}^n x_i y_i $$
      <p>The induced <b>Euclidean norm</b> is:</p>
      $$ \|x\|_2 = \sqrt{\langle x, x \rangle} = \sqrt{x^\top x} $$

      <h3>Angles and Orthogonality</h3>
      <p>For nonzero vectors $x, y$, the angle $\theta \in [0, \pi]$ is defined by:</p>
      $$ \cos \theta = \frac{\langle x, y \rangle}{\|x\|_2 \|y\|_2} $$
      <ul>
        <li><b>Orthogonal:</b> $\langle x, y \rangle = 0$ ($\theta = \pi/2$).</li>
        <li><b>Parallel:</b> $|\langle x, y \rangle| = \|x\|_2 \|y\|_2$ ($\theta = 0$ or $\pi$).</li>
      </ul>

      <h3>Matrix Inner Product and Norms</h3>
      <p>For matrices $A, B \in \mathbb{R}^{m \times n}$, we use the <b>Frobenius inner product</b>:</p>
      $$ \langle A, B \rangle_F = \mathrm{tr}(A^\top B) = \sum_{i,j} A_{ij} B_{ij} $$
      <p>This induces the <b>Frobenius norm</b> (the Euclidean norm of the vectorized matrix):</p>
      $$ \|A\|_F = \sqrt{\mathrm{tr}(A^\top A)} = \sqrt{\sum_{i,j} A_{ij}^2} $$

      <h3>Operator (Spectral) Norm</h3>
      <p>The <b>operator norm</b> induced by the Euclidean vector norm is the maximum "stretch" of the matrix:</p>
      $$ \|A\| = \sup_{x \ne 0} \frac{\|Ax\|_2}{\|x\|_2} = \max_{\|x\|_2=1} \|Ax\|_2 $$
      <p>This is distinct from the Frobenius norm. We will prove later that $\|A\| = \sigma_{\max}(A)$.</p>

      <h3>Positive Semidefinite (PSD) Matrices</h3>
      <p>A symmetric matrix $X \in \mathbb{S}^n$ is:</p>
      <ul>
        <li><b>Positive Semidefinite ($X \succeq 0$):</b> $v^\top X v \ge 0$ for all $v \in \mathbb{R}^n$.</li>
        <li><b>Positive Definite ($X \succ 0$):</b> $v^\top X v > 0$ for all $v \ne 0$.</li>
      </ul>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Rank & Nullspace</h3>
        <p>Explore the fundamental subspaces that define the structure of linear maps.</p>
        <div id="widget-rank-nullspace" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 1: DETERMINANT, TRACE, EIGENFACTS -->
    <section class="card-v2" id="section-1">
      <h2>1. Determinant, Trace, and Eigenvalues</h2>

      <h3>Characteristic Polynomial and Spectral Identities</h3>
      <p>For a square matrix $A \in \mathbb{R}^{n \times n}$, the <b>characteristic polynomial</b> is defined as $p_A(t) = \det(tI - A)$. Over $\mathbb{C}$, it factors into linear terms involving the eigenvalues $\lambda_i$:</p>
      $$ p_A(t) = \prod_{i=1}^n (t - \lambda_i) $$
      <p>Expanding the polynomial yields two fundamental identities matching the coefficients:</p>
      <div class="math-box">
        $$ \sum_{i=1}^n \lambda_i = \mathrm{tr}(A) \qquad \text{and} \qquad \prod_{i=1}^n \lambda_i = \det(A) $$
      </div>

      <h3>Shift and Scale Properties</h3>
      <p>If $Av = \lambda v$, then:</p>
      <ul>
        <li><b>Shift:</b> $(A + tI)v = (\lambda + t)v$. The spectrum shifts by $t$.</li>
        <li><b>Scale:</b> $(cA)v = (c\lambda)v$. The spectrum scales by $c$.</li>
      </ul>

      <h3>Similarity Invariants</h3>
      <p>Two matrices $A, B$ are <b>similar</b> if $B = PAP^{-1}$ for some invertible $P$. Similar matrices represent the same linear operator in different bases. They share the same:</p>
      <ul>
        <li>Spectrum (Eigenvalues)</li>
        <li>Trace</li>
        <li>Determinant</li>
      </ul>
      <div class="proof-enhanced">
        <h4>Proof of Trace Invariance</h4>
        <p>Using the cyclicity of the trace ($\mathrm{tr}(XY) = \mathrm{tr}(YX)$):</p>
        $$ \mathrm{tr}(PAP^{-1}) = \mathrm{tr}(A P^{-1} P) = \mathrm{tr}(A) $$
      </div>

      <h3>Trace Cyclicity and Determinant Multiplicativity</h3>
      <p>Fundamental properties to memorize:</p>
      $$ \mathrm{tr}(AB) = \mathrm{tr}(BA) \qquad \det(AB) = \det(A)\det(B) $$
      <p><b>Non-laws (Traps to Avoid):</b></p>
      <ul>
        <li>$\mathrm{tr}(AB) \neq \mathrm{tr}(A)\mathrm{tr}(B)$ generally.</li>
        <li>$\det(A+B) \neq \det(A) + \det(B)$ generally.</li>
      </ul>
    </section>

    <!-- SECTION 2: GEOMETRY OF LINEAR MAPS -->
    <section class="card-v2" id="section-2">
      <h2>2. Geometry of Linear Maps</h2>

      <h3>Unit Ball to Ellipsoid</h3>
      <p>A linear map $A$ transforms the Euclidean unit ball $B = \{x : \|x\|_2 \le 1\}$ into an <b>ellipsoid</b> $E = A(B)$.</p>
      <ul>
        <li>If $A$ is invertible, $E = \{ y : y^\top (A^{-\top} A^{-1}) y \le 1 \}$.</li>
        <li>The volume scales by the determinant: $\mathrm{vol}(E) = |\det A| \cdot \mathrm{vol}(B)$.</li>
      </ul>

      <h3>Angle and Length Preservation</h3>
      <p><b>Orthogonal Matrices ($A^\top A = I$):</b> These preserve inner products (and thus angles) and lengths:</p>
      $$ \langle Ax, Ay \rangle = x^\top A^\top A y = x^\top I y = \langle x, y \rangle $$
      <p>This implies $\|Ax\|_2 = \|x\|_2$. Orthogonal matrices represent rigid motions (rotations and reflections).</p>

      <p><b>Conformal Maps:</b> If a map preserves <i>angles</i> but allows lengths to scale uniformly, then:</p>
      $$ A^\top A = \alpha I \quad \text{for some } \alpha > 0 $$
      <div class="proof-enhanced">
        <h4>Proof Idea</h4>
        <p>Angle preservation implies $\frac{\langle Ax, Ay \rangle}{\|Ax\|\|Ay\|} = \frac{\langle x, y \rangle}{\|x\|\|y\|}$. Using the polarization identity, one can deduce that $\langle Ax, Ay \rangle = \alpha \langle x, y \rangle$ for a constant $\alpha$. This requires $x^\top (A^\top A - \alpha I) y = 0$ for all $x, y$, forcing $A^\top A = \alpha I$.</p>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Explorer: Matrix Geometry & Orthogonality</h3>
        <p>Visualize how linear maps transform the unit circle and affect orthogonality.</p>
        <div id="widget-matrix-geometry" style="width: 100%; position: relative;"></div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Orthogonality & Projections</h3>
        <p>Explore the geometric relationship between vectors, including dot products and projections.</p>
        <div id="widget-orthogonality" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 3: NORMS ON MATRICES -->
    <section class="card-v2" id="section-3">
      <h2>3. Matrix Norms and Spectral Value</h2>

      <h3>Operator Norm is Top Stretch</h3>
      <p>We formally defined the operator norm as $\|A\| = \max_{\|x\|=1} \|Ax\|_2$. We can compute it via optimization:</p>
      $$ \|A\|^2 = \max_{\|x\|_2=1} \|Ax\|_2^2 = \max_{\|x\|_2=1} x^\top (A^\top A) x $$
      <p>This is the Rayleigh quotient for the symmetric matrix $A^\top A$. The maximum value is the largest eigenvalue, $\lambda_{\max}(A^\top A)$. Thus:</p>
      <div class="math-box">
        $$ \|A\| = \sqrt{\lambda_{\max}(A^\top A)} = \sigma_1(A) $$
      </div>

      <h3>Submultiplicativity</h3>
      <p>For any induced norm, $\|AB\| \le \|A\| \|B\|$.
      <br><i>Proof:</i> $\|ABx\| \le \|A\| \|Bx\| \le \|A\| \|B\| \|x\|$. Taking the sup over unit $x$ proves the claim.</p>

      <h3>Frobenius vs. Spectral Norm</h3>
      <p>Let $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r > 0$ be the singular values of $A$.</p>
      <ul>
        <li><b>Spectral Norm:</b> $\|A\| = \sigma_1$</li>
        <li><b>Frobenius Norm:</b> $\|A\|_F = \sqrt{\sum_{i,j} A_{ij}^2} = \sqrt{\mathrm{tr}(A^\top A)} = \sqrt{\sum_i \sigma_i^2}$</li>
      </ul>
      <p>Key inequalities:</p>
      $$ \|A\| \le \|A\|_F \le \sqrt{\mathrm{rank}(A)} \|A\| $$

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Visualizer: Norm Geometry</h3>
        <p>Explore unit balls for different vector norms.</p>
        <div id="widget-norm-geometry" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 4: SVD AND POLAR DECOMPOSITION -->
    <section class="card-v2" id="section-4">
      <h2>4. SVD and Polar Decomposition</h2>

      <h3>Singular Value Decomposition (SVD)</h3>
      <p>Any matrix $A \in \mathbb{R}^{m \times n}$ can be factored as:</p>
      $$ A = U \Sigma V^\top $$
      <ul>
        <li>$U \in \mathbb{R}^{m \times m}$ is orthogonal ($U^\top U = I$).</li>
        <li>$V \in \mathbb{R}^{n \times n}$ is orthogonal ($V^\top V = I$).</li>
        <li>$\Sigma \in \mathbb{R}^{m \times n}$ is diagonal with entries $\sigma_1 \ge \sigma_2 \ge \dots \ge 0$.</li>
      </ul>
      <p><b>Geometric Interpretation:</b> A linear map is a rotation ($V^\top$), followed by an axis-aligned stretch ($\Sigma$), followed by another rotation ($U$).</p>

      <h3>Polar Decomposition</h3>
      <p>Every square matrix $A$ can be decomposed into a rotation and a symmetric stretch, analogous to $z = e^{i\theta}r$ for complex numbers:</p>
      $$ A = QH $$
      <ul>
        <li>$Q$ is orthogonal (rotation/reflection).</li>
        <li>$H = (A^\top A)^{1/2}$ is positive semidefinite (stretch).</li>
      </ul>
      <p>If $A$ is invertible, this factorization is unique.</p>

      <div class="proof-enhanced">
        <h4>Corollary: Metric Equivalence implies Orthogonal Relation</h4>
        <p>If $A^\top A = B^\top B$ and $B$ is invertible, then $A B^{-1}$ is orthogonal.</p>
        <p><i>Proof:</i> Let $Q = A B^{-1}$. Then:
        $$ Q^\top Q = (B^{-1})^\top A^\top A B^{-1} = (B^{-1})^\top (B^\top B) B^{-1} = (B B^{-1})^\top (B B^{-1}) = I $$
        Thus $Q$ is orthogonal.</p>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: SVD Approximator</h3>
        <p>Visualize low-rank approximation using SVD.</p>
        <div id="widget-svd-approximator" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Condition Number</h3>
        <p>Explore how the ratio of singular values ($\sigma_{\max}/\sigma_{\min}$) affects numerical stability.</p>
        <div id="widget-condition-number" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 5: MATRIX CALCULUS -->
    <section class="card-v2" id="section-5">
      <h2>5. Calculus with Matrices</h2>

      <p>Optimization requires differentiating scalar functions of matrices. We use the Fréchet derivative notation: $Df(X)[H]$ is the directional derivative of $f$ at $X$ in direction $H$. This is equivalent to $\frac{d}{dt} f(X + tH) \big|_{t=0}$.</p>

      <h3>Basic Derivatives</h3>
      <table class="data-table">
        <thead>
          <tr>
            <th>Function $f(X)$</th>
            <th>Directional Derivative $Df(X)[H]$</th>
            <th>Gradient $\nabla f(X)$</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>$\mathrm{tr}(AX)$</td>
            <td>$\mathrm{tr}(AH)$</td>
            <td>$A^\top$</td>
          </tr>
          <tr>
            <td>$\mathrm{tr}(X^\top A X)$</td>
            <td>$\mathrm{tr}(H^\top A X + X^\top A H)$</td>
            <td>$(A + A^\top)X$</td>
          </tr>
          <tr>
            <td>$\log \det X$ ($X \succ 0$)</td>
            <td>$\mathrm{tr}(X^{-1}H)$</td>
            <td>$X^{-1}$ (for symmetric $X$)</td>
          </tr>
        </tbody>
      </table>

      <h3>Derivative of the Inverse</h3>
      <p>For invertible $X$, the map $f(X) = X^{-1}$ has derivative:</p>
      $$ D(X^{-1})[H] = -X^{-1} H X^{-1} $$
      <div class="proof-enhanced">
        <h4>Derivation</h4>
        <p>Differentiate the identity $X X^{-1} = I$:</p>
        $$ D(X X^{-1})[H] = D(X)[H] X^{-1} + X D(X^{-1})[H] = 0 $$
        $$ H X^{-1} + X D(X^{-1})[H] = 0 $$
        $$ X D(X^{-1})[H] = - H X^{-1} \implies D(X^{-1})[H] = -X^{-1} H X^{-1} $$
      </div>

      <h3>Concavity of Log-Determinant</h3>
      <p>This is a fundamental result for interior point methods.</p>
      <p>Consider the function along a line $Y(t) = X + tV$ where $X \succ 0$.
      $$ g(t) = \log \det (X + tV) $$
      </p>
      <ul>
        <li><b>First Derivative:</b> $g'(t) = \mathrm{tr}(Y(t)^{-1}V)$.</li>
        <li><b>Second Derivative:</b>
          $$ g''(t) = \mathrm{tr}\left( \frac{d}{dt}(Y(t)^{-1}) V \right) = \mathrm{tr}\left( -Y(t)^{-1} V Y(t)^{-1} V \right) $$
          $$ = -\mathrm{tr}\left( (Y(t)^{-1/2} V Y(t)^{-1/2})^2 \right) = -\|Y(t)^{-1/2} V Y(t)^{-1/2}\|_F^2 \le 0 $$
        </li>
      </ul>
      <p>Since the second derivative is non-positive, $\log \det X$ is <b>concave</b>.</p>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Hessian Landscape</h3>
        <p>Visualize the curvature of functions (Hessians) and how positive definiteness relates to local minima.</p>
        <div id="widget-hessian-landscape" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 5.5: LEAST SQUARES (APPLICATION) -->
    <section class="card-v2" id="section-5-5">
      <h2>5.5 Application: Least Squares</h2>
      <p>The least squares problem $\min_x \|Ax - b\|_2^2$ is the canonical application of these concepts. The objective function is convex (Hessian $2A^\top A \succeq 0$). The solution is characterized by the normal equations $A^\top A x = A^\top b$, which represents the orthogonal projection of $b$ onto the range of $A$.</p>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Least Squares Projection</h3>
        <p>Visualize the geometry of the least squares solution as an orthogonal projection.</p>
        <div id="widget-least-squares" style="width: 100%; height: 500px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 6: WORKED PROBLEMS -->
    <section class="card-v2" id="section-6">
      <h2>6. Worked Problems (From "Linear Algebra You Actually Use")</h2>

      <div class="problem">
        <h3>Problem 8: Trace Commutator</h3>
        <p><b>(a)</b> Prove $\mathrm{tr}(AB) = \mathrm{tr}(BA)$.
        <br><i>Solution:</i> $\mathrm{tr}(AB) = \sum_i \sum_k A_{ik} B_{ki} = \sum_k \sum_i B_{ki} A_{ik} = \mathrm{tr}(BA)$.</p>
        <p><b>(b)</b> Prove no matrices satisfy $AB - BA = I$.
        <br><i>Solution:</i> Take trace: $\mathrm{tr}(AB - BA) = \mathrm{tr}(AB) - \mathrm{tr}(BA) = 0$. But $\mathrm{tr}(I) = n$. Contradiction.</p>
      </div>

      <div class="problem">
        <h3>Problem 9: Hilbert-Schmidt (Frobenius) Inner Product</h3>
        <p>Let $\langle A, B \rangle = \mathrm{tr}(A^\top B)$.</p>
        <p><b>(a)</b> Show this is an inner product. (Symmetric, Linear, Positive Definite).
        <br><b>(b)</b> Cauchy-Schwarz: $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.
        <br><b>(c)</b> Submultiplicativity: $\|AB\|_F \le \|A\|_2 \|B\|_F \le \|A\|_F \|B\|_F$.
        <br><i>Proof of (c):</i> $\|AB\|_F^2 = \mathrm{tr}(B^\top A^\top A B) = \mathrm{tr}(A^\top A B B^\top)$. Using trace inequalities, this is bounded by $\lambda_{\max}(A^\top A) \mathrm{tr}(BB^\top) = \|A\|_2^2 \|B\|_F^2$.</p>
      </div>

      <div class="problem">
        <h3>Problem 10: Operator Norm Characterization</h3>
        <p>Prove $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$.</p>
        <p><i>Solution:</i> See Section 3. Maximizing $x^\top A^\top A x$ on the unit sphere yields the largest eigenvalue of the symmetric matrix $A^\top A$.</p>
      </div>

      <div class="problem">
        <h3>Problem 12: Structure of $O(n)$</h3>
        <p>Let $O(n) = \{ Q \in \mathbb{R}^{n \times n} \mid Q^\top Q = I \}$.</p>
        <ul>
          <li><b>Group:</b> Closed under multiplication and inverse.</li>
          <li><b>Compactness:</b> Bounded ($\|Q\|_F^2 = n$) and closed (preimage of $I$).</li>
          <li><b>Characterization:</b> If a linear map preserves inner products, it is orthogonal. If it preserves lengths, it is orthogonal.</li>
        </ul>
      </div>

      <div class="problem">
        <h3>Problem 13: Spectral Theorem</h3>
        <p>Prove that symmetric matrices have real eigenvalues.</p>
        <p><i>Solution:</i> Let $Av = \lambda v$. Then $v^* A v = \lambda v^* v$. Since $A$ is real symmetric, $v^* A v$ is real (it equals its conjugate transpose). Since $v^* v > 0$, $\lambda$ must be real.</p>
      </div>

      <div class="problem">
        <h3>Problem 14: Spectral Radius</h3>
        <p>Prove Gelfand's formula: $\rho(A) = \lim_{k \to \infty} \|A^k\|^{1/k}$.</p>
        <p><i>Intuition:</i> For large $k$, $A^k$ is dominated by the largest eigenvalue $\lambda_{\max}^k$. The $k$-th root recovers $|\lambda_{\max}| = \rho(A)$.</p>
      </div>
    </section>

    <section class="card-v2" id="readings">
      <h2><i data-feather="book-open"></i> Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required Reading:</strong> Boyd & Vandenberghe, <em>Convex Optimization</em>, Appendix A.</li>
        <li><strong>Reference:</strong> Trefethen & Bau, <em>Numerical Linear Algebra</em> (Lecture 1-5).</li>
      </ul>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initNormGeometryVisualizer } from './widgets/js/norm-geometry-visualizer.js';
    initNormGeometryVisualizer('widget-norm-geometry');
  </script>
  <script type="module">
    import { initRankNullspace } from './widgets/js/rank-nullspace.js';
    initRankNullspace('widget-rank-nullspace');
  </script>
  <script type="module">
    import { initMatrixGeometry } from './widgets/js/matrix-geometry.js';
    initMatrixGeometry('widget-matrix-geometry');
  </script>
  <script type="module">
    import { initOrthogonality } from './widgets/js/orthogonality.js';
    initOrthogonality('widget-orthogonality');
  </script>
  <script type="module">
    import { initSvdApproximator } from './widgets/js/svd-approximator.js';
    initSvdApproximator('widget-svd-approximator');
  </script>
  <script type="module">
    import { initConditionNumber } from './widgets/js/condition-number.js';
    initConditionNumber('widget-condition-number');
  </script>
  <script type="module">
    import { initHessianLandscapeVisualizer } from './widgets/js/hessian-landscape-visualizer.js';
    initHessianLandscapeVisualizer('widget-hessian-landscape');
  </script>
  <script type="module">
    import { initLeastSquaresVisualizer } from './widgets/js/least-squares-visualizer.js';
    initLeastSquaresVisualizer('widget-least-squares');
  </script>
</body>
</html>
