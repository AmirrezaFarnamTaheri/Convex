<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>00. A Primer on Linear Algebra â€” Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.128/build/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../01-introduction/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>00. A Primer on Linear Algebra</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-14</a>
        <span>Duration: 90 min</a>
        <span>Tags: prerequisites, review, linear-algebra, foundational</a>
      </div>
      <div class="lecture-summary">
        <p><strong>Overview:</strong> This lecture establishes the linear algebra foundation required for convex optimization. Beginning with vectors and matrices, we progress through inner products, norms, orthogonality, and projections, building the mathematical background necessary for optimization theory.</p>
        <p><strong>Prerequisites:</strong> None.</p>
        <p><strong>Forward Connections:</strong> Projection techniques introduced here appear in least squares methods (<a href="../01-introduction/index.html">Lecture 01</a>). Positive Semidefinite (PSD) matrices form the basis for convex quadratic programs (<a href="../01-introduction/index.html">Lecture 01</a>). The four fundamental subspaces provide the geometric understanding of feasible sets (<a href="../02-convex-sets/index.html">Lecture 02</a>).</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to work with:</p>
      <ul>
        <li><b>Fundamental Objects and Subspaces:</b> Vectors, matrices, the four fundamental subspaces, and the Rank-Nullity Theorem.</li>
        <li><b>Inner Products and Norms:</b> Standard and generalized inner products, canonical vector norms, dual norms, HÃ¶lder's inequality, and the Cauchy-Schwarz inequality.</li>
        <li><b>Orthogonality and Projections:</b> Orthonormal bases, Gram-Schmidt process, QR decomposition, and projection onto subspaces and affine sets.</li>
        <li><b>Positive Semidefinite Matrices:</b> Definitions, eigenvalue characterization, quadratic forms, ellipsoids, and the Loewner order.</li>
        <li><b>Robust Numerical Methods:</b> Least squares via normal equations, QR, SVD, pseudoinverse, and condition numbers.</li>
        <li><b>Matrix Structures:</b> Trace inner product, Frobenius norm, and componentwise order.</li>
      </ul>
    </section>


    <!-- SECTION 0: NOTATION -->
    <section class="section-card" id="section-0">
      <h2>0. Notation and Basic Objects</h2>


      <h3>Preliminaries: Lines and Line Segments</h3>
      <p>We work in $\mathbb{R}^n$ equipped with the standard inner product $\langle x, y \rangle = x^\top y = \sum x_i y_i$ and Euclidean norm $\|x\|_2 = \sqrt{\langle x, x \rangle}$. Geometric objects like lines and segments are fundamental to defining convexity.</p>

      <h4>Line through two points</h4>
      <p>Given two distinct points $x_1, x_2 \in \mathbb{R}^n$, the <b>line</b> passing through them is the set of all affine combinations of these points:</p>
      $$
      y = \theta x_1 + (1-\theta)x_2, \quad \theta \in \mathbb{R}
      $$
      <p>As $\theta$ varies over $\mathbb{R}$, $y$ traces out the infinite line.</p>

      <h4>Line segment between two points</h4>
      <p>The <b>line segment</b> connecting $x_1$ and $x_2$, denoted $[x_1, x_2]$, corresponds to restricting the parameter $\theta$ to the interval $[0, 1]$:</p>
      $$
      y = \theta x_1 + (1-\theta)x_2, \quad 0 \le \theta \le 1
      $$
      <p>This parametrization is key: a set is convex if and only if it contains the line segment between any pair of its points.</p>

      <figure style="text-align: center;">
        <img src="assets/line-vs-segment.png"
             alt="Comparison of a line segment (blue) versus the infinite line (grey) passing through two points"
             style="max-width: 60%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 0.1:</i> The line segment (solid blue, $0 \le \theta \le 1$) is a subset of the infinite affine line (dashed grey, $\theta \in \mathbb{R}$).</figcaption>
      </figure>

      <h3>Scalars, Vectors, Matrices</h3>
      <ul>
        <li><b>Scalars:</b> Real numbers $a \in \mathbb{R}$.</li>
        <li><b>Column vectors:</b> $x \in \mathbb{R}^n$ are $n \times 1$ matrices with entries $x_i$.
          <br><i>Note on convention:</i> We treat vectors as columns to maintain algebraic consistency with matrix-vector multiplication $Ax$ representing linear maps $T(x)$. Treating $x$ as a row vector would necessitate the notation $xA^\top$. Throughout this course, unless specified otherwise, any vector $x$ is a column vector.</li>
        <li><b>Matrices:</b> $A \in \mathbb{R}^{m \times n}$; entry $a_{ij}$ lies in row $i$, column $j$. The first dimension $m$ is the number of rows, and $n$ is the number of columns.</li>
        <li><b>Transpose:</b> $A^\top \in \mathbb{R}^{n \times m}$ satisfies $(A^\top)_{ij} = a_{ji}$. It flips the matrix across its main diagonal.</li>
        <li><b>Identity:</b> $I_n$ is the $n \times n$ square matrix with ones on the diagonal and zeros elsewhere; it satisfies $I_n x = x$ for all $x \in \mathbb{R}^n$.</li>
        <li><b>Standard basis:</b> $e_1, \dots, e_n$ where $e_i$ has a 1 in position $i$ and zeros elsewhere. These are the columns of $I_n$. Every $x \in \mathbb{R}^n$ can be uniquely decomposed as $x = \sum_{i=1}^n x_i e_i$.</li>
      </ul>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/matrix-multiplication.svg"
             alt="Matrix multiplication diagram showing how rows and columns combine"
             style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 0.2:</i> Matrix multiplication visualizedâ€”how rows of the first matrix combine with columns of the second.</figcaption>
      </figure>

      <h3>Matrixâ€“Vector and Matrixâ€“Matrix Multiplication</h3>
      <ul>
        <li>$Ax$ is the <b>linear combination of columns of $A$</b> with coefficients from $x$.</li>
        <li>$(AB)_{ij} = \sum_k a_{ik} b_{kj}$.</li>
      </ul>

      <h3>Linear Maps</h3>
      <p>A function $T: \mathbb{R}^n \to \mathbb{R}^m$ is <b>linear</b> if and only if $T(\alpha x + \beta y) = \alpha T(x) + \beta T(y)$. Every linear map can be represented by a matrix $A$ with $T(x) = Ax$.</p>

      <h3>Componentwise Order Notation</h3>
      <p>A notation used throughout optimization is the componentwise order. For vectors $x, y \in \mathbb{R}^n$:</p>
      <ul>
        <li><b>$x \ge 0$</b> means $x_i \ge 0$ for all $i = 1, \dots, n$ (nonnegative).</li>
        <li><b>$x > 0$</b> means $x_i > 0$ for all $i = 1, \dots, n$ (strictly positive).</li>
        <li><b>$x \le y$</b> means $x_i \le y_i$ for all $i = 1, \dots, n$.</li>
      </ul>
      <p>This is the <b>standard partial order</b> on $\mathbb{R}^n$ induced by the nonnegative orthant $\mathbb{R}^n_+ = \{x \mid x \ge 0\}$. The vector $\mathbf{1}$ denotes the vector of all ones. This notation is essential for <a href="../02-convex-sets/index.html">Lecture 02</a>'s treatment of polyhedra and cones.</p>

      <h3>Matrix Calculus Basics</h3>
      <p>Optimization algorithms rely heavily on gradients and Hessians. Computing derivatives with respect to vectors and matrices is a core skill.</p>

      <h4>Gradients of Linear and Quadratic Forms</h4>
      <p>For $x \in \mathbb{R}^n$, $A \in \mathbb{R}^{n \times n}$, and $b \in \mathbb{R}^n$:</p>
      <table class="data-table">
        <thead>
          <tr>
            <th>Function $f(x)$</th>
            <th>Gradient $\nabla f(x)$</th>
            <th>Hessian $\nabla^2 f(x)$</th>
            <th>Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>$b^\top x$</td>
            <td>$b$</td>
            <td>$0$</td>
            <td>Linear function</td>
          </tr>
          <tr>
            <td>$x^\top A x$</td>
            <td>$(A + A^\top)x$</td>
            <td>$A + A^\top$</td>
            <td>General quadratic</td>
          </tr>
          <tr>
            <td>$x^\top A x$</td>
            <td>$2Ax$</td>
            <td>$2A$</td>
            <td>If $A$ is symmetric ($A=A^\top$)</td>
          </tr>
          <tr>
            <td>$\|Ax - b\|_2^2$</td>
            <td>$2A^\top(Ax - b)$</td>
            <td>$2A^\top A$</td>
            <td>Least Squares objective</td>
          </tr>
        </tbody>
      </table>

      <h4>Gradients with Respect to Matrices</h4>
      <p>For a matrix variable $X \in \mathbb{R}^{m \times n}$, the gradient is an $m \times n$ matrix.</p>
      <ul>
        <li><b>Rule 1: Linear Trace:</b> $\nabla_X \mathrm{tr}(A^\top X) = A$.
        <br><i>Proof:</i> $\mathrm{tr}(A^\top X) = \sum_{i,j} A_{ij} X_{ij}$. The partial derivative with respect to $X_{ij}$ is $A_{ij}$.</li>
        <li><b>Rule 2: Quadratic Trace:</b> $\nabla_X \mathrm{tr}(X^\top A X) = (A + A^\top)X$. If $A$ is symmetric, this is $2AX$.
        <br><i>Note:</i> This corresponds to $\frac{d}{dx}(ax^2) = 2ax$.</li>
        <li><b>Rule 3: Log Determinant:</b> $\nabla_X \log \det X = X^{-\top}$. If $X \in \mathbb{S}^n_{++}$ (symmetric positive definite), then $\nabla_X \log \det X = X^{-1}$.
        <br>
        <div class="proof-box">
          <h4>Derivation: Gradient of Log Determinant</h4>
        <p>We derive the gradient by identifying the linear term in the Taylor expansion of $f(X+\Delta)$ around $X$. This derivation relies on the connection between determinant, trace, and eigenvalues.</p>
          <div class="proof-step">
            <strong>Step 1: Factor out X.</strong>
            Assuming $X$ is invertible and $\Delta$ is small:
            $$ \log \det (X + \Delta) = \log \det (X(I + X^{-1}\Delta)) = \log (\det X \cdot \det(I + X^{-1}\Delta)) $$
          Using the property $\log(ab) = \log a + \log b$:
            $$ = \log \det X + \log \det(I + X^{-1}\Delta) $$
          </div>
          <div class="proof-step">
          <strong>Step 2: Relate Determinant to Trace.</strong>
          Let $\lambda_i$ be the eigenvalues of the matrix $E = X^{-1}\Delta$. Since the eigenvalues of $I+E$ are $1+\lambda_i$, we have:
          $$ \det(I + E) = \prod_{i=1}^n (1 + \lambda_i) $$
          Taking the log:
          $$ \log \det(I + E) = \sum_{i=1}^n \log(1 + \lambda_i) $$
        </div>
        <div class="proof-step">
          <strong>Step 3: Linearize via Taylor Series.</strong>
          For small $\lambda_i$, we use the Taylor expansion $\log(1+z) \approx z$.
          $$ \sum_{i=1}^n \log(1 + \lambda_i) \approx \sum_{i=1}^n \lambda_i = \mathrm{tr}(E) = \mathrm{tr}(X^{-1}\Delta) $$
          </div>
          <div class="proof-step">
          <strong>Step 4: Identify the Gradient.</strong>
            The first-order approximation is:
            $$ \log \det(X+\Delta) \approx \log \det X + \mathrm{tr}(X^{-1}\Delta) $$
          To extract the gradient, we write the trace term as an inner product $\langle G, \Delta \rangle = \mathrm{tr}(G^\top \Delta)$.
            $$ \mathrm{tr}(X^{-1}\Delta) = \mathrm{tr}((X^{-\top})^\top \Delta) = \langle X^{-\top}, \Delta \rangle $$
          Thus, by the definition of the gradient, $\nabla_X \log \det X = X^{-\top}$.
          </div>
        </div>
        </li>
      </ul>
      <div class="insight">
        <h4>ðŸ’¡ Chain Rule for Matrix Functions</h4>
        <p>If $f(X) = g(h(X))$, then the differential is $df = g'(h(X)) \circ dh$. For example, to differentiate $\|Ax - b\|_2^2$:</p>
        <ol>
          <li>Let $r = Ax - b$. Then $f = r^\top r$.</li>
          <li>$df = 2r^\top dr$.</li>
          <li>$dr = A dx$.</li>
          <li>Substitute: $df = 2r^\top A dx = (2A^\top r)^\top dx$.</li>
          <li>The gradient is the transpose of the coefficient of $dx^\top$ (or the vector multiplying $dx$), so $\nabla f = 2A^\top r = 2A^\top(Ax-b)$.</li>
        </ol>
      </div>

      <div class="proof-box">
        <h4>Derivation: Hessian of Least Squares</h4>
        <p>Let $f(x) = \|Ax - b\|_2^2$. We show $\nabla^2 f(x) = 2A^\top A$ and prove its convexity.</p>

        <div class="proof-step">
          <strong>Step 1: Expand.</strong>
          $$ f(x) = (Ax - b)^\top (Ax - b) = x^\top A^\top A x - 2b^\top A x + b^\top b $$
        </div>

        <div class="proof-step">
          <strong>Step 2: Gradient.</strong>
          Differentiating term by term:
          <ul>
            <li>$\nabla (x^\top A^\top A x) = 2A^\top A x$ (since $A^\top A$ is symmetric)</li>
            <li>$\nabla (-2b^\top A x) = \nabla (-2(A^\top b)^\top x) = -2A^\top b$</li>
            <li>$\nabla (b^\top b) = 0$</li>
          </ul>
          Summing these: $\nabla f(x) = 2A^\top A x - 2A^\top b = 2A^\top (Ax - b)$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Hessian.</strong>
          Differentiating $\nabla f(x)$ with respect to $x$:
          $$ \nabla^2 f(x) = \nabla_x (2A^\top A x) = 2A^\top A $$
          Note that the Hessian is a constant matrix (independent of $x$), which is typical for quadratic functions.
        </div>

        <div class="proof-step">
          <strong>Step 4: Convexity (PSD Check).</strong>
          For any vector $v \in \mathbb{R}^n$:
          $$ v^\top \nabla^2 f(x) v = v^\top (2A^\top A) v = 2(Av)^\top (Av) = 2\|Av\|_2^2 \ge 0 $$
          Since the quadratic form is always non-negative, the Hessian is Positive Semidefinite (PSD). Thus, the least squares objective is always <b>convex</b>.
        </div>
      </div>
    </section>

    <!-- SECTION 1: FOUR FUNDAMENTAL SUBSPACES -->
    <section class="section-card" id="section-1">
      <h2>1. Subspaces and the Four Fundamental Spaces</h2>

      <p>A <a href="#" class="definition-link">linear subspace</a> is a set of vectors that is closed under addition and scalar multiplication. Given a matrix $A \in \mathbb{R}^{m \times n}$, we can define four fundamental subspaces that characterize the behavior of the linear map $T(x) = Ax$.</p>
      <ul>
        <li><a href="#" class="definition-link" data-term="column space">Column Space (Range)</a>: $\mathcal{R}(A) = \{Ax \mid x \in \mathbb{R}^n\} \subseteq \mathbb{R}^m$. This is the set of all possible outputs of the linear map, i.e., the span of the columns of $A$.</li>
        <li><a href="#" class="definition-link" data-term="nullspace">Nullspace (Kernel)</a>: $\mathcal{N}(A) = \{x \in \mathbb{R}^n \mid Ax = 0\}$. This is the set of all input vectors that are mapped to the zero vector.</li>
        <li><b>Row Space:</b> $\mathcal{R}(A^\top) \subseteq \mathbb{R}^n$. This is the span of the rows of $A$.</li>
        <li><b>Left Nullspace:</b> $\mathcal{N}(A^\top) = \{y \in \mathbb{R}^m \mid A^\top y = 0\}$. This is the nullspace of the transpose of $A$.</li>
      </ul>

      <p>These subspaces are linked by two crucial orthogonality relationships:</p>
      <ul>
        <li>$\mathcal{R}(A) \perp \mathcal{N}(A^\top)$ in $\mathbb{R}^m$</li>
        <li>$\mathcal{R}(A^\top) \perp \mathcal{N}(A)$ in $\mathbb{R}^n$</li>
      </ul>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/linear-subspaces.svg"
             alt="Visual representation of intersecting planes showing linear subspaces"
             style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 1.1:</i> Three planes in 3D spaceâ€”the intersection represents solution sets of linear equations.</figcaption>
      </figure>

      <div class="proof-box">
        <h4>Proof: $\mathcal{R}(A) \perp \mathcal{N}(A^\top)$</h4>

        <div class="proof-step">
          <strong>Setup:</strong> Let $v \in \mathcal{R}(A)$ and $y \in \mathcal{N}(A^\top)$. By definition, $v = Ax$ for some $x \in \mathbb{R}^n$, and $A^\top y = 0$.
        </div>

        <div class="proof-step">
          <strong>Compute inner product:</strong> We compute the dot product $\langle v, y \rangle$:
          $$ v^\top y = (Ax)^\top y = x^\top A^\top y = x^\top (A^\top y) $$
        </div>

        <div class="proof-step">
          <strong>Substitute Nullspace Condition:</strong> Since $y \in \mathcal{N}(A^\top)$, we have $A^\top y = 0$.
          $$ x^\top (0) = 0 $$
        </div>

        <div class="proof-step">
          <strong>Conclusion:</strong> The inner product is zero for any $v \in \mathcal{R}(A)$ and $y \in \mathcal{N}(A^\top)$. Thus, the subspaces are orthogonal.
        </div>
      </div>

      <h3>Rank and Rankâ€“Nullity Theorem</h3>
      <p>$\mathrm{rank}(A) = \dim \mathcal{R}(A)$. For $A \in \mathbb{R}^{m \times n}$:
      $$ \dim \mathcal{N}(A) + \mathrm{rank}(A) = n $$
      This fundamental result relates the dimensions of the domain's subspaces to the rank of the mapping.</p>

      <div class="proof-box">
        <h4>Proof of the Rank-Nullity Theorem</h4>

        <div class="proof-step">
          <strong>Step 1: Construct a basis for the Nullspace.</strong> Let $k = \dim(\mathcal{N}(A))$ be the nullity. Let $\{v_1, \dots, v_k\}$ be a basis for the nullspace $\mathcal{N}(A)$. These are $k$ linearly independent vectors in $\mathbb{R}^n$ such that $Av_i = 0$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Extend to a basis for $\mathbb{R}^n$.</strong> By the Basis Extension Theorem, we can extend this set to form a basis for the entire space $\mathbb{R}^n$: $\{v_1, \dots, v_k, w_1, \dots, w_{n-k}\}$. Here, $r = n-k$ is the number of added vectors.
        </div>

        <div class="proof-step">
          <strong>Step 3: Analyze the image of the basis vectors.</strong> Consider the image of an arbitrary vector $x \in \mathbb{R}^n$. We can write $x$ as a linear combination of basis elements:
          $$ x = \sum_{i=1}^k c_i v_i + \sum_{j=1}^{r} d_j w_j $$
          Applying the linear map $A$:
          $$ Ax = \sum_{i=1}^k c_i \underbrace{A v_i}_{0} + \sum_{j=1}^{r} d_j A w_j = \sum_{j=1}^{r} d_j A w_j $$
          This shows that the vectors $\{Aw_1, \dots, Aw_r\}$ span the column space $\mathcal{R}(A)$.
        </div>

        <div class="proof-step">
          <strong>Step 4: Prove Linear Independence in the Range.</strong> We must show that $\{Aw_1, \dots, Aw_r\}$ are linearly independent. Suppose we have a linear combination summing to zero:
          $$ \sum_{j=1}^{r} \alpha_j A w_j = 0 $$
          By linearity, this implies $A \left( \sum_{j=1}^{r} \alpha_j w_j \right) = 0$.
          Let $u = \sum_{j=1}^{r} \alpha_j w_j$. Since $Au = 0$, the vector $u$ must lie in the nullspace $\mathcal{N}(A)$.
        </div>

        <div class="proof-step">
          <strong>Step 5: Contradiction via Basis Uniqueness.</strong> Since $u \in \mathcal{N}(A)$, it can be written as a linear combination of the nullspace basis $\{v_i\}$:
          $$ u = \sum_{i=1}^k \beta_i v_i $$
          Equating the two expressions for $u$:
          $$ \sum_{j=1}^{r} \alpha_j w_j = \sum_{i=1}^k \beta_i v_i \quad \implies \quad \sum_{j=1}^{r} \alpha_j w_j - \sum_{i=1}^k \beta_i v_i = 0 $$
          Since the combined set $\{v_1, \dots, v_k, w_1, \dots, w_r\}$ is a basis for $\mathbb{R}^n$, all coefficients must be zero. Specifically, $\alpha_j = 0$ for all $j$.
        </div>

        <div class="proof-step">
          <strong>Step 6: Count Dimensions.</strong> Since $\{Aw_1, \dots, Aw_r\}$ is a linearly independent spanning set for $\mathcal{R}(A)$, it is a basis.
          Thus, $\dim(\mathcal{R}(A)) = r = n - k$.
          Rearranging gives the theorem: $\dim(\mathcal{R}(A)) + \dim(\mathcal{N}(A)) = n$.
        </div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Rank & Nullspace</h3>
        <p><b>Explore the Four Fundamental Subspaces:</b> Define a 2Ã—3 or 3Ã—2 matrix and visualize how its column space, row space, nullspace, and left nullspace relate to each other. Try creating rank-deficient matrices (e.g., with linearly dependent columns) to see the Rank-Nullity Theorem in actionâ€”observe how the dimensions of the nullspace and column space always sum to the number of columns.</p>
        <p><i>Key insight:</i> The orthogonality relationships between these subspaces are fundamental to understanding linear transformations and will appear throughout convex optimization.</p>
        <div id="widget-rank-nullspace" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 2: INNER PRODUCTS AND NORMS -->

    <!-- SECTION 2: ALGEBRAIC INVARIANTS -->
    <section class="section-card" id="section-2">
      <h2>2. Algebraic Invariants: Determinant, Trace, and Eigenvalues</h2>

      <p>While a matrix $A$ represents a linear map in a specific basis, we are often interested in properties that are intrinsic to the map itself, independent of the basis. These are called <b>invariants</b>.</p>

      <h3>2.1 The "Big Three" Invariants</h3>
      <p>For a square matrix $A \in \mathbb{R}^{n \times n}$ with eigenvalues $\lambda_1, \dots, \lambda_n$ (counted with algebraic multiplicity):</p>
      <ul>
        <li><b>Trace:</b> $\mathrm{tr}(A) = \sum_{i=1}^n A_{ii} = \sum_{i=1}^n \lambda_i$.</li>
        <li><b>Determinant:</b> $\det(A) = \prod_{i=1}^n \lambda_i$.</li>
        <li><b>Eigenvalues:</b> Roots of the characteristic polynomial $p_A(\lambda) = \det(A - \lambda I)$.</li>
      </ul>

      <div class="insight">
        <h4>Geometric Interpretation</h4>
        <ul>
          <li><b>Determinant as Volume:</b> $|\det(A)|$ is the factor by which the linear map scales volume. If $S$ is a set with volume $V$, then $A(S)$ has volume $|\det(A)| V$. The sign indicates orientation (preservation vs. reversal).</li>
          <li><b>Eigenvalues as Stretch Factors:</b> If $Ax = \lambda x$, the map simply scales the vector $x$ by $\lambda$. If a matrix has a full set of eigenvectors, it maps a unit circle to an ellipse whose semiaxes are aligned with the eigenvectors and have lengths $|\lambda_i|$.</li>
        </ul>
      </div>


      <h3>2.2 The "Isotropic Scaling" Lemma</h3>
      <p>A classic result connects the geometry of eigenvectors to the structure of the matrix. What if <i>every</i> vector is an eigenvector?</p>
      <div class="theorem-box">
        <h4>Lemma: Scalar Matrices</h4>
        <p>If every non-zero vector $x \in \mathbb{R}^n$ is an eigenvector of $A$ (i.e., $Ax = \lambda(x)x$), then $A$ must be a scalar multiple of the identity: $A = \lambda I$.</p>
      </div>
      <div class="proof-box">
        <h4>Proof</h4>
        <p>Let $u, v$ be linearly independent vectors. By assumption, $Au = \lambda_u u$ and $Av = \lambda_v v$.
        Consider their sum $w = u+v$. We must have $A(u+v) = \lambda_w (u+v)$.
        $$ \lambda_w u + \lambda_w v = \lambda_w(u+v) = A(u+v) = Au + Av = \lambda_u u + \lambda_v v $$
        Rearranging: $(\lambda_w - \lambda_u)u + (\lambda_w - \lambda_v)v = 0$.
        Since $u$ and $v$ are independent, the coefficients must vanish: $\lambda_w = \lambda_u$ and $\lambda_w = \lambda_v$.
        Thus $\lambda_u = \lambda_v$ for any independent pair. By transitivity, all vectors share the same eigenvalue $\lambda$. Thus $Ax = \lambda x$ for all $x$, so $A = \lambda I$.</p>
      </div>
      <p><b>Geometric Intuition:</b> A general matrix distorts a circle into an ellipse. If <i>every</i> direction is a principal axis (eigenvector), the ellipse must be a circle (isotropic scaling).</p>

      <h3>2.3 Algebraic Properties</h3>

      <h4>Linearity of Trace</h4>
      <p>The trace is a linear functional:
      $$ \mathrm{tr}(A + B) = \mathrm{tr}(A) + \mathrm{tr}(B) \quad \text{and} \quad \mathrm{tr}(cA) = c\,\mathrm{tr}(A). $$
      </p>
      <div class="proof-box">
        <h4>Proof</h4>
        <p>By definition, $\mathrm{tr}(A) = \sum_{i=1}^n a_{ii}$. Then:</p>
        $$ \mathrm{tr}(A+B) = \sum_{i=1}^n (a_{ii} + b_{ii}) = \sum_{i=1}^n a_{ii} + \sum_{i=1}^n b_{ii} = \mathrm{tr}(A) + \mathrm{tr}(B) $$
      </div>

      <h4>Multiplicativity of Determinant</h4>
      <p>The determinant is multiplicative:
      $$ \det(AB) = \det(A)\det(B). $$
      This reflects the geometric fact that the volume scaling of a composite map is the product of the individual scalings. Note that $\det(AB) = \det(BA)$ even if $AB \neq BA$.
      </p>

      <h4>Trace of a Product (Cyclic Property)</h4>
      <p>In general, $\mathrm{tr}(AB) \neq \mathrm{tr}(A)\mathrm{tr}(B)$. However, the trace is <b>cyclic</b>:
      $$ \mathrm{tr}(AB) = \mathrm{tr}(BA). $$
      More generally, $\mathrm{tr}(ABC) = \mathrm{tr}(BCA) = \mathrm{tr}(CAB)$. This property is crucial for deriving matrix gradients.
      </p>

      <div class="proof-box">
        <h4>Proof: $\mathrm{tr}(AB) = \mathrm{tr}(BA)$</h4>
        <div class="proof-step">
          <strong>Coordinate Definition:</strong> Let $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times m}$. The diagonal elements of the product $AB$ are:
          $$ (AB)_{ii} = \sum_{k=1}^n a_{ik} b_{ki} $$
        </div>
        <div class="proof-step">
          <strong>Summing the diagonal:</strong>
          $$ \mathrm{tr}(AB) = \sum_{i=1}^m (AB)_{ii} = \sum_{i=1}^m \sum_{k=1}^n a_{ik} b_{ki} $$
        </div>
        <div class="proof-step">
          <strong>Switching order:</strong> Since the sums are finite, we can swap the order of summation:
          $$ \sum_{k=1}^n \sum_{i=1}^m b_{ki} a_{ik} = \sum_{k=1}^n (BA)_{kk} = \mathrm{tr}(BA) $$
        </div>
      </div>


      <div class="example">
        <h4>Worked Example: Trace and Determinant Caveats</h4>
        <p>While $\mathrm{tr}(A+B) = \mathrm{tr}(A) + \mathrm{tr}(B)$, other properties are subtler.</p>

        <p><b>1. Trace of Product:</b> $\mathrm{tr}(AB) \neq \mathrm{tr}(A)\mathrm{tr}(B)$.
        <br>Let $A = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ and $B = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}$.
        <br>$\mathrm{tr}(A)=1, \mathrm{tr}(B)=1 \implies \mathrm{tr}(A)\mathrm{tr}(B)=1$.
        <br>But $AB = 0$, so $\mathrm{tr}(AB) = 0$. Thus multiplicativity fails.</p>

        <p><b>2. Determinant of Sum:</b> $\det(A+B) \neq \det(A) + \det(B)$.
        <br>A nice conceptual counterexample: Take $A = I$ and $B = -I$ (for even $n$).
        <br>$\det(A) = 1$, $\det(B) = (-1)^n = 1$.
        <br>$\det(A+B) = \det(0) = 0$.
        <br>But $\det(A) + \det(B) = 1 + 1 = 2 \neq 0$.
        <br>This inequality is true generically; $\det$ is multiplicative, not additive.</p>
      </div>

      <h3>2.4 Similarity and Basis Independence</h3>
      <p>Two matrices $A$ and $B$ are <b>similar</b> if $B = P A P^{-1}$ for some invertible $P$. This represents the same linear operator in a different basis.</p>
      <div class="theorem-box">
        <h4>Theorem: Invariance under Similarity</h4>
        <p>If $A$ and $B$ are similar, they share the same algebraic invariants:</p>
        <ul>
          <li>$\det(PAP^{-1}) = \det(P)\det(A)\det(P^{-1}) = \det(A)$.</li>
          <li>$\mathrm{tr}(PAP^{-1}) = \mathrm{tr}(AP^{-1}P) = \mathrm{tr}(A)$.</li>
          <li>$p_{PAP^{-1}}(\lambda) = \det(PAP^{-1} - \lambda I) = \det(P(A-\lambda I)P^{-1}) = p_A(\lambda)$.</li>
        </ul>
        <p>Thus, they have the <b>same eigenvalues</b>.</p>
      </div>

      <h3>2.5 Spectral Shift</h3>
      <p>Adding a multiple of the identity shifts the eigenvalues but preserves eigenvectors.
      If $Ax = \lambda x$, then:
      $$ (A + tI)x = Ax + tIx = (\lambda + t)x. $$
      Thus, the eigenvalues of $A+tI$ are $\{\lambda_i + t\}$.
      <br><b>Warning:</b> In general, $\det(A+B) \neq \det(A) + \det(B)$. For example, $\det(I+I) = 2^n \neq 1+1$.
      </p>
    </section>


    <section class="section-card" id="section-3">
      <h2>3. Inner Products, Norms, and Angles</h2>

      <h3>Inner Product</h3>
      <p>An <a href="#" class="definition-link">inner product</a> on $\mathbb{R}^n$ is a mapping $\langle x, y \rangle$ that is bilinear, symmetric, and positive definite:</p>
      <ul>
        <li>$\langle ax + by, z \rangle = a\langle x, z \rangle + b\langle y, z \rangle$</li>
        <li>$\langle x, y \rangle = \langle y, x \rangle$</li>
        <li>$\langle x, x \rangle > 0$ for $x \neq 0$</li>
      </ul>
      <p>The <b>standard (Euclidean) inner product</b> is the most common example: $\langle x, y \rangle = x^\top y = \sum_{i=1}^n x_i y_i$.</p>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/vector-addition-parallelogram.png"
             alt="Parallelogram law for vector addition"
             style="max-width: 70%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 12px; background: white;" />
        <figcaption><i>Figure 3.1:</i> The parallelogram law of vector addition, a geometric reflection of the algebraic properties of inner products.</figcaption>
      </figure>


      <h3>The Adjoint: Moving Matrices Across Inner Products</h3>
      <p>A fundamental property connecting matrices and inner products is the definition of the <b>transpose</b> (or adjoint). For any $A \in \mathbb{R}^{m \times n}$, $x \in \mathbb{R}^n$, and $y \in \mathbb{R}^m$:
      $$ \boxed{\langle Ax, y \rangle = \langle x, A^\top y \rangle} $$
      or in dot product notation, $(Ax)^\top y = x^\top (A^\top y)$. This identity allows us to "move" the matrix $A$ from one side of the inner product to the other by taking its transpose. It is the workhorse behind gradients of quadratic forms (essential for <a href="../03-convex-functions/index.html">Lecture 03</a>) and the definition of orthogonality.
      </p>

      <h3>Norms</h3>
      <p>A <a href="#" class="definition-link">norm</a> is a function that assigns a strictly positive length or size to each vector in a vector space, except for the zero vector. A norm $\|\cdot\|$ must satisfy:</p>
      <ol>
        <li><b>Non-negativity:</b> $\|x\| \ge 0$, and $\|x\| = 0$ if and only if $x=0$.</li>
        <li><b>Absolute homogeneity:</b> $\|\alpha x\| = |\alpha| \|x\|$ for any scalar $\alpha$.</li>
        <li><b>Triangle inequality:</b> $\|x+y\| \le \|x\| + \|y\|$.</li>
      </ol>
      <p>Three of the most widely used norms are:</p>
      $$
      \|x\|_2 = \sqrt{\sum_i x_i^2} \quad (\text{Euclidean norm}), \quad \|x\|_1 = \sum_i |x_i| \quad (\text{L1 norm}), \quad \|x\|_\infty = \max_i |x_i| \quad (\text{Infinity norm})
      $$

      <figure style="text-align: center;">
        <img src="assets/norm-balls.png"
             alt="Comparison of L1, L2, and Infinity norm unit balls"
             style="max-width: 70%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 3.2:</i> The "unit ball" $\{x \mid \|x\| \le 1\}$ for the $\ell_1$ (diamond), $\ell_2$ (circle), and $\ell_\infty$ (square) norms.</figcaption>
      </figure>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: The Geometry of Norms</h3>
        <p><b>Visualize Norm Unit Balls:</b> Explore the unit balls (sets of vectors with norm â‰¤ 1) for the $\ell_1$, $\ell_2$, and $\ell_\infty$ norms in 2D. Each norm defines a different notion of "distance" and produces a distinctly shaped unit ball:</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>$\ell_2$ (Euclidean):</b> Forms a perfect circleâ€”the most "natural" geometric distance</li>
          <li><b>$\ell_1$ (Manhattan):</b> Forms a diamond with "pointy" corners at the axesâ€”this geometry is why $\ell_1$ regularization promotes sparse solutions in machine learning</li>
          <li><b>$\ell_\infty$ (Chebyshev):</b> Forms a squareâ€”measures the maximum component</li>
        </ul>
        <p><i>Connection to optimization:</i> The shape of these unit balls directly affects the solutions to optimization problems. The corners of the $\ell_1$ ball are what make LASSO regression find sparse solutions!</p>
        <div id="widget-norm-geometry" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>Cauchyâ€“Schwarz and Triangle Inequality</h3>

      <div class="proof-box">
        <h4>Proof of Cauchyâ€“Schwarz: $|x^\top y| \le \|x\|_2 \|y\|_2$</h4>

        <div class="proof-step">
          <strong>Step 1: Consider a norm.</strong> For any scalar $t$, the expression $\|x - ty\|_2^2$ is non-negative:
          $$ \|x - ty\|_2^2 = (x - ty)^\top(x - ty) = \|x\|_2^2 - 2t(x^\top y) + t^2 \|y\|_2^2 \ge 0 $$
        </div>

        <div class="proof-step">
          <strong>Step 2: Apply the discriminant condition.</strong> This quadratic in $t$ is non-negative for all $t$, so its discriminant must be non-positive:
          $$ ( -2(x^\top y) )^2 - 4(\|y\|_2^2)(\|x\|_2^2) \le 0 $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Simplify.</strong> Rearranging gives
          $$ 4(x^\top y)^2 \le 4\|x\|_2^2 \|y\|_2^2 $$
          Taking the square root of both sides yields the Cauchy-Schwarz inequality.
        </div>
      </div>

      <div class="proof-box">
        <h4>Proof of the Triangle Inequality: $\|x+y\|_2 \le \|x\|_2 + \|y\|_2$</h4>

        <div class="proof-step">
          <strong>Step 1: Expand the squared norm.</strong> Starting with the left side:
          $$ \|x+y\|_2^2 = (x+y)^\top(x+y) = \|x\|_2^2 + 2x^\top y + \|y\|_2^2 $$
        </div>

        <div class="proof-step">
          <strong>Step 2: Apply Cauchy-Schwarz.</strong> By the Cauchy-Schwarz inequality, $x^\top y \le |x^\top y| \le \|x\|_2 \|y\|_2$. Substituting:
          $$ \|x+y\|_2^2 \le \|x\|_2^2 + 2\|x\|_2 \|y\|_2 + \|y\|_2^2 = (\|x\|_2 + \|y\|_2)^2 $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Take square roots.</strong> Taking the square root of both sides yields the triangle inequality.
        </div>
      </div>

      <h3>Angles and Orthogonality</h3>
      <p>The Cauchy-Schwarz inequality allows us to define the angle between two vectors:
      $$ \cos \angle(x, y) = \frac{x^\top y}{\|x\|_2 \|y\|_2} $$
      Since the magnitude of the right-hand side is guaranteed to be between -1 and 1. Two vectors are <b>orthogonal</b> if their inner product is zero, $x^\top y = 0$, which corresponds to a 90-degree angle.</p>

      <h3>Dual Norms and HÃ¶lder's Inequality</h3>
      <p>The concept of duality is central to optimization. For any norm $\|\cdot\|$, we can define a corresponding <b>dual norm</b>, denoted $\|\cdot\|_*$, as:
      $$ \|y\|_* = \sup_{\|x\| \le 1} x^\top y $$
      The dual norm measures the maximum "stretch" of a vector $y$ when applied to unit-norm vectors $x$. This definition leads to some important pairings:</p>

      <div class="insight">
        <h4>ðŸ’¡ Geometric Intuition for Dual Norms</h4>
        <p>Think of the dual norm as measuring how "aligned" a vector $y$ can be with vectors in the unit ball of the primal norm.</p>
        <ul>
          <li><b>$\ell_2$ is self-dual:</b> The unit ball is a sphere. The direction of maximum inner product with $y$ is simply $y$ itself (normalized). Thus $\|y\|_{2,*} = \|y\|_2$.</li>
          <li><b>$\ell_1$ dual is $\ell_\infty$:</b> The $\ell_1$ unit ball is a diamond with corners at coordinate axes. To maximize $x^\top y$ over this diamond, you simply pick the corner $e_i$ (or $-e_i$) corresponding to the largest component of $y$. Thus, the maximum value is $\max_i |y_i| = \|y\|_\infty$.</li>
          <li><b>$\ell_\infty$ dual is $\ell_1$:</b> The $\ell_\infty$ unit ball is a box (corners at $(\pm 1, \dots, \pm 1)$). To maximize $x^\top y$ over the box, you choose each $x_i$ to match the sign of $y_i$. The result is $\sum y_i \mathrm{sign}(y_i) = \sum |y_i| = \|y\|_1$.</li>
        </ul>
      </div>

      <div class="proof-box">
        <h4>Proof: $(\|\cdot\|_2)^* = \|\cdot\|_2$ (Self-Duality of Euclidean Norm)</h4>
        <div class="proof-step">
          <strong>Step 1: Upper bound.</strong> By Cauchy-Schwarz, for $\|x\|_2 \le 1$:
          $$ x^\top y \le |x^\top y| \le \|x\|_2 \|y\|_2 \le \|y\|_2 $$
          Thus $\|y\|_* \le \|y\|_2$.
        </div>
        <div class="proof-step">
          <strong>Step 2: Achieve equality.</strong> Choose $x = y/\|y\|_2$ (assuming $y \neq 0$). Then $\|x\|_2 = 1$ and:
          $$ x^\top y = \frac{y^\top y}{\|y\|_2} = \frac{\|y\|_2^2}{\|y\|_2} = \|y\|_2 $$
          Hence $\|y\|_* \ge \|y\|_2$.
        </div>
        <div class="proof-step">
          <strong>Conclusion:</strong> Therefore $\|y\|_* = \|y\|_2$.
        </div>
      </div>

      <div class="proof-box">
        <h4>Proof: $(\|\cdot\|_1)^* = \|\cdot\|_\infty$</h4>
        <div class="proof-step">
          <strong>Step 1: Upper bound.</strong> For $\|x\|_1 \le 1$:
          $$ x^\top y = \sum_i x_i y_i \le \sum_i |x_i| |y_i| \le \max_i |y_i| \sum_i |x_i| \le \|y\|_\infty $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Achieve equality.</strong> Let $j = \arg\max_i |y_i|$. Choose $x = \mathrm{sign}(y_j) e_j$. Then $\|x\|_1 = 1$ and:
          $$ x^\top y = \mathrm{sign}(y_j) y_j = |y_j| = \|y\|_\infty $$
        </div>
        <div class="proof-step">
          <strong>Conclusion:</strong> $\|y\|_* = \|y\|_\infty$.
        </div>
      </div>

      <h3>HÃ¶lder's Inequality (Dual-Norm Cauchyâ€“Schwarz)</h3>
      <p>For all $x, y \in \mathbb{R}^n$ and any norm $\|\cdot\|$:</p>
      $$ x^\top y \le \|x\| \cdot \|y\|_* $$
      <div class="proof-box">
        <h4>Proof of HÃ¶lder's Inequality</h4>
        <div class="proof-step">
          <strong>Case 1: $x = 0$.</strong> The inequality is trivially true: $0 \le 0$.
        </div>
        <div class="proof-step">
          <strong>Case 2: $x \neq 0$.</strong> Write:
          $$ x^\top y = \|x\| \cdot \frac{x^\top y}{\|x\|} = \|x\| \cdot \left(\frac{x}{\|x\|}\right)^\top y $$
          Note that $\left\|\frac{x}{\|x\|}\right\| = 1$, so by the definition of dual norm:
          $$ \left(\frac{x}{\|x\|}\right)^\top y \le \sup_{\|z\| \le 1} z^\top y = \|y\|_* $$
          Combining: $x^\top y \le \|x\| \cdot \|y\|_*$.
        </div>
      </div>

      <h3>Generalized (Weighted) Inner Product</h3>
      <p>If $Q \in \mathbb{R}^{n \times n}$ is symmetric positive definite (SPD), then $\langle x, y \rangle_Q := x^\top Q y$ is an inner product with induced norm $\|x\|_Q = \sqrt{x^\top Q x}$. This yields <b>quadratic forms</b> and ellipsoidal distance.</p>

      <h3>Spectral Norm and Convexity</h3>
      <p>The <b>spectral norm</b> (or operator norm) of a matrix $X \in \mathbb{R}^{m \times n}$ is induced by the Euclidean vector norm:
      $$ \|X\|_2 = \sup_{v \neq 0} \frac{\|Xv\|_2}{\|v\|_2} $$
      It turns out this equals the maximum singular value:
      $$ \|X\|_2 = \sigma_{\max}(X) = \sqrt{\lambda_{\max}(X^\top X)} $$
      Since $X^\top X$ is always Positive Semidefinite ($v^\top X^\top X v = \|Xv\|_2^2 \ge 0$), its eigenvalues are non-negative.</p>

      <div class="proof-box">
        <h4>Proof: $\|X\|_2 = \sqrt{\lambda_{\max}(X^\top X)}$</h4>
        <div class="proof-step">
          <strong>Step 1: Squared Norm.</strong>
          $$ \|X\|_2^2 = \sup_{v \neq 0} \frac{\|Xv\|_2^2}{\|v\|_2^2} = \sup_{v \neq 0} \frac{v^\top X^\top X v}{v^\top v} $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Rayleigh Quotient.</strong> The expression $\frac{v^\top M v}{v^\top v}$ for a symmetric matrix $M$ is the Rayleigh Quotient. Its maximum value is exactly $\lambda_{\max}(M)$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Conclusion.</strong>
          $$ \|X\|_2^2 = \lambda_{\max}(X^\top X) \implies \|X\|_2 = \sqrt{\lambda_{\max}(X^\top X)} $$
        </div>
      </div>

      <p><b>Dual Representation (Crucial for Convexity Proofs):</b> Just like the max eigenvalue, the spectral norm can be written as a supremum:
      $$ \|X\|_2 = \sup_{\|u\|_2=1, \|v\|_2=1} u^\top X v $$
      Since $f(X) = u^\top X v$ is a <b>linear function</b> of $X$, and $\|X\|_2$ is the supremum of these linear functions, $\|X\|_2$ is a <b>convex function</b>.</p>

      <h3>Matrix Inner Product and Frobenius Norm</h3>
      <p>For matrices $X, Y \in \mathbb{R}^{m \times n}$, the <b>trace inner product</b> (also called the Hilbert-Schmidt inner product) is:
      $$ \langle X, Y \rangle = \mathrm{tr}(X^\top Y) = \sum_{i=1}^m \sum_{j=1}^n X_{ij} Y_{ij} $$
      The <b>Frobenius norm</b> (or Hilbert-Schmidt norm) is $\|X\|_F = \sqrt{\langle X, X \rangle}$.</p>

      <div class="proof-box">
        <h4>Why does $\mathrm{tr}(A^\top A) = \sum a_{ij}^2$? (Index Gymnastics)</h4>
        <p>Let $A \in \mathbb{R}^{m \times n}$. The diagonal entries of the product $A^\top A$ are formed by dotting the columns of $A$ with themselves.
        <br>Specifically, the $(j,j)$ entry of $A^\top A$ is:
        $$ (A^\top A)_{jj} = \sum_{k=1}^m (A^\top)_{jk} A_{kj} = \sum_{k=1}^m A_{kj} A_{kj} = \sum_{k=1}^m A_{kj}^2 $$
        This is the sum of squares of the $j$-th column.
        <br>The trace is the sum of these diagonal entries:
        $$ \mathrm{tr}(A^\top A) = \sum_{j=1}^n (A^\top A)_{jj} = \sum_{j=1}^n \sum_{k=1}^m A_{kj}^2 $$
        This is exactly the sum of squares of all entries.</p>
      </div>

      <h4>Submultiplicativity of the Frobenius Norm</h4>
      <p>For compatible matrices $A, B$, we have the inequality $\|AB\|_F \le \|A\|_F \|B\|_F$.
      <br><i>Proof via Cauchy-Schwarz:</i>
      $$ \|AB\|_F^2 = \sum_{i,j} (AB)_{ij}^2 = \sum_{i,j} \left( \sum_k a_{ik} b_{kj} \right)^2 $$
      Apply Cauchy-Schwarz to the inner sum:
      $$ \left( \sum_k a_{ik} b_{kj} \right)^2 \le \left( \sum_k a_{ik}^2 \right) \left( \sum_k b_{kj}^2 \right) $$
      Summing over $i$ and $j$:
      $$ \sum_{i,j} (AB)_{ij}^2 \le \sum_{i,j} \left( \sum_k a_{ik}^2 \right) \left( \sum_k b_{kj}^2 \right) = \left( \sum_i \sum_k a_{ik}^2 \right) \left( \sum_j \sum_k b_{kj}^2 \right) = \|A\|_F^2 \|B\|_F^2 $$
      Taking square roots yields the result.</p>
    </section>

    <!-- SECTION 3: ORTHOGONALITY AND QR -->
    <section class="section-card" id="section-4">
      <h2>4. Orthogonality and Orthonormal Bases</h2>

      <h3>Orthonormal Sets</h3>
      <p>A set of vectors $q_1, \dots, q_k$ is <b>orthonormal</b> if its elements are mutually orthogonal and each has a norm of 1. Formally, $q_i^\top q_j = \delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta (1 if $i=j$, 0 otherwise). A square matrix $Q$ with orthonormal columns is an <b>orthogonal matrix</b>, satisfying the important property $Q^\top Q = I$, which means $Q^{-1} = Q^\top$. Orthonormal bases are computationally desirable because they are numerically stable and simplify many calculations, such as projections.</p>

      <h3>Angle Preservation and Orthogonal Matrices</h3>
      <p>A linear map $A$ preserves angles if $\frac{\langle Ax, Ay \rangle}{\|Ax\|\|Ay\|} = \frac{\langle x, y \rangle}{\|x\|\|y\|}$ for all $x, y$.
      A stronger condition is that $A$ preserves inner products: $\langle Ax, Ay \rangle = \langle x, y \rangle$.
      <br>Using the adjoint property, $\langle x, A^\top A y \rangle = \langle x, y \rangle$ for all $x, y$, which implies $A^\top A = I$.
      Thus, <b>matrices that preserve geometry (lengths and angles) are exactly the orthogonal matrices</b> (possibly scaled).</p>

      <div class="insight">
        <h4>The Orthogonal Group $O_n$</h4>
        <p>The set of all $n \times n$ orthogonal matrices forms a group, denoted $O_n$.</p>
        <ul>
          <li><b>Closure:</b> If $Q_1, Q_2 \in O_n$, then $(Q_1 Q_2)^\top (Q_1 Q_2) = Q_2^\top Q_1^\top Q_1 Q_2 = Q_2^\top I Q_2 = I$.</li>
          <li><b>Inverse:</b> $Q^{-1} = Q^\top$, which is also orthogonal.</li>
          <li><b>Compactness:</b> $O_n$ is a compact set in $\mathbb{R}^{n \times n}$. It is closed (preimage of $I$ under continuous map $f(A)=A^\top A$) and bounded (columns have unit norm).</li>
        </ul>
      </div>

      <h3>The Gramâ€“Schmidt Process</h3>
      <p>The Gram-Schmidt process is an algorithm for constructing an orthonormal basis from a set of linearly independent vectors. Starting with a vector, it iteratively subtracts the components that lie in the direction of the previously processed vectors, leaving a new, orthogonal vector that is then normalized.
      $$
      \tilde{q}_k = a_k - \sum_{i=1}^{k-1} (q_i^\top a_k) q_i, \quad q_k = \frac{\tilde{q}_k}{\|\tilde{q}_k\|_2}
      $$
      </p>

      <h3>The QR Decomposition</h3>
      <p>The QR decomposition expresses a matrix $A$ as the product of an orthonormal matrix $Q$ and an upper triangular matrix $R$. This decomposition is a direct outcome of the Gram-Schmidt process and is fundamental to numerical linear algebra. For a matrix $A \in \mathbb{R}^{m \times n}$ with full column rank, we can write $A = QR$, where $Q \in \mathbb{R}^{m \times n}$ has orthonormal columns and $R \in \mathbb{R}^{n \times n}$ is upper-triangular.
      <br><b>Applications:</b></p>
      <ul>
        <li><b>Solving Linear Systems:</b> The system $Ax=b$ becomes $QRx=b$, which simplifies to $Rx = Q^\top b$. This is easily solved using back substitution and is far more numerically stable than forming the normal equations.</li>
        <li><b>Projections:</b> The projection onto the column space of $A$ is given by $P = QQ^\top$.</li>
      </ul>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Explorer: Orthogonality & Projections</h3>
        <p><b>Visualize Vector Relationships:</b> Drag two vectors in the 2D plane and observe how their geometric relationships change in real-time. The tool displays:</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Dot product ($x^\top y$):</b> Becomes zero when vectors are orthogonal (perpendicular)</li>
          <li><b>Angle:</b> Computed using $\cos \theta = \frac{x^\top y}{\|x\|_2 \|y\|_2}$ via the Cauchy-Schwarz inequality</li>
          <li><b>Orthogonal projection:</b> The shadow of one vector onto anotherâ€”this is the foundation of least squares!</li>
        </ul>
        <p><i>Key concept:</i> Projection is everywhere in optimizationâ€”from computing least squares solutions to understanding constraint gradients. The projection of $b$ onto the column space of $A$ gives us the best least-squares fit.</p>
        <div id="widget-orthogonality" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 4: PSD MATRICES -->
    <section class="section-card" id="section-5">
      <h2>5. Positive Semidefinite Matrices</h2>

      <h3>5.1 The Space of Symmetric Matrices ($\mathbb{S}^n$)</h3>
      <p>In convex optimization, a central object is the Hessian matrix $\nabla^2 f(x)$. By Schwarz's Theorem, if a function is twice continuously differentiable, its Hessian is symmetric. A matrix $A \in \mathbb{R}^{n \times n}$ is symmetric if $A = A^\top$. The set of such matrices is denoted $\mathbb{S}^n$.</p>

      <h4>The Spectral Theorem</h4>
      <p>If $A \in \mathbb{S}^n$, then:</p>
      <ol>
        <li>All <b>eigenvalues</b> ($\lambda_1, \dots, \lambda_n$) are <b>real numbers</b>.</li>
        <li>There exists a set of <b>orthonormal eigenvectors</b> $q_1, \dots, q_n$.</li>
        <li>$A$ can be diagonalized as $A = Q \Lambda Q^\top$, where $Q$ is an orthogonal matrix ($Q^\top Q = I$) containing the eigenvectors, and $\Lambda$ is a diagonal matrix containing the eigenvalues.</li>
      </ol>

      <h4>Why are Eigenvectors of Symmetric Matrices Orthogonal?</h4>
      <p>The Spectral Theorem guarantees orthogonal eigenvectors for symmetric matrices. The proof relies on the adjoint property.</p>
      <div class="proof-box">
        <h4>Proof of Orthogonality</h4>
        <p>Let $A$ be symmetric ($A^\top = A$). Let $v_1, v_2$ be eigenvectors with distinct eigenvalues $\lambda_1 \neq \lambda_2$.
        $$ A v_1 = \lambda_1 v_1, \quad A v_2 = \lambda_2 v_2 $$
        Consider the inner product $\langle A v_1, v_2 \rangle$. We can evaluate it two ways:</p>
        <ol>
          <li>$\langle A v_1, v_2 \rangle = \langle \lambda_1 v_1, v_2 \rangle = \lambda_1 \langle v_1, v_2 \rangle$</li>
          <li>$\langle A v_1, v_2 \rangle = \langle v_1, A^\top v_2 \rangle = \langle v_1, A v_2 \rangle = \langle v_1, \lambda_2 v_2 \rangle = \lambda_2 \langle v_1, v_2 \rangle$</li>
        </ol>
        <p>Subtracting gives $(\lambda_1 - \lambda_2) \langle v_1, v_2 \rangle = 0$. Since $\lambda_1 \neq \lambda_2$, we must have $\langle v_1, v_2 \rangle = 0$.</p>
      </div>

      <p><b>Implication for convexity:</b> The "shape" of a multivariable function (bowl vs. saddle) is entirely determined by the signs of the eigenvalues of its Hessian.</p>

      <h3>5.2 Positive Semidefinite (PSD) Matrices</h3>
      <p>This definition is central to the course. It provides the matrix equivalent of a "non-negative number" and underpins the definition of <b>Convex Functions</b> (<a href="../03-convex-functions/index.html">Lecture 03</a>) and <b>Semidefinite Programming</b> (<a href="../04-convex-opt-problems/index.html">Lecture 04</a>).</p>

      <div class="insight">
        <h4>Forward Connection: The PSD Cone</h4>
        <p>The set of all PSD matrices forms a convex cone, denoted $\mathbb{S}^n_+$. This geometric object is the foundation of <b>Semidefinite Programming (SDP)</b>, a powerful generalization of linear programming that we will study in <a href="../04-convex-opt-problems/index.html">Lecture 04</a>. Just as LP optimizes over the non-negative orthant, SDP optimizes over the PSD cone.</p>
      </div>

      <h4>Definition A: The Variational Definition</h4>
      <p>A symmetric matrix $A \in \mathbb{S}^n$ is <a href="#" class="definition-link" data-term="positive semidefinite">Positive Semidefinite (PSD)</a>, denoted as $A \succeq 0$, if:</p>
      $$ x^\top A x \ge 0 \quad \text{for all } x \in \mathbb{R}^n $$
      <p><b>Geometric Intuition (Curvature):</b> If $A$ represents the curvature of a function (the Hessian), the term $x^\top A x$ represents the <b>curvature in the direction of vector $x$</b>. If $x^\top A x \ge 0$ for all directions $x$, the function curves upward in every direction (it is a "bowl").</p>

      <h4>Definition B: The Eigenvalue Definition</h4>
      <p>A symmetric matrix $A \in \mathbb{S}^n$ is PSD if and only if all its eigenvalues are non-negative:</p>
      $$ \lambda_i(A) \ge 0 \quad \text{for all } i = 1, \dots, n $$

      <h4>Factorization of PSD Matrices</h4>
      <p>A symmetric matrix $A$ is PSD if and only if it can be written as a Gram matrix: $A = B^\top B$ for some $B$.
      <br><i>Proof:</i> If $A \succeq 0$, spectral decomposition gives $A = Q \Lambda Q^\top = (Q \Lambda^{1/2}) (Q \Lambda^{1/2})^\top$. Let $B = (Q \Lambda^{1/2})^\top$.
      Conversely, if $A = B^\top B$, then $x^\top A x = \|Bx\|_2^2 \ge 0$.
      </p>


      <div class="proof-box">
        <h4>Proof: Equivalence of Definitions</h4>
        <p>We prove that $(x^\top A x \ge 0) \iff (\lambda_i \ge 0)$.</p>

        <div class="proof-step">
          <strong>Step 1: $(\Rightarrow)$</strong> Assume $x^\top A x \ge 0$ for all vectors $x$. Let $v$ be an eigenvector of $A$ with eigenvalue $\lambda$.
          $$ A v = \lambda v $$
          Substitute $x = v$ into the variational definition:
          $$ v^\top A v = v^\top (\lambda v) = \lambda (v^\top v) = \lambda \|v\|_2^2 $$
          Since $x^\top A x \ge 0$, we have $\lambda \|v\|_2^2 \ge 0$. Since eigenvectors are non-zero, $\|v\|_2^2 > 0$. Therefore, <b>$\lambda \ge 0$</b>.
        </div>

        <div class="proof-step">
          <strong>Step 2: $(\Leftarrow)$</strong> Assume all $\lambda_i \ge 0$. Use the Spectral Decomposition $A = Q \Lambda Q^\top$. For any arbitrary vector $x$:
          $$ x^\top A x = x^\top (Q \Lambda Q^\top) x = (Q^\top x)^\top \Lambda (Q^\top x) $$
          Let $y = Q^\top x$. Then:
          $$ x^\top A x = y^\top \Lambda y = \sum_{i=1}^n \lambda_i y_i^2 $$
          Since $\lambda_i \ge 0$ and $y_i^2 \ge 0$, the sum is non-negative. Thus <b>$x^\top A x \ge 0$</b>.
        </div>
      </div>

      <h3>5.3 The Rayleigh Quotient and Eigenvalues</h3>
      <p>The <b>maximum eigenvalue</b> $\lambda_{\max}(A)$ can be defined as an optimization problem:</p>
      $$ \lambda_{\max}(A) = \sup_{x \neq 0} \frac{x^\top A x}{x^\top x} = \sup_{\|x\|_2 = 1} x^\top A x $$
      <p>This is the <b>Rayleigh Quotient</b>. This definition proves that $\lambda_{\max}(A)$ is a convex function of $A$. Notice that for a fixed $x$, the function $g(A) = x^\top A x$ is <b>linear</b> in $A$. Since $\lambda_{\max}(A)$ is the <b>pointwise supremum</b> of a family of linear functions (indexed by $x$), it is a convex function.</p>

      <h3>5.4 The Schur Complement Lemma</h3>
      <p>A powerful tool for handling block matrices in optimization is the Schur Complement. It allows us to convert complicated nonlinear constraints into Linear Matrix Inequalities (LMIs) and provides a systematic way to check positive semidefiniteness.</p>

      <figure style="text-align: center;">
        <img src="assets/schur_complement_block.png"
             alt="Schur Complement Block Matrix Transformation Visualization"
             style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 5.3:</i> Visualization of the Schur complement process. Left: Original block matrix. Center: Elimination matrix. Right: Resulting block upper-triangular matrix where the top-left block has been transformed into the Schur complement.</figcaption>
      </figure>

      <div class="proof-box">
        <h4>1. Block Matrix Setup and Dimensions</h4>
        <p>Consider a block matrix $M$ partitioned as follows:</p>
        $$ M = \begin{bmatrix} A & B \\ C & D \end{bmatrix} $$
        <p>where $A \in \mathbb{R}^{p \times p}$, $B \in \mathbb{R}^{p \times q}$, $C \in \mathbb{R}^{q \times p}$, and $D \in \mathbb{R}^{q \times q}$. Assume $D$ is invertible.</p>
        <p>The <b>Schur complement</b> of $D$ in $M$ is defined as:</p>
        $$ S := A - B D^{-1} C $$

        <h4>2. Constructing the Elimination Matrix</h4>
        <p>To analyze $M$, we perform block Gaussian elimination. Define the elimination matrix $R$ (often denoted as $E$ in elimination contexts):</p>
        $$ R = \begin{bmatrix} I_p & 0 \\ -D^{-1}C & I_q \end{bmatrix} $$
        <p>This matrix is block lower-triangular with unit diagonal blocks, so $\det(R) = 1$. Multiplying $M$ by $R$ on the right yields the factorization:</p>
        $$ MR = \begin{bmatrix} A & B \\ C & D \end{bmatrix} \begin{bmatrix} I_p & 0 \\ -D^{-1}C & I_q \end{bmatrix} = \begin{bmatrix} A - BD^{-1}C & B \\ 0 & D \end{bmatrix} = \begin{bmatrix} S & B \\ 0 & D \end{bmatrix} $$

        <div class="proof-box">
          <h4>Line-by-Line Verification of Block Multiplication</h4>
          <p>Let's verify this multiplication explicitly using the block multiplication rule $\begin{bmatrix} A & B \\ C & D \end{bmatrix} \begin{bmatrix} E & F \\ G & H \end{bmatrix} = \begin{bmatrix} AE+BG & AF+BH \\ CE+DG & CF+DH \end{bmatrix}$.</p>
          <p>Here $E=I_p, F=0, G=-D^{-1}C, H=I_q$.</p>
          <ul>
            <li><b>Top-left:</b> $A(I_p) + B(-D^{-1}C) = A - BD^{-1}C = S$.</li>
            <li><b>Top-right:</b> $A(0) + B(I_q) = B$.</li>
            <li><b>Bottom-left:</b> $C(I_p) + D(-D^{-1}C) = C - C = 0$. (This was the goal!)</li>
            <li><b>Bottom-right:</b> $C(0) + D(I_q) = D$.</li>
          </ul>
        </div>

        <div class="insight">
          <h4>ðŸ’¡ Choice of Elimination Matrix</h4>
          <p>The goal is to eliminate the bottom-left block $C$, analogous to Gaussian elimination. We require the $(2,1)$ block of the product to be zero:
          $$ C \cdot I_p + D \cdot X = 0 \implies D X = -C \implies X = -D^{-1}C $$
          This determines the entry $-D^{-1}C$ in the elimination matrix $E$.</p>
        </div>

        <p>Since $ME$ is block upper-triangular, its determinant is the product of the determinants of its diagonal blocks: $\det(ME) = \det(S)\det(D)$. Because $\det(E) = 1$, we derive the <b>Determinant Identity</b>:</p>
        $$ \det(M) = \det(D) \det(A - B D^{-1} C) $$

        <div class="example">
          <h4>Numerical Example (Sanity Check)</h4>
          <p>Let $A=[2], B=[1], C=[3], D=[4]$. Then $M = \begin{bmatrix} 2 & 1 \\ 3 & 4 \end{bmatrix}$.</p>
          <ul>
            <li><b>Direct Determinant:</b> $\det(M) = 2(4) - 1(3) = 8 - 3 = 5$.</li>
            <li><b>Schur Complement:</b> $S = A - B D^{-1} C = 2 - 1(\frac{1}{4})3 = 2 - 0.75 = 1.25$.</li>
            <li><b>Formula Check:</b> $\det(D)\det(S) = 4(1.25) = 5$.</li>
          </ul>
          <p>It matches perfectly.</p>
        </div>

        <h4>3. Symmetric Variant: Schur Complement of $A$</h4>
        <p>Similarly, if $A$ is invertible, we can eliminate $C$ using the Schur complement of $A$, defined as $D - C A^{-1} B$. The determinant identity becomes:</p>
        $$ \det(M) = \det(A) \det(D - C A^{-1} B) $$

        <h4>4. Schur Complement and Positive Semidefiniteness</h4>
        <p>This is the crucial property for convex optimization. Let $M$ be a symmetric matrix ($C = B^\top$):</p>
        $$ M = \begin{bmatrix} A & B \\ B^\top & D \end{bmatrix} $$
        <p><b>Theorem:</b> If $D \succ 0$, then $M \succeq 0$ if and only if the Schur complement $S = A - B D^{-1} B^\top \succeq 0$.</p>

        <div class="proof-step">
          <strong>Proof ($\Rightarrow$):</strong> Assume $M \succeq 0$.
          Consider test vectors $z = [x^\top, -(D^{-1}B^\top x)^\top]^\top$. Then:
          $$ z^\top M z = x^\top (A - B D^{-1} B^\top) x = x^\top S x $$
          Since $M \succeq 0$, $z^\top M z \ge 0$ for all $z$, which implies $x^\top S x \ge 0$ for all $x$. Thus $S \succeq 0$.
        </div>

        <div class="proof-step">
          <strong>Proof ($\Leftarrow$):</strong> Assume $S \succeq 0$ and $D \succ 0$.
          Take any $z = [x^\top, y^\top]^\top$. We can factor $M$ using the elimination matrix from the proof of the lemma:
          $$ z^\top M z = x^\top S x + (y + D^{-1}B^\top x)^\top D (y + D^{-1}B^\top x) $$
          Since $S \succeq 0$, the first term is non-negative. Since $D \succ 0$, the second term is non-negative. Thus $z^\top M z \ge 0$ for all $z$.
        </div>
      </div>

      <p><b>Forward Connection:</b> This result can also be viewed as minimizing the quadratic form $f(x, y) = [x^\top \ y^\top] M [x^\top \ y^\top]^\top$ with respect to $y$ (partial minimization), yielding a new quadratic form in $x$ defined by the Schur complement matrix. This perspective is explored in <a href="../03-convex-functions/index.html">Lecture 03</a>.</p>

        <div class="insight">
          <h4>The Triangle of Equivalence</h4>
          <p>For a positive definite matrix $Y \succ 0$, the following three conditions are equivalent. They represent three different "faces" of the same object:</p>
          $$
          \boxed{
          \begin{matrix}
          \textbf{Scalar Inequality} & & \textbf{Block Matrix LMI} \\
          t \ge x^\top Y^{-1} x & \iff & \begin{bmatrix} Y & x \\ x^\top & t \end{bmatrix} \succeq 0 \\
          & \Updownarrow & \\
          & \textbf{Rank-1 Update} & \\
          & tY - xx^\top \succeq 0 &
          \end{matrix}
          }
          $$
          <ul>
            <li><b>Scalar:</b> Useful for epigraphs of quadratic-over-linear functions.</li>
            <li><b>Block Matrix:</b> Useful for Semidefinite Programming (SDP) constraints.</li>
            <li><b>Rank-1 Update:</b> Useful for algebraic manipulation ($tY \succeq xx^\top$).</li>
          </ul>
          <p><i>Proof of $tY - xx^\top$:</i> Consider the quadratic form $v^\top(tY - xx^\top)v = t v^\top Y v - (x^\top v)^2$. This is non-negative for all $v$ if and only if $(x^\top v)^2 \le t (v^\top Y v)$. By Cauchy-Schwarz (specifically with inner product $\langle u, w \rangle_Y = u^\top Y w$), the maximum value of $\frac{(x^\top v)^2}{v^\top Y v}$ is $x^\top Y^{-1} x$. Thus we need $t \ge x^\top Y^{-1} x$.</p>
        </div>

      <p><b>Application:</b> This lemma turns the nonlinear constraint $\|Bx\|_2^2 \le c$ (equivalent to $x^\top B^\top B x \le c$) into an LMI:
      $$ \begin{bmatrix} I & Bx \\ (Bx)^\top & c \end{bmatrix} \succeq 0 $$
      This is central to Semidefinite Programming.</p>

      <p>A positive definite matrix $Q$ can be used to define a generalized norm, $\|x\|_Q = \sqrt{x^\top Q x}$. The unit ball for this norm, $\{x \mid x^\top Q x \le 1\}$, is an ellipsoid, whose geometry is determined by the eigenvalues and eigenvectors of $Q$.</p>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/eigenvalues-psd.png"
             alt="Eigenvalue visualization for PSD matrices"
             style="max-width: 80%; height: auto; border: 1px solid var(--border); border-radius: 8px;" />
        <figcaption><i>Figure 5.1:</i> Eigenvalues and positive semidefinitenessâ€”the signs determine the matrix's curvature properties.</figcaption>
      </figure>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-primer/eigenvectors.gif"
             alt="Animated eigenvector visualization"
             style="max-width: 70%; height: auto; border: 1px solid var(--border); border-radius: 8px;" />
        <figcaption><i>Figure 5.2:</i> Eigenvectors in actionâ€”how matrices transform space along principal directions.</figcaption>
      </figure>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Explorer: Matrix Geometry & Definiteness</h3>
        <p><b>Explore Linear Maps and Quadratic Forms:</b> This interactive tool connects the linear transformation $Ax$ with the quadratic form $x^\top Ax$. Toggle "Force Symmetric" to explore the specific properties relevant to optimization (Hessians).</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Linear Map:</b> See how the unit circle transforms into an ellipse. The axes of the ellipse correspond to eigenvectors (for symmetric matrices).</li>
          <li><b>Quadratic Form:</b> Visualize the level sets of $z = x^\top Ax$.
            <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
              <li><b>Positive Definite:</b> Ellipses (Convex Bowl)</li>
              <li><b>Indefinite:</b> Hyperbolas (Saddle Point)</li>
            </ul>
          </li>
        </ul>
        <div id="widget-matrix-geometry" style="width: 100%; position: relative;"></div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Hessian Landscape Visualizer</h3>
        <p><b>Connect Eigenvalues to Function Curvature:</b> This 3D visualizer renders the surface of a quadratic function $f(x) = \frac{1}{2}x^\top Q x$ and displays its Hessian matrix $Q$. The eigenvalues of the Hessian directly control the curvature:</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Large positive eigenvalues:</b> Steep curvature (fast convergence in optimization)</li>
          <li><b>Small positive eigenvalues:</b> Flat directions (slow convergence)</li>
          <li><b>Condition number ($\kappa = \lambda_{\max}/\lambda_{\min}$):</b> When this ratio is large, gradient descent converges slowly</li>
        </ul>
        <p><i>Practical insight:</i> This visualization explains why preconditioning (transforming to balance eigenvalues) dramatically speeds up iterative solvers!</p>
        <div id="widget-hessian-landscape" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>5.5 The Loewner Order</h3>
      <p>For symmetric matrices $X, Y \in \mathbb{S}^n$, we write <b>$X \succeq Y$</b> if and only if $X - Y$ is <b>positive semidefinite (PSD)</b>, meaning:</p>
      $$
      v^\top (X - Y) v \ge 0 \quad \text{for all } v \in \mathbb{R}^n
      $$

      <div class="proof-box">
        <h4>Microfacts About the Loewner Order</h4>
        <ul>
          <li><b>Eigenvalue characterization:</b> $X \succeq 0$ if and only if all eigenvalues of $X$ are nonnegative.</li>
          <li><b>Inner product property:</b> If $X \succeq 0$ and $Y \succeq 0$, then:
            $$ \langle X, Y \rangle = \mathrm{tr}(XY) \ge 0 $$
            (This follows because $XY$ has nonnegative trace when both are PSD.)
          </li>
          <li><b>Partial order:</b> The relation $\succeq$ is reflexive, transitive, and antisymmetric (hence a partial order).</li>
        </ul>
      </div>

      <p>The Loewner order is the language of:</p>
      <ul>
        <li><b>Ellipsoids</b> (defined by PSD matrices in quadratic forms)</li>
        <li><b>PSD cone</b> $\mathbb{S}^n_+ = \{X \in \mathbb{S}^n \mid X \succeq 0\}$ in <a href="../02-convex-sets/index.html">Lecture 02</a></li>
        <li><b>Semidefinite programs (SDPs)</b> in <a href="../04-convex-opt-problems/index.html">Lecture 04</a></li>
      </ul>
    </section>

    <!-- SECTION 5: PROJECTIONS -->
    <section class="section-card" id="section-6">
      <h2>6. Projections onto Subspaces and Affine Sets</h2>

      <h3>Orthogonal Projection onto a Subspace</h3>
      <p>Let $\mathcal{S} \subseteq \mathbb{R}^m$ be a subspace and $b \in \mathbb{R}^m$. The <b>orthogonal projection</b> $p \in \mathcal{S}$ of $b$ is the unique vector in $\mathcal{S}$ minimizing $\|b - p\|_2$. It is characterized by the <b>orthogonality condition</b>:
      $$ b - p \perp \mathcal{S} \quad \iff \quad v^\top(b - p) = 0 \ \ \forall v \in \mathcal{S} $$</p>

      <h3>Projection via a Basis</h3>
      <p>If $Q \in \mathbb{R}^{m \times k}$ has orthonormal columns spanning $\mathcal{S}$, the projector is:
      $$ P = QQ^\top \quad \text{and} \quad p = Pb = QQ^\top b $$
      Check: $P^2 = P$ (idempotent) and $P^\top = P$ (symmetric)â€”that's what makes it an <b>orthogonal</b> projector.</p>

      <h3>Projection onto $\mathcal{R}(A)$ using $A$</h3>
      <p>If $A \in \mathbb{R}^{m \times n}$ has full column rank:
      $$ P = A(A^\top A)^{-1} A^\top \quad \text{and} \quad p = Pb $$
      </p>

      <h3>Projection onto an Affine Set</h3>
      <p>Consider an affine set defined by $\mathcal{A} = \{ x \in \mathbb{R}^n \mid Fx = g \}$, where $F \in \mathbb{R}^{p \times n}$ has full row rank and $g \in \mathbb{R}^p$. Projecting onto this set reduces to subspace projection by translating. This concept is generalized in <a href="../02-convex-sets/index.html">Lecture 02</a> to <b>projection onto convex sets</b> (POCS).</p>

      <h4>Parametric Representation</h4>
      <p>Every point in $\mathcal{A}$ can be written as $x = x_0 + Zt$, where:</p>
      <ul>
        <li>$x_0$ is any particular solution satisfying $Fx_0 = g$.</li>
        <li>$Z$ is a matrix whose columns form a basis for $\mathcal{N}(F)$ (the nullspace of $F$).</li>
        <li>$t \in \mathbb{R}^k$ where $k = n - p$ (dimension of the nullspace).</li>
      </ul>

      <h4>Euclidean Projection Formula</h4>
      <p>The <b>Euclidean projection</b> of any point $y \in \mathbb{R}^n$ onto $\mathcal{A}$ is given by:</p>
      $$
      \Pi_{\mathcal{A}}(y) = x_0 + Z(Z^\top Z)^{-1} Z^\top (y - x_0)
      $$

      <div class="proof-box">
        <h4>Geometric Interpretation and Derivation</h4>
        <p>This formula says: "Project $(y - x_0)$ onto the nullspace $\mathcal{N}(F)$, then shift back by $x_0$."</p>

        <div class="proof-step">
          <strong>Step 1: Translate to subspace problem.</strong>
          We want to find $x \in \mathcal{A}$ minimizing $\|x - y\|_2$. Let $x = x_0 + w$, where $w \in \mathcal{N}(F)$.
          The problem becomes minimizing $\|(x_0 + w) - y\|_2 = \|w - (y - x_0)\|_2$ subject to $w \in \mathcal{N}(F)$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Project onto nullspace.</strong>
          This is now a projection of the vector $(y - x_0)$ onto the subspace $\mathcal{N}(F)$.
          Since the columns of $Z$ form a basis for $\mathcal{N}(F)$, the projection matrix onto this subspace is $P_{\mathcal{N}(F)} = Z(Z^\top Z)^{-1} Z^\top$.
          Thus, the optimal $w^*$ is:
          $$ w^* = \Pi_{\mathcal{N}(F)}(y - x_0) = Z(Z^\top Z)^{-1} Z^\top (y - x_0) $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Shift back.</strong>
          The projection onto the affine set is $x^* = x_0 + w^*$.
          $$ \Pi_{\mathcal{A}}(y) = x_0 + Z(Z^\top Z)^{-1} Z^\top (y - x_0) $$
        </div>
      </div>

      <div class="proof-box">
        <h3>Example 1: Projection onto a Line</h3>
        <p>Let's find the projection of the vector $b = (2, 3)^\top$ onto the line spanned by the vector $u = (1, 1)^\top$. The projection $p$ is given by the formula:
        $$ p = \frac{u^\top b}{u^\top u} u = \frac{(1)(2) + (1)(3)}{(1)(1) + (1)(1)} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \frac{5}{2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2.5 \\ 2.5 \end{pmatrix} $$
        The residual vector is $r = b - p = (2 - 2.5, 3 - 2.5)^\top = (-0.5, 0.5)^\top$. We can verify that the residual is orthogonal to the line: $u^\top r = (1)(-0.5) + (1)(0.5) = 0$.
        </p>
      </div>

    </section>

    <!-- SECTION 6: LEAST SQUARES -->
    <section class="section-card" id="section-7">
      <h2>7. The Method of Least Squares</h2>

      <h3>The Problem: Overdetermined Systems</h3>
      <p>Often in practice, we encounter a system of linear equations $Ax=b$ where there is no exact solution because the vector $b$ does not lie in the column space of $A$. This is common when $m > n$ (more equations than unknowns). The goal of least squares is to find the "best" approximate solution by minimizing the squared Euclidean norm of the residual vector $r = Ax-b$. This is the canonical example of an <b>unconstrained optimization problem</b> (<a href="../01-introduction/index.html">Lecture 01</a>):
      $$ \min_{x \in \mathbb{R}^n} \|Ax - b\|_2^2 $$</p>

      <div class="insight">
        <h4>Forward Connection: Optimization Formulation</h4>
        <p>This problem is a specific instance of <b>Quadratic Programming</b> (<a href="../04-convex-opt-problems/index.html">Lecture 04</a>). In the notation of L01, the "loss function" is the squared Euclidean norm, and there are no constraints (or the feasible set is $\mathbb{R}^n$). Recognizing least squares as a convex optimization problem allows us to easily add regularizers (like $\ell_1$ in LASSO) or constraints (like $x \ge 0$).</p>
      </div>

      <h3>Geometric Interpretation: Projection</h3>
      <p>The solution to the least squares problem has a clean geometric interpretation. The vector $Ax$ lies in the column space of $A$, denoted $\mathcal{R}(A)$. The problem $\min_x \|Ax - b\|_2$ is therefore equivalent to finding the point $p \in \mathcal{R}(A)$ that is closest to $b$ in Euclidean distance.</p>

      <p>By the Pythagorean theorem, the closest point $p$ must satisfy a specific geometric condition: the error vector $b - p$ must be orthogonal to the subspace $\mathcal{R}(A)$. If the error vector were not orthogonal, we could project it onto the subspace to find a "correction" that brings us closer to $b$, contradicting the optimality of $p$. Thus, the optimal $p = Ax^\star$ is the <b>orthogonal projection</b> of $b$ onto $\mathcal{R}(A)$.</p>

      <h3>The Normal Equations</h3>
      <p>The orthogonality condition, $(b - Ax^\star) \perp \mathcal{R}(A)$, implies that the residual vector must be orthogonal to every basis vector of the subspace. Since the columns of $A$ span $\mathcal{R}(A)$, the residual must be orthogonal to each column of $A$. This can be expressed compactly as:
      $$ A^\top (b - Ax^\star) = 0 $$
      Expanding this yields the <b>normal equations</b>:
      $$ A^\top A x^\star = A^\top b $$
      If the matrix $A$ has linearly independent columns (full column rank), then $A^\top A$ is invertible, and we can write the unique solution as $x^\star = (A^\top A)^{-1} A^\top b$.</p>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Least Squares Projection</h3>
        <p><b>See the Geometry in 3D:</b> The solution $Ax^\star$ is the orthogonal projection of the target vector $b$ onto the subspace spanned by the columns of $A$ (the plane). The residual vector $r = b - Ax^\star$ connects the projection to the target and is perpendicular (orthogonal) to the plane.</p>
        <div id="widget-least-squares" style="width: 100%; height: 500px; position: relative;"></div>
      </div>

      <h3>Uniqueness of the Solution</h3>
      <ul>
        <li>If $A$ has full column rank ($\mathrm{rank}(A) = n$), the solution $x^\star$ is unique.</li>
        <li>If $A$ is rank-deficient ($\mathrm{rank}(A) < n$), there are infinitely many solutions. In this case, the pseudoinverse is used to find the solution with the minimum norm.</li>
      </ul>

      <div class="proof-box">
        <h3>Example 2: Solving Least Squares with Normal Equations</h3>
        <p>Let's solve the least squares problem for the system $Ax=b$ with:
        $$ A = \begin{pmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 1 \end{pmatrix}, \quad b = \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix} $$
        First, we form the normal equations $A^\top A x = A^\top b$:
        $$ A^\top A = \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & 1 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 1 \end{pmatrix} = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} $$
        $$ A^\top b = \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & 1 \end{pmatrix} \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 3 \\ 3 \end{pmatrix} $$
        We solve the system $\begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} x = \begin{pmatrix} 3 \\ 3 \end{pmatrix}$, which yields the solution $x^\star = (3/4, 3/4)^\top$.
        The projection is $p = Ax^\star = (3/2, 0, 3/2)^\top$, and the residual is $r = b - p = (1/2, 0, -1/2)^\top$. We can verify the orthogonality condition: $A^\top r = (0, 0)^\top$.
        </p>
      </div>

      <div class="proof-box">
        <h3>Example 3: Rank-Deficient Case</h3>
        <p>Consider the system with:
        $$ A = \begin{pmatrix} 1 & 1 \\ 2 & 2 \end{pmatrix}, \quad b = \begin{pmatrix} 3 \\ 6 \end{pmatrix} $$
        The columns of $A$ are linearly dependent, so the system is rank-deficient. The vector $b$ is in the column space of $A$, so there are infinitely many solutions. The normal equations are $A^\top A x = \begin{pmatrix} 5 & 5 \\ 5 & 5 \end{pmatrix} x = \begin{pmatrix} 15 \\ 15 \end{pmatrix}$. This system has infinitely many solutions of the form $x_1 + x_2 = 3$. The pseudoinverse would provide the minimum-norm solution, $x_1 = x_2 = 1.5$.
        </p>
      </div>

      <h3>7.4 Variants: Weighted and Constrained Least Squares</h3>


      <h3>(a) Weighted Least Squares (WLS)</h3>
      <p>Given SPD weight $W \in \mathbb{R}^{m \times m}$:
      $$ \min_x \|Ax - b\|_W^2 := (Ax - b)^\top W(Ax - b) $$
      Let $C$ satisfy $W = C^\top C$ (e.g., Cholesky). Then the problem is ordinary least squares in the <b>whitened</b> system $(CA)x \approx Cb$. Normal equations: $A^\top W A x = A^\top W b$.</p>

      <h3>(b) Constrained to an Affine Set</h3>
      <p>Solve:
      $$ \min_x \|Ax - b\|_2^2 \quad \text{s.t.} \quad Fx = g $$
      One method: parametrize $x = x_0 + Zy$, where $Fx_0 = g$ and columns of $Z$ form a basis for $\mathcal{N}(F)$. Then minimize $\|A(x_0 + Zy) - b\|_2^2$ over $y$ (an unconstrained LS). QR on $AZ$ is typically best.</p>

    </section>

    <!-- SECTION 7: QR, SVD, PSEUDOINVERSE (COMPREHENSIVE) -->
    <section class="section-card" id="section-8">
      <h2>8. Advanced Factorizations: QR, SVD, Pseudoinverse, and Applications</h2>

      <p><strong>Motivation:</strong> While Gaussian elimination and normal equations work in theory, numerical stability and efficiency demand better methods. The QR decomposition and Singular Value Decomposition (SVD) are standard tools in modern numerical linear algebraâ€”providing robust, stable solutions to least squares, revealing matrix structure, and enabling dimensionality reduction and data compression.</p>

      <!-- 7.1 QR Decomposition -->
      <h3>8.1 QR Decomposition (Orthogonal-Triangular Factorization)</h3>

      <h4>Theorem: Existence of QR Decomposition</h4>
      <div class="proof-box">
        <p><strong>Statement:</strong> Every matrix $A \in \mathbb{R}^{m \times n}$ with $m \ge n$ and linearly independent columns can be factored as:
        $$ A = QR $$
        where $Q \in \mathbb{R}^{m \times n}$ has orthonormal columns ($Q^\top Q = I_n$) and $R \in \mathbb{R}^{n \times n}$ is upper triangular with positive diagonal entries.</p>

        <div class="proof-step">
          <strong>Step 1: Gram-Schmidt Process.</strong> Let $a_1, \dots, a_n$ be the columns of $A$. We construct orthonormal vectors $q_1, \dots, q_n$ iteratively.
          <br>For $k=1$: Set $u_1 = a_1$. Define $r_{11} = \|u_1\|_2$ and $q_1 = u_1 / r_{11}$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Iterative Step.</strong> For $k=2, \dots, n$:
          <br>Project $a_k$ onto the space spanned by $q_1, \dots, q_{k-1}$:
          $$ p_k = \sum_{j=1}^{k-1} (q_j^\top a_k) q_j $$
          Define the orthogonal residual: $u_k = a_k - p_k$.
          <br>Set $r_{jk} = q_j^\top a_k$ for $j < k$.
          <br>Set $r_{kk} = \|u_k\|_2$ and $q_k = u_k / r_{kk}$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Matrix Form.</strong> Rearranging the equation for $a_k$:
          $$ a_k = \sum_{j=1}^{k-1} r_{jk} q_j + r_{kk} q_k $$
          This expresses the $k$-th column of $A$ as a linear combination of the first $k$ columns of $Q$.
          $$ A = \begin{bmatrix} a_1 & \cdots & a_n \end{bmatrix} = \begin{bmatrix} q_1 & \cdots & q_n \end{bmatrix} \begin{bmatrix} r_{11} & r_{12} & \cdots \\ 0 & r_{22} & \cdots \\ \vdots & \vdots & \ddots \end{bmatrix} = QR $$
          Since $r_{kk} > 0$ (assuming independent columns), $R$ is invertible.
        </div>
      </div>

      <h4>Solving Least Squares via QR</h4>
      <p>Given $A = QR$, the least-squares problem $\min \|Ax - b\|_2$ becomes:
      $$
      \|Ax - b\|_2 = \|QRx - b\|_2 = \|Rx - Q^\top b\|_2
      $$
      (using $\|Qy\|_2 = \|y\|_2$). Thus, we solve:
      $$ Rx^\star = Q^\top b $$
      by back substitution. This is <strong>numerically stable</strong> and avoids forming $A^\top A$, which squares the condition number.</p>

      <p><strong>Complexity:</strong> $O(mn^2)$ for computing QR, $O(n^2)$ for back substitution. For $m \gg n$, this is far more efficient than forming $A^\top A$ ($O(mn^2)$) and solving $A^\top A x = A^\top b$ ($O(n^3)$).</p>

      <!-- 7.2 Singular Value Decomposition -->
      <h3>8.2 Singular Value Decomposition (SVD)</h3>

      <h4>8.2.1 Definition and Existence</h4>
      <div class="proof-box">
        <p><strong>Theorem (<a href="#" class="definition-link">SVD</a>):</strong> Every matrix $A \in \mathbb{R}^{m \times n}$ admits a factorization:
        $$ A = U \Sigma V^\top $$
        where:</p>
        <ul>
          <li>$U \in \mathbb{R}^{m \times m}$ is orthogonal ($U^\top U = UU^\top = I_m$), with columns $u_1, \dots, u_m$ called <strong>left singular vectors</strong></li>
          <li>$V \in \mathbb{R}^{n \times n}$ is orthogonal ($V^\top V = VV^\top = I_n$), with columns $v_1, \dots, v_n$ called <strong>right singular vectors</strong></li>
          <li>$\Sigma \in \mathbb{R}^{m \times n}$ is a "diagonal" matrix with nonnegative entries $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_r > 0$ (the <strong>singular values</strong>), where $r = \text{rank}(A)$, and $\sigma_{r+1} = \cdots = 0$</li>
        </ul>

        <div class="proof-step">
          <strong>Proof:</strong>
          <p><b>Step 1: Eigenvalues of $A^\top A$.</b> The matrix $A^\top A$ is symmetric positive semidefinite. By the Spectral Theorem, there exists an orthogonal matrix $V = [v_1, \dots, v_n]$ and eigenvalues $\lambda_1 \ge \dots \ge \lambda_n \ge 0$ such that $A^\top A = V \text{diag}(\lambda_1, \dots, \lambda_n) V^\top$.</p>
          <p><b>Step 2: Define Singular Values.</b> Let $r$ be the rank of $A$. Then $\lambda_1, \dots, \lambda_r > 0$ and $\lambda_{r+1} = \dots = \lambda_n = 0$. Define $\sigma_i = \sqrt{\lambda_i}$ for $i=1,\dots,r$.</p>
          <p><b>Step 3: Construct Left Singular Vectors $U$.</b> For $i=1,\dots,r$, define $u_i = \frac{1}{\sigma_i} A v_i$.
          We verify they are orthonormal:
          $$ u_i^\top u_j = \frac{1}{\sigma_i \sigma_j} (A v_i)^\top (A v_j) = \frac{1}{\sigma_i \sigma_j} v_i^\top (A^\top A v_j) = \frac{1}{\sigma_i \sigma_j} v_i^\top (\lambda_j v_j) = \frac{\sigma_j^2}{\sigma_i \sigma_j} \delta_{ij} = \delta_{ij} $$
          </p>
          <p><b>Step 4: Completion.</b> Extend $\{u_1, \dots, u_r\}$ to an orthonormal basis $\{u_1, \dots, u_m\}$ for $\mathbb{R}^m$. Let $U = [u_1, \dots, u_m]$ and $\Sigma$ be the $m \times n$ matrix with $\sigma_i$ on the diagonal and zeros elsewhere.</p>
          <p><b>Step 5: Verification.</b> We check $A V = U \Sigma$.
          <br>For $i \le r$: $(A V)_i = A v_i = \sigma_i u_i = (U \Sigma)_i$.
          <br>For $i > r$: $A^\top A v_i = 0 \implies \|A v_i\|^2 = v_i^\top A^\top A v_i = 0 \implies A v_i = 0$. Also $(U \Sigma)_i = U \cdot 0 = 0$.
          <br>Thus $A V = U \Sigma$, which implies $A = U \Sigma V^\top$.</p>
        </div>
      </div>

      <h4>8.2.2 Compact and Truncated SVD</h4>
      <p>When $m \gg n$ or $A$ has low rank $r \ll \min(m,n)$, we can use more efficient representations:</p>
      <ul>
        <li><strong>Full SVD:</strong> $A = U_{m \times m} \Sigma_{m \times n} V^\top_{n \times n}$ (standard form, $U$ and $V$ square)</li>
        <li><strong>Compact (Reduced) SVD:</strong> $A = U_r \Sigma_r V_r^\top$, where $U_r \in \mathbb{R}^{m \times r}$, $\Sigma_r = \text{diag}(\sigma_1, \dots, \sigma_r) \in \mathbb{R}^{r \times r}$, $V_r \in \mathbb{R}^{n \times r}$. Eliminates zero singular values.</li>
        <li><strong>Truncated SVD (rank-$k$ approximation):</strong> $A_k = U_k \Sigma_k V_k^\top = \sum_{i=1}^k \sigma_i u_i v_i^\top$, using only the top $k$ singular values ($k < r$). This is the <strong>best rank-$k$ approximation</strong> to $A$ in both Frobenius and spectral norms (Eckart-Young theorem).</li>
      </ul>

      <h4>8.2.3 Geometric Interpretation</h4>
      <p>The SVD reveals how $A$ acts as a linear transformation:</p>
      <ol>
        <li><strong>$V^\top$:</strong> Rotates (and possibly reflects) input space, aligning with principal axes</li>
        <li><strong>$\Sigma$:</strong> Scales along these axes by $\sigma_1, \dots, \sigma_r$</li>
        <li><strong>$U$:</strong> Rotates (and possibly reflects) the scaled space to output space</li>
      </ol>
      <p>Thus, $A$ transforms the unit sphere in $\mathbb{R}^n$ into an ellipsoid in $\mathbb{R}^m$ with semi-axes of lengths $\sigma_1, \dots, \sigma_r$.</p>

      <div class="proof-box">
        <h4>Walkthrough: The Geometry of SVD in 2D</h4>
        <p>Let's visualize exactly how $A = U \Sigma V^\top$ transforms a vector $x$. Consider the matrix:</p>
        $$ A = \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix} $$
        <p>This simple matrix just stretches. But let's look at a general $A$. The transformation $Ax = U (\Sigma (V^\top x))$ happens in three steps:</p>
        <ol>
          <li><b>Step 1: Rotate (Input Space).</b> First, multiplication by the orthogonal matrix $V^\top$ performs a rotation (or reflection) of the vector $x$. It changes the basis from the standard coordinates to the eigenbasis of $A^\top A$.
            <br><i>Effect:</i> The unit circle remains a circle, just rotated.
          </li>
          <li><b>Step 2: Stretch.</b> Next, multiplication by the diagonal matrix $\Sigma$ stretches the rotated vector along the coordinate axes. The $i$-th coordinate is scaled by the singular value $\sigma_i$.
            <br><i>Effect:</i> The circle becomes an axis-aligned ellipse.
          </li>
          <li><b>Step 3: Rotate (Output Space).</b> Finally, multiplication by $U$ rotates the ellipse to its final orientation in the output space. The major axes of the ellipse align with the columns of $U$ (left singular vectors).
          </li>
        </ol>
        <p><b>Summary:</b> Every linear map is just a rotation, followed by a stretch, followed by another rotation. SVD gives you the coordinate systems ($V$ and $U$) in which the map is purely diagonal (a simple stretch).</p>
      </div>

      <h4>8.2.4 SVD and Matrix Norms</h4>
      <p>Singular values determine all major matrix norms:</p>
      <ul>
        <li><strong>Spectral norm (2-norm):</strong> $\|A\|_2 = \sigma_1 = \sigma_{\max}(A)$ (largest singular value)</li>
        <li><strong>Frobenius norm:</strong> $\|A\|_F = \sqrt{\sum_{i,j} a_{ij}^2} = \sqrt{\sum_{i=1}^r \sigma_i^2}$ (square root of sum of squared singular values)</li>
        <li><strong>Nuclear norm (trace norm):</strong> $\|A\|_* = \sum_{i=1}^r \sigma_i$ (sum of singular values, convex envelope of rank)</li>
      </ul>

      <h4>Derivation: Operator Norm and Singular Values</h4>
      <p>The spectral norm $\|A\|_2$ has a fundamental connection to the singular values.
      $$ \|A\|_2 = \max_{\|x\|_2=1} \|Ax\|_2 $$
      Consider the squared norm: $\|Ax\|_2^2 = \langle Ax, Ax \rangle = \langle x, A^\top A x \rangle$.
      This is a quadratic form with the symmetric PSD matrix $A^\top A$. The maximum value of the Rayleigh quotient $\frac{x^\top M x}{x^\top x}$ is the maximum eigenvalue $\lambda_{\max}(M)$.
      Therefore:
      $$ \|A\|_2^2 = \lambda_{\max}(A^\top A) = \sigma_1^2 \implies \|A\|_2 = \sigma_1 $$
      This confirms that the operator norm is the largest singular value, representing the maximum stretch factor of the matrix.
      </p>

      <h4>8.2.5 The Eckart-Young-Mirsky Theorem</h4>
      <div class="proof-box">
        <p><strong>Theorem (Optimal Low-Rank Approximation):</strong> Let $A = U \Sigma V^\top$ be the SVD of $A \in \mathbb{R}^{m \times n}$ with rank $r$. For any $k < r$, the best rank-$k$ approximation to $A$ in the Frobenius norm is:
        $$ A_k = \sum_{i=1}^k \sigma_i u_i v_i^\top = U_k \Sigma_k V_k^\top $$
        and the approximation error is:
        $$ \min_{\text{rank}(B) \le k} \|A - B\|_F = \|A - A_k\|_F = \sqrt{\sum_{i=k+1}^r \sigma_i^2} $$
        The same result holds for the spectral norm:
        $$ \min_{\text{rank}(B) \le k} \|A - B\|_2 = \|A - A_k\|_2 = \sigma_{k+1} $$</p>

        <div class="proof-step">
          <strong>Proof (Spectral Norm Case):</strong>
          <p>We want to show that if $\mathrm{rank}(B) \le k$, then $\|A - B\|_2 \ge \sigma_{k+1}$.</p>
          <p><b>Step 1: Construct a test subspace.</b> Let $V_{k+1}$ be the subspace spanned by the first $k+1$ right singular vectors $v_1, \dots, v_{k+1}$. For any $x \in V_{k+1}$ with $\|x\|_2=1$:
          $$ \|Ax\|_2 = \|\sum_{i=1}^{k+1} \sigma_i u_i v_i^\top x\|_2 = \sqrt{\sum_{i=1}^{k+1} \sigma_i^2 (v_i^\top x)^2} \ge \sigma_{k+1} \sqrt{\sum (v_i^\top x)^2} = \sigma_{k+1} $$
          </p>
          <p><b>Step 2: Use Rank-Nullity.</b> Since $\mathrm{rank}(B) \le k$, the nullspace $\mathcal{N}(B)$ has dimension at least $n-k$. The subspace $V_{k+1}$ has dimension $k+1$. The sum of dimensions is $(n-k) + (k+1) = n+1 > n$, so these two subspaces must have a non-trivial intersection.</p>
          <p><b>Step 3: Evaluate error on intersection.</b> Let $z \in \mathcal{N}(B) \cap V_{k+1}$ with $\|z\|_2=1$.
          $$ \|(A-B)z\|_2 = \|Az - Bz\|_2 = \|Az\|_2 \ge \sigma_{k+1} $$
          Thus $\max_{\|x\|=1} \|(A-B)x\|_2 \ge \sigma_{k+1}$, so $\|A-B\|_2 \ge \sigma_{k+1}$.</p>
          <p><b>Step 4: Achievability.</b> The truncated SVD $A_k$ has rank $k$, and $\|A - A_k\|_2 = \|\sum_{i=k+1}^r \sigma_i u_i v_i^\top\|_2 = \sigma_{k+1}$. Thus the lower bound is achieved.</p>
        </div>
      </div>

      <p><strong>Applications:</strong> This theorem is the foundation for:</p>
      <ul>
        <li><strong>Data compression:</strong> Store only top-$k$ singular vectors and values (image/video compression)</li>
        <li><strong>Denoising:</strong> Remove noise by truncating small singular values</li>
        <li><strong>Dimensionality reduction:</strong> Project data onto top-$k$ principal components (PCA)</li>
        <li><strong>Recommender systems:</strong> Matrix completion via low-rank factorization (Netflix Prize)</li>
      </ul>

      <h4>8.2.6 Connection to Eigendecomposition</h4>
      <p>For <strong>symmetric</strong> $A = A^\top$, the SVD reduces to the spectral theorem:</p>
      <ul>
        <li>Eigenvalues $\lambda_i$ and singular values $\sigma_i$ are related: $\sigma_i = |\lambda_i|$</li>
        <li>If $A \succeq 0$ (PSD), then $\lambda_i \ge 0$ and $\sigma_i = \lambda_i$, so $A = Q \Lambda Q^\top = U \Sigma V^\top$ with $U = V = Q$</li>
      </ul>
      <p>For <strong>general</strong> (non-symmetric) $A$:</p>
      <ul>
        <li>Eigenvalues of $A$ may be complex; singular values are always real and nonnegative</li>
        <li>$A^\top A$ has eigenvalues $\lambda_i = \sigma_i^2$ and eigenvectors $v_i$ (right singular vectors of $A$)</li>
        <li>$A A^\top$ has eigenvalues $\lambda_i = \sigma_i^2$ and eigenvectors $u_i$ (left singular vectors of $A$)</li>
      </ul>

      <!-- 7.3 Pseudoinverse (Moore-Penrose Inverse) -->

      <h4>Note: Connection to Polar Decomposition</h4>
      <p>The SVD is closely related to the <b>Polar Decomposition</b> $A = U P$, where $U$ is orthogonal and $P$ is symmetric positive semidefinite.
      <br>Using SVD $A = U \Sigma V^\top$, we can write:
      $$ A = (U V^\top) (V \Sigma V^\top) $$
      Here, $Q = U V^\top$ is orthogonal (rotation), and $P = V \Sigma V^\top$ is symmetric PSD (stretch). This parallels the complex number form $z = e^{i\theta} r$. In the PSD case ($U=V$), this reduces to $A = I (U \Sigma U^\top)$, which is just the spectral decomposition.</p>

      <h3>8.3 The Pseudoinverse (Moore-Penrose Inverse)</h3>

      <h4>8.3.1 Definition via SVD</h4>
      <p>For $A = U \Sigma V^\top$ with singular values $\sigma_1, \dots, \sigma_r > 0$, the <strong>pseudoinverse</strong> is:
      $$ A^+ = V \Sigma^+ U^\top $$
      where $\Sigma^+ \in \mathbb{R}^{n \times m}$ is formed by taking the reciprocal of nonzero singular values and transposing:
      $$ \Sigma^+ = \begin{bmatrix} \text{diag}(1/\sigma_1, \dots, 1/\sigma_r) & 0 \\ 0 & 0 \end{bmatrix}^\top $$</p>

      <h4>8.3.2 Moore-Penrose Axioms</h4>
      <p>The pseudoinverse $A^+$ is the <strong>unique</strong> matrix satisfying:</p>
      <ol>
        <li>$A A^+ A = A$</li>
        <li>$A^+ A A^+ = A^+$</li>
        <li>$(A A^+)^\top = A A^+$ (symmetric)</li>
        <li>$(A^+ A)^\top = A^+ A$ (symmetric)</li>
      </ol>
      <p>These axioms uniquely define $A^+$ without reference to SVD.</p>

      <h4>8.3.3 Properties and Applications</h4>
      <ul>
        <li><strong>Least squares:</strong> $x^+ = A^+ b$ is the <strong>minimum-norm least-squares solution</strong>: it minimizes $\|Ax - b\|_2$ over all $x$, and among all minimizers, it has the smallest $\|x\|_2$.</li>
        <li><strong>Inverse when invertible:</strong> If $A$ is square and invertible, $A^+ = A^{-1}$.</li>
        <li><strong>Projectors:</strong> $P_{\mathcal{R}(A)} = A A^+$ projects onto $\mathcal{R}(A)$; $P_{\mathcal{R}(A^\top)} = A^+ A$ projects onto $\mathcal{R}(A^\top)$.</li>
        <li><strong>Solution to $Ax = b$:</strong> The general solution is $x = A^+ b + (I - A^+ A) z$ for any $z \in \mathbb{R}^n$ (parametrizes the nullspace component).</li>
      </ul>

      <!-- 7.4 Principal Component Analysis (PCA) -->
      <h3>8.4 Principal Component Analysis (PCA) via SVD</h3>

      <p><strong>Problem:</strong> Given data $X \in \mathbb{R}^{n \times d}$ ($n$ samples, $d$ features), find a $k$-dimensional subspace that captures maximum variance.</p>

      <h4>PCA Algorithm:</h4>
      <ol>
        <li><strong>Center the data:</strong> $\tilde{X} = X - \mathbf{1} \mu^\top$ where $\mu = \frac{1}{n} X^\top \mathbf{1}$ (column means)</li>
        <li><strong>Compute SVD:</strong> $\tilde{X} = U \Sigma V^\top$</li>
        <li><strong>Principal components:</strong> The columns of $V$ are the <strong>principal directions</strong> (eigenvectors of covariance matrix $C = \frac{1}{n} \tilde{X}^\top \tilde{X}$)</li>
        <li><strong>Variance explained:</strong> The $i$-th principal component explains variance $\sigma_i^2 / n$</li>
        <li><strong>Projection:</strong> Project onto top-$k$ components: $Z = \tilde{X} V_k \in \mathbb{R}^{n \times k}$</li>
      </ol>

      <p><strong>Why SVD?</strong> The covariance matrix $C = \frac{1}{n} \tilde{X}^\top \tilde{X} = \frac{1}{n} V \Sigma^2 V^\top$ (from SVD). The eigenvalues of $C$ are $\sigma_i^2 / n$, and its eigenvectors are the columns of $V$. Thus, SVD directly provides the principal components without forming $C$ explicitly, which is more numerically stable and efficient.</p>

      <p><strong>Applications:</strong></p>
      <ul>
        <li>Dimensionality reduction for visualization (e.g., projecting $\mathbb{R}^{1000}$ to $\mathbb{R}^2$)</li>
        <li>Feature extraction in machine learning (reducing input dimension)</li>
        <li>Data compression (JPEG, video codecs)</li>
        <li>Noise reduction (truncate components with low variance)</li>
      </ul>

      <!-- 7.5 Condition Numbers and Perturbation Theory -->
      <h3>8.5 Condition Numbers and Numerical Stability</h3>

      <h4>8.5.1 Definition</h4>
      <p>The <strong>condition number</strong> of a matrix $A \in \mathbb{R}^{m \times n}$ with $\text{rank}(A) = n$ is:
      $$ \kappa(A) = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} = \frac{\sigma_1}{\sigma_n} $$
      For square nonsingular $A$, this is equivalent to $\kappa(A) = \|A\|_2 \|A^{-1}\|_2$.</p>

      <h4>8.5.2 Interpretation</h4>
      <p>The condition number measures how <strong>sensitive</strong> the solution of $Ax = b$ is to perturbations in $A$ or $b$:</p>
      <ul>
        <li><strong>Well-conditioned ($\kappa \approx 1$):</strong> $\sigma_1 \approx \sigma_n$; matrix is "balanced"; small changes in input â†’ small changes in output</li>
        <li><strong>Ill-conditioned ($\kappa \gg 1$):</strong> $\sigma_1 \gg \sigma_n$; matrix is nearly singular; small changes in input â†’ large changes in output</li>
      </ul>

      <h4>8.5.3 Perturbation Bound</h4>
      <div class="proof-box">
        <p><strong>Theorem (Relative Error Bound):</strong> Let $Ax = b$ and $A(x + \Delta x) = b + \Delta b$. If $\|\Delta b\|_2 / \|b\|_2$ is small, then:
        $$ \frac{\|\Delta x\|_2}{\|x\|_2} \le \kappa(A) \frac{\|\Delta b\|_2}{\|b\|_2} $$</p>

        <p><strong>Implication:</strong> A perturbation of size $\epsilon$ in $b$ can cause a perturbation of size $\kappa(A) \cdot \epsilon$ in $x$. If $\kappa(A) = 10^{10}$ and we have machine precision $\epsilon_{\text{mach}} \approx 10^{-16}$, we lose $\log_{10}(\kappa) = 10$ digits of precision!</p>
      </div>

      <h4>8.5.4 Numerical Stability of Normal Equations</h4>
      <p>The normal equations $A^\top A x = A^\top b$ square the condition number:
      $$ \kappa(A^\top A) = \frac{\sigma_1^2}{\sigma_n^2} = \kappa(A)^2 $$
      If $A$ is already ill-conditioned, forming $A^\top A$ significantly degrades precision. <strong>Consequently, QR and SVD are preferred for robust numerical solutions.</strong></p>

      <!-- 7.6 Numerical Algorithms -->
      <h3>8.6 Numerical Algorithms for SVD</h3>

      <h4>8.6.1 Golub-Kahan-Reinsch Algorithm</h4>
      <p>The standard algorithm for computing SVD:</p>
      <ol>
        <li><strong>Bidiagonalization:</strong> Reduce $A$ to bidiagonal form $B$ using Householder reflections: $A = U_1 B V_1^\top$ (cost: $O(mn^2)$)</li>
        <li><strong>Iterative diagonalization:</strong> Apply QR iterations to $B$ to diagonalize it: $B = U_2 \Sigma V_2^\top$ (cost: $O(n^2)$ iterations)</li>
        <li><strong>Combine:</strong> $A = (U_1 U_2) \Sigma (V_1 V_2)^\top$</li>
      </ol>
      <p><strong>Total complexity:</strong> $O(mn^2)$ for $m \ge n$. For large-scale problems, randomized SVD algorithms achieve $O(mn \log k)$ for rank-$k$ approximation.</p>

      <h4>8.6.2 Randomized SVD (for Large Matrices)</h4>
      <p>For massive matrices where exact SVD is prohibitive, <strong>randomized algorithms</strong> provide fast, accurate rank-$k$ approximations:</p>
      <ol>
        <li>Sample $k + p$ random vectors and form $Q$ via QR of $A \Omega$ ($\Omega$ random)</li>
        <li>Compute SVD of small matrix $Q^\top A$</li>
        <li>Reconstruct approximate SVD of $A$</li>
      </ol>
      <p>Complexity: $O(mn \log k)$ vs $O(mn^2)$ for full SVD. Used in big data applications (Netflix, Google).</p>

      <!-- Interactive Widgets -->
      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: SVD for Image Compression</h3>
        <p><b>The Power of Low-Rank Approximation:</b> The Singular Value Decomposition (SVD) reveals that many real-world matrices (like images) can be accurately approximated using only their largest singular values. This demo lets you:</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Load an image:</b> Any image is just a matrix of pixel values</li>
          <li><b>Compute SVD:</b> $A = U\Sigma V^\top$ where $\Sigma$ contains singular values in decreasing order</li>
          <li><b>Reconstruct with k values:</b> Use only the top $k$ singular values and see the quality vs. compression tradeoff</li>
          <li><b>Observe compression ratio:</b> Often 90%+ of the "energy" is captured by just 10-20% of singular values!</li>
        </ul>
        <p><i>Applications:</i> This principle underpins data compression, dimensionality reduction (PCA), recommender systems, and denoising. In optimization, low-rank structure enables efficient large-scale algorithms.</p>
        <div id="widget-svd-approximator" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Condition Number & Numerical Stability</h3>
        <p><b>Why Some Systems Are Harder to Solve:</b> This tool demonstrates how the condition number $\kappa(A) = \sigma_{\max}/\sigma_{\min}$ affects the convergence of iterative solvers. Compare two linear systems $Ax = b$ side by side:</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Well-conditioned ($\kappa \approx 1$):</b> Balanced eigenvalues â†’ fast, stable convergence</li>
          <li><b>Ill-conditioned ($\kappa \gg 1$):</b> Widely varying eigenvalues â†’ slow convergence, numerical instability</li>
        </ul>
        <p><i>Key takeaway:</i> This is why we avoid forming $A^\top A$ in least squares (it squares the condition number!) and prefer QR or SVD methods. In optimization, ill-conditioning makes gradient descent crawlâ€”motivating preconditioning and second-order methods.</p>
        <div id="widget-condition-number" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <!-- 7.7 Projection Revisited -->
      <h3>8.7 Projection Revisited (with SVD)</h3>
      <p>Using the SVD $A = U \Sigma V^\top$ with rank $r$, the projector onto $\mathcal{R}(A)$ is:
      $$ P_{\mathcal{R}(A)} = U_r U_r^\top = A A^+ $$
      where $U_r$ consists of the first $r$ columns of $U$ (corresponding to nonzero singular values). Similarly:
      $$ P_{\mathcal{R}(A^\top)} = V_r V_r^\top = A^+ A $$
      Both are symmetric ($P^\top = P$) and idempotent ($P^2 = P$), and they send any vector to its closest point in the respective subspace.</p>

      <h3>8.8 Worked Examples</h3>

      <h4>Example 8.1: Computing SVD by Hand (2Ã—2 Matrix)</h4>
      <div class="proof-box">
        <p><strong>Problem:</strong> Compute the SVD of $A = \begin{bmatrix} 3 & 0 \\ 4 & 5 \end{bmatrix}$.</p>

        <div class="proof-step">
          <strong>Step 1: Compute $A^\top A$.</strong>
          $$
          A^\top A = \begin{bmatrix} 3 & 4 \\ 0 & 5 \end{bmatrix} \begin{bmatrix} 3 & 0 \\ 4 & 5 \end{bmatrix} = \begin{bmatrix} 25 & 20 \\ 20 & 25 \end{bmatrix}
          $$
        </div>

        <div class="proof-step">
          <strong>Step 2: Find eigenvalues of $A^\top A$.</strong> The characteristic polynomial is:
          $$
          \det(A^\top A - \lambda I) = \det\begin{bmatrix} 25-\lambda & 20 \\ 20 & 25-\lambda \end{bmatrix} = (25-\lambda)^2 - 400 = \lambda^2 - 50\lambda + 225
          $$
          Solving: $\lambda_1 = 45$, $\lambda_2 = 5$. Thus $\sigma_1 = \sqrt{45} = 3\sqrt{5} \approx 6.708$ and $\sigma_2 = \sqrt{5} \approx 2.236$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Find eigenvectors (right singular vectors $V$).</strong>
          <ul>
            <li>For $\lambda_1 = 45$: $(A^\top A - 45I)v = 0$ gives $v_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$</li>
            <li>For $\lambda_2 = 5$: $(A^\top A - 5I)v = 0$ gives $v_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}$</li>
          </ul>
          Thus $V = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}$.
        </div>

        <div class="proof-step">
          <strong>Step 4: Compute left singular vectors $U$.</strong> Use $u_i = Av_i / \sigma_i$:
          $$
          u_1 = \frac{1}{\sigma_1} A v_1 = \frac{1}{3\sqrt{5}} \begin{bmatrix} 3 & 0 \\ 4 & 5 \end{bmatrix} \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{1}{3\sqrt{10}} \begin{bmatrix} 3 \\ 9 \end{bmatrix} = \frac{1}{\sqrt{10}} \begin{bmatrix} 1 \\ 3 \end{bmatrix}
          $$
          $$
          u_2 = \frac{1}{\sigma_2} A v_2 = \frac{1}{\sqrt{5}} \begin{bmatrix} 3 & 0 \\ 4 & 5 \end{bmatrix} \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1 \end{bmatrix} = \frac{1}{\sqrt{10}} \begin{bmatrix} 3 \\ -1 \end{bmatrix}
          $$
          Thus $U = \frac{1}{\sqrt{10}} \begin{bmatrix} 1 & 3 \\ 3 & -1 \end{bmatrix}$.
        </div>

        <div class="proof-step">
          <strong>Step 5: Assemble the SVD.</strong>
          $$
          A = U \Sigma V^\top = \frac{1}{\sqrt{10}} \begin{bmatrix} 1 & 3 \\ 3 & -1 \end{bmatrix} \begin{bmatrix} 3\sqrt{5} & 0 \\ 0 & \sqrt{5} \end{bmatrix} \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}
          $$
        </div>

        <div class="proof-step">
          <strong>Verification:</strong> Check that $U^\top U = I$, $V^\top V = I$, and $U \Sigma V^\top = A$.
        </div>
      </div>

      <h4>Example 8.2: Low-Rank Approximation for Data Compression</h4>
      <div class="proof-box">
        <p><strong>Problem:</strong> Given data matrix $X \in \mathbb{R}^{100 \times 50}$ (100 samples, 50 features) with $\text{rank}(X) = 50$, compute a rank-10 approximation that retains 90% of the "energy" (Frobenius norm squared).</p>

        <div class="proof-step">
          <strong>Step 1: Compute SVD.</strong> $X = U \Sigma V^\top$ where $\Sigma = \text{diag}(\sigma_1, \dots, \sigma_{50})$ with $\sigma_1 \ge \cdots \ge \sigma_{50} > 0$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Energy in each component.</strong> The total energy is:
          $$
          \|X\|_F^2 = \sum_{i=1}^{50} \sigma_i^2
          $$
          The energy retained by the rank-$k$ approximation is:
          $$
          \|X_k\|_F^2 = \sum_{i=1}^k \sigma_i^2
          $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Choose $k$ to retain 90% energy.</strong> Find the smallest $k$ such that:
          $$
          \frac{\sum_{i=1}^k \sigma_i^2}{\sum_{i=1}^{50} \sigma_i^2} \ge 0.90
          $$
          Suppose this gives $k = 10$ (typical for real data with decaying singular values).
        </div>

        <div class="proof-step">
          <strong>Step 4: Form approximation.</strong>
          $$
          X_{10} = U_{:,1:10} \Sigma_{1:10,1:10} V_{:,1:10}^\top = \sum_{i=1}^{10} \sigma_i u_i v_i^\top
          $$
        </div>

        <div class="proof-step">
          <strong>Storage savings:</strong>
          <ul>
            <li><strong>Original:</strong> $100 \times 50 = 5000$ numbers</li>
            <li><strong>Rank-10 approximation:</strong> $10(100 + 1 + 50) = 1510$ numbers (storing $U_{:,1:10}$, $\Sigma_{1:10,1:10}$, $V_{:,1:10}$)</li>
            <li><strong>Compression ratio:</strong> $5000/1510 \approx 3.3\times$ with 90% energy retained</li>
          </ul>
        </div>
      </div>

      <h4>Example 8.3: Solving Rank-Deficient Least Squares</h4>
      <div class="proof-box">
        <p><strong>Problem:</strong> Solve $\min \|Ax - b\|_2$ where $A = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \end{bmatrix}$ (rank 1) and $b = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$.</p>

        <div class="proof-step">
          <strong>Step 1: Recognize rank deficiency.</strong> The second row is $2 \times$ the first row, so $\text{rank}(A) = 1$. Normal equations $A^\top A x = A^\top b$ will be singular.
        </div>

        <div class="proof-step">
          <strong>Step 2: Compute (compact) SVD.</strong> For this simple case:
          $$
          A = \sigma_1 u_1 v_1^\top \quad \text{where } \sigma_1 = \sqrt{1^2 + 2^2 + 3^2 + 2^2 + 4^2 + 6^2} = \sqrt{70}
          $$
          $$
          u_1 = \frac{1}{\sqrt{5}} \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \quad v_1 = \frac{1}{\sqrt{14}} \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
          $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Compute pseudoinverse.</strong>
          $$
          A^+ = V \Sigma^+ U^\top = \frac{1}{\sigma_1} v_1 u_1^\top = \frac{1}{70} \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} \begin{bmatrix} 1 & 2 \end{bmatrix} = \frac{1}{70} \begin{bmatrix} 1 & 2 \\ 2 & 4 \\ 3 & 6 \end{bmatrix}
          $$
        </div>

        <div class="proof-step">
          <strong>Step 4: Minimum-norm solution.</strong>
          $$
          x^+ = A^+ b = \frac{1}{70} \begin{bmatrix} 1 & 2 \\ 2 & 4 \\ 3 & 6 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \frac{1}{70} \begin{bmatrix} 5 \\ 10 \\ 15 \end{bmatrix} = \frac{1}{14} \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
          $$
        </div>

        <div class="proof-step">
          <strong>Verification:</strong>
          <ul>
            <li>$Ax^+ = A \cdot \frac{1}{14}v_1 = \frac{\sigma_1}{14} u_1 v_1^\top v_1 = \frac{\sqrt{70}}{14} \cdot \sqrt{14} u_1 = \frac{1}{2}\sqrt{5} \cdot \frac{1}{\sqrt{5}}\begin{bmatrix} 1 \\ 2 \end{bmatrix} = \frac{1}{2}\begin{bmatrix} 1 \\ 2 \end{bmatrix}$</li>
            <li>Residual: $\|Ax^+ - b\|_2 = \left\|\frac{1}{2}\begin{bmatrix} 1 \\ 2 \end{bmatrix} - \begin{bmatrix} 1 \\ 2 \end{bmatrix}\right\|_2 = \frac{1}{2}\sqrt{5}$</li>
            <li>This is the minimum possible (since $b \notin \mathcal{R}(A)$)</li>
            <li>Among all minimizers, $x^+$ has the smallest norm: $\|x^+\|_2 = \frac{1}{14}\sqrt{14} = \frac{1}{\sqrt{14}}$</li>
          </ul>
        </div>
      </div>

      <h4>Example 8.4: Condition Number Analysis</h4>
      <div class="proof-box">
        <p><strong>Problem:</strong> Compare the condition numbers of $A$ and $A^\top A$ for $A = \begin{bmatrix} 1 & 0 \\ 0 & 0.01 \\ 0 & 0 \end{bmatrix}$.</p>

        <div class="proof-step">
          <strong>Step 1: Compute singular values of $A$.</strong> For this matrix, $\sigma_1 = 1$ and $\sigma_2 = 0.01$ (diagonal structure makes this immediate). Thus:
          $$
          \kappa(A) = \frac{\sigma_1}{\sigma_2} = \frac{1}{0.01} = 100
          $$
        </div>

        <div class="proof-step">
          <strong>Step 2: Compute $A^\top A$.</strong>
          $$
          A^\top A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0.01 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 0.01 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 0.0001 \end{bmatrix}
          $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Condition number of $A^\top A$.</strong> Eigenvalues are 1 and 0.0001, so:
          $$
          \kappa(A^\top A) = \frac{1}{0.0001} = 10000 = \kappa(A)^2
          $$
        </div>

        <div class="proof-step">
          <strong>Implication:</strong> If $A$ already has $\kappa(A) = 100$, forming $A^\top A$ makes it $\kappa = 10^4$â€”catastrophically worse. With machine precision $\epsilon_{\text{mach}} \approx 10^{-16}$, we lose $\log_{10}(10^4) = 4$ digits of precision!
        </div>

        <div class="proof-step">
          <strong>Solution:</strong> Use QR or SVD to avoid forming $A^\top A$ explicitly.
        </div>
      </div>

      <!-- 7.10 Applications to Optimization -->
      <h3>8.10 SVD Applications in Convex Optimization</h3>

      <h4>8.10.1 Preconditioning Gradient Descent</h4>
      <p>Consider minimizing a quadratic $f(x) = \frac{1}{2}x^\top A x - b^\top x$ where $A = U \Sigma^2 U^\top \succ 0$. Standard gradient descent has convergence rate dependent on $\kappa(A)$:</p>
      $$
      \left\|x^{(k)} - x^*\right\|_A \le \left(\frac{\kappa(A) - 1}{\kappa(A) + 1}\right)^k \|x^{(0)} - x^*\|_A
      $$
      <p>For $\kappa(A) = 10^4$, convergence is painfully slow. <strong>Preconditioning</strong> via SVD:</p>
      <ol>
        <li>Change variables: $y = \Sigma U^\top x$ (whitening transformation)</li>
        <li>In $y$-space, the problem becomes $\min \frac{1}{2}\|y\|_2^2 - \tilde{b}^\top y$ where $\tilde{b} = \Sigma^{-1} U^\top b$</li>
        <li>Hessian in $y$-space is $I$ (perfectly conditioned!), so gradient descent converges in one step</li>
        <li>Transform back: $x^* = U \Sigma^{-1} y^*$</li>
      </ol>
      <p>This is the essence of <strong>Newton's method</strong>: use second-order information (Hessian = $A$) to precondition.</p>

      <h4>8.10.2 Total Least Squares (TLS)</h4>
      <p>Standard least squares minimizes $\|Ax - b\|_2$ assuming $A$ is exact and $b$ is noisy. <strong>Total Least Squares</strong> accounts for errors in both:</p>
      $$
      \min_{\Delta A, \Delta b, x} \left\|\begin{bmatrix} \Delta A & \Delta b \end{bmatrix}\right\|_F \quad \text{s.t.} \quad (A + \Delta A)x = b + \Delta b
      $$
      <p><strong>Solution via SVD:</strong> Form augmented matrix $[A \mid b]$ and compute SVD. The TLS solution is given by the right singular vector corresponding to the smallest singular value. This is used in errors-in-variables regression.</p>

      <h4>8.10.3 Matrix Completion and Recommender Systems</h4>
      <p>The <strong>Netflix Prize</strong> problem: Given a sparse ratings matrix $R \in \mathbb{R}^{m \times n}$ (users Ã— movies) with only $\sim$1% entries observed, recover the full matrix. Assumption: $R$ is low-rank (users have a few latent preferences).</p>
      <p><strong>Formulation:</strong></p>
      $$
      \min_{X \in \mathbb{R}^{m \times n}} \text{rank}(X) \quad \text{s.t.} \quad X_{ij} = R_{ij} \text{ for observed } (i,j)
      $$
      <p>Since $\text{rank}(\cdot)$ is non-convex, relax to the <strong>nuclear norm</strong> (convex envelope of rank):</p>
      $$
      \min_{X \in \mathbb{R}^{m \times n}} \|X\|_* = \sum_i \sigma_i(X) \quad \text{s.t.} \quad X_{ij} = R_{ij} \text{ for observed } (i,j)
      $$
      <p>This is a <strong>semidefinite program</strong> (SDP), solvable via convex optimization. The nuclear norm promotes low-rank solutions.</p>

      <h4>8.10.4 Robust PCA: Separating Low-Rank and Sparse Components</h4>
      <p>Given data $M = L + S$ where $L$ is low-rank (signal) and $S$ is sparse (outliers/noise), recover both:</p>
      $$
      \min_{L, S} \|L\|_* + \lambda \|S\|_1 \quad \text{s.t.} \quad L + S = M
      $$
      <p>This convex formulation (Principal Component Pursuit) provably recovers $L$ and $S$ under mild conditions. Applications: video surveillance (background = low-rank, foreground = sparse), data cleaning, anomaly detection.</p>

      <h4>8.10.5 Dimensionality Reduction for Large-Scale Optimization</h4>
      <p>For problems with $n \gg 1$ variables but low effective dimension (e.g., $x$ lies near a $k$-dimensional subspace), use SVD to reduce dimension:</p>
      <ol>
        <li>Collect samples $x^{(1)}, \dots, x^{(m)}$ from the feasible set or initialization</li>
        <li>Form data matrix $X = [x^{(1)} \cdots x^{(m)}]$ and compute SVD: $X = U \Sigma V^\top$</li>
        <li>Identify dominant subspace: keep top $k$ left singular vectors $U_k$</li>
        <li>Parametrize $x = U_k y$ where $y \in \mathbb{R}^k$ (dimension reduction from $n$ to $k$)</li>
        <li>Solve optimization problem in reduced space (much faster)</li>
      </ol>
      <p>This is used in <strong>active subspace methods</strong> for high-dimensional inverse problems and PDE-constrained optimization.</p>

      <!-- 7.11 Computational Considerations -->
      <h3>8.11 Computational Considerations and Software</h3>

      <h4>8.11.1 When NOT to Compute Full SVD</h4>
      <p>For a matrix $A \in \mathbb{R}^{m \times n}$ with $m, n \approx 10^6$, full SVD costs $O(mn^2) \approx 10^{18}$ operationsâ€”infeasible! Alternatives:</p>
      <ul>
        <li><strong>Truncated SVD:</strong> Use iterative methods (Lanczos, Arnoldi) to compute only top $k$ singular values/vectors. Cost: $O(mnk)$ with $k \ll n$. Libraries: ARPACK, PROPACK.</li>
        <li><strong>Randomized SVD:</strong> Cost $O(mn \log k)$. Libraries: scikit-learn (<code>TruncatedSVD</code>), Facebook's <code>fbpca</code>, <code>redsvd</code>.</li>
        <li><strong>Streaming SVD:</strong> For data that doesn't fit in memory, update SVD incrementally as new rows arrive.</li>
      </ul>

      <h4>8.11.2 Numerical Stability: Avoiding $A^\top A$</h4>
      <p><strong>Bad practice (loses precision):</strong></p>
      <pre style="background: var(--panel); padding: 12px; border-radius: 4px; overflow-x: auto;">
# Python / NumPy example (DON'T DO THIS for ill-conditioned A!)
import numpy as np
AtA = A.T @ A
eigenvalues, V = np.linalg.eigh(AtA)
sigma = np.sqrt(eigenvalues)  # numerical errors amplified!</pre>

      <p><strong>Good practice (numerically stable):</strong></p>
      <pre style="background: var(--panel); padding: 12px; border-radius: 4px; overflow-x: auto;">
# Python / NumPy example
import numpy as np
U, sigma, Vt = np.linalg.svd(A, full_matrices=False)  # uses bidiagonalization</pre>

      <h4>8.11.3 Software Recommendations</h4>
      <table style="width: 100%; border-collapse: collapse; margin: 16px 0;">
        <thead>
          <tr style="background: var(--panel); border-bottom: 2px solid var(--border);">
            <th style="padding: 8px; text-align: left;">Language</th>
            <th style="padding: 8px; text-align: left;">Library</th>
            <th style="padding: 8px; text-align: left;">Function</th>
            <th style="padding: 8px; text-align: left;">Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr style="border-bottom: 1px solid var(--border);">
            <td style="padding: 8px;">Python</td>
            <td style="padding: 8px;"><code>numpy.linalg</code></td>
            <td style="padding: 8px;"><code>svd(A)</code></td>
            <td style="padding: 8px;">Full SVD via LAPACK (dgesdd)</td>
          </tr>
          <tr style="border-bottom: 1px solid var(--border);">
            <td style="padding: 8px;">Python</td>
            <td style="padding: 8px;"><code>scipy.sparse.linalg</code></td>
            <td style="padding: 8px;"><code>svds(A, k)</code></td>
            <td style="padding: 8px;">Sparse truncated SVD (top k)</td>
          </tr>
          <tr style="border-bottom: 1px solid var(--border);">
            <td style="padding: 8px;">MATLAB</td>
            <td style="padding: 8px;">Built-in</td>
            <td style="padding: 8px;"><code>[U,S,V] = svd(A)</code></td>
            <td style="padding: 8px;">Full or economy SVD</td>
          </tr>
          <tr style="border-bottom: 1px solid var(--border);">
            <td style="padding: 8px;">Julia</td>
            <td style="padding: 8px;"><code>LinearAlgebra</code></td>
            <td style="padding: 8px;"><code>svd(A)</code></td>
            <td style="padding: 8px;">Fast, via OpenBLAS/MKL</td>
          </tr>
          <tr>
            <td style="padding: 8px;">C/C++</td>
            <td style="padding: 8px;">LAPACK</td>
            <td style="padding: 8px;"><code>dgesvd</code> / <code>dgesdd</code></td>
            <td style="padding: 8px;">Production-grade, highly optimized</td>
          </tr>
        </tbody>
      </table>

      <h4>8.11.4 Sparse vs Dense SVD</h4>
      <p><strong>Sparse matrices:</strong> If $A$ has mostly zeros (e.g., graph Laplacians, finite element matrices), use specialized sparse SVD algorithms that exploit sparsity. Never convert to dense!</p>
      <p><strong>Structured matrices:</strong> For Toeplitz, circulant, or FFT-structured matrices, fast $O(n \log n)$ multiplication enables iterative SVD via Krylov methods.</p>

      <p><strong>Key Takeaway:</strong> <em>Prefer QR or SVD over normal equations for numerical stability. Use SVD when rank-deficiency, condition number analysis, or low-rank approximation is needed.</em></p>
    </section>

    <section class="section-card" id="section-9">
      <h2>9. Review & Implementation Guide</h2>

      <h3>Method Comparison: When to Use What</h3>
      <table style="width: 100%; border-collapse: collapse; margin: 16px 0;">
        <thead>
          <tr style="background: var(--panel); border-bottom: 2px solid var(--border);">
            <th style="padding: 8px; text-align: left;">Method</th>
            <th style="padding: 8px; text-align: left;">When to Use</th>
            <th style="padding: 8px; text-align: left;">Complexity</th>
          </tr>
        </thead>
        <tbody>
          <tr style="border-bottom: 1px solid var(--border);">
            <td style="padding: 8px;"><strong>Normal Equations</strong></td>
            <td style="padding: 8px;">Small, well-conditioned problems; theoretical derivations</td>
            <td style="padding: 8px;">$O(mn^2 + n^3)$</td>
          </tr>
          <tr style="border-bottom: 1px solid var(--border);">
            <td style="padding: 8px;"><strong>QR Decomposition</strong></td>
            <td style="padding: 8px;">Standard least squares; numerically stable; full rank</td>
            <td style="padding: 8px;">$O(mn^2)$</td>
          </tr>
          <tr style="border-bottom: 1px solid var(--border);">
            <td style="padding: 8px;"><strong>SVD</strong></td>
            <td style="padding: 8px;">Rank-deficient, ill-conditioned; minimum-norm solution; analysis</td>
            <td style="padding: 8px;">$O(mn^2)$</td>
          </tr>
          <tr>
            <td style="padding: 8px;"><strong>Randomized SVD</strong></td>
            <td style="padding: 8px;">Large-scale, low-rank approximation; big data</td>
            <td style="padding: 8px;">$O(mn \log k)$</td>
          </tr>
        </tbody>
      </table>

      <p><strong>Key Takeaway:</strong> <em>Prefer QR or SVD over normal equations for numerical stability. Use SVD when rank-deficiency, condition number analysis, or low-rank approximation is needed.</em></p>

      <h3>Key Formulas</h3>
      <ul>
        <li><b>Normal Equations:</b> $A^\top A x^\star = A^\top b$</li>
        <li><b>Projection onto $\mathcal{R}(A)$:</b> $P = A(A^\top A)^{-1}A^\top$</li>
        <li><b>QR Solution:</b> $Rx^\star = Q^\top b$</li>
        <li><b>Pseudoinverse:</b> $A^+ = V\Sigma^+ U^\top$</li>
        <li><b>Condition Number:</b> $\kappa(A) = \sigma_{\max} / \sigma_{\min}$</li>
        <li><b>Spectral Norm:</b> $\|A\|_2 = \sigma_{\max}(A) = \sqrt{\lambda_{\max}(A^\top A)}$</li>
        <li><b>Frobenius Norm:</b> $\|A\|_F = \sqrt{\sum \sigma_i^2} = \sqrt{\mathrm{tr}(A^\top A)}$</li>
      </ul>

      <h3>Implementation Mini-Guide</h3>
      <ul>
        <li>For standard, well-conditioned problems, <b>QR decomposition</b> is the recommended method. It is numerically stable and computationally efficient.</li>
        <li>For problems that are ill-conditioned or rank-deficient, the <b>SVD method</b> is the most robust choice. It is also necessary when the minimum-norm solution is required.</li>
        <li>Avoid explicitly forming the product $A^\top A$ and solving the normal equations, as this squares the condition number and can lead to a loss of numerical precision.</li>
        <li>Before solving, it is often a good practice to <b>preprocess data</b> by centering and scaling features. This can significantly improve the condition number.</li>
        <li>Always <b>verify the solution</b> by checking that the residual is orthogonal to the column space: $A^\top(b - Ax^*) \approx 0$.</li>
      </ul>

      <h3>Sanity Checklist</h3>
      <ul>
        <li>Always check the <b>dimensions</b> of your matrices and vectors.</li>
        <li>For least squares, check if your matrix has <b>full column rank</b> to determine if the solution is unique.</li>
        <li>Prefer <b>QR</b> or <b>SVD</b> over the normal equations for better numerical stability.</li>
        <li>If your results are unstable, check the <b>condition number</b> of your matrix and consider preprocessing your data.</li>
        <li>Remember the geometric interpretation: the least squares solution is a <b>projection</b>.</li>
      </ul>
    </section>


    <!-- SECTION 12: EXERCISES -->
    <section class="section-card" id="section-10">
      <h2><i data-feather="edit-3"></i> 10. Exercises</h2>

<div class="problem">
      <h3>P0.1 â€” Dual of $\ell_p$ Norm</h3>
      <p><b>Problem:</b> Show that for $1 < p < \infty$, the dual norm of $\|\cdot\|_p$ is $\|\cdot\|_q$ where $\frac{1}{p} + \frac{1}{q} = 1$. Handle the boundary cases $p = 1 \Rightarrow q = \infty$ and $p = \infty \Rightarrow q = 1$.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Dual Norms as "Worst-Case" Alignment:</b> The dual norm $\|y\|_* = \sup_{\|x\| \le 1} x^\top y$ measures the maximum possible projection of $y$ onto any vector in the primal unit ball. This concept is not just algebraic; it is the geometric foundation for <b>subgradients</b> (<a href="../03-convex-functions/index.html">Lecture 03</a>) and the definition of <b>dual cones</b> (<a href="../02-convex-sets/index.html">Lecture 02</a>).</li>
            <li><b>HÃ¶lder's Inequality:</b> $|x^\top y| \le \|x\|_p \|y\|_q$. <a href="#section-3">[Section 3]</a></li>
            <li><b>Achievability and Alignment:</b> The inequality is always "tight": for every $y$, there exists a specific unit vector $x$ (aligned with the geometry of the norm) such that $x^\top y = \|y\|_*$. This "alignment" vector is often the subgradient of the norm function.</li>
        </ul>
      </div>

      <div class="solution-box">
        <h4>Solution</h4>

        <div class="proof-step">
          <strong>Part 1: Upper Bound (HÃ¶lder's Inequality).</strong>
          Let $1/p + 1/q = 1$. For any $x$ with $\|x\|_p \le 1$:
          $$ x^\top y = \sum x_i y_i \le \sum |x_i y_i| \le \left(\sum |x_i|^p\right)^{1/p} \left(\sum |y_i|^q\right)^{1/q} = \|x\|_p \|y\|_q \le \|y\|_q $$
          Thus $\|y\|_* = \sup_{\|x\|_p \le 1} x^\top y \le \|y\|_q$.
        </div>

        <div class="proof-step">
          <strong>Part 2: Lower Bound (Achievability).</strong>
          We construct a specific $x$ that achieves the bound. Let $y \neq 0$. Define $x$ by:
          $$ x_i = \frac{\mathrm{sign}(y_i) |y_i|^{q-1}}{\|y\|_q^{q-1}} $$
          Check the norm of $x$:
          $$ \|x\|_p^p = \sum |x_i|^p = \frac{\sum |y_i|^{p(q-1)}}{\|y\|_q^{p(q-1)}} $$
          Since $1/p + 1/q = 1 \implies p+q = pq \implies p(q-1) = q$:
          $$ \|x\|_p^p = \frac{\sum |y_i|^q}{\|y\|_q^q} = \frac{\|y\|_q^q}{\|y\|_q^q} = 1 \implies \|x\|_p = 1 $$
          Now compute the inner product:
          $$ x^\top y = \sum \frac{\mathrm{sign}(y_i) |y_i|^{q-1} y_i}{\|y\|_q^{q-1}} = \frac{\sum |y_i|^q}{\|y\|_q^{q-1}} = \frac{\|y\|_q^q}{\|y\|_q^{q-1}} = \|y\|_q $$
          Since we found an $x$ with $\|x\|_p=1$ such that $x^\top y = \|y\|_q$, we have $\|y\|_* \ge \|y\|_q$.
        </div>

        <div class="proof-step">
          <strong>Conclusion:</strong> Since $\|y\|_* \le \|y\|_q$ and $\|y\|_* \ge \|y\|_q$, we have $\|y\|_* = \|y\|_q$.
          <br>The boundary cases ($p=1, q=\infty$) were proven in Section 13.
        </div>
      </div>

</div>
<div class="problem">
      <h3>P0.2 â€” Frobenius Cauchyâ€“Schwarz</h3>
      <p><b>Problem:</b> Prove that for matrices $X, Y \in \mathbb{R}^{m \times n}$:</p>
      $$
      |\langle X, Y \rangle| \le \|X\|_F \|Y\|_F
      $$


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Vectorization Isomorphism:</b> Matrices in $\mathbb{R}^{m \times n}$ behaves exactly like vectors in $\mathbb{R}^{mn}$ under the operation $\mathrm{vec}(A)$. This allows us to lift all standard Euclidean geometry resultsâ€”like angles and distancesâ€”directly to matrix spaces.</li>
            <li><b>The Frobenius Inner Product:</b> The definition $\langle A, B \rangle_F = \mathrm{tr}(A^\top B)$ is the natural inner product for matrices. It induces the Frobenius norm, which measures the "energy" or magnitude of a matrix just like the Euclidean norm measures a vector.</li>
            <li><b>Cauchy-Schwarz Generality:</b> Because the space of matrices equipped with the trace inner product is a valid Hilbert space, the Cauchy-Schwarz inequality $|\langle A, B \rangle| \le \|A\|_F \|B\|_F$ holds automatically. This is used to bound errors in matrix approximations.</li>
        </ul>
      </div>

<div class="proof-box">
        <h4>Solution</h4>
        <p>View matrices as vectors in $\mathbb{R}^{mn}$ by stacking columns. The Frobenius inner product corresponds to the standard Euclidean inner product on $\mathbb{R}^{mn}$, and the Frobenius norm corresponds to the Euclidean norm. Therefore, the standard Cauchy-Schwarz inequality applies directly.</p>
      </div>

</div>
<div class="problem">
      <h3>P0.3 â€” Loewner Order Transitivity</h3>
      <p><b>Problem:</b> If $X \succeq Y$ and $Y \succeq Z$ (all in $\mathbb{S}^n$), prove that $X \succeq Z$.</p>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>The Loewner Partial Order:</b> The relation $A \succeq B$ means that $A - B$ is Positive Semidefinite. This defines a <b>partial order</b> on symmetric matrices, which is fundamental to <b>Semidefinite Programming (SDP)</b> (<a href="../04-convex-opt-problems/index.html">Lecture 04</a>).</li>
            <li><b>Convex Cone Property:</b> The set of PSD matrices $\mathbb{S}^n_+$ forms a <b>convex cone</b>. A key property of any convex cone is closure under addition: if $X \in \mathbb{S}^n_+$ and $Y \in \mathbb{S}^n_+$, then $X+Y \in \mathbb{S}^n_+$.</li>
            <li><b>Transitivity from Closure:</b> The transitivity of the order ($X \succeq Y, Y \succeq Z \implies X \succeq Z$) follows directly from the cone's closure under addition: $(X-Y) + (Y-Z) = X-Z$. This allows us to chain matrix inequalities just like scalar inequalities.</li>
        </ul>
      </div>

<div class="proof-box">
        <h4>Solution</h4>
        <p>We have $X - Y \succeq 0$ and $Y - Z \succeq 0$. Therefore:</p>
        $$ X - Z = (X - Y) + (Y - Z) $$
        <p>Since the sum of two PSD matrices is PSD (each has nonnegative eigenvalues in their respective quadratic forms), we conclude $X - Z \succeq 0$, i.e., $X \succeq Z$.</p>
      </div>

</div>
<div class="problem">
      <h3>P0.4 â€” Projection onto Affine Set Example</h3>
      <p><b>Problem:</b> Find the Euclidean projection of $y = (1, 2, 3)^\top$ onto the affine set:</p>
      $$
      \mathcal{A} = \{ x \in \mathbb{R}^3 \mid x_1 + x_2 + x_3 = 1 \}
      $$


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Affine vs. Linear Subspaces:</b> An affine set $\mathcal{A} = \{x \mid Ax=b\}$ is just a shifted linear subspace. Geometrically, it's a "flat" surface that doesn't necessarily pass through the origin. Projections onto affine sets are computed by first shifting the problem to the origin (subspace projection) and then shifting back.</li>
            <li><b>The Structure of Solutions:</b> The general solution to a linear system is the sum of a <b>particular solution</b> $x_0$ and a generic element from the <b>nullspace</b> $\mathcal{N}(A)$. This decomposition is ubiquitous in optimization when dealing with equality constraints (<a href="../10-equality-constrained-minimization/index.html">Lecture 10</a>).</li>
            <li><b>Explicit Projection Formula:</b> For the specific case of a hyperplane $a^\top x = b$, the projection has a simple geometric form: move from $y$ in the direction of the normal vector $a$ until you hit the plane. This is the simplest instance of a constrained least-squares problem.</li>
        </ul>
      </div>

<div class="proof-box">
        <h4>Solution</h4>

        <div class="proof-step">
          <strong>Step 1: Particular solution.</strong> Choose $x_0 = (1, 0, 0)^\top$ (satisfies $x_1 + x_2 + x_3 = 1$).
        </div>

        <div class="proof-step">
          <strong>Step 2: Nullspace basis.</strong> $F = [1\ 1\ 1]$. A basis for the nullspace is given by any two linearly independent vectors whose components sum to zero. For example:
          $$ Z = \begin{bmatrix} 1 & 1 \\ -1 & 0 \\ 0 & -1 \end{bmatrix} $$
          (These are two linearly independent vectors orthogonal to $[1, 1, 1]^\top$.)
        </div>

        <div class="proof-step">
          <strong>Step 3: Alternative approach (direct formula).</strong> The projection onto the hyperplane $\{x \mid a^\top x = b\}$ is:
          $$ \Pi(y) = y - \frac{a^\top y - b}{\|a\|_2^2} a $$
          Here $a = (1, 1, 1)^\top$, $b = 1$, $a^\top y = 6$, $\|a\|_2^2 = 3$:
          $$ \Pi(y) = (1, 2, 3)^\top - \frac{6 - 1}{3}(1, 1, 1)^\top = (1, 2, 3)^\top - \frac{5}{3}(1, 1, 1)^\top $$
          $$ = \left(-\frac{2}{3}, \frac{1}{3}, \frac{4}{3}\right)^\top $$
        </div>

        <div class="proof-step">
          <strong>Verification:</strong> $-\frac{2}{3} + \frac{1}{3} + \frac{4}{3} = 1$ âœ“
        </div>
      </div>

</div>
<div class="problem">
      <h3>P0.5 â€” Subspace Fundamentals</h3>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Subspace Definition:</b> A subspace is a "flat" subset passing through the origin that is closed under linear combinations. In optimization, feasible sets for equality constraints ($Ax=0$) are subspaces.</li>
            <li><b>Fundamental Theorem of Linear Algebra:</b> The four fundamental subspaces ($\mathcal{R}(A), \mathcal{N}(A), \mathcal{R}(A^\top), \mathcal{N}(A^\top)$) are paired orthogonal complements. For example, $\mathcal{R}(A)^\perp = \mathcal{N}(A^\top)$. This orthogonality allows us to decompose any vector uniquely into a "range component" and a "nullspace component," which is the basis for the <b>Lagrange Multiplier</b> method.</li>
        </ul>
      </div>

<div class="proof-box">
        <h4>A1. Prove that the set of all linear combinations of a fixed set of vectors is a subspace.</h4>
        <p><b>Solution:</b> Let $S = \mathrm{span}\{v_1, \dots, v_k\}$.
        <br>1. <b>Closure under addition:</b> Let $u = \sum c_i v_i$ and $w = \sum d_i v_i$ be in $S$. Then $u+w = \sum (c_i+d_i)v_i$, which is also a linear combination, so $u+w \in S$.
        <br>2. <b>Closure under scalar multiplication:</b> Let $u = \sum c_i v_i \in S$ and $\alpha \in \mathbb{R}$. Then $\alpha u = \sum (\alpha c_i)v_i$, which is also a linear combination, so $\alpha u \in S$.
        <br>3. <b>Contains zero vector:</b> The zero vector is in $S$ because $0 = \sum 0 \cdot v_i$.
        <br>Since $S$ satisfies these three properties, it is a subspace.</p>
      </div>

      <div class="proof-box">
        <h4>A2. Show that $\mathcal{R}(A) \perp \mathcal{N}(A^\top)$.</h4>
        <p><b>Solution:</b> Let $v \in \mathcal{R}(A)$ and $y \in \mathcal{N}(A^\top)$. By definition, $v = Ax$ for some vector $x$, and $A^\top y = 0$. We want to show their inner product is zero:
        $$ v^\top y = (Ax)^\top y = x^\top A^\top y = x^\top (A^\top y) = x^\top 0 = 0 $$
        Since this holds for any $v \in \mathcal{R}(A)$ and $y \in \mathcal{N}(A^\top)$, the subspaces are orthogonal.</p>
      </div>

</div>
<div class="problem">
      <h3>P0.6 â€” Projections and Least Squares</h3>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>The Normal Equations:</b> The condition $A^\top A x = A^\top b$ arises from setting the gradient of the least-squares objective to zero. Geometrically, it expresses that the error vector $b - Ax$ must be orthogonal to every column of $A$.</li>
            <li><b>Projection as Optimization:</b> The least squares solution $x^*$ yields the orthogonal projection $p = Ax^*$ of $b$ onto the column space $\mathcal{R}(A)$. This connects approximation theory (finding the closest point) with optimization (minimizing a cost function).</li>
            <li><b>Numerical Stability (Critical):</b> While the normal equations are theoretically correct, solving them directly involves forming $A^\top A$, which squares the condition number $\kappa(A) \to \kappa(A)^2$. In practice, using <b>QR factorization</b> or <b>SVD</b> avoids this squaring and provides a much more stable solution, especially for ill-conditioned data.</li>
        </ul>
      </div>

<div class="proof-box">
        <h4>C1. Derive normal equations and prove uniqueness iff $\mathrm{rank}(A) = n$.</h4>
        <p><b>Solution:</b> The objective function is $f(x) = \|Ax-b\|_2^2 = x^\top A^\top A x - 2b^\top A x + b^\top b$. This is a quadratic function of $x$. To find the minimum, we take the gradient with respect to $x$ and set it to zero:
        $$ \nabla_x f(x) = 2A^\top A x - 2A^\top b = 0 \implies A^\top A x = A^\top b $$
        The solution is unique if and only if the matrix $A^\top A$ is invertible. This is true if and only if $A$ has full column rank, i.e., $\mathrm{rank}(A) = n$.
        </p>
      </div>

      <div class="proof-box">
        <h4>C2. Show that the residual at the LS solution is orthogonal to each column of $A$.</h4>
        <p><b>Solution:</b> Columns of $A$ span $\mathcal{R}(A)$. Orthogonality condition $A^\top(b - Ax^\star) = 0$ means $r^\star \perp \mathcal{R}(A)$.</p>
      </div>

      <div class="proof-box">
        <h4>C3. Solve a small overdetermined system by (i) normal equations, (ii) QR, (iii) SVD, and compare answers.</h4>
        <p><b>Solution:</b> Let's solve the system $Ax = b$ where:
        $$ A = \begin{pmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{pmatrix}, \quad b = \begin{pmatrix} 2 \\ 2 \\ 1 \end{pmatrix} $$
        This system is overdetermined (3 equations, 2 unknowns).</p>

        <p><b>(i) Normal Equations:</b> We solve $A^\top A x = A^\top b$.
        <br>Step 1: Compute $A^\top A = \begin{pmatrix} 1 & 0 & 1 \\ 1 & 1 & 0 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{pmatrix} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$.
        <br>Step 2: Compute $A^\top b = \begin{pmatrix} 1 & 0 & 1 \\ 1 & 1 & 0 \end{pmatrix} \begin{pmatrix} 2 \\ 2 \\ 1 \end{pmatrix} = \begin{pmatrix} 3 \\ 4 \end{pmatrix}$.
        <br>Step 3: Solve $\begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} x = \begin{pmatrix} 3 \\ 4 \end{pmatrix}$.
        <br>Multiplying by the inverse $(A^\top A)^{-1} = \frac{1}{3} \begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix}$, we get:
        $$ x = \frac{1}{3} \begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix} \begin{pmatrix} 3 \\ 4 \end{pmatrix} = \frac{1}{3} \begin{pmatrix} 2 \\ 5 \end{pmatrix} = \begin{pmatrix} 2/3 \\ 5/3 \end{pmatrix} \approx \begin{pmatrix} 0.667 \\ 1.667 \end{pmatrix} $$</p>

        <p><b>(ii) QR Decomposition:</b> We factor $A = QR$ where $Q$ has orthonormal columns.
        <br>Using Gram-Schmidt:
        <br>$q_1 = \frac{a_1}{\|a_1\|} = \frac{1}{\sqrt{2}} (1, 0, 1)^\top$.
        <br>$\tilde{q}_2 = a_2 - (q_1^\top a_2) q_1 = (1, 1, 0)^\top - \frac{1}{2}(1, 0, 1)^\top = (0.5, 1, -0.5)^\top$.
        <br>$\|\tilde{q}_2\| = \sqrt{0.25 + 1 + 0.25} = \sqrt{1.5} = \sqrt{3/2}$.
        <br>$q_2 = \frac{\tilde{q}_2}{\|\tilde{q}_2\|} = \sqrt{\frac{2}{3}} (0.5, 1, -0.5)^\top = \frac{1}{\sqrt{6}}(1, 2, -1)^\top$.
        <br>Then we solve $Rx = Q^\top b$:
        <br>$Q^\top b = \begin{pmatrix} \frac{1}{\sqrt{2}}(2+0+1) \\ \frac{1}{\sqrt{6}}(2+4-1) \end{pmatrix} = \begin{pmatrix} \frac{3}{\sqrt{2}} \\ \frac{5}{\sqrt{6}} \end{pmatrix}$.
        <br>$R = \begin{pmatrix} \sqrt{2} & \frac{1}{\sqrt{2}} \\ 0 & \frac{\sqrt{3}}{\sqrt{2}} \end{pmatrix}$.
        <br>Solving $\begin{pmatrix} \sqrt{2} & \frac{1}{\sqrt{2}} \\ 0 & \frac{\sqrt{3}}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} \frac{3}{\sqrt{2}} \\ \frac{5}{\sqrt{6}} \end{pmatrix}$ by back-substitution yields $x = (2/3, 5/3)^\top$.</p>

        <p><b>(iii) SVD:</b> $A = U \Sigma V^\top$. The solution is $x = V \Sigma^{-1} U^\top b$.
        <br>This computation yields the same result $x = (2/3, 5/3)^\top$.
        <br><b>Comparison:</b> All three methods agree. The Normal Equations are fastest but least stable ($\kappa^2$). QR is standard and stable ($\kappa$). SVD is most robust but most expensive.</p>
      </div>

      <div class="proof-box">
        <h4>C4. Show that the projection matrix $P = A(A^\top A)^{-1}A^\top$ is idempotent ($P^2=P$) and symmetric ($P^\top = P$).</h4>
        <p><b>Solution:</b>
        <br><b>Symmetry:</b>
        $$ P^\top = (A(A^\top A)^{-1}A^\top)^\top = (A^\top)^\top ((A^\top A)^{-1})^\top A^\top = A ((A^\top A)^\top)^{-1} A^\top = A (A^\top (A^\top)^\top)^{-1} A^\top = A(A^\top A)^{-1}A^\top = P $$
        <br><b>Idempotency:</b>
        $$ P^2 = (A(A^\top A)^{-1}A^\top)(A(A^\top A)^{-1}A^\top) = A(A^\top A)^{-1}(A^\top A)(A^\top A)^{-1}A^\top = A(A^\top A)^{-1} I A^\top = A(A^\top A)^{-1}A^\top = P $$
        </p>
      </div>

</div>
<div class="problem">
      <h3>P0.7 â€” Pseudoinverse & Rank Deficiency</h3>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Handling Rank Deficiency:</b> When a system $Ax=b$ has infinitely many solutions (non-trivial nullspace), the <b>Moore-Penrose Pseudoinverse</b> $A^+$ selects the unique solution with the <b>minimum Euclidean norm</b>. This acts as a natural "regularizer".</li>
            <li><b>General Solution Structure:</b> Any least-squares solution can be written as $x = A^+b + (I - A^+A)z$. The term $A^+b$ is the particular solution in the row space, while $(I - A^+A)z$ represents an arbitrary component in the nullspace.</li>
            <li><b>Projectors via Pseudoinverse:</b> The matrix $P = AA^+$ is the orthogonal projector onto the range $\mathcal{R}(A)$, and $I - AA^+$ projects onto the orthogonal complement. These operators are fundamental in <b>Projected Gradient Descent</b> methods.</li>
        </ul>
      </div>

<div class="proof-box">
        <h4>D1. Prove that every least-squares solution $x^\star$ satisfies $x^\star = A^+b + (I - A^+A)z$ for some $z$.</h4>
        <p><b>Solution:</b> From SVD, the set of minimizers is $A^+b + \mathcal{N}(A)$; note $(I - A^+A)$ projects onto $\mathcal{N}(A)$.</p>
      </div>

      <div class="proof-box">
        <h4>D2. Show $P = AA^+$ is the projector onto $\mathcal{R}(A)$ and $P^\perp = I - AA^+$ projects onto $\mathcal{N}(A^\top)$.</h4>
        <p><b>Solution:</b> Use SVD or basic projector algebra: $P^2 = P$, $P^\top = P$, range/nullspace relations.</p>
      </div>

</div>
<div class="problem">
      <h3>P0.8 â€” Practice Problems</h3>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Isometry and Orthogonality:</b> Orthogonal matrices $Q$ (where $Q^\top Q = I$) are <b>isometries</b>: they preserve lengths ($\|Qx\| = \|x\|$) and angles. They represent rigid rotations or reflections of space and are crucial for stable numerical algorithms (like QR and SVD) because they do not amplify errors.</li>
            <li><b>Projector Characterization:</b> A matrix $P$ represents an <b>orthogonal projection</b> if and only if it is both <b>symmetric</b> ($P=P^\top$) and <b>idempotent</b> ($P^2=P$). If it is idempotent but not symmetric, it represents a "skew" projection.</li>
            <li><b>Constructing Projectors:</b> To project onto a subspace spanned by columns of $A$, we use $P = A(A^\top A)^{-1}A^\top$. If the columns are orthonormal ($A=Q$), this simplifies to $P = QQ^\top$, avoiding the matrix inversion.</li>
        </ul>
      </div>

<div class="proof-box">
        <h4>A3. Show that $Q^\top Q = I$ implies $\|Qx\|_2 = \|x\|_2$.</h4>
        <p><b>Solution:</b> We start with the squared norm:
        $$ \|Qx\|_2^2 = (Qx)^\top(Qx) $$
        Using the property $(AB)^\top = B^\top A^\top$, this becomes:
        $$ \|Qx\|_2^2 = x^\top Q^\top Q x $$
        Since we are given that $Q^\top Q = I$, we substitute this in:
        $$ \|Qx\|_2^2 = x^\top I x = x^\top x = \|x\|_2^2 $$
        Taking the square root of both sides (and since norms are non-negative), we get $\|Qx\|_2 = \|x\|_2$. This shows that orthogonal transformations preserve the Euclidean norm (length) of vectors.</p>
      </div>

      <div class="proof-box">
        <h4>B1. Let $S = \mathrm{span}\{u, v\}$ with $u, v$ independent. Derive the projector $P$ onto $S$.</h4>
        <p><b>Solution:</b> Form $A = [u \ v]$. If $A$ has full column rank, $P = A(A^\top A)^{-1}A^\top$.</p>
      </div>

      <div class="proof-box">
        <h4>B2. Show that if $P$ is symmetric and idempotent, then it is an orthogonal projector.</h4>
        <p><b>Solution:</b> For any $b$, set $p = Pb$ and $r = b - Pb$. Then $Pr = 0$ (because $P^2 = P$); thus $r \in \mathcal{N}(P)$. For any $y = Pw \in \mathcal{R}(P)$, $y^\top r = w^\top P^\top(b - Pb) = w^\top P(b - Pb) = w^\top(Pb - P^2b) = 0$, so $r \perp \mathcal{R}(P)$. Hence $p$ is the orthogonal projection.</p>
      </div>

      <div class="proof-box">
        <h4>B3. Project $b = (1, 2, 3)^\top$ onto the affine set $\{x \mid [1 \ 1 \ 1]x = 3\}$.</h4>
        <p><b>Solution:</b> We want to find the point $p$ in the plane $x_1 + x_2 + x_3 = 3$ that is closest to $b = (1, 2, 3)^\top$.</p>

        <div class="proof-step">
          <strong>Step 1: Use the explicit formula.</strong>
          The projection of a vector $y$ onto the hyperplane $H = \{x \mid a^\top x = \beta\}$ is given by:
          $$ \Pi_H(y) = y - \frac{a^\top y - \beta}{\|a\|_2^2} a $$
          Here, $a = (1, 1, 1)^\top$, $\beta = 3$, and $y = (1, 2, 3)^\top$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Compute terms.</strong>
          <ul>
            <li>$a^\top y = 1(1) + 1(2) + 1(3) = 6$</li>
            <li>$\|a\|_2^2 = 1^2 + 1^2 + 1^2 = 3$</li>
            <li>Residual scalar: $\frac{a^\top y - \beta}{\|a\|_2^2} = \frac{6 - 3}{3} = \frac{3}{3} = 1$</li>
          </ul>
        </div>

        <div class="proof-step">
          <strong>Step 3: Compute projection.</strong>
          $$ p = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} - 1 \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \\ 2 \end{pmatrix} $$
        </div>

        <div class="proof-step">
          <strong>Step 4: Verify.</strong>
          Check if $p$ satisfies the constraint: $0 + 1 + 2 = 3$. Yes.
          Check if $b - p$ is orthogonal to the plane: $b - p = (1, 1, 1)^\top$, which is the normal vector $a$. Yes.
        </div>
      </div>

</div>

            <div class="problem">
        <h3>P0.9 â€” Trace Properties and Commutators</h3>
        <p>Let $A \in M_{m \times n}(\mathbb{R})$ and $B \in M_{n \times m}(\mathbb{R})$.</p>
        <ol type="a">
          <li>Prove that $\mathrm{tr}(AB) = \mathrm{tr}(BA)$.</li>
          <li>Prove that there do not exist $A, B \in M_n(\mathbb{R})$ such that $AB - BA = I_n$.</li>
        </ol>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Cyclic Property of Trace:</b> The identity $\mathrm{tr}(ABC) = \mathrm{tr}(BCA) = \mathrm{tr}(CAB)$ allows us to "rotate" matrices inside the trace. This is the key trick for deriving matrix gradients, such as $\nabla_X \mathrm{tr}(A X B) = A^\top B^\top$.</li>
            <li><b>Commutators and Quantum Mechanics:</b> The commutator $[A, B] = AB - BA$ captures the extent to which two matrices fail to commute. The fact that $\mathrm{tr}([A,B]) = 0$ implies that no two finite-dimensional matrices can satisfy the canonical commutation relation $AB - BA = I$ (a fundamental result in quantum mechanics).</li>
        </ul>
      </div>

      <div class="solution-box">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>(a) Trace Cyclic Property:</strong>
            <p><b>Coordinate Proof:</b></p>
            <p>Let $A = (a_{ij})$ and $B = (b_{jk})$. The $((i,i))$-th entry of $AB$ is $(AB)_{ii} = \sum_{k=1}^n a_{ik} b_{ki}$.</p>
            $$ \mathrm{tr}(AB) = \sum_{i=1}^m (AB)_{ii} = \sum_{i=1}^m \sum_{k=1}^n a_{ik} b_{ki} $$
            <p>Similarly, $(BA)_{kk} = \sum_{i=1}^m b_{ki} a_{ik}$.</p>
            $$ \mathrm{tr}(BA) = \sum_{k=1}^n (BA)_{kk} = \sum_{k=1}^n \sum_{i=1}^m b_{ki} a_{ik} $$
            <p>Since scalar multiplication commutes and finite sums can be reordered, the two expressions are identical.</p>

            <p><b>Conceptual Proof (Basis Independence):</b></p>
            <p>Interpret $A: \mathbb{R}^n \to \mathbb{R}^m$ and $B: \mathbb{R}^m \to \mathbb{R}^n$ as linear maps. The trace is defined as $\sum \langle e_i, T e_i \rangle$ for any orthonormal basis.</p>
            <p>Then $\mathrm{tr}(AB) = \sum_{i=1}^m \langle e_i, A B e_i \rangle = \sum_{i=1}^m \langle A^\top e_i, B e_i \rangle$.</p>
            <p>Similarly $\mathrm{tr}(BA) = \sum_{k=1}^n \langle f_k, B A f_k \rangle = \sum_{k=1}^n \langle B^\top f_k, A f_k \rangle$. Expanding these in coordinates yields the same double sum.</p>
          </div>

          <div class="proof-step">
            <strong>(b) Non-existence of Commutator Identity:</strong>
            <p>Assume for contradiction that $AB - BA = I_n$. Take the trace of both sides:</p>
            $$ \mathrm{tr}(AB - BA) = \mathrm{tr}(I_n) $$
            <p>Using linearity and part (a): $\mathrm{tr}(AB) - \mathrm{tr}(BA) = n \implies 0 = n$. This is a contradiction.</p>
            <div class="proof-step">
              <strong>Alternative Proof (Structural View):</strong> Define the linear map $T: M_n(\mathbb{R}) \to M_n(\mathbb{R})$ by $T(X) = AX - XA$.
              We proved in part (a) that for any $X$, $\mathrm{tr}(T(X)) = \mathrm{tr}(AX - XA) = 0$.
              Thus, the image of $T$ ($\mathrm{im}(T)$) is contained in the subspace of traceless matrices.
              Since $\mathrm{tr}(I_n) = n \neq 0$, the identity matrix does not lie in the image of $T$. Therefore, there is no $B$ such that $T(B) = AB - BA = I_n$.
            </div>
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P0.10 â€” Frobenius Inner Product and Norm</h3>
        <p>For $A, B \in M_{m \times n}(\mathbb{R})$, define $\langle A, B \rangle := \mathrm{tr}(A^\top B)$.</p>
        <ol type="a">
          <li>Show that $\langle \cdot, \cdot \rangle$ is an inner product and $\|A\|_{HS} := \sqrt{\langle A, A \rangle}$ is a norm.</li>
          <li>Prove the Cauchy-Schwarz inequality: $|\mathrm{tr}(A^\top B)| \le \|A\|_{HS} \|B\|_{HS}$.</li>
          <li>Prove submultiplicativity: $\|AB\|_{HS} \le \|A\|_{HS} \|B\|_{HS}$.</li>
        </ol>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Geometry of Matrix Space:</b> By equipping the vector space of matrices with the inner product $\langle A, B \rangle = \mathrm{tr}(A^\top B)$, we gain access to geometric tools like angles and projections for matrices. This space is isomorphic to $\mathbb{R}^{mn}$.</li>
            <li><b>Frobenius Norm:</b> The induced norm $\|A\|_F = \sqrt{\mathrm{tr}(A^\top A)}$ is simply the Euclidean norm of the matrix elements. It is the most common "entry-wise" measure of matrix size and is easier to work with than the operator norm for calculus purposes.</li>
            <li><b>Submultiplicativity:</b> The property $\|AB\|_F \le \|A\|_F \|B\|_F$ ensures that matrix multiplication is compatible with the norm topology, guaranteeing that the product of matrices is a continuous operation.</li>
        </ul>
      </div>

      <div class="solution-box">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>(a) Inner Product Axioms:</strong>
            <ul>
              <li><b>Symmetry:</b> $\langle A, B \rangle = \mathrm{tr}(A^\top B) = \mathrm{tr}((B^\top A)^\top) = \mathrm{tr}(B^\top A) = \langle B, A \rangle$.</li>
              <li><b>Bilinearity:</b> Linear in first argument by trace linearity: $\mathrm{tr}((\alpha A + C)^\top B) = \alpha \mathrm{tr}(A^\top B) + \mathrm{tr}(C^\top B)$.</li>
              <li><b>Positive Definiteness:</b> $\langle A, A \rangle = \sum_{i,j} a_{ij}^2 \ge 0$. Equality holds iff all $a_{ij}=0$.</li>
            </ul>
            <p>Since it is an inner product, $\|A\|_{HS}$ automatically satisfies norm axioms (homogeneity, triangle inequality via C-S).</p>
          </div>

          <div class="proof-step">
            <strong>(b) Cauchy-Schwarz:</strong>
            <p><b>Method 1 (Vectorization):</b> $\langle A, B \rangle = \langle \mathrm{vec}(A), \mathrm{vec}(B) \rangle_{\mathbb{R}^{mn}}$. Standard C-S applies.</p>
            <p><b>Method 2 (Quadratic Polynomial):</b> Consider $\phi(t) = \|A + tB\|_{HS}^2 \ge 0$.
            $$ \|A+tB\|_{HS}^2 = \langle A, A \rangle + 2t \langle A, B \rangle + t^2 \langle B, B \rangle $$
            This is a quadratic $at^2 + bt + c \ge 0$. The discriminant must be non-positive:
            $$ (2\langle A, B \rangle)^2 - 4 \langle A, A \rangle \langle B, B \rangle \le 0 \implies \langle A, B \rangle^2 \le \|A\|_{HS}^2 \|B\|_{HS}^2 $$
            Taking square roots gives the result.</p>
          </div>

          <div class="proof-step">
            <strong>(c) Submultiplicativity:</strong>
            <p>We want to show $\|AB\|_{HS} \le \|A\|_{HS} \|B\|_{HS}$.
            $$ \|AB\|_{HS}^2 = \sum_{i,j} \left(\sum_k a_{ik} b_{kj}\right)^2 $$
            By Cauchy-Schwarz on vectors (row $i$ of A, col $j$ of B): $(\sum_k a_{ik} b_{kj})^2 \le (\sum_k a_{ik}^2)(\sum_k b_{kj}^2)$.
            $$ \|AB\|_{HS}^2 \le \sum_{i,j} \left(\sum_k a_{ik}^2\right)\left(\sum_p b_{pj}^2\right) = \left(\sum_{i,k} a_{ik}^2\right)\left(\sum_{j,p} b_{pj}^2\right) = \|A\|_{HS}^2 \|B\|_{HS}^2 $$
            </p>
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P0.11 â€” Operator Norm</h3>
        <p>For $A \in M_{m \times n}(\mathbb{R})$, define the operator norm $\|A\| := \sup_{x \ne 0} \frac{\|Ax\|_2}{\|x\|_2}$.</p>
        <ol type="a">
          <li>Show that $\|\cdot\|$ is a norm.</li>
          <li>Show that $\|A\| = \sqrt{\lambda_{\max}(A^\top A)}$.</li>
          <li>Show that $\|AB\| \le \|A\| \|B\|$.</li>
          <li>Prove $|\det A| \le \|A\|^n$ for $A \in M_n(\mathbb{R})$.</li>
        </ol>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Operator Norm as "Maximum Gain":</b> The operator norm $\|A\|_2 = \sup_{\|x\|=1} \|Ax\|_2$ measures the maximum factor by which the matrix stretches a vector. Unlike the Frobenius norm (which measures average energy), this measures the <b>worst-case</b> amplification.</li>
            <li><b>Spectral Connection:</b> For the Euclidean norm, the operator norm is exactly the largest singular value, $\|A\|_2 = \sigma_{\max}(A)$. This links the geometric stretching property directly to the SVD.</li>
            <li><b>Determinant Bound:</b> The inequality $|\det A| \le \|A\|^n$ essentially says that the volume of the image of the unit cube (the determinant) cannot exceed the volume of a cube with side length equal to the maximum stretch factor.</li>
        </ul>
      </div>

      <div class="solution-box">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>(a) Norm Axioms:</strong>
            <ul>
              <li><b>Definiteness:</b> $\|A\| \ge 0$ is clear. If $\|A\|=0$, then $\|Ax\|=0$ for all $x$, so $Ax=0 \forall x$, implies $A=0$.</li>
              <li><b>Homogeneity:</b> $\|\alpha A\| = \sup \frac{\|\alpha A x\|}{\|x\|} = |\alpha| \sup \frac{\|Ax\|}{\|x\|} = |\alpha| \|A\|$.</li>
              <li><b>Triangle Inequality:</b> $\|(A+B)x\| \le \|Ax\| + \|Bx\| \le (\|A\| + \|B\|)\|x\|$. Taking sup gives $\|A+B\| \le \|A\| + \|B\|$.</li>
            </ul>
          </div>

          <div class="proof-step">
            <strong>(b) Spectral Characterization:</strong>
            <p>We show $\|A\| = \sqrt{\lambda_{\max}(A^\top A)}$.</p>
            <p><b>Step 1: Upper Bound.</b> For any $x \neq 0$:
            $$ \frac{\|Ax\|_2^2}{\|x\|_2^2} = \frac{x^\top A^\top A x}{x^\top x} \le \lambda_{\max}(A^\top A) $$
            (by the Rayleigh quotient bound). Thus $\|A\| \le \sqrt{\lambda_{\max}(A^\top A)}$.</p>
            <p><b>Step 2: Lower Bound.</b> Let $v$ be a unit eigenvector of $A^\top A$ corresponding to $\lambda_{\max}$. Then:
            $$ \|Av\|_2^2 = v^\top A^\top A v = v^\top (\lambda_{\max} v) = \lambda_{\max} \|v\|_2^2 = \lambda_{\max} $$
            Since the supremum is at least the value at this specific $v$, $\|A\| \ge \sqrt{\lambda_{\max}(A^\top A)}$.
            <br>Combining bounds gives equality.</p>
          </div>

          <div class="proof-step">
            <strong>(c) Submultiplicativity:</strong>
            <p>$\|ABx\| \le \|A\| \|Bx\| \le \|A\| \|B\| \|x\|$. Thus $\frac{\|ABx\|}{\|x\|} \le \|A\| \|B\|$. Taking sup yields $\|AB\| \le \|A\| \|B\|$.</p>
          </div>

          <div class="proof-step">
            <strong>(d) Determinant Bound:</strong>
            <p>$|\det A| = \prod \sigma_i$. Since $\|A\| = \sigma_1$ (largest singular value), all $\sigma_i \le \|A\|$.
            $$ |\det A| \le \prod_{i=1}^n \|A\| = \|A\|^n $$
            Equality holds if $A$ is a scaled orthogonal matrix.</p>
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P0.12 â€” Norm Equivalence and Orthogonal Invariance</h3>
        <p>Let $\mathcal{O}_n = \{ U \in M_n(\mathbb{R}) \mid U^\top U = I \}$.</p>
        <ol type="a">
          <li>Show that for $U \in \mathcal{O}_m, V \in \mathcal{O}_n$, $\|UAV\|_{HS} = \|A\|_{HS}$ and $\|UAV\| = \|A\|$.</li>
          <li>Show that $\|A\| \le \|A\|_{HS} \le \sqrt{n} \|A\|$.</li>
          <li>Show that convergence in one norm implies convergence in the other: $\|A_k - A\| \to 0 \iff \|A_k - A\|_{HS} \to 0$.</li>
        </ol>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Unitary Invariance:</b> A norm is unitary invariant if $\|UAV\| = \|A\|$ for all orthogonal $U, V$. Both the Frobenius and Operator norms satisfy this, meaning they depend only on the <b>singular values</b> of $A$. This makes them "natural" norms for coordinate-independent analysis.</li>
            <li><b>Equivalence of Norms:</b> In finite-dimensional spaces, all norms are equivalent: they define the same open sets and convergence sequences. However, the constants relating them (e.g., $\|A\|_{op} \le \|A\|_F \le \sqrt{n}\|A\|_{op}$) matter for tight convergence bounds in numerical linear algebra.</li>
        </ul>
      </div>

      <div class="solution-box">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>(a) Orthogonal Invariance:</strong>
            <p><b>HS Norm:</b> $\|UAV\|_{HS}^2 = \mathrm{tr}(V^\top A^\top U^\top U A V) = \mathrm{tr}(V^\top A^\top A V) = \mathrm{tr}(A^\top A V V^\top) = \mathrm{tr}(A^\top A) = \|A\|_{HS}^2$.</p>
            <p><b>Operator Norm:</b> Since orthogonal matrices are isometries, they preserve vector lengths. $\sup \frac{\|UAVx\|}{\|x\|} = \sup \frac{\|A(Vx)\|}{\|Vx\|} = \sup_{y \ne 0} \frac{\|Ay\|}{\|y\|} = \|A\|$.</p>
          </div>

          <div class="proof-step">
            <strong>(b) Comparison:</strong>
            <p>We prove $\|A\| \le \|A\|_{HS} \le \sqrt{n} \|A\|$ using algebraic properties.</p>
            <p><b>Lower Bound ($C_1=1$):</b> Recall that $\|A\|^2 = \lambda_{\max}(A^\top A)$. Also $\|A\|_{HS}^2 = \mathrm{tr}(A^\top A) = \sum_{i=1}^n \lambda_i(A^\top A)$.
            Since eigenvalues of $A^\top A$ are non-negative, $\lambda_{\max} \le \sum \lambda_i$. Thus:
            $$ \|A\|^2 \le \|A\|_{HS}^2 \implies \|A\| \le \|A\|_{HS} $$</p>
            <p><b>Upper Bound ($C_2=\sqrt{n}$):</b> Write $A$ in terms of its columns $A = [v_1 \ \dots \ v_n]$. Then $\|A\|_{HS}^2 = \sum_{j=1}^n \|v_j\|_2^2$.
            Note that $\|v_j\|_2 = \|A e_j\|_2 \le \|A\| \|e_j\|_2 = \|A\|$. Summing these bounds:
            $$ \|A\|_{HS}^2 = \sum_{j=1}^n \|v_j\|_2^2 \le \sum_{j=1}^n \|A\|^2 = n \|A\|^2 $$
            Taking square roots gives $\|A\|_{HS} \le \sqrt{n} \|A\|$.</p>
          </div>

          <div class="proof-step">
            <strong>(c) Equivalence of Convergence:</strong>
            <p>Apply the inequalities from (b) to the difference matrix $E_k = A_k - A$:
            $$ \|E_k\| \le \|E_k\|_{HS} \le \sqrt{n} \|E_k\| $$
            We examine both directions:</p>
            <ul>
                <li>If $\|A_k - A\| \to 0$, then by the upper bound, $0 \le \|A_k - A\|_{HS} \le \sqrt{n}\|A_k - A\| \to 0$.</li>
                <li>Conversely, if $\|A_k - A\|_{HS} \to 0$, then by the lower bound, $0 \le \|A_k - A\| \le \|A_k - A\|_{HS} \to 0$.</li>
            </ul>
            <p>So convergence in one norm implies convergence in the other. This illustrates that all norms on finite-dimensional spaces induce the same topology.</p>
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P0.13 â€” Properties of Orthogonal Matrices</h3>
        <p>Let $\mathcal{O}_n$ be the set of $n \times n$ orthogonal matrices.</p>
        <ol type="a">
          <li>Show that $\mathcal{O}_n$ is a group under multiplication.</li>
          <li>Show that $\mathcal{O}_n$ is a compact set.</li>
          <li>Prove that if $\langle Ax, Ay \rangle = \langle x, y \rangle$ for all $x, y$, then $A \in \mathcal{O}_n$.</li>
          <li>Prove that if $\|Ax\| = \|x\|$ for all $x$, then $A \in \mathcal{O}_n$.</li>
          <li>Prove that if $x \perp y \implies Ax \perp Ay$, then $A = cU$ for some $c \ge 0, U \in \mathcal{O}_n$.</li>
        </ol>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Compactness of $O(n)$:</b> The group of orthogonal matrices is a closed and bounded subset of matrix space. Its compactness is why optimization problems over orthogonal matrices (like finding the best rotation) are guaranteed to have solutions.</li>
            <li><b>Characterizations of Orthogonality:</b> A matrix is orthogonal iff it preserves inner products ($\langle Ax, Ay \rangle = \langle x, y \rangle$) iff it preserves norms ($\|Ax\| = \|x\|$). This equivalence underscores that preserving geometry (lengths and angles) is the defining feature of orthogonal transformations.</li>
            <li><b>Rigid Motions:</b> Transformations of the form $Ax + b$ with $A \in O(n)$ are rigid motions. They move objects without shearing or stretching them, preserving the "shape" of convex sets.</li>

        </ul>
      </div>

      <div class="solution-box">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>(a) Group:</strong> Closed under mult ($(AB)^\top AB = B^\top I B = I$) and inverse ($(A^{-1})^\top A^{-1} = A A^\top = I$).
          </div>

          <div class="proof-step">
            <strong>(b) Compactness:</strong> A subset of $\mathbb{R}^{n^2}$ is compact if and only if it is closed and bounded (Heine-Borel).
            <ul>
                <li><b>Boundedness:</b> For any $U \in \mathcal{O}_n$, $\|U\|_{HS}^2 = \mathrm{tr}(U^\top U) = \mathrm{tr}(I) = n$. Thus $\mathcal{O}_n$ lies on the sphere of radius $\sqrt{n}$, so it is bounded.</li>
                <li><b>Closedness:</b> Consider the continuous map $f(A) = A^\top A$. Then $\mathcal{O}_n = f^{-1}(\{I\})$. Since the singleton set $\{I\}$ is closed, its preimage $\mathcal{O}_n$ is closed.</li>
            </ul>
            Thus, $\mathcal{O}_n$ is compact.
          </div>

          <div class="proof-step">
            <strong>(c) Preserves IP $\implies$ Orthogonal:</strong>
            $\langle Ax, Ay \rangle = x^\top A^\top A y$. Condition implies $x^\top (A^\top A - I) y = 0$ for all $x,y$. Thus $A^\top A = I$.
          </div>

          <div class="proof-step">
            <strong>(d) Preserves Norm $\implies$ Orthogonal:</strong>
            Using the polarization identity:
            $$ \langle Ax, Ay \rangle = \frac{1}{2}(\|Ax\|^2 + \|Ay\|^2 - \|Ax - Ay\|^2) $$
            If norms are preserved, the RHS becomes $\frac{1}{2}(\|x\|^2 + \|y\|^2 - \|x-y\|^2) = \langle x, y \rangle$.
            Since inner products are preserved, by part (c), $A$ is orthogonal.
          </div>

          <div class="proof-step">
            <strong>(e) Preserves Orthogonality $\implies$ Scaled Orthogonal:</strong>
            <p>We are given that $\langle x, y \rangle = 0 \implies \langle Ax, Ay \rangle = 0$. We derive the structure of $A$ in steps.</p>
            <ul>
                <li><b>Step 1: Basis vectors.</b> Let $e_i$ be the standard basis. Since $\langle e_i, e_j \rangle = 0$ for $i \ne j$, we must have $\langle A e_i, A e_j \rangle = 0$. Thus, the columns $u_i = A e_i$ are pairwise orthogonal. Let $\alpha_i = \|u_i\| = \|A e_i\|$.</li>
                <li><b>Step 2: Weighted Inner Product.</b> For any $x, y$, we have $Ax = \sum x_i u_i$ and $Ay = \sum y_j u_j$.
                $$ \langle Ax, Ay \rangle = \sum_{i,j} x_i y_j \langle u_i, u_j \rangle = \sum_i \alpha_i^2 x_i y_i $$
                This defines a weighted inner product.</li>
                <li><b>Step 3: Force equal weights.</b> Fix $i \ne j$. Let $x = e_i + e_j$ and $y = e_i - e_j$. Then $\langle x, y \rangle = 1 - 1 = 0$.
                By hypothesis, $\langle Ax, Ay \rangle = 0$. Using the formula from Step 2:
                $$ \langle Ax, Ay \rangle = \alpha_i^2(1)(1) + \alpha_j^2(1)(-1) = \alpha_i^2 - \alpha_j^2 = 0 $$
                Thus $\alpha_i^2 = \alpha_j^2$ for all $i,j$. Let this common value be $c^2$.</li>
                <li><b>Step 4: Identify Matrix.</b> Since $\alpha_i = c$ for all $i$, we have $\langle Ax, Ay \rangle = c^2 \sum x_i y_i = c^2 \langle x, y \rangle$.
                This implies $\langle x, A^\top A y \rangle = \langle x, c^2 I y \rangle$ for all $x, y$, so $A^\top A = c^2 I$.</li>
                <li><b>Step 5: Factor.</b> If $c > 0$, let $U = \frac{1}{c} A$. Then $U^\top U = I$, so $U \in \mathcal{O}_n$, and $A = cU$. If $c=0$, $A=0$, which is $0 \cdot I$.</li>
            </ul>
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P0.14 â€” Generalized Inner Product</h3>
        <p>Let $A$ be a symmetric positive definite $n \times n$ matrix. Define $\langle x, y \rangle_A := x^\top A y$.</p>
        <ol type="a">
          <li>Show that $\langle \cdot, \cdot \rangle_A$ satisfies the inner product axioms.</li>
          <li>Prove the generalized Cauchy-Schwarz inequality: $(x^\top A y)^2 \le (x^\top A x)(y^\top A y)$.</li>
          <li>Show that the set $E = \{x \in \mathbb{R}^n \mid x^\top A x \le 1\}$ is convex.</li>
        </ol>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Mahalanobis Distance:</b> The inner product $\langle x, y \rangle_A = x^\top A y$ (where $A \succ 0$) defines a geometry where distance is measured relative to the "ellipsoid" defined by $A$. This is standard in statistics (covariance) and optimization (Newton's method uses the local Hessian metric).</li>
            <li><b>Cholesky Factorization:</b> Since $A \succ 0$, we can write $A = L L^\top$. This effectively transforms the generalized inner product back to the standard Euclidean one: $x^\top A y = (L^\top x)^\top (L^\top y)$. This "change of variables" is a powerful proof technique.</li>
            <li><b>Convexity of Ellipsoids:</b> The unit ball $\{x \mid x^\top A x \le 1\}$ is an ellipsoid. Since it is the sublevel set of a convex quadratic function (or simply a linear transformation of the Euclidean unit ball), it is a convex set.</li>
        </ul>
      </div>

      <div class="solution-box">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>(a) Inner Product Axioms:</strong>
            <ul>
              <li><b>Symmetry:</b> $\langle x, y \rangle_A = x^\top A y = (x^\top A y)^\top = y^\top A^\top x = y^\top A x = \langle y, x \rangle_A$ (since $A=A^\top$).</li>
              <li><b>Linearity:</b> $(ax+bz)^\top A y = a(x^\top A y) + b(z^\top A y)$.</li>
              <li><b>Positive Definiteness:</b> $\langle x, x \rangle_A = x^\top A x$. Since $A$ is PD, this is $\ge 0$ and equals $0$ iff $x=0$.</li>
            </ul>
          </div>

          <div class="proof-step">
            <strong>(b) Cauchy-Schwarz:</strong>
            <p>We prove $(x^\top A y)^2 \le (x^\top A x)(y^\top A y)$.</p>
            <p><b>Method 1: Quadratic Nonnegativity.</b> Consider the function $\phi(t) = \langle x - ty, x - ty \rangle_A$ for $t \in \mathbb{R}$.
            Since it is a squared norm, $\phi(t) \ge 0$ for all $t$. Expanding:
            $$ \phi(t) = \langle x, x \rangle_A - 2t \langle x, y \rangle_A + t^2 \langle y, y \rangle_A $$
            This is a quadratic $at^2 + bt + c \ge 0$. The discriminant must be non-positive:
            $$ b^2 - 4ac \le 0 \implies 4\langle x, y \rangle_A^2 - 4\langle x, x \rangle_A \langle y, y \rangle_A \le 0 $$
            Simplifying gives the inequality.</p>
            <p><b>Method 2: Factorization.</b> Let $A = T^\top T$. Then $\langle x, y \rangle_A = (Tx)^\top (Ty) = \langle Tx, Ty \rangle_2$.
            By standard Euclidean C-S: $|\langle Tx, Ty \rangle_2|^2 \le \|Tx\|_2^2 \|Ty\|_2^2$.
            Substituting back: $(x^\top A y)^2 \le (x^\top A x)(y^\top A y)$.</p>
          </div>

          <div class="proof-step">
            <strong>(c) Convexity of Ellipsoid:</strong>
            <p>The set $E$ is the unit ball for the induced norm $\|x\|_A = \sqrt{x^\top A x}$.
            Since every norm ball is convex (follows from triangle inequality), $E$ is convex.
            Alternatively, $f(x) = x^\top A x$ is a convex function (Hessian $2A \succ 0$), so its sublevel set is convex.</p>
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P0.15 â€” Explicit Description of PSD Cone</h3>
        <p><b>Problem:</b> We work in the space of real symmetric $n \times n$ matrices $\mathbb{S}^n$. The <b>positive semidefinite (PSD) cone</b> is defined as $\mathbb{S}^n_+ = \{X \in \mathbb{S}^n \mid z^\top X z \ge 0 \ \forall z \in \mathbb{R}^n\}$.
        Derive explicit inequalities in terms of the matrix entries for $X \in \mathbb{S}^n_+$ for the cases $n=1$, $n=2$, and $n=3$.</p>

        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
          <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
          <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
              <li><b>Sylvester's Criterion (General):</b> A symmetric matrix is Positive Definite ($X \succ 0$) if and only if all <b>leading</b> principal minors are positive. However, for Positive Semidefinite ($X \succeq 0$) matrices, we require <b>all</b> principal minors (not just the leading ones) to be non-negative.</li>
              <li><b>Principal Minors:</b> A principal minor is the determinant of a submatrix formed by selecting the same set of row and column indices. This includes the diagonal entries ($1 \times 1$ principal minors) and the full determinant.</li>
              <li><b>Geometric Intuition:</b> These algebraic conditions define the "walls" of the PSD cone. For $n=2$, it forms a specific rotated cone in 3D space.</li>
          </ul>
        </div>

        <div class="solution-box">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>Case $n=1$:</strong>
            Let $X = [x_1]$. The quadratic form is $z^\top X z = x_1 z^2$.
            For this to be non-negative for all $z \in \mathbb{R}$, we simply need:
            $$ x_1 \ge 0 $$
            Thus $\mathbb{S}^1_+ = \{ [x_1] \mid x_1 \ge 0 \}$, which is the non-negative ray.
          </div>

          <div class="proof-step">
            <strong>Case $n=2$:</strong>
            Let $X = \begin{bmatrix} x_1 & x_2 \\ x_2 & x_3 \end{bmatrix}$. The conditions are that all principal minors must be non-negative.
            <ul>
              <li><b>$1 \times 1$ minors (diagonal):</b> $x_1 \ge 0$ and $x_3 \ge 0$.</li>
              <li><b>$2 \times 2$ minor (determinant):</b> $\det(X) = x_1 x_3 - x_2^2 \ge 0$.</li>
            </ul>
            <p><b>Necessity:</b> Testing $z=e_1$ gives $x_1 \ge 0$. Testing $z=e_2$ gives $x_3 \ge 0$. Since $\det(X)$ is the product of eigenvalues $\lambda_1 \lambda_2$, and trace $x_1+x_3$ is the sum $\lambda_1+\lambda_2$, non-negative trace and determinant imply non-negative eigenvalues.</p>
            $$ \mathbb{S}^2_+ = \left\{ \begin{bmatrix} x_1 & x_2 \\ x_2 & x_3 \end{bmatrix} \bigg| x_1 \ge 0, \ x_3 \ge 0, \ x_1 x_3 - x_2^2 \ge 0 \right\} $$
          </div>

          <div class="proof-step">
            <strong>Case $n=3$:</strong>
            Let $X = \begin{bmatrix} x_1 & x_2 & x_3 \\ x_2 & x_4 & x_5 \\ x_3 & x_5 & x_6 \end{bmatrix}$.
            We require non-negativity of all principal minors:
            <ul>
              <li><b>$1 \times 1$ (Diagonal entries):</b>
              $$ x_1 \ge 0, \quad x_4 \ge 0, \quad x_6 \ge 0 $$
              </li>
              <li><b>$2 \times 2$ (Principal submatrices):</b>
              $$ x_1 x_4 - x_2^2 \ge 0 \quad (\text{indices } \{1,2\}) $$
              $$ x_1 x_6 - x_3^2 \ge 0 \quad (\text{indices } \{1,3\}) $$
              $$ x_4 x_6 - x_5^2 \ge 0 \quad (\text{indices } \{2,3\}) $$
              </li>
              <li><b>$3 \times 3$ (Full Determinant):</b>
              $$ \det(X) = x_1 x_4 x_6 + 2 x_2 x_3 x_5 - x_1 x_5^2 - x_4 x_3^2 - x_6 x_2^2 \ge 0 $$
              </li>
            </ul>
            These 7 inequalities fully characterize the PSD cone in 6 dimensions (since a $3 \times 3$ symmetric matrix has 6 degrees of freedom).
          </div>
        </div>
      </div>
</section>

    <!-- SECTION 13: SUMMARY -->


    <!-- READINGS -->
    <section class="section-card" id="section-11">
      <h2>11. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required Reading:</strong> Boyd & Vandenberghe, <em>Convex Optimization</em>, Appendix A.</li>
        <li><strong>Video Resource:</strong> Gilbert Strang's "The Fundamental Theorem of Linear Algebra" (MIT OpenCourseWare)</li>
        <li><strong>Additional Reference:</strong> Trefethen & Bau, <em>Numerical Linear Algebra</em> (for QR, SVD, and conditioning)</li>
      </ul>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>Â© <span id="year"></a> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initNormGeometryVisualizer } from './widgets/js/norm-geometry-visualizer.js';
    initNormGeometryVisualizer('widget-norm-geometry');
  </script>
  <script type="module">
    import { initOrthogonality } from './widgets/js/orthogonality.js';
    initOrthogonality('widget-orthogonality');
  </script>
  <script type="module">
    import { initRankNullspace } from './widgets/js/rank-nullspace.js';
    initRankNullspace('widget-rank-nullspace');
  </script>
  <script type="module">
    import { initMatrixGeometry } from './widgets/js/matrix-geometry.js';
    initMatrixGeometry('widget-matrix-geometry');
  </script>
  <script type="module">
    import { initSvdApproximator } from './widgets/js/svd-approximator.js';
    initSvdApproximator('widget-svd-approximator');
  </script>
  <script type="module">
    import { initHessianLandscapeVisualizer } from './widgets/js/hessian-landscape-visualizer.js';
    initHessianLandscapeVisualizer('widget-hessian-landscape');
  </script>
  <script type="module">
    import { initConditionNumber } from './widgets/js/condition-number.js';
    initConditionNumber('widget-condition-number');
  </script>
  <script type="module">
    import { initLeastSquaresVisualizer } from './widgets/js/least-squares-visualizer.js';
    initLeastSquaresVisualizer('widget-least-squares');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
