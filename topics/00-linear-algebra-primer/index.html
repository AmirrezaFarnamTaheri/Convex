<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>00. A Primer on Linear Algebra ‚Äî Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.128/build/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">
          <img src="../../static/assets/branding/logo.svg" alt="Logo" />
          <span>Convex Optimization</span>
        </a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../01-introduction/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header card-v2">
      <h1>00. A Primer on Linear Algebra</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-14</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, review, linear-algebra, foundational</span>
      </div>
      <div class="lecture-summary">
        <p><strong>Overview:</strong> This lecture establishes the rigorous linear algebra foundation required for convex optimization. We go beyond basic matrix operations to understand the geometry of linear maps (ellipsoids, volume scaling), spectral theory (eigenvalues, trace, determinant), and advanced factorizations (SVD, Polar Decomposition). These tools are the language of modern optimization.</p>
        <p><strong>Prerequisites:</strong> None.</p>
        <p><strong>Forward Connections:</strong> This algebra is the engine for the rest of the course. PSD matrices drive convex quadratic programs. The SVD underpins low-rank approximation and robust statistics. Separating hyperplanes (Lecture 02) and KKT conditions (Lecture 05) rely on the geometric intuition built here.</p>
      </div>
    </header>

    <section class="card-v2">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <ul>
        <li><b>Master Algebraic Invariants:</b> Rigorously define trace and determinant, prove their spectral identities ($\mathrm{tr}(A)=\sum \lambda_i$, $\det(A)=\prod \lambda_i$), and understand their invariance under similarity.</li>
        <li><b>Geometric Intuition:</b> Visualize linear maps as "rotate-stretch-rotate" operations that transform the unit ball into an ellipsoid. Understand volume scaling via the determinant.</li>
        <li><b>Quantify "Stretch":</b> Derive the operator norm $\|A\|_2$ from first principles using the Rayleigh quotient, and connect it to the maximum singular value.</li>
        <li><b>SVD and Polar Decomposition:</b> Construct the Singular Value Decomposition (SVD) step-by-step and derive the Polar Decomposition ($A=QH$) as the matrix equivalent of the polar form of a complex number.</li>
        <li><b>Matrix Calculus:</b> Compute gradients of trace, determinant, and log-determinant functions, essential for second-order optimization methods.</li>
      </ul>
    </section>

    <!-- SECTION 0: NOTATION -->
    <section class="card-v2" id="section-0">
      <h2>0. Notation and Primitives</h2>

      <p>We work in $\mathbb{R}^n$ unless otherwise stated. Vectors are column vectors.</p>

      <h3>Inner Product, Norm, and Angle</h3>
      <ul>
        <li><b>Standard Inner Product:</b> $\langle x, y \rangle = x^\top y = \sum_{i=1}^n x_i y_i$.</li>
        <li><b>Euclidean Norm:</b> $\|x\|_2 = \sqrt{\langle x, x \rangle} = \sqrt{\sum x_i^2}$.</li>
        <li><b>Angle:</b> For nonzero $x, y$, defined by $\cos \theta = \frac{\langle x, y \rangle}{\|x\|_2 \|y\|_2}$.
          <ul>
            <li>$\theta = 0$: Same direction.</li>
            <li>$\theta = \pi/2$: Orthogonal ($x^\top y = 0$).</li>
            <li>$\theta = \pi$: Opposite direction.</li>
          </ul>
        </li>
      </ul>

      <h3>Matrix Space Structure</h3>
      <p>For matrices $A, B \in \mathbb{R}^{m \times n}$:</p>
      <ul>
        <li><b>Frobenius Inner Product:</b> $\langle A, B \rangle_F = \mathrm{tr}(A^\top B) = \sum_{i,j} A_{ij} B_{ij}$.</li>
        <li><b>Frobenius Norm:</b> $\|A\|_F = \sqrt{\mathrm{tr}(A^\top A)} = \sqrt{\sum_{i,j} A_{ij}^2}$. This is just the Euclidean norm of the matrix elements flattened into a vector.</li>
      </ul>

      <div class="insight">
        <h4>üí° Why "Inner Product"?</h4>
        <p>The trace inner product allows us to do calculus on matrix spaces. For example, the directional derivative of a function $f(X)$ in direction $H$ is often written as $\langle \nabla f(X), H \rangle = \mathrm{tr}(\nabla f(X)^\top H)$.</p>
      </div>
    </section>

    <!-- SECTION 1: ALGEBRAIC INVARIANTS -->
    <section class="card-v2" id="section-1">
      <h2>1. Algebraic Invariants: Trace, Determinant, and Eigenvalues</h2>

      <p>Three scalars characterize a square matrix $A \in \mathbb{R}^{n \times n}$. They are "invariants" because they do not change under a change of basis (similarity transformation $B = PAP^{-1}$).</p>

      <h3>1.1 The Big Three</h3>
      <ul>
        <li><b>Trace:</b> $\mathrm{tr}(A) = \sum_{i=1}^n A_{ii}$. (Linear functional).</li>
        <li><b>Determinant:</b> $\det(A)$. (Multiplicative functional, volume scaling).</li>
        <li><b>Spectrum:</b> The set of eigenvalues $\{\lambda_1, \dots, \lambda_n\}$ (roots of the characteristic polynomial).</li>
      </ul>

      <h3>1.2 The Spectral Identities</h3>
      <div class="theorem-box">
        <h4>Theorem (Trace = Sum, Det = Product)</h4>
        <p>Let $\lambda_1, \dots, \lambda_n$ be the eigenvalues of $A$ (counted with algebraic multiplicity). Then:</p>
        $$ \sum_{i=1}^n \lambda_i = \mathrm{tr}(A) \qquad \text{and} \qquad \prod_{i=1}^n \lambda_i = \det(A) $$
      </div>

      <div class="proof-enhanced">
        <h4>Proof via Characteristic Polynomial</h4>
        <div class="proof-step">
          <strong>Step 1: Definition.</strong> The characteristic polynomial is $p_A(t) = \det(tI - A)$. Its roots are the eigenvalues.
        </div>
        <div class="proof-step">
          <strong>Step 2: Factored Form.</strong> Over $\mathbb{C}$, the polynomial factors as:
          $$ p_A(t) = \prod_{i=1}^n (t - \lambda_i) = t^n - \left(\sum \lambda_i\right) t^{n-1} + \cdots + (-1)^n \prod \lambda_i $$
        </div>
        <div class="proof-step">
          <strong>Step 3: Coefficient Matching.</strong> Expand $\det(tI - A)$ directly.
          <ul>
            <li>The coefficient of $t^{n-1}$ comes from terms choosing diagonal entries $(t-A_{ii})$ for $n-1$ positions. It evaluates to $-\sum A_{ii} = -\mathrm{tr}(A)$. Matching coefficients: $-\sum \lambda_i = -\mathrm{tr}(A)$.</li>
            <li>The constant term is $p_A(0) = \det(-A) = (-1)^n \det(A)$. Matching constant terms: $(-1)^n \prod \lambda_i = (-1)^n \det(A)$.</li>
          </ul>
        </div>
      </div>

      <h3>1.3 Trace Cyclicity and Similarity Invariance</h3>
      <p>A crucial property for matrix calculus is the <b>cyclicity of the trace</b>.</p>

      <div class="proof-enhanced">
        <h4>Proof: $\mathrm{tr}(AB) = \mathrm{tr}(BA)$</h4>
        <p>Let $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times m}$.
        $$
        \mathrm{tr}(AB) = \sum_{i=1}^m (AB)_{ii} = \sum_{i=1}^m \sum_{k=1}^n A_{ik} B_{ki} = \sum_{k=1}^n \sum_{i=1}^m B_{ki} A_{ik} = \sum_{k=1}^n (BA)_{kk} = \mathrm{tr}(BA)
        $$
        This implies invariance under similarity ($B = PAP^{-1}$):
        $$
        \mathrm{tr}(PAP^{-1}) = \mathrm{tr}(AP^{-1}P) = \mathrm{tr}(AI) = \mathrm{tr}(A)
        $$
        Similarly, $\det(PAP^{-1}) = \det(P)\det(A)\det(P^{-1}) = \det(A)$.</p>
      </div>

      <div class="insight">
        <h4>‚ö†Ô∏è Common Pitfalls (Traps)</h4>
        <ul>
          <li>$\det(A+B) \neq \det(A) + \det(B)$ in general. (Try $A=B=I$).</li>
          <li>$\mathrm{tr}(AB) \neq \mathrm{tr}(A)\mathrm{tr}(B)$ in general.</li>
          <li>The trace of a product is invariant under <i>cyclic</i> permutations only: $\mathrm{tr}(ABC) = \mathrm{tr}(BCA) \neq \mathrm{tr}(BAC)$.</li>
        </ul>
      </div>
    </section>

    <!-- SECTION 2: GEOMETRY OF LINEAR MAPS -->
    <section class="card-v2" id="section-2">
      <h2>2. Geometry of Linear Maps</h2>

      <p>A matrix $A$ is not just a table of numbers; it is a machine that transforms space. We can characterize it by how it transforms the fundamental shape: the unit ball.</p>

      <h3>2.1 Unit Ball to Ellipsoid</h3>
      <p>Let $\mathbb{B} = \{x : \|x\|_2 \le 1\}$ be the unit ball. The image $E = A(\mathbb{B})$ is an <b>ellipsoid</b>.</p>
      <p>If $A$ is invertible, we can derive the equation of this ellipsoid:</p>
      $$ y = Ax \iff x = A^{-1}y $$
      $$ \|x\|_2 \le 1 \iff \|A^{-1}y\|_2^2 \le 1 \iff y^\top (A^{-\top} A^{-1}) y \le 1 \iff y^\top (AA^\top)^{-1} y \le 1 $$
      <p>The matrix $M = AA^\top$ determines the shape. The semi-axes of the ellipsoid are the square roots of the eigenvalues of $AA^\top$ (which correspond to the singular values of $A$).</p>

      <h3>2.2 Volume Scaling</h3>
      <p>The determinant $|\det(A)|$ is the <b>volume expansion factor</b>.</p>
      $$ \mathrm{vol}(A(S)) = |\det(A)| \cdot \mathrm{vol}(S) $$
      <p>This explains why $\det(AB) = \det(A)\det(B)$: scaling by $B$ then $A$ is the same as scaling by the product.</p>

      <h3>2.3 Conformal Maps (Angle Preserving)</h3>
      <p>When does a linear map preserve angles? That is, when is $\angle(Ax, Ay) = \angle(x, y)$ for all $x, y$?</p>
      <div class="theorem-box">
        <h4>Theorem: Conformality</h4>
        <p>A matrix $A$ preserves angles if and only if $A^\top A = \alpha I$ for some scalar $\alpha > 0$.</p>
        <p>This means $A = \sqrt{\alpha} Q$ for some orthogonal matrix $Q$. Such maps are simply a <b>uniform scaling</b> followed by a <b>rotation/reflection</b>.</p>
      </div>
    </section>

    <!-- SECTION 3: QUANTIFYING STRETCH (NORM & SVD) -->
    <section class="card-v2" id="section-3">
      <h2>3. Quantifying "Stretch": Operator Norm and SVD</h2>

      <p>We want to measure the "size" of a matrix $A$ by how much it can stretch a vector.</p>

      <h3>3.1 Operator (Spectral) Norm</h3>
      <p>The operator norm induced by the Euclidean norm is:</p>
      $$ \|A\|_2 = \sup_{x \neq 0} \frac{\|Ax\|_2}{\|x\|_2} = \max_{\|x\|_2=1} \|Ax\|_2 $$
      <p>This is the length of the longest semi-axis of the image ellipsoid.</p>

      <div class="proof-enhanced">
        <h4>Derivation via Optimization (Rayleigh Quotient)</h4>
        <p>We want to maximize $\|Ax\|_2^2 = x^\top A^\top A x$ subject to $\|x\|_2^2 = 1$. Let $M = A^\top A$. $M$ is symmetric positive semidefinite.</p>
        <div class="proof-step">
          <strong>Step 1: Lagrangian.</strong>
          $$ \mathcal{L}(x, \lambda) = x^\top M x - \lambda(x^\top x - 1) $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Stationarity.</strong>
          $$ \nabla_x \mathcal{L} = 2Mx - 2\lambda x = 0 \implies Mx = \lambda x $$
        </div>
        <div class="proof-step">
          <strong>Step 3: Conclusion.</strong> The critical points are eigenvectors of $A^\top A$. The value of the objective at a critical point is $x^\top M x = x^\top (\lambda x) = \lambda$.
          The maximum is the largest eigenvalue, $\lambda_{\max}(A^\top A)$.
        </div>
        <div class="proof-step">
          <strong>Result:</strong> $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$.
        </div>
      </div>

      <h3>3.2 Singular Value Decomposition (SVD)</h3>
      <p>The geometric picture "Rotate input $\to$ Stretch axes $\to$ Rotate output" is formalized by the SVD.</p>
      <div class="theorem-box">
        <h4>Theorem (SVD)</h4>
        <p>Every matrix $A \in \mathbb{R}^{m \times n}$ can be factored as:</p>
        $$ A = U \Sigma V^\top $$
        <p>where:</p>
        <ul>
          <li>$U \in \mathbb{R}^{m \times m}$ is orthogonal ($U^\top U = I$).</li>
          <li>$V \in \mathbb{R}^{n \times n}$ is orthogonal ($V^\top V = I$).</li>
          <li>$\Sigma \in \mathbb{R}^{m \times n}$ is diagonal with entries $\sigma_1 \ge \sigma_2 \ge \dots \ge 0$.</li>
        </ul>
      </div>
      <p>The $\sigma_i$ are the <b>singular values</b>. They are the square roots of the eigenvalues of $A^\top A$.</p>

      <h3>3.3 Norms via Singular Values</h3>
      <p>Matrix norms can be read directly from the singular values:</p>
      <ul>
        <li><b>Spectral Norm:</b> $\|A\|_2 = \sigma_1$ (max stretch).</li>
        <li><b>Frobenius Norm:</b> $\|A\|_F = \sqrt{\sum \sigma_i^2}$ (aggregate energy).</li>
        <li><b>Nuclear Norm:</b> $\|A\|_* = \sum \sigma_i$ (convex relaxation of rank).</li>
      </ul>
      <p>Inequality chain: $\|A\|_2 \le \|A\|_F \le \sqrt{\mathrm{rank}(A)} \|A\|_2$.</p>
    </section>

    <!-- SECTION 4: POLAR DECOMPOSITION -->
    <section class="card-v2" id="section-4">
      <h2>4. Symmetric Matrices and Polar Decomposition</h2>

      <h3>4.1 The Real Spectral Theorem</h3>
      <p>If $S \in \mathbb{R}^{n \times n}$ is symmetric ($S = S^\top$), then:</p>
      <ol>
        <li>All eigenvalues are real.</li>
        <li>Eigenvectors corresponding to distinct eigenvalues are orthogonal.</li>
        <li>$S$ is orthogonally diagonalizable: $S = Q \Lambda Q^\top$ with $Q^\top Q = I$.</li>
      </ol>

      <h3>4.2 Positive Semidefinite (PSD) Matrices</h3>
      <p>$S$ is PSD ($S \succeq 0$) if all eigenvalues $\lambda_i \ge 0$. Equivalently, $S$ has a square root $B$ such that $S = B^\top B$.</p>

      <h3>4.3 Polar Decomposition</h3>
      <p>Just as a complex number $z$ can be written as $re^{i\theta}$ (magnitude $\times$ phase), a matrix $A$ can be factored into a "magnitude" and a "phase".</p>

      <div class="theorem-box">
        <h4>Theorem (Polar Decomposition)</h4>
        <p>Any square matrix $A$ can be factored as:</p>
        $$ A = QH $$
        <p>where $Q$ is orthogonal (rotation/reflection) and $H$ is symmetric PSD (pure stretch).</p>
      </div>

      <div class="proof-enhanced">
        <h4>Construction via SVD</h4>
        <p>Let $A = U \Sigma V^\top$. Insert $V^\top V = I$:</p>
        $$ A = U \Sigma V^\top = (U V^\top) (V \Sigma V^\top) $$
        <p>Define:</p>
        <ul>
          <li>$Q = U V^\top$. Since $U, V$ are orthogonal, $Q$ is orthogonal.</li>
          <li>$H = V \Sigma V^\top$. This is symmetric ($H^\top = H$) and its eigenvalues are $\sigma_i \ge 0$, so it is PSD.</li>
        </ul>
        <p>Note: $H = \sqrt{A^\top A}$. If $A$ is invertible, $Q$ is unique ($Q = AH^{-1}$).</p>
      </div>

      <div class="insight">
        <h4>üí° Corollary: Same Stretch Implies Orthogonal Relation</h4>
        <p>If $A^\top A = B^\top B$ and $B$ is invertible, then $AB^{-1}$ is orthogonal.
        <br><i>Proof:</i> Let $Q = AB^{-1}$. Then $Q^\top Q = B^{-\top} A^\top A B^{-1} = B^{-\top} (B^\top B) B^{-1} = I$.
        <br>Meaning: If two maps stretch space identically, they differ only by a rigid rotation.</p>
      </div>
    </section>

    <!-- SECTION 5: PROJECTIONS -->
    <section class="card-v2" id="section-5">
      <h2>5. Projections and Least Squares</h2>
      <p>The solution to $\min \|Ax - b\|_2^2$ is the projection of $b$ onto the column space $\mathcal{R}(A)$.</p>
      <ul>
        <li><b>Normal Equations:</b> $A^\top A x = A^\top b$.</li>
        <li><b>Projection Matrix:</b> $P = A(A^\top A)^{-1}A^\top$ (if full rank).</li>
        <li><b>Pseudoinverse:</b> For general $A$, the minimum norm solution is $x = A^+ b$, where $A^+ = V \Sigma^+ U^\top$.</li>
      </ul>
    </section>

    <!-- SECTION 6: MATRIX CALCULUS -->
    <section class="card-v2" id="section-6">
      <h2>6. Matrix Calculus You Will Use</h2>

      <p>Optimization requires taking derivatives with respect to matrices. Here are the identities you need for the course.</p>

      <table class="data-table">
        <thead>
          <tr>
            <th>Function $f(X)$</th>
            <th>Gradient $\nabla f(X)$</th>
            <th>Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>$\mathrm{tr}(A^\top X)$</td>
            <td>$A$</td>
            <td>Linear function</td>
          </tr>
          <tr>
            <td>$\mathrm{tr}(X^\top A X)$</td>
            <td>$(A + A^\top)X$</td>
            <td>Quadratic form</td>
          </tr>
          <tr>
            <td>$\log \det(X)$</td>
            <td>$X^{-1}$ (if symmetric)</td>
            <td>Crucial for interior point methods</td>
          </tr>
          <tr>
            <td>$X^{-1}$</td>
            <td>$-X^{-1} H X^{-1}$</td>
            <td>Directional derivative in direction $H$</td>
          </tr>
        </tbody>
      </table>

      <div class="proof-enhanced">
        <h4>Derivation: Gradient of $\log \det X$</h4>
        <p>Let $f(X) = \log \det X$. The directional derivative in direction $H$ is:</p>
        $$ \frac{d}{dt} \log \det(X + tH) \Big|_{t=0} = \mathrm{tr}(X^{-1} H) $$
        <p>Since the directional derivative is $\langle \nabla f(X), H \rangle = \mathrm{tr}(\nabla f(X)^\top H)$, we identify $\nabla f(X) = X^{-\top}$. For symmetric matrices, $\nabla f(X) = X^{-1}$.</p>
      </div>
    </section>

    <!-- SECTION 7: EXERCISES -->
    <section class="card-v2" id="section-7">
      <h2><i data-feather="edit-3"></i> 7. Exercises</h2>

      <div class="problem">
        <h3>Problem 1: Trace Cyclicity</h3>
        <p>Prove that $\mathrm{tr}(ABC) = \mathrm{tr}(BCA) = \mathrm{tr}(CAB)$. Does $\mathrm{tr}(ABC) = \mathrm{tr}(BAC)$ hold in general?</p>
      </div>

      <div class="problem">
        <h3>Problem 2: No-Go Commutator</h3>
        <p>Prove that there exist no matrices $A, B$ such that $AB - BA = I$. (Hint: Take the trace).</p>
      </div>

      <div class="problem">
        <h3>Problem 3: Rank-One Update</h3>
        <p>Let $A = I + uv^\top$. Compute the eigenvalues and determinant of $A$.</p>
      </div>

      <div class="problem">
        <h3>Problem 4: Operator Norm Properties</h3>
        <p>Prove that $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$. Use this to show $\|A\|_2 \le \|A\|_F$.</p>
      </div>

      <div class="problem">
        <h3>Problem 5: Polar Decomposition Uniqueness</h3>
        <p>If $A = Q H$ where $Q$ is orthogonal and $H$ is PD, show that $H = \sqrt{A^\top A}$. Conclude that if $A$ is invertible, the decomposition is unique.</p>
      </div>
    </section>

    <!-- SECTION 8: ADVANCED MATRIX ANALYSIS PROBLEMS -->
    <section class="card-v2" id="section-8">
      <h2>8. Advanced Matrix Analysis Problems</h2>

      <div class="problem">
        <h3>Problem 8: Trace Cyclic Property & Commutator</h3>
        <p><b>(a)</b> Prove that for square matrices $A, B$, $\mathrm{tr}(AB) = \mathrm{tr}(BA)$.
        <br><b>(b)</b> Prove that there are no matrices $A, B$ such that $AB - BA = I$.
        <br><i>Solution:</i> (a) $\mathrm{tr}(AB) = \sum_i \sum_k a_{ik}b_{ki} = \sum_k \sum_i b_{ki}a_{ik} = \mathrm{tr}(BA)$. (b) $\mathrm{tr}(AB - BA) = \mathrm{tr}(AB) - \mathrm{tr}(BA) = 0$, but $\mathrm{tr}(I) = n$. Contradiction.</p>
      </div>

      <div class="problem">
        <h3>Problem 9: Frobenius Inner Product</h3>
        <p>Let $\langle A, B \rangle = \mathrm{tr}(A^\top B)$. Prove:
        <br><b>(a)</b> It is a valid inner product.
        <br><b>(b)</b> Cauchy-Schwarz: $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.
        <br><b>(c)</b> Submultiplicativity: $\|AB\|_F \le \|A\|_F \|B\|_F$ (actually stronger: $\le \|A\|_2 \|B\|_F$).</p>
      </div>

      <div class="problem">
        <h3>Problem 10: Operator Norm & Spectral Radius</h3>
        <p>Define $\|A\| = \sup_{\|x\|=1} \|Ax\|$. Show that $\|A\| = \sqrt{\lambda_{\max}(A^\top A)}$. Deduce that for square $A$, $|\det(A)| \le \|A\|^n$.</p>
      </div>

      <div class="problem">
        <h3>Problem 11: Orthogonal Invariance</h3>
        <p>Show that if $U, V$ are orthogonal matrices, then $\|UAV\|_F = \|A\|_F$ and $\|UAV\|_2 = \|A\|_2$.
        <br><i>Solution:</i> $\|UAV\|_F^2 = \mathrm{tr}(V^\top A^\top U^\top U A V) = \mathrm{tr}(V^\top A^\top A V) = \mathrm{tr}(A^\top A V V^\top) = \mathrm{tr}(A^\top A) = \|A\|_F^2$.</p>
      </div>

      <div class="problem">
        <h3>Problem 12: The Orthogonal Group $O_n$</h3>
        <p><b>(a)</b> Show that $O_n = \{Q \mid Q^\top Q = I\}$ is a group.
        <br><b>(b)</b> Show that $O_n$ is compact.
        <br><b>(c)</b> Prove: if a linear map preserves inner products, it is orthogonal. If it preserves lengths, it is orthogonal.</p>
      </div>

      <div class="problem">
        <h3>Problem 13: Spectral Theorem Proof</h3>
        <p>Prove that if $A$ is real symmetric, all its eigenvalues are real.
        <br><i>Solution:</i> Let $Ax = \lambda x$. Then $x^* A x = \lambda x^* x$. Since $A$ is real symmetric, $(x^* A x)^* = x^* A^* x = x^* A x$, so $x^* A x$ is real. Since $x^* x > 0$ is real, $\lambda$ must be real.</p>
      </div>

      <div class="problem">
        <h3>Problem 14: Spectral Radius Formula</h3>
        <p>Prove Gelfand's formula: $\rho(A) = \lim_{k \to \infty} \|A^k\|^{1/k}$.
        <br><i>Hint:</i> Use Jordan form to bound $\|A^k\|$. The spectral radius is the infimum of the norm over all consistent norms.</p>
      </div>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>¬© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initNormGeometryVisualizer } from './widgets/js/norm-geometry-visualizer.js';
    initNormGeometryVisualizer('widget-norm-geometry');
  </script>
  <script type="module">
    import { initOrthogonality } from './widgets/js/orthogonality.js';
    initOrthogonality('widget-orthogonality');
  </script>
  <script type="module">
    import { initRankNullspace } from './widgets/js/rank-nullspace.js';
    initRankNullspace('widget-rank-nullspace');
  </script>
  <script type="module">
    import { initMatrixGeometry } from './widgets/js/matrix-geometry.js';
    initMatrixGeometry('widget-matrix-geometry');
  </script>
  <script type="module">
    import { initSvdApproximator } from './widgets/js/svd-approximator.js';
    initSvdApproximator('widget-svd-approximator');
  </script>
  <script type="module">
    import { initHessianLandscapeVisualizer } from './widgets/js/hessian-landscape-visualizer.js';
    initHessianLandscapeVisualizer('widget-hessian-landscape');
  </script>
  <script type="module">
    import { initConditionNumber } from './widgets/js/condition-number.js';
    initConditionNumber('widget-condition-number');
  </script>
  <script type="module">
    import { initLeastSquaresVisualizer } from './widgets/js/least-squares-visualizer.js';
    initLeastSquaresVisualizer('widget-least-squares');
  </script>
</body>
</html>
