<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>01. Introduction to Convex Optimization ‚Äî Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/styles.css" />
  <link rel="stylesheet" href="../../static/css/modern-widgets.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">
          <img src="../../static/assets/branding/logo.svg" alt="Logo" />
          <span>Convex Optimization</span>
        </a>
      </div>
      <nav class="nav">
        <a href="../../index.html">All Lectures</a>
        <a href="../00-linear-algebra-primer/index.html">‚Üê Previous</a>
        <a href="../02-convex-sets/index.html">Next ‚Üí</a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2>Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header card">
      <h1>01. Introduction: The What, Why, and How of Convex Optimization</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: intro, motivation, overview, modeling</span>
      </div>
      <div class="lecture-summary">
        <p><strong>Overview:</strong> This lecture introduces convex optimization by defining what makes a problem "convex" and proving that any local minimum is also global. We examine the canonical problem families (LP, QP, SOCP, SDP) and present the "loss + regularizer + constraints" modeling framework.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-primer/index.html">Lecture 00: Linear Algebra Primer</a> is required, particularly projections, PSD matrices, and norms.</p>
        <p><strong>Forward Connections:</strong> Least squares and QP examples build on projection theory from <a href="../00-linear-algebra-primer/index.html">Lecture 00</a>. Feasible sets introduced here connect to geometric concepts in <a href="../02-convex-sets/index.html">Lecture 02</a>.</p>
      </div>
    </header>

    <section class="card">
      <h2>Learning Objectives</h2>
      <ul>
        <li><b>Define Convex Problems:</b> Articulate the precise three-part definition and distinguish convex from nonconvex problems.</li>
        <li><b>Prove Global Optimality:</b> Show that every local minimum is global, and understand why this property fundamentally changes optimization.</li>
        <li><b>Recognize Canonical Families:</b> Identify LP, QP, SOCP, SDP by sight and know which real-world scenarios each family models.</li>
        <li><b>Apply Loss + Regularizer Paradigm:</b> Formulate problems using the "loss + regularizer + constraints" template and explain the bias-variance tradeoff.</li>
        <li><b>Use Safe Rewrites:</b> Transform norms, absolute values, and max functions into standard convex forms.</li>
        <li><b>Formulate and Sanity-Check:</b> Translate real-world problems to mathematical form, verify feasibility, and check for unboundedness.</li>
        <li><b>Understand the Solver Workflow:</b> Grasp the "formulate ‚Üí canonicalize ‚Üí solve ‚Üí verify" pipeline.</li>
      </ul>
    </section>

    <nav class="card table-of-contents">
      <h2>Table of Contents</h2>
      <ol>
        <li><a href="#section-1">What is a Convex Optimization Problem?</a></li>
        <li><a href="#section-2">The Crown Jewel: Local = Global</a></li>
        <li><a href="#section-3">Canonical Problem Families</a></li>
        <li><a href="#section-4">The Loss + Regularizer Paradigm</a></li>
        <li><a href="#section-5">Problem Set & Solutions</a></li>
        <li><a href="#section-6">Constraint Grammar (Explicit, Searchable)</a></li>
        <li><a href="#section-7">Tiny Modeling Rewrites You Can Already Use</a></li>
      </ol>
    </nav>

    <article>
      <section class="card" id="section-1">
        <h2>1. What is a Convex Optimization Problem?</h2>
        <p>In the vast landscape of optimization, problems divide into two categories: <b>convex</b> and <b>nonconvex</b>. This is not arbitrary‚Äîit's the fundamental line between problems we can solve efficiently and reliably, and those we generally cannot.</p>
        <figure>
          <img src="../../static/assets/topics/01-introduction/optimization-schematic.png" alt="Schematic overview of the optimization problem landscape" />
          <figcaption>Figure 1.1: The optimization landscape‚Äîconvex vs. nonconvex problems.</figcaption>
        </figure>
        <h3>Formal Definition</h3>
        <p>An optimization problem is <b>convex</b> if it can be written in the form:</p>
        $$
        \begin{aligned}
        \min_{x \in \mathbb{R}^n} \quad & f_0(x) && \text{(Objective)} \\
        \text{subject to} \quad & f_i(x) \le 0, && i=1,\dots,m \quad \text{(Inequalities)}\\
        & Ax = b && \text{(Equalities)}
        \end{aligned}
        $$
        <p>where <b>three critical conditions</b> are met:</p>
        <ol>
          <li>The objective function $f_0$ is a <b>convex function</b>.</li>
          <li>All inequality constraint functions $f_i$ are <b>convex functions</b>.</li>
          <li>All equality constraints are <b>affine</b> (i.e., of the form $Ax = b$).</li>
        </ol>
        <div class="insight">
          <h4>üí° Key Insight</h4>
          <p>This structure guarantees two profound properties: the <b>feasible set</b> (all points satisfying the constraints) is a convex set, and, as we'll prove next, <b>every local minimum is a global minimum</b>. This is the magic of convexity.</p>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Visualizer: Convex Combinations</h3>
          <p>Explore how convex combinations work by adjusting the parameter Œ∏ ‚àà [0,1]. Watch how the point moves along the line segment between two points, always staying within the feasible region.</p>
          <div id="widget-convex-combination" style="width: 100%; height: 400px; position: relative;"></div>
        </div>
      </section>

      <section class="card" id="section-2">
        <h2>2. The Crown Jewel: Local = Global</h2>
        <p>This property is what makes convex optimization fundamentally different from general optimization. In nonconvex problems, algorithms can get stuck in local minima, but for convex problems, any local minimum is guaranteed to be the global solution.</p>
        <div class="proof">
          <h4>Proof: Any Local Minimum is a Global Minimum</h4>

          <div class="proof-step">
            <strong>Step 1: Assume for contradiction.</strong> Let $x^*$ be a local minimum that is not a global minimum. Then there exists a feasible point $y$ with $f_0(y) < f_0(x^*)$.
          </div>

          <div class="proof-step">
            <strong>Step 2: Construct line segment.</strong> Since the feasible set is convex, any point on the line segment between $x^*$ and $y$, given by $z(\theta) = \theta y + (1-\theta)x^*$ for $\theta \in [0, 1]$, is also feasible.
          </div>

          <div class="proof-step">
            <strong>Step 3: Apply function convexity.</strong> By convexity of $f_0$:
            $$ f_0(z(\theta)) = f_0(\theta y + (1-\theta)x^*) \le \theta f_0(y) + (1-\theta)f_0(x^*) $$
          </div>

          <div class="proof-step">
            <strong>Step 4: Use the assumption.</strong> Since $f_0(y) < f_0(x^*)$:
            $$ f_0(z(\theta)) < \theta f_0(x^*) + (1-\theta)f_0(x^*) = f_0(x^*) $$
            This holds for any $\theta \in (0, 1)$.
          </div>

          <div class="proof-step">
            <strong>Step 5: Derive contradiction.</strong> By choosing $\theta > 0$ sufficiently small, we can find points $z(\theta)$ arbitrarily close to $x^*$ with $f_0(z(\theta)) < f_0(x^*)$. This contradicts the assumption that $x^*$ is a local minimum.
          </div>

          <div class="proof-step">
            <strong>Step 6: Conclude.</strong> Our assumption must be false. Therefore, any local minimum is also a global minimum.
          </div>
        </div>
        <figure>
          <img src="../../static/assets/topics/01-introduction/convex_function_chord.gif" alt="Animated visualization of the chord property for convex functions" />
          <figcaption>Figure 2.1: Animation showing how the chord between any two points stays above a convex function's graph.</figcaption>
        </figure>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Explorer: Convex vs Non-Convex Functions</h3>
          <p>Test Jensen's inequality by selecting two points on different functions. See directly whether a function is convex, concave, or neither by checking if the chord lies above, below, or crosses the curve.</p>
          <div id="widget-convex-vs-nonconvex" style="width: 100%; height: 400px; position: relative;"></div>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Visualizer: Optimization Landscape Viewer</h3>
          <p>Explore 3D landscapes of convex and non-convex functions. Drop a "marble" to simulate gradient descent and observe how convex landscapes always lead to the global minimum, while non-convex ones can get stuck.</p>
          <div id="widget-landscape-viewer" style="width: 100%; height: 450px; position: relative;"></div>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Demo: Convergence Comparison</h3>
          <p>Watch the convergence behavior of optimization algorithms on convex vs non-convex problems. Observe how convex problems converge reliably to the optimal solution.</p>
          <div id="widget-convergence-comparison" style="width: 100%; height: 400px; position: relative;"></div>
        </div>
      </section>

      <section class="card" id="section-3">
        <h2>3. Canonical Problem Families</h2>
        <p>A vast number of real-world models reduce to one of a few standard forms. Recognizing them is a crucial modeling skill.</p>
        <figure>
          <img src="../../static/assets/topics/01-introduction/hierarchy-of-convex-optimization-problems-dark.svg" alt="Hierarchy diagram showing relationships between LP, QP, SOCP, and SDP" />
          <figcaption>Figure 3.1: The hierarchy of convex optimization problems.</figcaption>
        </figure>

        <div class="subsection">
            <h3>3.1 Linear Program (LP)</h3>
            <p>An LP has a linear objective and linear constraints. The feasible set is a polyhedron, and the solution is always at a vertex.</p>
            $$ \min_x\ c^\top x \quad \text{s.t. } Ax \le b, \quad Fx = g $$
            <div class="example">
              <h4>Example: Resource Allocation (Factory Production)</h4>
              <p>A factory produces $n$ products using $m$ raw materials.</p>
              <ul>
                <li><b>Variables:</b> $x_j$ = units of product $j$ to produce.</li>
                <li><b>Objective:</b> Maximize profit $\sum_j p_j x_j$.</li>
                <li><b>Constraints:</b> Material limits $\sum_j A_{ij} x_j \le S_i$ for each material $i$, and non-negativity $x_j \ge 0$.</li>
              </ul>
            </div>
        </div>

        <div class="subsection">
            <h3>3.2 Quadratic Program (QP)</h3>
            <p>A QP has a convex quadratic objective and linear constraints. The level sets of the objective are ellipsoids.</p>
            $$ \min_x\ \frac{1}{2} x^\top Q x + c^\top x \quad \text{s.t. } Ax \le b, \quad Fx = g, \quad Q \succeq 0 $$
            <div class="example">
              <h4>Example: Markowitz Portfolio Optimization</h4>
              <p>An investor allocates capital among assets to minimize risk for a target return.</p>
              <ul>
                <li><b>Variables:</b> $w_i$ = portfolio weight for asset $i$.</li>
                <li><b>Objective:</b> Minimize variance (risk) $w^\top \Sigma w$, where $\Sigma$ is the covariance matrix.</li>
                <li><b>Constraints:</b> Target return $\mu^\top w \ge R_{\text{target}}$ and budget $\mathbf{1}^\top w = 1$.</li>
              </ul>
            </div>
        </div>

        <div class="subsection">
            <h3>3.3 Second-Order Cone Program (SOCP)</h3>
            <p>An SOCP is an LP with additional second-order cone constraints, which involve Euclidean norms.</p>
             $$ \min_x\ c^\top x \quad \text{s.t. } \|A_i x + b_i\|_2 \le c_i^\top x + d_i, \quad Fx = g $$
            <div class="example">
                <h4>Example: Robust Least Squares</h4>
                <p>When solving least squares, there might be uncertainty in the data matrix $A$. A robust formulation that accounts for this uncertainty can often be modeled as an SOCP, protecting the solution against worst-case measurement errors.</p>
            </div>
        </div>

        <div class="subsection">
            <h3>3.4 Semidefinite Program (SDP)</h3>
            <p>In an SDP, the variable is a symmetric matrix that is constrained to be positive semidefinite.</p>
             $$ \min_X\ \langle C, X \rangle \quad \text{s.t. } \langle A_i, X \rangle = b_i, \quad X \succeq 0 $$
            <div class="example">
                <h4>Example: Minimum Volume Enclosing Ellipsoid (MVEE)</h4>
                <p>Given a set of points, finding the smallest ellipsoid that contains all of them can be formulated as an SDP. This has applications in outlier detection and experimental design.</p>
            </div>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Explorer: Problem Classification Flowchart</h3>
          <p>Use this interactive flowchart to classify your optimization problem. Answer questions about your objective and constraints to determine which problem family (LP, QP, SOCP, SDP) your problem belongs to.</p>
          <div id="widget-problem-flowchart" style="width: 100%; height: 450px; position: relative;"></div>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Gallery: Real-World Applications</h3>
          <p>Explore a gallery of real-world optimization problems from finance, operations research, machine learning, and more. See how abstract mathematical formulations map to practical applications.</p>
          <div id="widget-problem-gallery" style="width: 100%; height: 400px; position: relative;"></div>
        </div>
      </section>

      <section class="card" id="section-4">
        <h2>4. The Loss + Regularizer Paradigm</h2>
        <p>Many problems in statistics and machine learning use this modeling template:</p>
        $$
        \min_x \quad \underbrace{\text{loss}(x; \text{data})}_{\text{Data Fidelity}} + \underbrace{\lambda \cdot \text{regularizer}(x)}_{\text{Model Complexity}}
        $$
        <p>This balances fitting the data (loss) with preventing overfitting (regularizer). The parameter $\lambda \ge 0$ controls the trade-off. Common examples include:</p>
        <ul>
          <li><b>Loss Functions:</b> Least Squares $(\|Ax-b\|_2^2)$, Logistic Loss.</li>
          <li><b>Regularizers:</b> Ridge $(\|x\|_2^2)$, LASSO $(\|x\|_1)$ for promoting sparsity.</li>
        </ul>
        <div class="insight">
          <h4>‚ö° Why LASSO Promotes Sparsity</h4>
          <p>The $\ell_1$ "ball" has sharp corners at the coordinate axes. When the contours of the loss function expand, they are more likely to first touch the $\ell_1$ region at one of these corners, where many coordinates are exactly zero. This geometric property makes LASSO effective for feature selection.</p>
        </div>
      </section>

      <section class="card" id="section-5">
        <h2>5. Problem Set & Solutions</h2>
        <div class="problem">
          <h3>P1.1 ‚Äî Classify as Convex / Not Convex</h3>
          <p>For each problem, state if it is convex and provide a one-sentence justification.</p>
          <ol type="a">
            <li>$\min \|Ax - b\|_2^2$ subject to $Fx = g$.</li>
            <li>$\min -\|x\|_2$ subject to $Ax \le b$.</li>
            <li>$\min \|x\|_1$ subject to $\|Bx - c\|_\infty \le 1$.</li>
            <li>$\min x^\top Q x$ subject to $Dx \le e$, where $Q$ is not guaranteed to be PSD.</li>
            <li>$\min \|x\|_2^2$ subject to $x_i \in \{0, 1\}$ for all $i$.</li>
          </ol>
          <div class="solution">
            <h4>Solutions</h4>
            <ol type="a">
              <li><b>Convex.</b> The objective is a convex quadratic and the constraint is affine.</li>
              <li><b>Not Convex.</b> The objective function, negative of the L2-norm, is concave.</li>
              <li><b>Convex.</b> The objective and the constraint function are both norms, which are convex.</li>
              <li><b>Not Convex.</b> The objective is only guaranteed to be convex if $Q \succeq 0$.</li>
              <li><b>Not Convex.</b> The integer constraint $x_i \in \{0, 1\}$ makes the feasible set non-convex.</li>
            </ol>
          </div>
        </div>
        <div class="problem">
            <h3>P1.2 ‚Äî Real-World Modeling</h3>
            <p>You are tasked with optimizing the placement of a distribution warehouse. The goal is to minimize the sum of squared Euclidean distances to a set of $k$ retail locations, $r_1, \dots, r_k \in \mathbb{R}^2$. The warehouse must be located within a region defined by the linear inequalities $C x \le d$. Formulate this as a convex optimization problem.</p>
            <div class="solution">
                <h4>Solution</h4>
                <p>Let the warehouse location be $x \in \mathbb{R}^2$. The objective is to minimize the sum of squared distances to the retail locations.</p>
                <p>The objective function is:</p>
                $$ f_0(x) = \sum_{i=1}^{k} \|x - r_i\|_2^2 $$
                This is a convex quadratic function. The constraints are given as $Cx \le d$, which are linear. The problem is a Quadratic Program (QP):
                $$ \min_{x \in \mathbb{R}^2} \sum_{i=1}^{k} \|x - r_i\|_2^2 \quad \text{s.t.} \quad Cx \le d $$
                This is a convex optimization problem.
            </div>
        </div>
        <div class="problem">
          <h3>P1.3 ‚Äî (New) Reformulation</h3>
          <p>Show how to reformulate the following problem as a linear program:</p>
          $$ \min_{x \in \mathbb{R}^n} \|Ax - b\|_1 \quad \text{s.t.} \quad \|x\|_\infty \le 1 $$
          <div class="solution">
            <h4>Solution</h4>
            <p>We can introduce a new variable $t \in \mathbb{R}^m$ to represent the absolute values in the objective function. The problem can be rewritten as:</p>
            $$ \min_{x, t} \mathbf{1}^\top t \quad \text{s.t.} \quad -t \le Ax - b \le t, \quad -1 \le x_i \le 1 \text{ for all } i $$
            This is an LP because the objective and all constraints are linear.
          </div>
        </div>
      </section>

      <!-- SECTION 6: CONSTRAINT GRAMMAR -->
      <section class="card" id="section-6">
        <h2>6. Constraint Grammar (Explicit, Searchable)</h2>

        <p>Throughout this course, we will use a consistent vocabulary to describe convex constraints. Learning this "grammar" now makes later lectures easier to follow and helps you quickly recognize convex forms.</p>

        <h3>Standard Constraint Types</h3>

        <h4>1. Affine Equalities and Inequalities</h4>
        <ul>
          <li><b>Affine equality:</b> $Ax = b$ where $A \in \mathbb{R}^{m \times n}$, $b \in \mathbb{R}^m$</li>
          <li><b>Affine inequality:</b> $a^\top x \le b$ or componentwise $Cx \le d$</li>
          <li><b>Why convex:</b> The set $\{x \mid Ax = b\}$ is an affine subspace (convex). The set $\{x \mid a^\top x \le b\}$ is a halfspace (convex).</li>
        </ul>

        <h4>2. Componentwise Bounds</h4>
        <ul>
          <li><b>Nonnegativity:</b> $x \ge 0$ means $x_i \ge 0$ for all $i$</li>
          <li><b>Box constraints:</b> $\ell \le x \le u$ means $\ell_i \le x_i \le u_i$ for all $i$</li>
          <li><b>Why convex:</b> Each constraint defines a halfspace; their intersection (the nonnegative orthant $\mathbb{R}^n_+$ or a box) is convex.</li>
        </ul>

        <h4>3. Norm Constraints</h4>
        <ul>
          <li><b>Euclidean ball:</b> $\|Px + q\|_2 \le r$ (defines an ellipsoid when $P$ is invertible)</li>
          <li><b>Infinity norm:</b> $\|x\|_\infty \le t$ equivalent to $-t \le x_i \le t$ for all $i$</li>
          <li><b>$\ell_1$ norm:</b> $\|x\|_1 \le t$ (defines a cross-polytope)</li>
          <li><b>Why convex:</b> Sublevel sets of norms are convex (unit balls of norms are convex).</li>
        </ul>

        <h4>4. Second-Order Cone (SOC)</h4>
        <p>The constraint $\|Ax + b\|_2 \le c^\top x + d$ defines membership in a <b>second-order cone</b>. This is the natural extension of the Euclidean ball constraint and is fundamental to <b>SOCP</b> (Second-Order Cone Programming).</p>

        <h4>5. Positive Semidefinite (PSD) Constraints</h4>
        <ul>
          <li><b>Matrix variable:</b> $X \in \mathbb{S}^n$ (symmetric $n \times n$ matrices)</li>
          <li><b>PSD constraint:</b> $X \succeq 0$ means $v^\top X v \ge 0$ for all $v \in \mathbb{R}^n$</li>
          <li><b>Why convex:</b> The PSD cone $\mathbb{S}^n_+ = \{X \in \mathbb{S}^n \mid X \succeq 0\}$ is a convex cone (see <a href="../02-convex-sets/index.html">Lecture 02</a>).</li>
        </ul>

        <div class="insight">
          <h4>üí° Recognition Strategy</h4>
          <p>When you see a new problem, scan for these patterns. If all constraints fit this grammar and the objective is a convex function, the problem is convex.</p>
        </div>

        <h3>Standard Sets Arising from Constraints</h3>
        <table class="data-table" style="width: 100%; margin-top: 16px;">
          <thead>
            <tr>
              <th>Constraint Form</th>
              <th>Geometric Object</th>
              <th>Used In</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>$Ax = b$</td>
              <td>Affine subspace</td>
              <td>Equality-constrained problems</td>
            </tr>
            <tr>
              <td>$a^\top x \le b$</td>
              <td>Halfspace</td>
              <td>LP, general inequalities</td>
            </tr>
            <tr>
              <td>$x \ge 0$</td>
              <td>Nonnegative orthant $\mathbb{R}^n_+$</td>
              <td>LP, QP (nonnegative variables)</td>
            </tr>
            <tr>
              <td>$\|x\|_2 \le r$</td>
              <td>Euclidean ball</td>
              <td>Robust optimization, trust regions</td>
            </tr>
            <tr>
              <td>$\|Ax + b\|_2 \le c^\top x + d$</td>
              <td>Second-order cone</td>
              <td>SOCP</td>
            </tr>
            <tr>
              <td>$X \succeq 0$</td>
              <td>PSD cone $\mathbb{S}^n_+$</td>
              <td>SDP</td>
            </tr>
          </tbody>
        </table>
      </section>

      <!-- SECTION 7: TINY MODELING REWRITES -->
      <section class="card" id="section-7">
        <h2>7. Tiny Modeling Rewrites You Can Already Use</h2>

        <p>These are <b>safe transformations</b> that convert common patterns into standard convex forms. Memorize them‚Äîthey appear constantly in modeling.</p>

        <h3>Absolute Value Elimination</h3>

        <h4>Scalar Case</h4>
        <p><b>Pattern:</b> $|x_i| \le u_i$</p>
        <p><b>Rewrite:</b> $-u_i \le x_i \le u_i$ (two linear inequalities)</p>

        <h4>Objective with Absolute Values</h4>
        <p><b>Pattern:</b> $\min \sum_i |x_i|$</p>
        <p><b>Rewrite:</b> Introduce slack variables $t_i \ge 0$ and add constraints:
        $$
        \min \sum_i t_i \quad \text{s.t.} \quad -t_i \le x_i \le t_i
        $$
        This is now an LP (all constraints linear).

        <h3>Infinity Norm as Box Constraint</h3>
        <p><b>Pattern:</b> $\|x\|_\infty \le t$</p>
        <p><b>Rewrite:</b> $-t \mathbf{1} \le x \le t \mathbf{1}$ (componentwise: $-t \le x_i \le t$ for all $i$)</p>
        <p><b>Why this works:</b> $\|x\|_\infty = \max_i |x_i|$, so the constraint is equivalent to $|x_i| \le t$ for all $i$.</p>

        <h3>$\ell_1$ Norm as Sum of Absolute Values</h3>
        <p><b>Pattern:</b> $\|x\|_1 \le t$</p>
        <p><b>Rewrite:</b> Introduce slack variables $s_i$ with:
        $$
        -s_i \le x_i \le s_i, \quad \sum_i s_i \le t
        $$
        Or equivalently (epigraph form): minimize $t$ subject to $-t \le x_i \le t$ and $\sum_i s_i = t$ where $s_i = |x_i|$.</p>

        <h3>Euclidean Norm (Second-Order Cone Form)</h3>
        <p><b>Pattern:</b> $\|Ax - b\|_2 \le t$</p>
        <p><b>Standard form:</b> This is already in <b>second-order cone (SOC)</b> form:
        $$
        (Ax - b, t) \in \mathcal{Q}^{m+1} := \{(y, s) \in \mathbb{R}^{m+1} \mid \|y\|_2 \le s\}
        $$
        This is the canonical constraint type for <b>SOCP</b> (Second-Order Cone Programming).</p>

        <h3>Max Function as Slack Variable</h3>
        <p><b>Pattern:</b> Minimize $\max\{f_1(x), f_2(x), \dots, f_m(x)\}$ where each $f_i$ is convex</p>
        <p><b>Rewrite (epigraph form):</b>
        $$
        \min t \quad \text{s.t.} \quad f_i(x) \le t \text{ for all } i
        $$
        <b>Why this works:</b> The minimum value of $t$ that satisfies all constraints is exactly $\max_i f_i(x)$.</p>

        <h3>PSD Constraint Notation</h3>
        <p><b>Pattern:</b> Matrix $X$ must be positive semidefinite</p>
        <p><b>Standard notation:</b> $X \succeq 0$ or $X \in \mathbb{S}^n_+$</p>
        <p><b>Linear Matrix Inequality (LMI):</b> Constraints of the form
        $$
        F_0 + \sum_{i=1}^n x_i F_i \succeq 0
        $$
        where $F_i \in \mathbb{S}^m$ are given symmetric matrices. This is the standard form for <b>SDP</b> (Semidefinite Programming).</p>

        <div class="example-box">
          <h4>Example: Reformulating $\ell_1$ Regression</h4>
          <p><b>Original:</b> $\min \|Ax - b\|_1$</p>
          <p><b>Step 1:</b> Write $\|Ax - b\|_1 = \sum_i |a_i^\top x - b_i|$</p>
          <p><b>Step 2:</b> Introduce slack $t_i$ for each residual:
          $$
          \min \sum_i t_i \quad \text{s.t.} \quad -t_i \le a_i^\top x - b_i \le t_i, \quad t_i \ge 0
          $$
          <p><b>Result:</b> A <b>linear program</b> with $n + m$ variables and $2m$ inequality constraints.</p>
        </div>

        <div class="insight">
          <h4>üí° Pro Tip</h4>
          <p>Always prefer standard forms (LP, QP, SOCP, SDP) over custom implementations. Standard forms have decades of algorithmic optimization and mature solvers.</p>
        </div>
      </section>

    </article>

    <footer class="site-footer">
      <div class="container">
        <p>¬© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/theme-switcher.js"></script>
<script src="../../static/js/toc.js"></script>
<script src="../../static/js/theme-switcher.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initConvexCombination } from './widgets/js/convex-combination.js';
    initConvexCombination('widget-convex-combination');
  </script>
  <script type="module">
    import { initConvexVsNonconvex } from './widgets/js/convex-vs-nonconvex.js';
    initConvexVsNonconvex('widget-convex-vs-nonconvex');
  </script>
  <script type="module">
    import { initLandscapeViewer } from './widgets/js/landscape-viewer.js';
    initLandscapeViewer('widget-landscape-viewer');
  </script>
  <script type="module">
    import { initConvergenceComparison } from './widgets/js/convergence-comparison.js';
    initConvergenceComparison('widget-convergence-comparison');
  </script>
  <script type="module">
    import { initProblemFlowchart } from './widgets/js/problem-flowchart.js';
    initProblemFlowchart('widget-problem-flowchart');
  </script>
  <script type="module">
    import { initProblemGallery } from './widgets/js/problem-gallery.js';
    initProblemGallery('widget-problem-gallery');
  </script>
</body>
</html>
