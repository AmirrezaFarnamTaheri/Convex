<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>01. Introduction to Convex Optimization ‚Äî Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/styles.css" />
  <link rel="stylesheet" href="/static/css/modern-widgets.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">
          <img src="../../static/assets/branding/logo.svg" alt="Logo" />
          <span>Convex Optimization</span>
        </a>
      </div>
      <nav class="nav">
        <a href="../../index.html#sessions">All Lectures</a>
        <a href="../00-linear-algebra-primer/index.html">‚Üê Previous</a>
        <a href="../02-convex-sets/index.html">Next ‚Üí</a>
      </nav>
    </div>
  </header>

  <main class="container" style="padding-top: 2rem;">
    <header class="lecture-header card">
      <h1>01. Introduction: The What, Why, and How of Convex Optimization</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: intro, motivation, overview, modeling</span>
      </div>
      <div class="lecture-summary">
        <p><strong>Overview:</strong> This foundational lecture introduces the universe of convex optimization. We define what makes a problem "convex," explore why this property is the critical dividing line between computational tractability and intractability, and prove the crown jewel: that any local minimum is also global. We tour the canonical problem families (LP, QP, SOCP, SDP) that power modern applications, introduce the powerful "loss + regularizer + constraints" modeling paradigm, and equip you with the practical skills to read, write, and sanity-check convex programs.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-primer/index.html">Lecture 00: Linear Algebra Primer</a> is essential‚Äîespecially projections, PSD matrices, and norms.</p>
        <p><strong>Scope Guard:</strong> No deep set theory (that's <a href="../02-convex-sets/index.html">Lecture 02</a>), no convex-function calculus/epigraphs (that's Lecture 03), and no KKT/duality/algorithms here. This is purely about problem structure, recognition, and formulation.</p>
        <p><strong>Connections:</strong> The least squares and QP examples draw directly from <a href="../00-linear-algebra-primer/index.html">Lecture 00</a>'s projection theory. The feasible sets introduced here motivate <a href="../02-convex-sets/index.html">Lecture 02</a>'s geometric foundations.</p>
      </div>
    </header>

    <section class="card">
      <h2>Learning Objectives</h2>
      <ul>
        <li><b>Define Convex Problems:</b> Articulate the precise three-part definition and distinguish convex from nonconvex problems.</li>
        <li><b>Prove Global Optimality:</b> Walk through the rigorous proof that every local minimum is global, and explain why this property is transformative.</li>
        <li><b>Recognize Canonical Families:</b> Identify LP, QP, SOCP, SDP by sight and know which real-world scenarios each family models.</li>
        <li><b>Apply Loss + Regularizer Paradigm:</b> Formulate problems using the "loss + regularizer + constraints" template and explain the bias-variance tradeoff.</li>
        <li><b>Use Safe Rewrites:</b> Transform norms, absolute values, and max functions into standard convex forms.</li>
        <li><b>Formulate and Sanity-Check:</b> Translate real-world problems to mathematical form, verify feasibility, and check for unboundedness.</li>
        <li><b>Understand the Solver Workflow:</b> Grasp the "formulate ‚Üí canonicalize ‚Üí solve ‚Üí verify" pipeline.</li>
      </ul>
    </section>

    <nav class="card table-of-contents">
      <h2>Table of Contents</h2>
      <ol>
        <li><a href="#section-1">What is a Convex Optimization Problem?</a></li>
        <li><a href="#section-2">The Crown Jewel: Local = Global</a></li>
        <li><a href="#section-3">Canonical Problem Families</a></li>
        <li><a href="#section-4">The Loss + Regularizer Paradigm</a></li>
        <li><a href="#section-5">Problem Set & Solutions</a></li>
      </ol>
    </nav>

    <article>
      <section class="card" id="section-1">
        <h2>1. What is a Convex Optimization Problem?</h2>
        <p>In the vast landscape of optimization, problems divide into two categories: <b>convex</b> and <b>nonconvex</b>. This is not arbitrary‚Äîit's the fundamental line between problems we can solve efficiently and reliably, and those we generally cannot.</p>
        <figure>
          <img src="../../static/assets/topics/01-introduction/optimization-schematic.png" alt="Schematic overview of the optimization problem landscape" />
          <figcaption>Figure 1.1: The optimization landscape‚Äîconvex vs. nonconvex problems.</figcaption>
        </figure>
        <h3>Formal Definition</h3>
        <p>An optimization problem is <b>convex</b> if it can be written in the form:</p>
        $$
        \begin{aligned}
        \min_{x \in \mathbb{R}^n} \quad & f_0(x) && \text{(Objective)} \\
        \text{subject to} \quad & f_i(x) \le 0, && i=1,\dots,m \quad \text{(Inequalities)}\\
        & Ax = b && \text{(Equalities)}
        \end{aligned}
        $$
        <p>where <b>three critical conditions</b> are met:</p>
        <ol>
          <li>The objective function $f_0$ is a <b>convex function</b>.</li>
          <li>All inequality constraint functions $f_i$ are <b>convex functions</b>.</li>
          <li>All equality constraints are <b>affine</b> (i.e., of the form $Ax = b$).</li>
        </ol>
        <div class="insight">
          <h4>üí° Key Insight</h4>
          <p>This structure guarantees two profound properties: the <b>feasible set</b> (all points satisfying the constraints) is a convex set, and, as we'll prove next, <b>every local minimum is a global minimum</b>. This is the magic of convexity.</p>
        </div>
      </section>

      <section class="card" id="section-2">
        <h2>2. The Crown Jewel: Local = Global</h2>
        <p>This property is what makes convex optimization so powerful. In nonconvex problems, algorithms can get stuck in local minima, but for convex problems, any local minimum is guaranteed to be the best possible solution.</p>
        <div class="proof">
          <h4>Proof: Any Local Minimum is a Global Minimum</h4>
          <p>The proof is by contradiction. We assume a local minimum is not global and show this leads to an inconsistency.</p>
          <ol>
              <li><b>Assumption:</b> Let $x^*$ be a local minimum, and assume it is <em>not</em> a global minimum.</li>
              <li><b>Implication:</b> This means there must exist another feasible point $y$ such that $f_0(y) < f_0(x^*)$.</li>
              <li><b>Line Segment Construction:</b> Since the feasible set is convex, any point on the line segment between $x^*$ and $y$, defined as $z(\theta) = \theta y + (1-\theta)x^*$ for $\theta \in [0, 1]$, is also feasible.</li>
              <li><b>Applying Function Convexity:</b> By the definition of a convex function $f_0$, we have:
              $$ f_0(z(\theta)) = f_0(\theta y + (1-\theta)x^*) \le \theta f_0(y) + (1-\theta)f_0(x^*) $$</li>
              <li><b>Using the Assumption:</b> Since we assumed $f_0(y) < f_0(x^*)$, we can substitute this into the inequality:
              $$ f_0(z(\theta)) < \theta f_0(x^*) + (1-\theta)f_0(x^*) = f_0(x^*) $$
              This holds for any $\theta \in (0, 1)$.</li>
              <li><b>The Contradiction:</b> The result $f_0(z(\theta)) < f_0(x^*)$ means that we can find points $z(\theta)$ arbitrarily close to $x^*$ (by choosing a very small $\theta > 0$) that have a smaller objective value. This contradicts the definition of $x^*$ as a local minimum.</li>
              <li><b>Conclusion:</b> Our initial assumption must be false. Therefore, any local minimum $x^*$ must also be a global minimum.</li>
          </ol>
        </div>
        <figure>
          <img src="../../static/assets/topics/01-introduction/convex_function_chord.gif" alt="Animated visualization of the chord property for convex functions" />
          <figcaption>Figure 2.1: Animation showing how the chord between any two points stays above a convex function's graph.</figcaption>
        </figure>
      </section>

      <section class="card" id="section-3">
        <h2>3. Canonical Problem Families</h2>
        <p>A vast number of real-world models reduce to one of a few standard forms. Recognizing them is a crucial modeling skill.</p>
        <figure>
          <img src="../../static/assets/topics/01-introduction/hierarchy-of-convex-optimization-problems.svg" alt="Hierarchy diagram showing relationships between LP, QP, SOCP, and SDP" />
          <figcaption>Figure 3.1: The hierarchy of convex optimization problems.</figcaption>
        </figure>

        <div class="subsection">
            <h3>3.1 Linear Program (LP)</h3>
            <p>An LP has a linear objective and linear constraints. The feasible set is a polyhedron, and the solution is always at a vertex.</p>
            $$ \min_x\ c^\top x \quad \text{s.t. } Ax \le b, \quad Fx = g $$
            <div class="example">
              <h4>Example: Resource Allocation (Factory Production)</h4>
              <p>A factory produces $n$ products using $m$ raw materials.</p>
              <ul>
                <li><b>Variables:</b> $x_j$ = units of product $j$ to produce.</li>
                <li><b>Objective:</b> Maximize profit $\sum_j p_j x_j$.</li>
                <li><b>Constraints:</b> Material limits $\sum_j A_{ij} x_j \le S_i$ for each material $i$, and non-negativity $x_j \ge 0$.</li>
              </ul>
            </div>
        </div>

        <div class="subsection">
            <h3>3.2 Quadratic Program (QP)</h3>
            <p>A QP has a convex quadratic objective and linear constraints. The level sets of the objective are ellipsoids.</p>
            $$ \min_x\ \frac{1}{2} x^\top Q x + c^\top x \quad \text{s.t. } Ax \le b, \quad Fx = g, \quad Q \succeq 0 $$
            <div class="example">
              <h4>Example: Markowitz Portfolio Optimization</h4>
              <p>An investor allocates capital among assets to minimize risk for a target return.</p>
              <ul>
                <li><b>Variables:</b> $w_i$ = portfolio weight for asset $i$.</li>
                <li><b>Objective:</b> Minimize variance (risk) $w^\top \Sigma w$, where $\Sigma$ is the covariance matrix.</li>
                <li><b>Constraints:</b> Target return $\mu^\top w \ge R_{\text{target}}$ and budget $\mathbf{1}^\top w = 1$.</li>
              </ul>
            </div>
        </div>

        <div class="subsection">
            <h3>3.3 Second-Order Cone Program (SOCP)</h3>
            <p>An SOCP is an LP with additional second-order cone constraints, which involve Euclidean norms.</p>
             $$ \min_x\ c^\top x \quad \text{s.t. } \|A_i x + b_i\|_2 \le c_i^\top x + d_i, \quad Fx = g $$
            <div class="example">
                <h4>Example: Robust Least Squares</h4>
                <p>When solving least squares, there might be uncertainty in the data matrix $A$. A robust formulation that accounts for this uncertainty can often be modeled as an SOCP, protecting the solution against worst-case measurement errors.</p>
            </div>
        </div>

        <div class="subsection">
            <h3>3.4 Semidefinite Program (SDP)</h3>
            <p>In an SDP, the variable is a symmetric matrix that is constrained to be positive semidefinite.</p>
             $$ \min_X\ \langle C, X \rangle \quad \text{s.t. } \langle A_i, X \rangle = b_i, \quad X \succeq 0 $$
            <div class="example">
                <h4>Example: Minimum Volume Enclosing Ellipsoid (MVEE)</h4>
                <p>Given a set of points, finding the smallest ellipsoid that contains all of them can be formulated as an SDP. This has applications in outlier detection and experimental design.</p>
            </div>
        </div>
      </section>

      <section class="card" id="section-4">
        <h2>4. The Loss + Regularizer Paradigm</h2>
        <p>Many problems in statistics and machine learning use a powerful modeling template:</p>
        $$
        \min_x \quad \underbrace{\text{loss}(x; \text{data})}_{\text{Data Fidelity}} + \underbrace{\lambda \cdot \text{regularizer}(x)}_{\text{Model Complexity}}
        $$
        <p>This balances fitting the data (loss) with preventing overfitting (regularizer). The parameter $\lambda \ge 0$ controls the trade-off. Common examples include:</p>
        <ul>
          <li><b>Loss Functions:</b> Least Squares $(\|Ax-b\|_2^2)$, Logistic Loss.</li>
          <li><b>Regularizers:</b> Ridge $(\|x\|_2^2)$, LASSO $(\|x\|_1)$ for promoting sparsity.</li>
        </ul>
        <div class="insight">
          <h4>‚ö° Why LASSO Promotes Sparsity</h4>
          <p>The $\ell_1$ "ball" has sharp corners at the coordinate axes. When the contours of the loss function expand, they are more likely to first touch the $\ell_1$ region at one of these corners, where many coordinates are exactly zero. This geometric property makes LASSO a powerful tool for feature selection.</p>
        </div>
      </section>

      <section class="card" id="section-5">
        <h2>5. Problem Set & Solutions</h2>
        <div class="problem">
          <h3>P1.1 ‚Äî Classify as Convex / Not Convex</h3>
          <p>For each problem, state if it is convex and provide a one-sentence justification.</p>
          <ol type="a">
            <li>$\min \|Ax - b\|_2^2$ subject to $Fx = g$.</li>
            <li>$\min -\|x\|_2$ subject to $Ax \le b$.</li>
            <li>$\min \|x\|_1$ subject to $\|Bx - c\|_\infty \le 1$.</li>
            <li>$\min x^\top Q x$ subject to $Dx \le e$, where $Q$ is not guaranteed to be PSD.</li>
            <li>$\min \|x\|_2^2$ subject to $x_i \in \{0, 1\}$ for all $i$.</li>
          </ol>
          <div class="solution">
            <h4>Solutions</h4>
            <ol type="a">
              <li><b>Convex.</b> The objective is a convex quadratic and the constraint is affine.</li>
              <li><b>Not Convex.</b> The objective function, negative of the L2-norm, is concave.</li>
              <li><b>Convex.</b> The objective and the constraint function are both norms, which are convex.</li>
              <li><b>Not Convex.</b> The objective is only guaranteed to be convex if $Q \succeq 0$.</li>
              <li><b>Not Convex.</b> The integer constraint $x_i \in \{0, 1\}$ makes the feasible set non-convex.</li>
            </ol>
          </div>
        </div>
        <div class="problem">
            <h3>P1.2 ‚Äî Real-World Modeling</h3>
            <p>You are tasked with optimizing the placement of a distribution warehouse. The goal is to minimize the sum of squared Euclidean distances to a set of $k$ retail locations, $r_1, \dots, r_k \in \mathbb{R}^2$. The warehouse must be located within a region defined by the linear inequalities $C x \le d$. Formulate this as a convex optimization problem.</p>
            <div class="solution">
                <h4>Solution</h4>
                <p>Let the warehouse location be $x \in \mathbb{R}^2$. The objective is to minimize the sum of squared distances to the retail locations.</p>
                <p>The objective function is:</p>
                $$ f_0(x) = \sum_{i=1}^{k} \|x - r_i\|_2^2 $$
                This is a convex quadratic function. The constraints are given as $Cx \le d$, which are linear. The problem is a Quadratic Program (QP):
                $$ \min_{x \in \mathbb{R}^2} \sum_{i=1}^{k} \|x - r_i\|_2^2 \quad \text{s.t.} \quad Cx \le d $$
                This is a convex optimization problem.
            </div>
        </div>
      </section>

    </article>

    <footer class="site-footer">
      <div class="container">
        <p>¬© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main>

  <script src="../../static/js/math-renderer.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
</body>
</html>
