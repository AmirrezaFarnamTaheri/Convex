<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>02. Introduction to Convex Optimization ‚Äî Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.128/build/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../01-linear-algebra-advanced/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../03-convex-sets-geometry/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>02. Introduction: What Makes a Problem Convex?</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: intro, motivation, overview, modeling, fundamentals</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture introduces convex optimization by defining what makes a problem "convex" and proving the fundamental theorem that any local minimum is also global. We examine the canonical problem families (LP, QP, SOCP, SDP), present the "loss + regularizer + constraints" modeling framework, and develop practical techniques for reformulating problems into standard convex forms.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-basics/index.html">Lecture 00: Linear Algebra Basics</a> and <a href="../01-linear-algebra-advanced/index.html">Lecture 01: Linear Algebra Advanced</a> are recommended, particularly projections, PSD matrices, norms, and the fundamental subspaces.</p>
        <p><strong>Forward Connections:</strong> The definition of convex problems relies on convex functions and sets (Lectures 03-06). The modeling techniques introduced here are used throughout the course. Duality theory (Lecture 13) generalizes the optimality conditions proven here.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Define Convex Optimization Problems:</b> State the precise three-part definition and distinguish convex from nonconvex problems at a glance.</li>
        <li><b>Prove the Fundamental Theorem:</b> Show that every local minimum of a convex problem is a global minimum, and understand why this property is essential.</li>
        <li><b>Understand the Problem Hierarchy:</b> Identify LP, QP, SOCP, and SDP by their structure and know which solvers and complexity classes apply to each.</li>
        <li><b>Apply the Loss + Regularizer Paradigm:</b> Formulate machine learning and statistical problems using the "loss + regularizer + constraints" template.</li>
        <li><b>Transform to Standard Forms:</b> Use safe rewrites to convert norms, absolute values, and max functions into equivalent standard convex forms.</li>
        <li><b>Verify Problem Convexity:</b> Check feasibility, recognize unboundedness, and apply sanity checks before invoking solvers.</li>
        <li><b>Understand Solver Architecture:</b> Grasp the "model ‚Üí transform ‚Üí canonicalize ‚Üí solve ‚Üí verify" pipeline used by CVX, CVXPY, and JuMP.</li>
      </ul>
    </section>



    <article>
      <section class="section-card" id="section-1">
        <h2>1. What is a Convex Optimization Problem?</h2>

        <h3>1.1 The Landscape of Optimization</h3>
        <p>Optimization is the mathematical discipline of finding the "best" solution from a set of alternatives. In the vast landscape of optimization problems, there exists a fundamental divide between two classes:</p>
        <ul>
          <li><b>Convex problems:</b> Solvable efficiently with guaranteed global optimality.</li>
          <li><b>Nonconvex problems:</b> Generally NP-hard, prone to local minima, no polynomial-time guarantees.</li>
        </ul>
        <p>This distinction is not merely academic‚Äîit determines whether a real-world problem can be solved reliably at scale.</p>

        <figure style="text-align: center; margin: 24px 0;">
          <img src="../../static/assets/topics/02-introduction/convex_vs_nonconvex_schematic.svg"
               alt="Schematic overview of the optimization problem landscape"
               style="max-width: 100%; height: auto; border-radius: 8px;" />
          <figcaption><i>Figure 1.1:</i> The optimization landscape‚Äîconvex vs. nonconvex problems represent fundamentally different computational complexity classes.</figcaption>
        </figure>

        <h3>1.2 Formal Definition</h3>
        <p>A mathematical optimization problem is <a href="#" class="definition-link" data-term="convex function">convex</a> if it can be written in the standard form:</p>
        $$
        \begin{aligned}
        \min_{x \in \mathbb{R}^n} \quad & f_0(x) && \text{(Objective function)} \\
        \text{subject to} \quad & f_i(x) \le 0, && i=1,\dots,m \quad \text{(Inequality constraints)}\\
        & Ax = b && \text{(Equality constraints)}
        \end{aligned}
        $$
        <p>where <b>three critical structural conditions</b> must be satisfied:</p>
        <ol>
          <li><b>Convex objective:</b> The objective function $f_0 : \mathbb{R}^n \to \mathbb{R}$ is convex (see <a href="../05-convex-functions-basics/index.html">Lecture 05</a> for the formal definition).</li>
          <li><b>Convex inequality constraints:</b> Each constraint function $f_i : \mathbb{R}^n \to \mathbb{R}$ is convex (see <a href="../05-convex-functions-basics/index.html">Lecture 05</a>).</li>
          <li><b>Affine equality constraints:</b> All equality constraints are <a href="#" class="definition-link">linear (affine)</a>, of the form $Ax = b$. Recall from <a href="../00-linear-algebra-basics/index.html">Lecture 00</a> that the set of solutions to $Ax=b$ is an affine set (a shifted subspace). If we allowed nonlinear equality constraints (like $x^2=1$), the feasible set would essentially consist of the boundary of a convex set, which is generally not convex.</li>
        </ol>

        <div class="insight">
          <h4>Geometric Insight: Linearity vs. Convexity</h4>
          <p>It is crucial to distinguish between "flat" and "curved" geometry:</p>
          <ul>
            <li><b>Linearity (Affine):</b> Represents "flat" structures‚Äîlines, planes, hyperplanes. These are sets that extend infinitely in all directions within a subspace. They are both convex and concave. In optimization, we use them for equality constraints because the intersection of hyperplanes is a stable, convex object (a smaller affine set).</li>
            <li><b>Convexity:</b> Represents "bulging" or "bowl-like" structures‚Äîballs, ellipsoids, halfspaces. These sets have an "inside" and an "outside". In optimization, we use them for inequality constraints ("stay inside the bowl") or objectives ("find the bottom of the bowl").</li>
          </ul>
          <div class="example">
            <h4>üö´ Counter-Example Corner: Why $x^2=1$ is Forbidden</h4>
            <p>Suppose we try to enforce the constraint $x^2 = 1$ in a convex problem.
            <br>The feasible set is $\{x \in \mathbb{R} \mid x^2 = 1\} = \{-1, +1\}$.
            <br>Is this set convex? No! The line segment connecting $-1$ and $+1$ contains the point 0, which is not in the set ($0^2 \neq 1$).
            <br>This is why equality constraints $h(x)=0$ must be affine (flat). If $h(x)$ is convex (like $x^2-1$), its zero-set is the boundary of a convex set, which is generally not convex.</p>
          </div>
        </div>

        <div class="insight">
          <h4>üí° Why These Conditions Matter</h4>
          <p>These three conditions guarantee that:</p>
          <ul>
            <li>The <b>feasible set</b> $\mathcal{F} = \{x \mid f_i(x) \le 0, Ax = b\}$ is a convex set.</li>
            <li>Every local minimum is guaranteed to be a <b>global minimum</b>.</li>
            <li>Efficient algorithms (interior-point, first-order methods) converge with <b>provable guarantees</b>.</li>
          </ul>
          <p>Violating even one condition (e.g., a non-affine equality constraint like $x^2=1$) can make the problem NP-hard.</p>
        </div>

        <h3>1.3 The Feasible Set</h3>
        <p>The <a href="#" class="definition-link">feasible set</a> (also called feasible region or constraint set) is:</p>
        $$
        \mathcal{F} = \{x \in \mathbb{R}^n \mid f_i(x) \le 0 \ \forall i, \ Ax = b\}
        $$
        <p>This is the set of all points satisfying all constraints. For a convex problem, $\mathcal{F}$ is convex‚Äîa property we'll prove in <a href="../03-convex-sets-geometry/index.html">Lecture 03</a> using operations preserving convexity.</p>

        <h3>1.4 Optimal Value and Optimal Points</h3>
        <ul>
          <li><b>Optimal value:</b> $p^* = \inf\{f_0(x) \mid x \in \mathcal{F}\}$ (can be $-\infty$ if unbounded, $+\infty$ if infeasible).</li>
          <li><b>Optimal point (solution):</b> $x^* \in \mathcal{F}$ is optimal if $f_0(x^*) = p^*$.</li>
          <li><b>$\varepsilon$-suboptimal:</b> $x$ is $\varepsilon$-suboptimal if $f_0(x) \le p^* + \varepsilon$.</li>
        </ul>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Visualizer: Convex Hull Explorer</h3>
          <p><b>Understand the Building Block of Convexity:</b> A convex combination is the foundation of all convex geometry. This widget demonstrates how mixing points creates the convex hull:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Drag the vertices ($v_1, v_2, v_3$):</b> Shape the triangle (convex hull).</li>
            <li><b>Drag the target point ($x$):</b> See if it can be formed as a convex combination of vertices.</li>
            <li><b>Barycentric Coordinates:</b> Observe the weights $\theta_i$. Inside the hull, all $\theta_i \ge 0$. Outside, at least one is negative.</li>
          </ul>
          <p><i>Geometric intuition:</i> A set is convex if it contains all convex combinations of its points. This prevents "holes" or "dents" that could trap optimization algorithms.</p>
          <div id="widget-convex-combination" style="width: 100%; height: 450px; position: relative;"></div>
        </div>
      </section>

      <section class="section-card" id="section-2">
        <h2>2. The Fundamental Theorem: Local = Global</h2>

        <h3>2.1 Global Optimality</h3>
        <p>In general (nonconvex) optimization, algorithms can get stuck in <b>local minima</b>‚Äîpoints that are optimal within a small neighborhood but not globally. For convex problems, this pathology cannot occur:</p>

        <div class="insight">
          <h4>‚ö†Ô∏è Common Misconception: Unimodal vs. Convex</h4>
          <p>It is often thought that any function with a single global minimum ("unimodal") is easy to optimize. This is <b>false</b>.</p>
          <ul>
            <li><b>Quasiconvexity:</b> A function can have a single minimum but still be non-convex (e.g., "bowl" shape with flat spots or weird kinks). While quasiconvex problems are solvable, they require specialized methods (bisection).</li>
            <li><b>Stationary Points:</b> A non-convex function can have a single global minimum but many saddle points or local maxima, which can trap gradient-based algorithms (vanishing gradients).</li>
            <li><b>Convexity is stronger:</b> Convexity guarantees not only uniqueness (under strict convexity) but also that <b>every</b> direction of descent points towards the optimum. This geometric structure allows for polynomial-time guarantees that mere unimodality does not.</li>
          </ul>
        </div>

        <div class="theorem-box">
          <h4>Theorem (Fundamental Property of Convex Optimization)</h4>
          <p>For a convex optimization problem, any local minimum is also a global minimum.</p>
          <p><b>Formally:</b> If $x^*$ is a local minimum (i.e., there exists $\delta > 0$ such that $f_0(x^*) \le f_0(x)$ for all feasible $x$ with $\|x - x^*\|_2 < \delta$), then $x^*$ is a global minimum.</p>
        </div>

        <div class="proof-box">
          <h4>Proof</h4>

          <div class="proof-step">
            <strong>Setup (proof by contradiction):</strong> Assume $x^*$ is a local minimum but not a global minimum. Then there exists a feasible point $y \in \mathcal{F}$ with $f_0(y) < f_0(x^*)$.
          </div>

          <div class="proof-step">
            <strong>Step 1: Convexity of feasible set.</strong> Since the problem is convex, the feasible set $\mathcal{F}$ is convex. Therefore, for any $\theta \in [0,1]$, the point:
            $$
            z(\theta) = \theta y + (1-\theta)x^*
            $$
            is also feasible, i.e., $z(\theta) \in \mathcal{F}$.
          </div>

          <div class="proof-step">
            <strong>Step 2: Apply convexity of objective.</strong> By convexity of $f_0$:
            $$
            f_0(z(\theta)) = f_0(\theta y + (1-\theta)x^*) \le \theta f_0(y) + (1-\theta)f_0(x^*)
            $$
          </div>

          <div class="proof-step">
            <strong>Step 3: Use the assumption $f_0(y) < f_0(x^*)$.</strong> Rearranging the convexity inequality:
            $$
            f_0(z(\theta)) \le \theta f_0(y) + (1-\theta)f_0(x^*) < \theta f_0(x^*) + (1-\theta)f_0(x^*) = f_0(x^*)
            $$
            Therefore, $f_0(z(\theta)) < f_0(x^*)$ for all $\theta \in (0, 1)$.
          </div>

          <div class="proof-step">
            <div style="float: right; margin-left: 20px; width: 150px; padding: 10px; background: #f8f9fa; border: 1px solid #ddd; font-size: 0.9em;">
                <strong>Visual Guide:</strong> Imagine a bowl. $x^*$ is a local dip. If $y$ is lower (global min), draw a line from $x^*$ to $y$. The line must go <em>down</em> from $x^*$. But that means points immediately next to $x^*$ are lower than $x^*$, so $x^*$ wasn't even a local min!
            </div>
            <strong>Step 4: Construct a contradiction.</strong> As $\theta \to 0^+$, the point $z(\theta)$ approaches $x^*$. Specifically:
            $$
            \|z(\theta) - x^*\|_2 = \|\theta(y - x^*)\|_2 = \theta \|y - x^*\|_2
            $$
            Since $x^*$ is a local minimum, there exists a radius $\delta > 0$ such that $f_0(x^*) \le f_0(x)$ for all feasible $x$ with $\|x - x^*\|_2 < \delta$.
            <br>We can choose a sufficiently small $\theta \in (0, 1)$ such that $\theta \|y - x^*\|_2 < \delta$.
            <br>For this choice of $\theta$, the point $z(\theta)$ lies within the local neighborhood ($\|z(\theta) - x^*\|_2 < \delta$).
            <br>However, we established in Step 3 that $f_0(z(\theta)) < f_0(x^*)$. This contradicts the definition of $x^*$ as a local minimum.
          </div>

          <div class="proof-step">
            <strong>Conclusion:</strong> The assumption that $x^*$ is a local minimum but not a global minimum leads to a contradiction. Therefore, any local minimum must be a global minimum.
          </div>
        </div>

        <h3>2.2 Practical Implications</h3>
        <div class="insight">
          <h4>Why This Matters</h4>
          <ul>
            <li><b>No need for global search:</b> Local search methods (gradient descent, Newton's method) are guaranteed to find the global optimum.</li>
            <li><b>No dependence on initialization:</b> Any reasonable starting point will converge to the global solution.</li>
            <li><b>Certificates of optimality:</b> First-order conditions provide verifiable certificates that a solution is globally optimal.</li>
            <li><b>Polynomial-time solvability:</b> Interior-point methods solve convex problems to arbitrary precision in polynomial time.</li>
          </ul>
        </div>

        <figure style="text-align: center; margin: 24px 0;">
          <img src="../../static/assets/topics/02-introduction/convex_function_illustration.svg"
               alt="Visualization of the chord property for convex functions"
               style="max-width: 100%; height: auto; border-radius: 8px;" />
          <figcaption><i>Figure 2.1:</i> The chord between any two points on a convex function's graph lies above the function‚Äîthis geometric property ensures no local minima can exist.</figcaption>
        </figure>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Laboratory: Convexity & Landscapes</h3>
          <p><b>From Definition to Global Optimality:</b> This unified tool connects the mathematical definition of convexity with its powerful consequences for optimization. Toggle between the two tabs below:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>The Definition (1D):</b> Interactively verify <b>Jensen's Inequality</b>. Click two points to draw a chord. If the chord stays <i>above</i> the curve everywhere, the function is convex. This geometric "bowl shape" prevents hidden valleys.</li>
            <li><b>The Consequence (3D):</b> See what this means for optimization. Drop a marble on a <b>Convex Bowl</b> and it always finds the bottom (Global Minimum). Drop it on a <b>Non-Convex Landscape</b> and it gets stuck in local dips.</li>
          </ul>
          <p><i>Key Insight:</i> The simple 1D geometric property (chord above curve) guarantees that in ANY dimension, there are no local traps. Local optimality implies global optimality.</p>
          <div id="widget-optimization-landscape" style="width: 100%; position: relative;"></div>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Demo: Convergence Rate Comparison</h3>
          <p><b>Compare Algorithm Performance:</b> Watch different optimization algorithms race to the solution on both convex and non-convex problems:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Algorithms compared:</b>
              <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
                <li><b>Gradient Descent:</b> The standard method‚Äîfollows the negative gradient.</li>
                <li><b>Momentum:</b> Accelerated variants (Nesterov, Heavy Ball) to cross flat regions.</li>
                <li><b>Newton's Method:</b> Uses second-order (Hessian) information for quadratic convergence.</li>
              </ul>
            </li>
            <li><b>Metrics tracked:</b> Objective value, gradient norm, distance to optimum, iteration count.</li>
            <li><b>Key observations:</b>
              <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
                <li>On convex problems: All methods converge to the same optimum (only speed differs).</li>
                <li>On non-convex problems: Different methods find different local minima.</li>
              </ul>
            </li>
          </ul>
          <p><i>Convergence guarantees:</i> For strongly convex problems, gradient descent achieves linear convergence: $f(x_k) - f(x^*) \le (1-\mu/L)^k (f(x_0) - f(x^*))$ where $\mu$ is the strong convexity parameter and $L$ is the Lipschitz constant.</p>
          <div id="widget-convergence-comparison" style="width: 100%; height: 450px; position: relative;"></div>
        </div>
      </section>

      <section class="section-card" id="section-3">
        <h2>3. Why Convexity Matters: Practical Implications</h2>

        <h3>3.1 Computational Complexity</h3>
        <p>The distinction between convex and nonconvex problems is not merely about ease of solution‚Äîit reflects fundamental differences in computational complexity:</p>

        <table class="data-table" style="width: 100%; margin-top: 16px;">
          <thead>
            <tr>
              <th>Property</th>
              <th>Convex Problems</th>
              <th>Nonconvex Problems</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><b>Complexity class</b></td>
              <td>Polynomial time (P)</td>
              <td>Generally NP-hard</td>
            </tr>
            <tr>
              <td><b>Local vs. global</b></td>
              <td>Local = Global (proven)</td>
              <td>Local ‚â† Global (local minima exist)</td>
            </tr>
            <tr>
              <td><b>Solver guarantees</b></td>
              <td>Provably converge to global optimum</td>
              <td>May get stuck in local minima</td>
            </tr>
            <tr>
              <td><b>Initialization</b></td>
              <td>Any feasible point works</td>
              <td>Critical‚Äîaffects final solution</td>
            </tr>
            <tr>
              <td><b>Duality gap</b></td>
              <td>Zero at optimum (strong duality)</td>
              <td>May have duality gap</td>
            </tr>
            <tr>
              <td><b>Scalability</b></td>
              <td>Millions of variables routine</td>
              <td>Limited to smaller instances</td>
            </tr>
          </tbody>
        </table>

        <h3>3.2 Real-World Impact</h3>
        <div class="example">
          <h4>Example: Large-Scale Machine Learning</h4>
          <p>Modern machine learning relies heavily on convex optimization:</p>
          <ul>
            <li><b>Support Vector Machines (SVM):</b> Formulated as a convex QP, SVMs are solved reliably for millions of training examples.</li>
            <li><b>Logistic Regression:</b> Convex likelihood maximization ensures unique global optimum.</li>
            <li><b>LASSO Regression:</b> $\ell_1$-regularized least squares is convex, enabling sparse feature selection at scale.</li>
            <li><b>Deep Learning (first-order phase):</b> Even though neural networks are nonconvex, the convex optimization techniques (SGD, Adam) dominate the field.</li>
          </ul>
        </div>

        <h3>3.3 When Convexity is Lost</h3>
        <p>Certain problem features immediately destroy convexity:</p>
        <ul>
          <li><b>Integer constraints:</b> $x_i \in \{0,1\}$ makes the feasible set non-convex.</li>
          <li><b>Equality constraints with convex functions:</b> $f(x) = c$ where $f$ is convex (not affine).</li>
          <li><b>Maximizing a convex function:</b> Equivalent to minimizing a concave function.</li>
          <li><b>Products of variables:</b> Bilinear or quadratic terms with indefinite Hessian.</li>
          <li><b>Non-monotone compositions:</b> $f(g(x))$ where $g$ is convex but $f$ is non-monotone.</li>
        </ul>

        <div class="insight">
          <h4>‚ö†Ô∏è Convex Relaxations</h4>
          <p>When faced with a nonconvex problem, a common strategy is <b>convex relaxation</b>‚Äîreplacing non-convex constraints with convex approximations. The solution to the relaxed problem provides:</p>
          <ul>
            <li>A <b>lower bound</b> on the optimal value (for minimization).</li>
            <li>Often a <b>high-quality approximate solution</b>.</li>
            <li>A starting point for local refinement methods.</li>
          </ul>
        </div>
      </section>

      <section class="section-card" id="section-4">
        <h2>4. The Hierarchy of Convex Problem Families</h2>

        <p>Within the broad class of convex optimization problems, there exists a hierarchy of increasingly expressive standard forms. Each has dedicated algorithms and complexity results.</p>

        <figure style="text-align: center; margin: 24px 0;">
          <img src="../../static/assets/topics/02-introduction/hierarchy-of-convex-optimization-problems-dark.svg"
               alt="Hierarchy diagram showing relationships between LP, QP, SOCP, and SDP"
               style="max-width: 100%; height: auto; border-radius: 8px;" />
          <figcaption><i>Figure 4.1:</i> The hierarchy of convex optimization: each class contains the previous ones as special cases. LP ‚äÇ QP ‚äÇ SOCP ‚äÇ SDP ‚äÇ General Convex.</figcaption>
        </figure>

        <h3>4.1 Linear Programming (LP)</h3>
        <p>The simplest convex problem has both a linear objective and linear constraints. A <a href="#" class="definition-link" data-term="linear program">Linear Program (LP)</a> is defined as:</p>
        $$
        \begin{aligned}
        \min_{x} \quad & c^\top x \\
        \text{s.t.} \quad & Ax \le b \\
        & Fx = g
        \end{aligned}
        $$

        <div class="subsection">
          <h4>Properties and Characteristics</h4>
          <ul>
            <li><b>Feasible set:</b> A <a href="#" class="definition-link">polyhedron</a> (intersection of finitely many halfspaces and hyperplanes).</li>
            <li><b>Optimal solution:</b> Always at a vertex (extreme point) if it exists.</li>
            <li><b>Algorithms:</b> Simplex method (exponential worst-case, fast in practice), Interior-point methods (polynomial-time).</li>
            <li><b>Complexity:</b> Polynomial time via interior-point methods.</li>
            <li><b>Duality:</b> Strong duality always holds (primal-dual relationship).</li>
          </ul>
        </div>

        <div class="example">
          <h4>Example 1: Resource Allocation (Production Planning)</h4>
          <p>A factory produces $n$ products using $m$ types of raw materials.</p>
          <ul>
            <li><b>Decision variables:</b> $x_j$ = units of product $j$ to produce.</li>
            <li><b>Objective:</b> Maximize profit $\max \sum_{j=1}^n p_j x_j$ where $p_j$ is profit per unit.</li>
            <li><b>Constraints:</b>
              <ul>
                <li>Material limits: $\sum_{j=1}^n A_{ij} x_j \le S_i$ (amount of material $i$ used ‚â§ supply).</li>
                <li>Non-negativity: $x_j \ge 0$.</li>
              </ul>
            </li>
          </ul>
          <p><b>Standard form:</b> $\min -p^\top x$ subject to $Ax \le S$, $x \ge 0$.</p>
        </div>

        <div class="example">
          <h4>Example 2: Network Flow</h4>
          <p>Find minimum-cost flow through a network from sources to sinks.</p>
          <ul>
            <li><b>Variables:</b> $f_{ij}$ = flow on edge $(i,j)$.</li>
            <li><b>Objective:</b> $\min \sum_{(i,j)} c_{ij} f_{ij}$ (minimize total cost).</li>
            <li><b>Constraints:</b>
              <ul>
                <li>Flow conservation: $\sum_j f_{ij} - \sum_k f_{ki} = b_i$ at each node.</li>
                <li>Capacity: $0 \le f_{ij} \le u_{ij}$.</li>
              </ul>
            </li>
          </ul>
        </div>

        <h3>4.2 Quadratic Programming (QP)</h3>
        <p><a href="#" class="definition-link" data-term="quadratic program">Quadratic Programs (QP)</a> have a convex quadratic objective and linear constraints:</p>
        $$
        \begin{aligned}
        \min_{x} \quad & \frac{1}{2}x^\top Q x + c^\top x \\
        \text{s.t.} \quad & Ax \le b \\
        & Fx = g
        \end{aligned}
        $$
        <p>where $Q \succeq 0$ (positive semidefinite) to ensure convexity.</p>

        <div class="subsection">
          <h4>Properties and Characteristics</h4>
          <ul>
            <li><b>Level sets:</b> Ellipsoids (when $Q \succ 0$).</li>
            <li><b>Optimality:</b> Characterized by KKT conditions (covered in <a href="../09-duality/index.html">Lecture 13</a>).</li>
            <li><b>Algorithms:</b> Active-set methods, Interior-point methods.</li>
            <li><b>Complexity:</b> Polynomial time.</li>
            <li><b>Special case:</b> When $Q = I$ and no inequality constraints are present, this reduces to unconstrained least squares.</li>
          </ul>
        </div>

        <div class="example">
          <h4>Example 1: Markowitz Portfolio Optimization</h4>
          <p>An investor allocates capital among $n$ assets to minimize risk for a target return.</p>
          <ul>
            <li><b>Decision variables:</b> $w_i$ = fraction of wealth in asset $i$.</li>
            <li><b>Objective:</b> Minimize variance (risk) $\min \frac{1}{2}w^\top \Sigma w$ where $\Sigma$ is the covariance matrix of returns.</li>
            <li><b>Constraints:</b>
              <ul>
                <li>Budget: $\mathbf{1}^\top w = 1$ (weights sum to 1).</li>
                <li>Target return: $\mu^\top w \ge R_{\text{target}}$ where $\mu$ is expected return vector.</li>
                <li>Long-only: $w \ge 0$ (no short-selling).</li>
              </ul>
            </li>
          </ul>
        </div>

        <div class="example">
          <h4>Example 2: Least Squares with Constraints</h4>
          <p>Fit a model $y \approx Ax$ minimizing squared error, subject to constraints on $x$:</p>
          $$
          \min_{x} \|Ax - y\|_2^2 \quad \text{s.t.} \quad x \ge 0, \ \mathbf{1}^\top x = 1
          $$
          <p>This arises in nonnegative matrix factorization, constrained regression, and image denoising.</p>
        </div>

        <h3>4.3 Second-Order Cone Programming (SOCP)</h3>
        <p><a href="#" class="definition-link" data-term="socp">SOCPs</a> involve second-order cone constraints (Euclidean norm constraints):</p>
        $$
        \begin{aligned}
        \min_{x} \quad & c^\top x \\
        \text{s.t.} \quad & \|A_i x + b_i\|_2 \le c_i^\top x + d_i, \quad i=1,\dots,m \\
        & Fx = g
        \end{aligned}
        $$

        <div class="subsection">
          <h4>The Second-Order Cone (Lorentz Cone)</h4>
          <p>The constraint $\|y\|_2 \le t$ defines membership in the second-order cone (also called the Lorentz cone or ice-cream cone):</p>
          $$
          \mathcal{Q}^{n+1} = \{(y, t) \in \mathbb{R}^{n+1} \mid \|y\|_2 \le t\}
          $$
          <p>This cone is convex and self-dual (covered in <a href="../03-convex-sets-geometry/index.html">Lecture 03</a>).</p>
        </div>

        <div class="example">
          <h4>Example 1: Robust Least Squares</h4>
          <p>When the data matrix $A$ is uncertain (known up to some perturbation), robust formulations protect against worst-case errors:</p>
          $$
          \min_{x} \max_{\|\Delta A\|_2 \le \rho} \|(A + \Delta A)x - b\|_2^2
          $$
          <p>This can be reformulated as an SOCP.</p>
        </div>

        <div class="example">
          <h4>Example 2: Antenna Array Weight Design</h4>
        <p>Design antenna weights to achieve a desired beam pattern while minimizing side lobes‚Äînaturally formulated as SOCP.
        <br><i>Explanation:</i> We want to minimize the maximum magnitude of the array response in "side lobe" directions, subject to a unity response in the "main lobe" direction.
        <br>Minimize $\max_k |w^\top a(\theta_k)|$ subject to $w^\top a(\theta_{\text{target}}) = 1$.
        <br>This transforms to: $\min t$ s.t. $|w^\top a(\theta_k)| \le t$.
        <br>Since these are complex magnitudes, $|z| \le t$ is a second-order cone constraint in the real/imaginary parts.</p>
        </div>

        <h3>4.4 Semidefinite Programming (SDP)</h3>
        <p><a href="#" class="definition-link" data-term="sdp">SDPs</a> optimize over positive semidefinite matrix variables:</p>
        $$
        \begin{aligned}
        \min_{X} \quad & \langle C, X \rangle = \mathrm{tr}(C^\top X) \\
        \text{s.t.} \quad & \langle A_i, X \rangle = b_i, \quad i=1,\dots,m \\
        & X \succeq 0
        \end{aligned}
        $$
        <p>where $X \in \mathbb{S}^n$ is a symmetric matrix, and $X \succeq 0$ means $X$ is positive semidefinite (see <a href="../00-linear-algebra-basics/index.html">Lecture 00</a> for PSD properties).</p>

        <div class="subsection">
          <h4>Properties and Applications</h4>
          <ul>
            <li><b>Matrix variables:</b> Optimizing over matrix spaces (in addition to vectors).</li>
            <li><b>PSD cone:</b> $\mathbb{S}^n_+ = \{X \in \mathbb{S}^n \mid X \succeq 0\}$ is a convex cone.</li>
            <li><b>Applications:</b> Control theory, combinatorial optimization relaxations, robust optimization, polynomial optimization.</li>
            <li><b>Algorithms:</b> Interior-point methods.</li>
            <li><b>Complexity:</b> Polynomial time, but more expensive than LP/QP/SOCP.</li>
          </ul>
        </div>

        <div class="example">
          <h4>Example: Minimum Volume Enclosing Ellipsoid (MVEE)</h4>
          <p>Given points $x_1, \ldots, x_N \in \mathbb{R}^n$, find the smallest-volume ellipsoid containing all of them. This can be formulated as an SDP and has applications in outlier detection and robust statistics.</p>
        <p><b>Formulation:</b> An ellipsoid can be parameterized as $\mathcal{E} = \{x \mid \|Ax + b\|_2 \le 1\}$ where $A \succ 0$.
        The volume is proportional to $\det(A^{-1})$. Maximizing $\det(A)$ is equivalent to minimizing $-\log\det(A)$.
        <br>Objective: $\min -\log\det(A)$.
        <br>Constraints: $\|Ax_i + b\|_2 \le 1$ for all $i=1,\dots,N$.
        <br>Using the Schur complement, $\|Ax_i + b\|_2 \le 1 \iff \|Ax_i + b\|_2^2 \le 1 \iff \begin{bmatrix} I & Ax_i+b \\ (Ax_i+b)^\top & 1 \end{bmatrix} \succeq 0$.
        <br>Thus, it is an SDP (with a convex log-det objective).</p>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Tool: Problem Classification Flowchart</h3>
          <p><b>Identify Your Problem Type:</b> Not sure whether your problem is LP, QP, SOCP, or SDP? This decision tree guides you:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Step 1:</b> Is the objective linear? ‚Üí LP (if constraints are linear)</li>
            <li><b>Step 2:</b> Is the objective quadratic? ‚Üí QP (if constraints are linear)</li>
            <li><b>Step 3:</b> Do you have norm constraints? ‚Üí SOCP</li>
            <li><b>Step 4:</b> Do you have matrix variables with PSD constraints? ‚Üí SDP</li>
          </ul>
          <p><i>Why classification matters:</i> Different problem families have different solver requirements and computational costs:</p>
          <ul style="margin-left: 1rem;">
            <li>LP: Fastest, millions of variables routine</li>
            <li>QP: Fast, thousands to millions of variables</li>
            <li>SOCP: Moderate, hundreds of thousands of variables</li>
            <li>SDP: Slowest (due to matrix size), but still polynomial-time</li>
          </ul>
          <div id="widget-problem-flowchart" style="width: 100%; height: 500px; position: relative;"></div>
        </div>
      </section>

      <section class="section-card" id="section-5">
        <h2>5. The Loss + Regularizer Modeling Paradigm</h2>

        <h3>5.1 The Fundamental Template</h3>
        <p>Many problems in machine learning, statistics, and signal processing follow a unified template:</p>
        $$
        \min_{x} \quad \underbrace{\text{loss}(x; \text{data})}_{\text{Data Fidelity}} + \underbrace{\lambda \cdot \text{regularizer}(x)}_{\text{Model Complexity Penalty}}
        $$
        <p>This balances two competing objectives:</p>
        <ul>
          <li><b>Loss (data fidelity):</b> Measures how well the model fits the observed data.</li>
          <li><b>Regularizer (complexity penalty):</b> Prevents overfitting by penalizing complex models.</li>
          <li><b>Trade-off parameter $\lambda \ge 0$:</b> Controls the balance between the two terms.</li>
        </ul>

        <h3>5.2 Common Loss Functions</h3>

        <table class="data-table" style="width: 100%; margin-top: 16px;">
          <thead>
            <tr>
              <th>Loss Function</th>
              <th>Formula</th>
              <th>Application</th>
              <th>Convex?</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><b>Least Squares</b></td>
              <td>$\|Ax - b\|_2^2$</td>
              <td>Regression, system identification</td>
              <td>‚úÖ Yes</td>
            </tr>
            <tr>
              <td><b>Absolute Error</b></td>
              <td>$\|Ax - b\|_1$</td>
              <td>Robust regression</td>
              <td>‚úÖ Yes</td>
            </tr>
            <tr>
              <td><b>Logistic Loss</b></td>
              <td>$\sum_i \log(1 + e^{-y_i (a_i^\top x)})$</td>
              <td>Binary classification</td>
              <td>‚úÖ Yes</td>
            </tr>
            <tr>
              <td><b>Hinge Loss</b></td>
              <td>$\sum_i \max\{0, 1 - y_i (a_i^\top x)\}$</td>
              <td>SVM classification</td>
              <td>‚úÖ Yes</td>
            </tr>
            <tr>
              <td><b>Huber Loss</b></td>
              <td>Quadratic near 0, linear for large residuals</td>
              <td>Robust regression</td>
              <td>‚úÖ Yes</td>
            </tr>
          </tbody>
        </table>

        <h3>5.3 Common Regularizers</h3>

        <table class="data-table" style="width: 100%; margin-top: 16px;">
          <thead>
            <tr>
              <th>Regularizer</th>
              <th>Formula</th>
              <th>Effect</th>
              <th>Convex?</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><b>$\ell_2$ (Ridge)</b></td>
              <td>$\|x\|_2^2$</td>
              <td>Shrinks all coefficients smoothly</td>
              <td>‚úÖ Yes</td>
            </tr>
            <tr>
              <td><b>$\ell_1$ (LASSO)</b></td>
              <td>$\|x\|_1$</td>
              <td>Promotes sparsity (many coefficients = 0)</td>
              <td>‚úÖ Yes</td>
            </tr>
            <tr>
              <td><b>Elastic Net</b></td>
              <td>$\alpha \|x\|_1 + (1-\alpha)\|x\|_2^2$</td>
              <td>Combines $\ell_1$ and $\ell_2$ benefits</td>
              <td>‚úÖ Yes</td>
            </tr>
            <tr>
              <td><b>Nuclear Norm</b></td>
              <td>$\|X\|_* = \sum_i \sigma_i(X)$</td>
              <td>Low-rank matrix recovery</td>
              <td>‚úÖ Yes</td>
            </tr>
            <tr>
              <td><b>Total Variation</b></td>
              <td>$\sum_i |x_{i+1} - x_i|$</td>
              <td>Piecewise-constant solutions (imaging)</td>
              <td>‚úÖ Yes</td>
            </tr>
          </tbody>
        </table>

        <h3>5.4 Why LASSO Promotes Sparsity</h3>
        <div class="insight">
          <h4>‚ö° Geometric Intuition</h4>
          <p>The $\ell_1$ regularizer promotes sparsity due to the geometry of its unit ball:</p>
          <ul>
            <li>The $\ell_1$ ball $\{x \mid \|x\|_1 \le 1\}$ is a <b>cross-polytope</b> (diamond in 2D) with sharp corners at the coordinate axes.</li>
            <li>When the loss function's level sets (ellipsoids for least squares) expand outward, they tend to first touch the $\ell_1$ constraint set at a corner.</li>
            <li>At these corners, many coordinates are exactly zero.</li>
            <li>In contrast, the $\ell_2$ ball is round‚Äîcontact points typically have no zero coordinates.</li>
          </ul>
        </div>

        <div class="proof-box">
          <h4>Analytical Reason: Subgradients at Zero</h4>
          <p>We can also see this algebraically. Consider minimizing a 1D objective $f(x) = \frac{1}{2}(x - y)^2 + \lambda |x|$.</p>
          <div class="proof-step">
            <strong>Smooth case ($\ell_2$):</strong> If we used $\lambda x^2$, the derivative is $(x-y) + 2\lambda x = 0$, giving $x = y / (1+2\lambda)$. This shrinks $y$ towards 0, but never hits exactly 0 unless $y=0$.
          </div>
          <div class="proof-step">
            <strong>Sparse case ($\ell_1$):</strong> The subdifferential of $|x|$ at $x=0$ is the interval $[-1, 1]$. The optimality condition is:
            $$ 0 \in \partial f(x) \iff 0 \in (x - y) + \lambda \partial |x| $$
            If we test $x=0$, the condition becomes $0 \in -y + [-\lambda, \lambda]$, which is true if $|y| \le \lambda$.
          </div>
          <div class="proof-step">
            <strong>Conclusion:</strong> If the data evidence ($y$) is small (specifically $|y| \le \lambda$), the optimal solution is <b>exactly zero</b>. The "kink" in the absolute value function at zero creates a "force field" that traps small values at zero. This generalizes to high dimensions.
          </div>
        </div>

        <div class="example">
          <h4>Example: LASSO Regression</h4>
          <p>The LASSO (Least Absolute Shrinkage and Selection Operator) combines least squares loss with $\ell_1$ regularization:</p>
          $$
          \min_{x} \quad \frac{1}{2}\|Ax - b\|_2^2 + \lambda \|x\|_1
          $$
          <p><b>Properties:</b></p>
          <ul>
            <li>Convex (sum of two convex functions).</li>
            <li>Produces sparse solutions (automatic feature selection).</li>
            <li>Can be reformulated as a QP.</li>
            <li>Widely used in high-dimensional statistics and compressed sensing (covered in <a href="../10-approximation-fitting/index.html">Lecture 14</a>).</li>
          </ul>
        </div>
      </section>

      <section class="section-card" id="section-6">
        <h2>6. Standard Form Transformations and Rewrites</h2>

        <p>Many problems do not initially appear in standard convex form but can be <b>equivalently transformed</b> into LP, QP, SOCP, or SDP. Applying these transformations is essential for practical modeling.</p>

        <h3>6.1 Absolute Value Elimination</h3>

        <div class="subsection">
          <h4>Pattern 1: Absolute Value in Constraints</h4>
          <p><b>Original:</b> $|x_i| \le t_i$</p>
          <p><b>Equivalent (LP-representable):</b> $-t_i \le x_i \le t_i$ (two linear inequalities).</p>
        </div>

        <div class="subsection">
          <h4>Pattern 2: $\ell_1$ Norm in Objective</h4>
          <p><b>Original:</b> $\min \|x\|_1 = \min \sum_{i=1}^n |x_i|$</p>
          <p><b>Reformulation:</b> Introduce slack variables $t_i \ge 0$:</p>
          $$
          \min \sum_{i=1}^n t_i \quad \text{s.t.} \quad -t_i \le x_i \le t_i
          $$
          <p>Now it's an LP with $2n$ variables and $2n$ inequality constraints.</p>
        </div>

        <div class="example">
          <h4>Example: $\ell_1$-Norm Minimization</h4>
          <p>The problem $\min \|Ax - b\|_1$ can be written as:</p>
          $$
          \begin{aligned}
          \min_{x, t} \quad & \mathbf{1}^\top t \\
          \text{s.t.} \quad & -t \le Ax - b \le t, \quad t \ge 0
          \end{aligned}
          $$
          <p>This is an LP with $n + m$ variables.</p>
        </div>

        <h3>6.2 Infinity Norm Transformations</h3>

        <div class="subsection">
          <h4>Pattern: $\ell_\infty$ Norm</h4>
          <p><b>Original:</b> $\|x\|_\infty = \max_i |x_i| \le t$</p>
          <p><b>Equivalent (LP-representable):</b> $-t \mathbf{1} \le x \le t \mathbf{1}$ (componentwise: $-t \le x_i \le t$ for all $i$).</p>
        </div>

        <div class="example">
          <h4>Example: Chebyshev Approximation</h4>
          <p>Minimize the maximum absolute error: $\min \|Ax - b\|_\infty$</p>
          <p><b>Reformulation as LP:</b></p>
          $$
          \begin{aligned}
          \min_{x, t} \quad & t \\
          \text{s.t.} \quad & -t \mathbf{1} \le Ax - b \le t \mathbf{1}
          \end{aligned}
          $$
        </div>

        <h3>6.3 Euclidean Norm (SOCP Form)</h3>

        <div class="subsection">
          <h4>Pattern: $\ell_2$ Norm Constraint</h4>
          <p><b>Form:</b> $\|Ax - b\|_2 \le t$ is already in <b>second-order cone (SOC)</b> form:</p>
          $$
          (Ax - b, t) \in \mathcal{Q}^{m+1} = \{(y, s) \in \mathbb{R}^{m+1} \mid \|y\|_2 \le s\}
          $$
        </div>

        <div class="example">
          <h4>Example: Robust Least Squares</h4>
          <p>Minimize worst-case residual norm subject to ellipsoidal uncertainty. Suppose the matrix $A$ is uncertain, modeled as $A + U$ where the uncertainty matrix $U$ has bounded spectral norm $\|U\|_2 \le \varepsilon$. A simplified but common version considers row-wise uncertainty or a structured uncertainty.</p>
          <p>Let's consider the specific form where the uncertainty is in the matrix $A$, bounded by a norm. A standard formulation is:</p>
          $$
          \min_{x} \max_{\|\Delta\|_2 \le \rho} \|(A + \Delta)x - b\|_2
          $$
          <p>We derive the reformulation as follows.</p>

          <div class="proof-box">
            <h4>Derivation of Robust Least Squares SOCP</h4>

            <div class="proof-step">
              <strong>Step 1: Expand the term.</strong>
              The objective is to minimize the worst-case error. Let $r(x) = \max_{\|\Delta\|_2 \le \rho} \|(A + \Delta)x - b\|_2$.
              Notice that $(A + \Delta)x - b = (Ax - b) + \Delta x$.
              So we want to maximize $\|(Ax - b) + \Delta x\|_2$ over $\|\Delta\|_2 \le \rho$.
            </div>

            <div class="proof-step">
              <strong>Step 2: Apply Triangle Inequality.</strong>
              By the triangle inequality:
              $$ \|(Ax - b) + \Delta x\|_2 \le \|Ax - b\|_2 + \|\Delta x\|_2 $$
              Also, $\|\Delta x\|_2 \le \|\Delta\|_2 \|x\|_2 \le \rho \|x\|_2$.
              Thus, $\|(A + \Delta)x - b\|_2 \le \|Ax - b\|_2 + \rho \|x\|_2$.
            </div>

            <div class="proof-step">
              <strong>Step 3: Show the bound is tight.</strong>
              We need to find a perturbation $\Delta$ with $\|\Delta\|_2 \le \rho$ that achieves this upper bound.
              Let $u = Ax - b$. If $x=0$, the term is $\|b\|_2$, which matches the formula.
              If $x \ne 0$, we choose $\Delta$ to align the perturbation $\Delta x$ with the residual $u$. Define the rank-1 matrix:
              $$ \Delta = \rho \frac{u}{\|u\|_2} \frac{x^\top}{\|x\|_2} $$
              The spectral norm is $\|\Delta\|_2 = \rho \|\frac{u}{\|u\|_2}\|_2 \|\frac{x}{\|x\|_2}\|_2 = \rho \cdot 1 \cdot 1 = \rho$, so it is feasible.
              Now, computing $\Delta x$:
              $$ \Delta x = \rho \frac{u}{\|u\|_2} \frac{x^\top x}{\|x\|_2} = \rho \frac{u}{\|u\|_2} \|x\|_2 = \left( \frac{\rho \|x\|_2}{\|u\|_2} \right) u $$
              This vector is a positive scalar multiple of $u = Ax - b$. Since $\|u + ku\|_2 = (1+k)\|u\|_2 = \|u\|_2 + k\|u\|_2$ for $k \ge 0$, the norms add linearly:
              $$ \|(Ax - b) + \Delta x\|_2 = \|u\|_2 + \|\Delta x\|_2 = \|Ax - b\|_2 + \rho \|x\|_2 $$
            </div>

            <div class="proof-step">
              <strong>Step 4: Formulate as Optimization.</strong>
              The problem becomes:
              $$ \min_x \left( \|Ax - b\|_2 + \rho \|x\|_2 \right) $$
            </div>

            <div class="proof-step">
              <strong>Step 5: Convert to SOCP Standard Form.</strong>
              Introduce auxiliary variables $t_1, t_2$:
              $$ \min_{x, t_1, t_2} \quad t_1 + \rho t_2 $$
              Subject to:
              $$ \|Ax - b\|_2 \le t_1 $$
              $$ \|x\|_2 \le t_2 $$
              These are two second-order cone constraints. Thus, Robust Least Squares is an SOCP.
            </div>
          </div>
        </div>

        <h3>6.4 Maximum Function (Epigraph Form)</h3>

        <div class="subsection">
          <h4>Pattern: Max in Objective</h4>
          <p><b>Original:</b> $\min_x \max\{f_1(x), f_2(x), \ldots, f_m(x)\}$ where each $f_i$ is convex.</p>
          <p><b>Epigraph reformulation:</b></p>
          $$
          \begin{aligned}
          \min_{x, t} \quad & t \\
          \text{s.t.} \quad & f_i(x) \le t, \quad i=1,\ldots,m
          \end{aligned}
          $$

          <div class="proof-box">
            <h4>Derivation: Why this works</h4>
            <div class="proof-step">
              <strong>Step 1: Analyze the constraints.</strong> The constraints $f_i(x) \le t$ for all $i=1,\dots,m$ are equivalent to:
              $$ \max_{i=1,\dots,m} f_i(x) \le t $$
            </div>
            <div class="proof-step">
              <strong>Step 2: Optimize over t.</strong> For a fixed $x$, the smallest possible value of $t$ that satisfies the constraints is $t^*(x) = \max_i f_i(x)$.
            </div>
            <div class="proof-step">
              <strong>Step 3: Joint minimization.</strong> Minimizing $t$ jointly with $x$ subject to these constraints is therefore equivalent to minimizing $t^*(x)$ over $x$:
              $$ \min_{x, t} \{ t \mid \max_i f_i(x) \le t \} = \min_x \left( \min_{t \ge \max_i f_i(x)} t \right) = \min_x \max_i f_i(x) $$
            </div>
          </div>
        </div>

        <div class="example">
          <h4>Example: Minimax Linear Program</h4>
          <p>Minimize the maximum of several linear functions:</p>
          $$
          \min_{x} \max\{c_1^\top x, c_2^\top x, \ldots, c_m^\top x\}
          $$
          <p><b>LP reformulation:</b></p>
          $$
          \begin{aligned}
          \min_{x, t} \quad & t \\
          \text{s.t.} \quad & c_i^\top x \le t, \quad i=1,\ldots,m
          \end{aligned}
          $$
        </div>

        <h3>6.5 Quadratic-Over-Linear (Perspective Form)</h3>

        <div class="subsection">
          <h4>Pattern: Perspective Transformation</h4>
          <p><b>Key result:</b> The function $f(x,t) = \frac{\|x\|_2^2}{t}$ for $t > 0$ is convex and can be represented as a second-order cone constraint:</p>
          $$
          \frac{\|x\|_2^2}{t} \le s \quad \iff \quad \|(2x, s-t)\|_2 \le s + t
          $$
        </div>

        <div class="example">
          <h4>Example: Linear-Fractional Programming</h4>
          <p>Consider minimizing a ratio of affine functions over a polyhedron:</p>
          $$ \min_{x} \frac{c^\top x + d}{e^\top x + f} \quad \text{s.t.} \quad Ax \le b, \ e^\top x + f > 0 $$
          <p>This is <b>quasiconvex</b> but not convex. However, we can transform it into a Linear Program (LP).</p>

          <div class="proof-box">
            <h4>Step-by-Step Reformulation</h4>
            <div class="proof-step">
              <strong>Step 1: Change of variables.</strong>
              Let $y = \frac{x}{e^\top x + f}$ and $z = \frac{1}{e^\top x + f}$. Note that $z > 0$.
              Then $x = y/z$.
            </div>

            <div class="proof-step">
              <strong>Step 2: Transform objective.</strong>
              The objective becomes:
              $$ \frac{c^\top x + d}{e^\top x + f} = c^\top \left(\frac{x}{e^\top x + f}\right) + d \left(\frac{1}{e^\top x + f}\right) = c^\top y + dz $$
              This is linear in $y$ and $z$.
            </div>

            <div class="proof-step">
              <strong>Step 3: Transform constraints.</strong>
              Substituting $x = y/z$ into $Ax \le b$:
              $$ A(y/z) \le b \iff Ay \le bz \iff Ay - bz \le 0 $$
              (Since $z > 0$, we can multiply through without flipping inequality).
              We also need the constraint that defines $z$:
              $$ e^\top x + f = 1/z \implies e^\top (y/z) + f = 1/z \implies e^\top y + fz = 1 $$
              And finally $z \ge 0$.
            </div>

            <div class="proof-step">
              <strong>Step 4: Final LP.</strong>
              $$
              \begin{aligned}
              \min_{y, z} \quad & c^\top y + dz \\
              \text{s.t.} \quad & Ay - bz \le 0 \\
              & e^\top y + fz = 1 \\
              & z \ge 0
              \end{aligned}
              $$
              Solving this LP gives $y^*, z^*$, and we recover $x^* = y^*/z^*$.
            </div>
          </div>
        </div>

        <h3>6.6 Quick Reference Table</h3>

        <table class="data-table" style="width: 100%; margin-top: 16px;">
          <thead>
            <tr>
              <th>Pattern</th>
              <th>Reformulation</th>
              <th>Standard Form</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>$\min \|x\|_1$</td>
              <td>$\min \sum_i t_i$ s.t. $-t \le x \le t$</td>
              <td>LP</td>
            </tr>
            <tr>
              <td>$\min \|x\|_\infty$</td>
              <td>$\min t$ s.t. $-t \le x_i \le t$</td>
              <td>LP</td>
            </tr>
            <tr>
              <td>$\min \max_i f_i(x)$</td>
              <td>$\min t$ s.t. $f_i(x) \le t$</td>
              <td>Depends on $f_i$</td>
            </tr>
            <tr>
              <td>$\|Ax-b\|_2 \le c^\top x + d$</td>
              <td>Direct SOC constraint</td>
              <td>SOCP</td>
            </tr>
            <tr>
              <td>$\frac{\|x\|_2^2}{t} \le s$</td>
              <td>$\|(2x, s-t)\|_2 \le s+t$</td>
              <td>SOCP</td>
            </tr>
          </tbody>
        </table>
      </section>

      <section class="section-card" id="section-7">
        <h2>7. Disciplined Convex Programming (DCP)</h2>

        <h3>7.1 The DCP Philosophy</h3>
        <p><b>Disciplined Convex Programming</b> is a methodology for constructing convex optimization problems by combining basic building blocks using a set of rules that preserve convexity. This approach underlies modeling languages like CVX (MATLAB), CVXPY (Python), and Convex.jl (Julia).</p>

        <h3>7.2 DCP Rules</h3>

        <div class="subsection">
          <h4>Atom Library</h4>
          <p>Each atomic function has a known convexity property:</p>
          <ul>
            <li><b>Convex atoms:</b> $x^2$, $e^x$, $|x|$, $\max\{f_1, f_2\}$, $\|x\|_p$ (for $p \ge 1$).</li>
            <li><b>Concave atoms:</b> $\sqrt{x}$, $\log(x)$, $\min\{f_1, f_2\}$.</li>
            <li><b>Affine atoms:</b> $Ax + b$, $\sum_i x_i$.</li>
          </ul>
        </div>

        <div class="subsection">
          <h4>Composition Rules</h4>
          <ul>
            <li><b>Sum:</b> Convex + Convex = Convex.</li>
            <li><b>Nonnegative scaling:</b> $\alpha \cdot \text{Convex} = \text{Convex}$ (for $\alpha \ge 0$).</li>
            <li><b>Affine composition:</b> $f(Ax + b)$ preserves convexity of $f$.</li>
            <li><b>Monotone composition:</b>
              <ul>
                <li>$h(g(x))$ is convex if $h$ convex increasing and $g$ convex.</li>
                <li>$h(g(x))$ is convex if $h$ convex decreasing and $g$ concave.</li>
              </ul>
            </li>
            <li><b>Pointwise maximum:</b> $\max\{f_1, \ldots, f_m\}$ is convex if all $f_i$ are convex.</li>
          </ul>
        </div>
      </section>

      <section class="section-card" id="section-8">
        <h2>8. Verifying Convexity: A Practical Checklist</h2>

        <h3>8.1 Pre-Solver Sanity Checks</h3>
        <p>Before invoking a solver, verify:</p>

        <ol>
          <li>
            <b>Feasibility Check:</b>
            <ul>
              <li>Does there exist any $x$ satisfying all constraints?</li>
              <li>Are equality constraints consistent? (check $\mathrm{rank}([A \ b]) = \mathrm{rank}(A)$).</li>
            </ul>
          </li>
          <li>
            <b>Unboundedness Check:</b>
            <ul>
              <li>Can the objective be made arbitrarily negative?</li>
              <li>Common cause: missing constraints (e.g., forgot $x \ge 0$ in an LP).</li>
            </ul>
          </li>
          <li>
            <b>Convexity Verification:</b>
            <ul>
              <li>Objective: Is $f_0$ convex? (Check Hessian PSD, or verify DCP).</li>
              <li>Inequality constraints: Are all $f_i$ convex?</li>
              <li>Equality constraints: Are they affine?</li>
            </ul>
          </li>
        </ol>

        <h3>8.2 Common Pitfalls</h3>

        <div class="example">
          <h4>Pitfall 1: Nonconvex Equality Constraints</h4>
          <p><b>Problem:</b> $\min f_0(x)$ s.t. $f(x) = c$ where $f$ is convex but not affine.</p>
          <p><b>Issue:</b> The constraint set $\{x \mid f(x) = c\}$ is generally non-convex (only the epigraph is convex).</p>
          <p><b>Example:</b> $x^2 = 1$ defines $\{-1, +1\}$, a non-convex set.</p>
        </div>

        <div class="example">
          <h4>Pitfall 2: Maximizing a Convex Function</h4>
          <p><b>Problem:</b> $\max f(x)$ where $f$ is convex.</p>
          <p><b>Issue:</b> This is equivalent to $\min -f(x)$ where $-f$ is concave (non-convex).</p>
          <p><b>Fix:</b> Either minimize instead, or reformulate.</p>
        </div>
      </section>

      <section class="section-card" id="section-9">
        <h2>9. The Convex Optimization Workflow</h2>

        <h3>9.1 The Five-Step Process</h3>

        <ol>
          <li>
            <b>Model:</b> Translate the real-world problem into mathematical form.
            <ul>
              <li>Define decision variables.</li>
              <li>Write objective function.</li>
              <li>List all constraints.</li>
            </ul>
          </li>
          <li>
            <b>Transform:</b> Apply standard form rewrites.
            <ul>
              <li>Eliminate absolute values, max functions.</li>
              <li>Convert norms to standard forms.</li>
              <li>Introduce slack variables as needed.</li>
            </ul>
          </li>
          <li>
            <b>Canonicalize:</b> Cast into LP/QP/SOCP/SDP.
            <ul>
              <li>Identify the problem family.</li>
              <li>Ensure all constraints fit the standard form.</li>
            </ul>
          </li>
          <li>
            <b>Solve:</b> Invoke a solver.
            <ul>
              <li>Choose appropriate solver (ECOS, OSQP, MOSEK, Gurobi, etc.).</li>
              <li>Set solver parameters (tolerance, iteration limits).</li>
            </ul>
          </li>
          <li>
            <b>Verify:</b> Check the solution.
            <ul>
              <li>Constraint satisfaction: Are all constraints met?</li>
              <li>Optimality: Does the solution satisfy KKT conditions?</li>
            </ul>
          </li>
        </ol>
      </section>

    <!-- SECTION 10: REVIEW -->
    <section class="section-card" id="section-10">
      <h2>10. Review & Cheat Sheet</h2>

      <h3>Convex vs. Non-Convex: The Litmus Test</h3>
      <table class="data-table" style="width: 100%; margin-bottom: 24px;">
        <thead>
          <tr>
            <th>Feature</th>
            <th>Convex Problem</th>
            <th>Non-Convex Problem</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>Local Optima</b></td>
            <td>Any local optimum is global.</td>
            <td>Can get stuck in local traps.</td>
          </tr>
          <tr>
            <td><b>Objective</b></td>
            <td>Convex function (bowl-shaped).</td>
            <td>Any function (wavy, multimodal).</td>
          </tr>
          <tr>
            <td><b>Constraints</b></td>
            <td>$f_i(x) \le 0$ (convex), $Ax=b$ (affine).</td>
            <td>Non-convex sets, integer constraints.</td>
          </tr>
          <tr>
            <td><b>Solvability</b></td>
            <td>Polynomial time (reliable).</td>
            <td>NP-Hard (generally heuristics).</td>
          </tr>
        </tbody>
      </table>

      <h3>Standard Problem Forms</h3>
      <ul>
        <li><b>LP (Linear Program):</b> Linear objective, linear constraints.
          <br>$\min c^\top x$ s.t. $Ax \le b$.</li>
        <li><b>QP (Quadratic Program):</b> Convex quadratic objective, linear constraints.
          <br>$\min \frac{1}{2}x^\top Px + q^\top x$ s.t. $Ax \le b$ ($P \succeq 0$).</li>
        <li><b>SOCP (Second-Order Cone Program):</b> Linear objective, norm constraints.
          <br>$\min c^\top x$ s.t. $\|A_i x + b_i\|_2 \le c_i^\top x + d_i$.</li>
        <li><b>SDP (Semidefinite Program):</b> Linear objective, LMI constraints.
          <br>$\min \mathrm{tr}(CX)$ s.t. $\sum x_i A_i \succeq B$.</li>
      </ul>

      <h3>Common Reformulation Tricks</h3>
      <ul>
        <li><b>Maximize Convex?</b> No! Minimize the negative (concave).</li>
        <li><b>$|x|$ or $\|x\|_1$:</b> Split into variables $-t \le x \le t$.</li>
        <li><b>$\max_i f_i(x)$:</b> Introduce epigraph variable $t$: $f_i(x) \le t$.</li>
        <li><b>Linear-Fractional:</b> Homogenize variables (Charnes-Cooper transformation).</li>
      </ul>
    </section>

    <!-- SECTION 11: EXERCISES -->
    <section class="section-card" id="section-11">
      <h2><i data-feather="edit-3"></i> 11. Exercises</h2>

      <p>These exercises cover the classification of convex problems, problem reformulation, and properties of convex sets and functions.
      Detailed, step-by-step solutions are provided to ensure complete understanding.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 24px;">
          <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Exercise Map</h4>
          <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
              <li><b>P2.1 - P2.8:</b> Problem Reformulation & Basic Definitions</li>
              <li><b>P2.9 - P2.15:</b> Geometric Sets (Voronoi, Dual Cones, Separation)</li>
              <li><b>P2.16 - P2.24:</b> Theoretical Foundations (Convex Hulls, Combinations, Topology)</li>
              <li><b>P2.25 - P2.31:</b> Advanced Conic Geometry (Duals, Epigraphs, Schur Complement)</li>
          </ul>
      </div>

<div class="problem">
  <h3>P2.1 ‚Äî Classify as Convex / Not Convex</h3>
  <p>For each problem, state if it is convex and explain your answer. <ol type="a"> <li>$\min \|Ax - b\|_2^2$ subject to $Fx = g$.</li> <li>$\min -\|x\|_2$ subject to $Ax \le b$.</li> <li>$\min \|x\|_1$ subject to $\|Bx - c\|_\infty \le 1$.</li> <li>$\min x^\top Q x$ subject to $Dx \le e$, where $Q$ is not guaranteed to be PSD.</li> <li>$\min \|x\|_2^2$ subject to $x_i \in \{0, 1\}$ for all $i$.</li> </ol></p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Composition Rules:</b> The key to identifying convexity is to peel back the layers of functions. Norms are convex, affine maps preserve convexity, and sublevel sets of convex functions are convex.</li> <li><b>The PSD Condition:</b> Quadratic forms $x^\top Q x$ are the boundary between easy and hard; if $Q$ has even one negative eigenvalue, the function is nonconvex (has saddle points). This relies on the eigenvalue characterization from <a href="../00-linear-algebra-basics/index.html">Lecture 00</a>.</li> <li><b>Integer Constraints:</b> Discreteness destroys convexity because the domain is no longer a connected set, eliminating local-to-global gradient information.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <ol type="a">
      <li><b>‚úÖ Convex.</b> The objective $\|Ax - b\|_2^2$ is a convex quadratic function (composition of convex $\|\cdot\|_2^2$ with affine $Ax - b$), and the constraint $Fx = g$ is affine.</li>
      <li><b>‚ùå Not Convex.</b> The objective function $-\|x\|_2$ is concave (negative of a convex function), making this a concave maximization problem.</li>
      <li><b>‚úÖ Convex.</b> Both $\|x\|_1$ and $\|Bx - c\|_\infty$ are norms (hence convex), and the constraint $\|Bx - c\|_\infty \le 1$ defines a convex set (sublevel set of a norm).</li>
      <li><b>‚ùå Not Convex.</b> The quadratic form $x^\top Q x$ is only guaranteed to be convex if $Q \succeq 0$ (positive semidefinite). Without this assumption, the problem is nonconvex.</li>
      <li><b>‚ùå Not Convex.</b> The integer constraint $x_i \in \{0, 1\}$ makes the feasible set non-convex (it's a finite set of points, not a convex set).</li>
    </ol>
  </div>
</div>

<div class="problem">
  <h3>P2.2 ‚Äî Real-World Modeling: Warehouse Location</h3>
  <p>You are tasked with optimizing the placement of a distribution warehouse to minimize the sum of squared Euclidean distances to $k$ retail locations $r_1, \dots, r_k \in \mathbb{R}^2$. The warehouse must be located within a region defined by linear inequalities $C x \le d$. Formulate this as a convex optimization problem.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Summation Preserves Convexity:</b> The objective function is a sum of convex terms (squared Euclidean distances). Since the sum of convex functions is always convex, the overall problem is convex.</li> <li><b>Geometric Interpretation:</b> This problem finds the "centroid" weighted by squared distance. It is mathematically distinct from the Fermat-Weber problem (minimizing sum of Euclidean distances, not squared), which is an SOCP. Using squared distances makes the objective a quadratic form, simplifying the problem to a Quadratic Program (QP).</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Define decision variable.</strong> Let $x \in \mathbb{R}^2$ be the warehouse location (coordinates in the plane).</div>
    <div class="proof-step"><strong>Step 2: Write the objective.</strong> The sum of squared distances to the retail locations is:
    $$ f_0(x) = \sum_{i=1}^k \|x - r_i\|_2^2 $$</div>
    <div class="proof-step"><strong>Step 3: Verify convexity of objective.</strong> Each term $\|x - r_i\|_2^2$ is convex (composition of convex $\|\cdot\|_2^2$ with affine $x - r_i$). The sum of convex functions is convex.</div>
    <div class="proof-step"><strong>Step 4: State the constraints.</strong> The warehouse must satisfy $Cx \le d$ (linear inequalities).</div>
    <div class="proof-step"><strong>Step 5: Complete formulation.</strong> The convex optimization problem is:
    $$ \begin{aligned} \min_{x \in \mathbb{R}^2} \quad & \sum_{i=1}^k \|x - r_i\|_2^2 \\ \text{s.t.} \quad & Cx \le d \end{aligned} $$
    This is a <b>Quadratic Program (QP)</b> since the objective is quadratic and constraints are linear.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.3 ‚Äî Reformulation: $\ell_1$ Regression as LP</h3>
  <p>Show how to reformulate the following problem as a linear program: $$ \min_{x \in \mathbb{R}^n} \|Ax - b\|_1 \quad \text{s.t.} \quad \|x\|_\infty \le 1 $$</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Epigraph Transformation (The "Minimax" Trick):</b> To minimize a function like $f(x) = \max_i g_i(x)$ or a norm, we introduce a scalar variable $t$ and constrain the function to be below it ($f(x) \le t$). This lifts the problem into a higher dimension where the nonlinearity becomes a set of linear constraints.</li> <li><b>Linearizability of Polyhedral Norms:</b> The $\ell_1$ and $\ell_\infty$ norms have unit balls that are polyhedra (defined by linear inequalities). This means any optimization involving them can be cast as a Linear Program (LP).</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Introduce slack variables for the objective.</strong> Write $\|Ax - b\|_1 = \sum_{i=1}^m |[Ax - b]_i|$. Introduce variables $t_i \ge 0$ to represent the absolute values:
    $$ |[Ax - b]_i| \le t_i \quad \iff \quad -t_i \le [Ax - b]_i \le t_i $$</div>
    <div class="proof-step"><strong>Step 2: Rewrite the objective.</strong> Minimizing $\|Ax - b\|_1$ is equivalent to minimizing $\sum_{i=1}^m t_i = \mathbf{1}^\top t$.</div>
    <div class="proof-step"><strong>Step 3: Convert the infinity norm constraint.</strong> The constraint $\|x\|_\infty \le 1$ is equivalent to:
    $$ -\mathbf{1} \le x \le \mathbf{1} \quad (\text{componentwise}) $$</div>
    <div class="proof-step"><strong>Step 4: Complete LP formulation.</strong> The equivalent linear program is:
    $$ \begin{aligned} \min_{x \in \mathbb{R}^n, t \in \mathbb{R}^m} \quad & \mathbf{1}^\top t \\ \text{s.t.} \quad & -t \le Ax - b \le t \\ & -\mathbf{1} \le x \le \mathbf{1} \\ & t \ge 0 \end{aligned} $$
    This is an LP with $n + m$ variables and $2m + 2n$ inequality constraints (plus nonnegativity on $t$).</div>
  </div>
</div>

<div class="problem">
  <h3>P2.4 ‚Äî Prove: Feasible Set is Convex</h3>
  <p>Prove that for a convex optimization problem, the feasible set $\mathcal{F} = \{x \mid f_i(x) \le 0, Ax = b\}$ is convex.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Definition of Convex Set:</b> Contains the line segment between any two points.</li> <li><b>Intersection Property:</b> The feasible set is the intersection of sublevel sets of convex functions (inequalities) and hyperplanes (equalities). Since intersections of convex sets are convex, $\mathcal{F}$ is convex.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Setup:</strong> Take any $x, y \in \mathcal{F}$ and $\theta \in [0,1]$. We must show $z = \theta x + (1-\theta)y \in \mathcal{F}$.</div>
    <div class="proof-step"><strong>Step 1: Verify inequality constraints.</strong> Since $x, y \in \mathcal{F}$, we have $f_i(x) \le 0$ and $f_i(y) \le 0$ for all $i$. By convexity of $f_i$:
    $$ f_i(z) = f_i(\theta x + (1-\theta)y) \le \theta f_i(x) + (1-\theta)f_i(y) \le 0 $$
    So $z$ satisfies all inequality constraints.</div>
    <div class="proof-step"><strong>Step 2: Verify equality constraints.</strong> Since $Ax = b$ and $Ay = b$:
    $$ Az = A(\theta x + (1-\theta)y) = \theta Ax + (1-\theta)Ay = \theta b + (1-\theta)b = b $$
    So $z$ satisfies all equality constraints.</div>
    <div class="proof-step"><strong>Conclusion:</strong> Since $z$ satisfies all constraints, $z \in \mathcal{F}$. Therefore, $\mathcal{F}$ is convex.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.5 ‚Äî Proving Uniqueness</h3>
  <p>Prove that if $f_0$ is strictly convex, then the optimization problem has at most one global minimizer.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Strict Convexity implies Uniqueness:</b> Geometrically, a strictly convex function curves upward everywhere; it has no flat regions.</li> <li><b>Proof Logic:</b> If there were two distinct global minima, the segment connecting them would lie strictly <i>above</i> the function graph (by definition of strict convexity), but also <i>on</i> the graph (since both endpoints are minimal and values cannot go lower), which is a contradiction. Thus, the solution must be unique.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Setup (proof by contradiction):</strong> Assume there are two distinct global minimizers, $x$ and $y$, with $x \neq y$. Let the optimal value be $p^* = f_0(x) = f_0(y)$.</div>
    <div class="proof-step"><strong>Step 1: Form a convex combination.</strong> Consider the point $z = \frac{1}{2}x + \frac{1}{2}y$. Since the feasible set is convex, $z$ is feasible.</div>
    <div class="proof-step"><strong>Step 2: Apply strict convexity.</strong> Since $f_0$ is strictly convex and $x \neq y$:
    $$ f_0(z) = f_0\left(\frac{1}{2}x + \frac{1}{2}y\right) < \frac{1}{2}f_0(x) + \frac{1}{2}f_0(y) $$</div>
    <div class="proof-step"><strong>Step 3: Substitute optimal values.</strong>
    $$ f_0(z) < \frac{1}{2}p^* + \frac{1}{2}p^* = p^* $$</div>
    <div class="proof-step"><strong>Conclusion:</strong> We found a feasible point $z$ with $f_0(z) < p^*$, which contradicts that $p^*$ is the global minimum. Thus, the assumption that there are two distinct minimizers must be false.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.6 ‚Äî Modeling: Maximum Likelihood Estimation</h3>
  <p>Consider a logistic regression model for binary classification. We have data points $(x_i, y_i)$ where $x_i \in \mathbb{R}^n$ are feature vectors and $y_i \in \{-1, 1\}$ are labels. The probability that $y_i = 1$ is modeled as: $$ P(y_i=1 \mid x_i; w) = \frac{1}{1 + e^{-w^\top x_i}} $$ Formulate the problem of finding the weight vector $w$ that maximizes the log-likelihood of the data as a convex optimization problem. (Note: $P(y_i=-1 \mid x_i; w) = 1 - P(y_i=1) = \frac{1}{1 + e^{w^\top x_i}}$).</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Log-Concavity in Statistics:</b> Many fundamental probability distributions (Exponential family) have log-concave density functions. This means maximizing the likelihood (product of probabilities) is equivalent to maximizing the log-likelihood (sum of logs), which is a concave maximization problem (hence convex).</li> <li><b>Logistic Regression:</b> The specific loss function $f(z) = \log(1+e^{-z})$ is convex. This ensures that training logistic regression classifiers is a convex optimization problem with unique global optima (for regularized versions).</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Write the likelihood function.</strong> Assuming independent samples, the likelihood is:
    $$ L(w) = \prod_{i=1}^m P(y_i \mid x_i; w) $$
    We can unify the probability expression. Notice that $P(y_i \mid x_i; w) = \frac{1}{1 + e^{-y_i w^\top x_i}}$.
    <br>Check: If $y_i=1$, $\frac{1}{1+e^{-w^\top x_i}}$. If $y_i=-1$, $\frac{1}{1+e^{w^\top x_i}} = \frac{e^{-w^\top x_i}}{1+e^{-w^\top x_i}} = 1 - \frac{1}{1+e^{-w^\top x_i}}$. Using the identity $1 - \sigma(z) = \sigma(-z)$:
    $$ P(y=-1) = \frac{1}{1+e^{w^\top x}} = \frac{1}{1+e^{-(-1)w^\top x}} $$
    So generally, $P(y \mid x; w) = \sigma(y w^\top x) = \frac{1}{1 + e^{-y w^\top x}}$.</div>
    <div class="proof-step"><strong>Step 2: Formulate the negative log-likelihood.</strong> Maximizing likelihood is equivalent to minimizing negative log-likelihood:
    $$ \ell(w) = -\log L(w) = -\sum_{i=1}^m \log\left( \frac{1}{1 + e^{-y_i w^\top x_i}} \right) $$
    $$ \ell(w) = \sum_{i=1}^m \log(1 + e^{-y_i w^\top x_i}) $$</div>
    <div class="proof-step"><strong>Step 3: Verify convexity.</strong> The function $f(z) = \log(1+e^z)$ (softplus) is convex.
    We compute the gradient and Hessian to verify:
    $$ f'(z) = \frac{e^z}{1+e^z} = \frac{1}{1+e^{-z}} \quad (\text{sigmoid function}) $$
    $$ f''(z) = \frac{e^z(1+e^z) - e^z(e^z)}{(1+e^z)^2} = \frac{e^z}{(1+e^z)^2} > 0 $$
    Our objective is a sum of terms $f(-y_i w^\top x_i)$. Since $f$ is convex and the argument $-y_i w^\top x_i$ is affine in $w$, the composition is convex.
    The sum of convex functions is convex.</div>
    <div class="proof-step"><strong>Conclusion:</strong> The optimization problem is:
    $$ \min_{w \in \mathbb{R}^n} \sum_{i=1}^m \log(1 + e^{-y_i w^\top x_i}) $$
    This is an unconstrained convex optimization problem.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.7 ‚Äî Proving Convexity from First Principles</h3>
  <p>Prove that $f(x) = \|Ax - b\|_2^2$ is a convex function using only the definition of convexity: $$ f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) \quad \forall \theta \in [0,1] $$ Do not use the Hessian condition or the composition rule.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Definition is King:</b> While Hessians and composition rules are convenient shortcuts, the inequality definition is the "ground truth" of convexity. Being able to prove convexity directly from the definition is a crucial skill for understanding <i>why</i> a function is convex.</li> <li><b>Quadratic Behavior:</b> The function $\|Ax-b\|^2$ is a "quadratic bowl". Along any line, its values trace a parabola opening upward. The condition $f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)$ states that the chord connecting two points on the parabola lies above the curve.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Simplify using affine composition.</strong>
    Let $g(u) = \|u\|_2^2 = u^\top u$. We know $f(x) = g(Ax-b)$.
    It is sufficient to prove that $g(u)$ is convex, because if $g$ is convex, then $f(x) = g(Ax-b)$ is convex (affine composition).
    Let's prove $g(u) = \|u\|_2^2$ is convex.</div>
    <div class="proof-step"><strong>Step 2: Expand the convex combination.</strong>
    Let $u, v \in \mathbb{R}^n$ and $\theta \in [0,1]$. Let $z = \theta u + (1-\theta)v$.
    $$ g(z) = \|\theta u + (1-\theta)v\|_2^2 = (\theta u + (1-\theta)v)^\top (\theta u + (1-\theta)v) $$
    $$ = \theta^2 u^\top u + 2\theta(1-\theta) u^\top v + (1-\theta)^2 v^\top v $$
    $$ = \theta^2 \|u\|^2 + 2\theta(1-\theta) u^\top v + (1-\theta)^2 \|v\|^2 $$</div>
    <div class="proof-step"><strong>Step 3: Analyze the RHS of the definition.</strong>
    The target is $\theta g(u) + (1-\theta)g(v) = \theta \|u\|^2 + (1-\theta)\|v\|^2$.</div>
    <div class="proof-step"><strong>Step 4: Compute the difference (RHS - LHS).</strong>
    $$ \Delta = [\theta \|u\|^2 + (1-\theta)\|v\|^2] - [\theta^2 \|u\|^2 + 2\theta(1-\theta) u^\top v + (1-\theta)^2 \|v\|^2] $$
    Group terms by $\|u\|^2$. Coefficient is $\theta - \theta^2 = \theta(1-\theta)$.
    <br>Group terms by $\|v\|^2$. Coefficient is $(1-\theta) - (1-\theta)^2 = (1-\theta)(1 - (1-\theta)) = (1-\theta)\theta$.
    <br>The cross term is $-2\theta(1-\theta) u^\top v$.
    <br>Factor out the common term $\theta(1-\theta)$:
    $$ \Delta = \theta(1-\theta) \left[ \|u\|^2 + \|v\|^2 - 2u^\top v \right] $$
    Recognize the term in brackets as the squared norm of difference:
    $$ \Delta = \theta(1-\theta) \|u - v\|_2^2 $$</div>
    <div class="proof-step"><strong>Step 5: Conclusion.</strong>
    Since $\theta \in [0,1]$, both $\theta$ and $(1-\theta)$ are non-negative. Also, the squared norm $\|u-v\|_2^2$ is always non-negative.
    Thus, $\Delta \ge 0$, which implies:
    $$ \theta g(u) + (1-\theta)g(v) \ge g(\theta u + (1-\theta)v) $$
    So $g(u)$ is convex, and consequently $f(x) = \|Ax-b\|_2^2$ is convex.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.8 ‚Äî Strict vs. Strong Convexity</h3>
  <p>Give an example of a function $f: \mathbb{R} \to \mathbb{R}$ that is strictly convex but not strongly convex. Explain why.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Strict Convexity:</b> The graph lies strictly above the tangent (except at the contact point). $f''(x) > 0$ is sufficient but not necessary ($x^4$).</li> <li><b>Strong Convexity:</b> The function curves up at least as fast as a quadratic. Requires $f''(x) \ge m > 0$ everywhere.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <p><b>Example 1:</b> $f(x) = x^4$.
    <br>Second derivative is $f''(x) = 12x^2$.
    <br><b>Strictly Convex:</b> For any $x \neq y$ and $\theta \in (0,1)$, the strict inequality holds. Note that $f''(x) > 0$ almost everywhere is sufficient for strict convexity.
    <br><b>Not Strongly Convex:</b> At $x=0$, $f''(0) = 0$. Strong convexity requires $f''(x) \ge m > 0$ for all $x$. Since the curvature vanishes at 0, it is not strongly convex.</p>
    <p><b>Example 2:</b> $f(x) = e^x$.
    <br><b>Strictly Convex:</b> $f''(x) = e^x > 0$ for all $x$.
    <br><b>Not Strongly Convex:</b> As $x \to -\infty$, $f''(x) \to 0$. There is no lower bound $m > 0$ that holds for all $x$.</p>
  </div>
</div>

<div class="problem">
  <h3>P2.9 ‚Äî Voronoi Regions</h3>
  <p>Let $x_1, \dots, x_k \in \mathbb{R}^n$ be a set of points (seeds). The Voronoi region associated with $x_i$ is defined as $V_i = \{x \in \mathbb{R}^n \mid \|x - x_i\|_2 \le \|x - x_j\|_2, \forall j \neq i\}$. Prove that $V_i$ is a convex set.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Polyhedra:</b> A polyhedron is the intersection of a finite number of halfspaces.</li><li><b>Geometric Locus:</b> The set of points closer to A than B is a halfspace defined by the perpendicular bisector of segment AB.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Analyze the inequality.</strong> The condition $\|x - x_i\|_2 \le \|x - x_j\|_2$ can be squared (since norms are non-negative):
    $$ \|x - x_i\|_2^2 \le \|x - x_j\|_2^2 $$
    Expanding this:
    $$ x^\top x - 2x_i^\top x + x_i^\top x_i \le x^\top x - 2x_j^\top x + x_j^\top x_j $$
    Canceling $x^\top x$ from both sides:
    $$ -2x_i^\top x + \|x_i\|_2^2 \le -2x_j^\top x + \|x_j\|_2^2 $$
    Rearranging terms to isolate $x$:
    $$ 2(x_j - x_i)^\top x \le \|x_j\|_2^2 - \|x_i\|_2^2 $$</div>
    <div class="proof-step"><strong>Step 2: Identify the geometric object.</strong> The inequality $a^\top x \le b$ with $a = 2(x_j - x_i)$ and $b = \|x_j\|_2^2 - \|x_i\|_2^2$ defines a <b>closed halfspace</b>. The boundary is the perpendicular bisector of the segment connecting $x_i$ and $x_j$.</div>
    <div class="proof-step"><strong>Step 3: Intersection of convex sets.</strong> The Voronoi region $V_i$ is the intersection of $k-1$ such halfspaces (one for each $j \neq i$). Since halfspaces are convex sets, and the intersection of any number of convex sets is convex, $V_i$ is a convex set (specifically, a polyhedron).</div>
  </div>
</div>

<div class="problem">
  <h3>P2.10 ‚Äî Linear-Fractional Functions</h3>
  <p>A function $f: \mathbb{R}^n \to \mathbb{R}$ is linear-fractional if $f(x) = \frac{a^\top x + b}{c^\top x + d}$ with domain $\text{dom } f = \{x \mid c^\top x + d > 0\}$. Prove that the sublevel sets $S_\alpha = \{x \in \text{dom } f \mid f(x) \le \alpha\}$ are convex for all $\alpha \in \mathbb{R}$. Is $f$ convex?</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Quasiconvexity:</b> A function is quasiconvex if all its sublevel sets are convex.</li><li><b>Geometry:</b> While the graph of a linear-fractional function may be curved, its "shadow" at any level is cut by a straight line.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Write out the condition.</strong> $x \in S_\alpha$ means:
    $$ \frac{a^\top x + b}{c^\top x + d} \le \alpha $$</div>
    <div class="proof-step"><strong>Step 2: Clear the denominator.</strong> Since we are in the domain where $c^\top x + d > 0$, we can multiply by the denominator without flipping the inequality:
    $$ a^\top x + b \le \alpha(c^\top x + d) $$</div>
    <div class="proof-step"><strong>Step 3: Rearrange into affine form.</strong>
    $$ a^\top x + b - \alpha c^\top x - \alpha d \le 0 $$
    $$ (a - \alpha c)^\top x + (b - \alpha d) \le 0 $$</div>
    <div class="proof-step"><strong>Step 4: Conclusion for Sublevel Sets.</strong> This is a linear inequality of the form $h^\top x + k \le 0$, which defines a halfspace. A halfspace (intersected with the open halfspace domain) is a convex set. Thus, $f$ is <b>quasiconvex</b>.</div>
    <div class="proof-step"><strong>Step 5: Is it Convex?</strong> Generally, <b>no</b>. Consider $f(x) = 1/x$ on $x>0$. The second derivative $f''(x) = 2/x^3 > 0$, so it is convex. However, $f(x) = -1/x$ on $x>0$ has $f''(x) = -2/x^3 < 0$, so it is concave. A general linear-fractional function is <b>quasilinear</b> (both quasiconvex and quasiconcave), but usually neither convex nor concave.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.11 ‚Äî The Convex Hull</h3>
  <p>The convex hull of a set $S$, denoted $\text{conv}(S)$, is the set of all convex combinations of points in $S$. Prove that $\text{conv}(S)$ is the smallest convex set containing $S$.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Definition:</b> Smallest means it is a subset of any other convex set containing S.</li><li><b>Intersection Definition:</b> Often defined as the intersection of all convex sets containing S. We must show equivalence.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Show $\text{conv}(S)$ is convex.</strong>
    Let $x, y \in \text{conv}(S)$. Then $x = \sum \alpha_i u_i$ and $y = \sum \beta_j v_j$ where $u_i, v_j \in S$, $\sum \alpha_i = 1, \sum \beta_j = 1$.
    Consider $z = \theta x + (1-\theta)y$.
    $$ z = \theta \sum_i \alpha_i u_i + (1-\theta) \sum_j \beta_j v_j = \sum_i (\theta \alpha_i) u_i + \sum_j ((1-\theta)\beta_j) v_j $$
    The coefficients are non-negative and sum to:
    $$ \sum \theta \alpha_i + \sum (1-\theta)\beta_j = \theta(1) + (1-\theta)(1) = 1 $$
    Thus, $z$ is a convex combination of points in $S$, so $z \in \text{conv}(S)$.</div>
    <div class="proof-step"><strong>Step 2: Show $\text{conv}(S)$ contains $S$.</strong>
    For any $x \in S$, we can write $x = 1 \cdot x$. This is a convex combination with coefficient 1. So $S \subseteq \text{conv}(S)$.</div>
    <div class="proof-step"><strong>Step 3: Show minimality.</strong>
    Let $C$ be ANY convex set such that $S \subseteq C$. We must show $\text{conv}(S) \subseteq C$.
    If $x \in \text{conv}(S)$, then $x = \sum_{i=1}^k \theta_i x_i$ with $x_i \in S$.
    Since $S \subseteq C$, we have $x_i \in C$.
    Since $C$ is convex, it must contain all convex combinations of its elements (by induction on $k$, see P2.16).
    Therefore, $x \in C$.
    </div>
    <div class="proof-step"><strong>Conclusion:</strong> $\text{conv}(S)$ is a convex set containing $S$, and it is contained in every convex set that contains $S$. Thus, it is the unique smallest convex set containing $S$.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.12 ‚Äî Dual Cones & Polar Sets</h3>
  <p>Let $K$ be a cone. The dual cone is $K^* = \{y \mid x^\top y \ge 0, \forall x \in K\}$. Find the dual cone of the non-negative orthant $\mathbb{R}^n_+$ and the semidefinite cone $\mathbb{S}^n_+$.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Self-Duality:</b> Some cones are their own duals ($K = K^*$).</li><li><b>Inner Product:</b> For matrices, the inner product is $\text{tr}(XY)$.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Part A: Non-negative Orthant.</strong>
    $K = \mathbb{R}^n_+ = \{x \mid x_i \ge 0\}$.
    $y \in K^*$ iff $x^\top y \ge 0$ for all $x \ge 0$.
    <br>1. If $y_i < 0$ for some $i$, choose $x = e_i$ (standard basis vector). Then $x \in K$ but $x^\top y = y_i < 0$, violating the condition. So we must have $y_i \ge 0$.
    <br>2. If $y \ge 0$, then for any $x \ge 0$, $x^\top y = \sum x_i y_i \ge 0$.
    <br>Conclusion: $K^* = \mathbb{R}^n_+$. It is self-dual.</div>
    <div class="proof-step"><strong>Part B: Semidefinite Cone.</strong>
    $K = \mathbb{S}^n_+ = \{X \in \mathbb{S}^n \mid z^\top X z \ge 0 \forall z\}$.
    $Y \in K^*$ iff $\langle X, Y \rangle = \text{tr}(XY) \ge 0$ for all $X \succeq 0$.
    <br>Let $Y$ have eigendecomposition $Y = \sum \lambda_i u_i u_i^\top$.
    <br>1. Suppose $Y \notin \mathbb{S}^n_+$. Then some eigenvalue $\lambda_k < 0$. Let corresponding eigenvector be $u_k$.
    Choose $X = u_k u_k^\top$. $X$ is PSD (eigenvalues 1 and 0).
    $\text{tr}(XY) = \text{tr}(u_k u_k^\top Y) = u_k^\top Y u_k = \lambda_k < 0$. Contradiction. So $Y$ must be PSD.
    <br>2. Suppose $Y \succeq 0$. Let $Y = LL^\top$ (Cholesky). Then $\text{tr}(XY) = \text{tr}(X LL^\top) = \text{tr}(L^\top X L)$.
    Since $X \succeq 0$, $L^\top X L$ is congruent to $X$ and thus also PSD. The trace of a PSD matrix is the sum of its eigenvalues (which are non-negative), so $\text{tr}(XY) \ge 0$.
    <br>Conclusion: $K^* = \mathbb{S}^n_+$. It is self-dual.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.13 ‚Äî Euclidean Distance Matrices (EDM)</h3>
  <p>A matrix $D \in \mathbb{S}^n$ is a Euclidean Distance Matrix if there exist points $x_1, \dots, x_n$ such that $D_{ij} = \|x_i - x_j\|_2^2$. Show that the set of all EDMs is a convex cone.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Schoenberg Mapping:</b> There is a linear bijection relating EDMs to Gram matrices (PSD matrices).</li><li><b>Linear Map of Cone:</b> The image of a convex cone under a linear map is a convex cone.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Expand definition.</strong>
    $D_{ij} = \|x_i - x_j\|^2 = \|x_i\|^2 + \|x_j\|^2 - 2x_i^\top x_j$.
    Let $G$ be the Gram matrix where $G_{ij} = x_i^\top x_j$. Then $G \succeq 0$.
    Let $v$ be the vector where $v_i = G_{ii} = \|x_i\|^2$.
    We can write $D = v\mathbf{1}^\top + \mathbf{1}v^\top - 2G$.</div>
    <div class="proof-step"><strong>Step 2: Identify Linear Structure.</strong>
    The mapping $\mathcal{L}(G) = \text{diag}(G)\mathbf{1}^\top + \mathbf{1}\text{diag}(G)^\top - 2G$ is a linear map from $\mathbb{S}^n$ to $\mathbb{S}^n$.
    The set of EDMs is the image of the PSD cone $\mathbb{S}^n_+$ under this linear map $\mathcal{L}$.</div>
    <div class="proof-step"><strong>Step 3: Convexity.</strong>
    Since $\mathbb{S}^n_+$ is a convex cone and linear maps preserve convexity and conical structure, the set of EDMs is a convex cone.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.14 ‚Äî The Separation Theorem</h3>
  <p>Let $C$ and $D$ be disjoint convex sets. Prove there exists a hyperplane $a^\top x = b$ that separates them (i.e., $a^\top x \le b$ for all $x \in C$ and $a^\top x \ge b$ for all $x \in D$). Assume $C$ is closed and $D$ is a singleton $\{y\}$ disjoint from $C$.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Projection Theorem:</b> The proof relies on projecting the point $y$ onto the set $C$.</li><li><b>Normal Vector:</b> The separating normal is parallel to the vector connecting $y$ to its projection.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Projection.</strong>
    Since $C$ is closed and convex and $y \notin C$, there exists a unique projection $P_C(y) = \text{argmin}_{x \in C} \|x - y\|_2$. Let $x^* = P_C(y)$.
    Since $y \notin C$, $x^* \neq y$.</div>
    <div class="proof-step"><strong>Step 2: Variational Inequality.</strong>
    The optimality condition for the projection is:
    $$ (y - x^*)^\top (z - x^*) \le 0 \quad \forall z \in C $$
    Let $a = y - x^*$. Note $a \neq 0$. The inequality says $a^\top z \le a^\top x^*$ for all $z \in C$.</div>
    <div class="proof-step"><strong>Step 3: Analyze point y.</strong>
    Consider $a^\top y = a^\top (x^* + a) = a^\top x^* + \|a\|^2$.
    Since $\|a\|^2 > 0$, we have $a^\top y > a^\top x^*$.</div>
    <div class="proof-step"><strong>Step 4: Construct Hyperplane.</strong>
    Let $b = \frac{a^\top x^* + a^\top y}{2}$.
    For any $z \in C$, $a^\top z \le a^\top x^* < b$.
    For the point $y$, $a^\top y > b$.
    Thus the hyperplane $a^\top x = b$ strictly separates $C$ and $\{y\}$.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.15 ‚Äî Supporting Hyperplanes</h3>
  <p>A hyperplane $a^\top x = b$ supports a set $C$ at $x_0 \in \partial C$ if $a^\top x \le b$ for all $x \in C$ and $a^\top x_0 = b$. Prove that if $C$ is a convex set, then for every point $x_0$ in the relative boundary of $C$, there exists a supporting hyperplane to $C$ at $x_0$.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Limit Argument:</b> Construct a sequence of separating hyperplanes for points approaching the boundary and take the limit.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Sequence outside C.</strong>
    Let $x_0$ be in the boundary. There exists a sequence $y_k \notin \text{cl}(C)$ such that $y_k \to x_0$.</div>
    <div class="proof-step"><strong>Step 2: Separate.</strong>
    By the separation theorem, for each $y_k$, there exists a non-zero vector $a_k$ (normalized so $\|a_k\|=1$) such that:
    $$ a_k^\top z \le a_k^\top y_k \quad \forall z \in C $$</div>
    <div class="proof-step"><strong>Step 3: Convergence.</strong>
    The sequence $a_k$ lies on the unit sphere (compact). By Bolzano-Weierstrass, there is a convergent subsequence $a_{k_j} \to a$ with $\|a\|=1$.</div>
    <div class="proof-step"><strong>Step 4: Limit.</strong>
    Taking limits in the inequality $a_{k_j}^\top z \le a_{k_j}^\top y_{k_j}$:
    $$ a^\top z \le a^\top x_0 \quad \forall z \in C $$
    Since $x_0 \in \text{cl}(C)$, this inequality holds for $x_0$ (equality).
    Thus $a^\top x = a^\top x_0$ is a supporting hyperplane.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.16 ‚Äî Finite Convex Combinations</h3>
  <p>Prove by induction that a set $C$ is convex if and only if it contains every convex combination of its points. Specifically, show that if $C$ is convex, then $x = \sum_{i=1}^k \theta_i x_i \in C$ for any $k \ge 2$, $x_i \in C$, $\theta_i \ge 0, \sum \theta_i = 1$.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Base Case:</b> $k=2$ is the definition of convexity.</li><li><b>Inductive Step:</b> Reduce a $k$-point combination to a 2-point combination of a point and a $(k-1)$-point combination.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Base Case ($k=2$):</strong> True by definition of convexity.</div>
    <div class="proof-step"><strong>Inductive Hypothesis:</strong> Assume true for $k-1$.</div>
    <div class="proof-step"><strong>Inductive Step ($k$):</strong>
    Let $x = \sum_{i=1}^k \theta_i x_i$. Assume $\theta_k < 1$ (if $\theta_k=1$, $x=x_k \in C$).
    Rewrite $x$:
    $$ x = \theta_k x_k + (1-\theta_k) \sum_{i=1}^{k-1} \frac{\theta_i}{1-\theta_k} x_i $$
    Let $y = \sum_{i=1}^{k-1} \mu_i x_i$ where $\mu_i = \frac{\theta_i}{1-\theta_k}$.
    Note $\sum_{i=1}^{k-1} \mu_i = \frac{1}{1-\theta_k} \sum_{i=1}^{k-1} \theta_i = \frac{1-\theta_k}{1-\theta_k} = 1$.
    Also $\mu_i \ge 0$.
    By the inductive hypothesis, $y \in C$.
    Now $x = \theta_k x_k + (1-\theta_k) y$. This is a convex combination of two points in $C$ ($x_k$ and $y$).
    Since $C$ is convex, $x \in C$.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.17 ‚Äî Convex & Affine via Lines</h3>
  <p>Prove that a set $S \subseteq \mathbb{R}^n$ is convex if and only if its intersection with any line is convex. (Note: A convex subset of a line is either empty, a point, or an interval).</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Restriction to Lines:</b> Convexity is a 1D property essentially.</li><li><b>Verification:</b> This is often used to check convexity of complex sets by checking $g(t) = x + tv$.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Direction $\implies$:</strong>
    Let $S$ be convex. Let $L = \{x + tv \mid t \in \mathbb{R}\}$ be a line.
    The intersection of convex sets is convex. A line is an affine set (hence convex). Thus $S \cap L$ is convex.</div>
    <div class="proof-step"><strong>Direction $\impliedby$:</strong>
    Assume $S \cap L$ is convex for all lines $L$.
    Let $x, y \in S$. Let $L$ be the line passing through $x$ and $y$.
    By assumption, $S \cap L$ is a convex set in 1D containing $x$ and $y$.
    Thus it must contain the segment $[x, y]$.
    Since $S \cap L \subseteq S$, the segment $[x, y] \subseteq S$.
    Thus $S$ is convex.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.18 ‚Äî Midpoint Convexity</h3>
  <p>A set $C$ is midpoint convex if for any $x,y \in C$, $\frac{x+y}{2} \in C$. Prove that if $C$ is closed and midpoint convex, it is convex.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Dyadic Rationals:</b> Midpoint convexity implies any combination with weights $k/2^n$ is in $C$.</li><li><b>Closure:</b> Dyadic rationals are dense in $[0,1]$. Closure fills the gaps.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Dyadic Rationals.</strong>
    By induction, any convex combination with weights $\theta = k/2^N$ (dyadic rationals) is in $C$.
    For example, $1/4 x + 3/4 y = 1/2(1/2 x + 1/2 y) + 1/2 y$. Since $1/2 x + 1/2 y \in C$, the average with $y$ is also in $C$.</div>
    <div class="proof-step"><strong>Step 2: Density.</strong>
    The set of dyadic rationals $\mathbb{D} = \{ k/2^N \mid k, N \in \mathbb{N} \}$ is dense in $[0,1]$.
    For any $\theta \in [0,1]$, there exists a sequence $d_i \in \mathbb{D}$ such that $d_i \to \theta$.</div>
    <div class="proof-step"><strong>Step 3: Closure.</strong>
    Let $x, y \in C$ and $\theta \in [0,1]$.
    Consider the sequence $z_i = d_i x + (1-d_i) y$. Since $d_i$ are dyadic, $z_i \in C$.
    Since $C$ is closed, $\lim z_i = \theta x + (1-\theta)y$ must be in $C$.
    Thus $C$ is convex.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.19 ‚Äî Hyperbolic Sets</h3>
  <p>Show that the set $S = \{ x \in \mathbb{R}^2_{++} \mid x_1 x_2 \ge 1 \}$ is convex. Generalize this to the hyperbolic cone $\{ (x, t) \in \mathbb{R}^n \times \mathbb{R} \mid x^\top x \le t^2, t \ge 0 \}$.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Sublevel Set:</b> $x_1 x_2 \ge 1 \iff \log x_1 + \log x_2 \ge 0$.</li><li><b>SOCP Connection:</b> The hyperbolic cone is actually the Second Order Cone rotated.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Part A: 2D Hyperbola.</strong>
    $x_1 x_2 \ge 1$ on $x > 0$. Take logs: $\ln x_1 + \ln x_2 \ge 0$.
    Or $-\ln x_1 - \ln x_2 \le 0$.
    Since $-\ln x$ is convex, the sum is convex. The sublevel set of a convex function is convex.</div>
    <div class="proof-step"><strong>Part B: Hyperbolic Cone.</strong>
    $x^\top x \le t^2$ with $t \ge 0$ is equivalent to $\|x\|_2 \le t$.
    This is exactly the Second Order Cone (Lorentz Cone), which is convex (it is the epigraph of the convex function $\|\cdot\|_2$).</div>
  </div>
</div>

<div class="problem">
  <h3>P2.20 ‚Äî Set Classification</h3>
  <p>Classify the following as convex or not: 1. A wedge (intersection of two halfspaces). 2. A torus. 3. The set of rank-1 matrices. 4. The set of matrices with trace 1.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Intersection:</b> Intersection preserves convexity.</li><li><b>Affine:</b> Linear constraints define affine sets (convex).</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>1. Wedge:</strong> YES. Intersection of two halfspaces (convex sets).</div>
    <div class="proof-step"><strong>2. Torus:</strong> NO. It has a hole (doughnut shape). The segment connecting two points on opposite sides passes through the hole.</div>
    <div class="proof-step"><strong>3. Rank-1 Matrices:</strong> NO. Let $A = e_1 e_1^\top$ and $B = e_2 e_2^\top$. Both rank 1. $A+B = \text{diag}(1,1,0\dots)$ has rank 2. Not closed under addition.</div>
    <div class="proof-step"><strong>4. Trace 1:</strong> YES. This is an affine set defined by the linear equality $\text{tr}(X) = 1$.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.21 ‚Äî Partial Sums</h3>
  <p>Let $S_k(x) = \sum_{i=1}^k x_i$. Show that the set $C = \{ x \in \mathbb{R}^n \mid S_k(x) \ge 0, k=1\dots n \}$ is a convex cone.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Linear Map:</b> The partial sums are linear functions of $x$.</li><li><b>Polyhedral Cone:</b> Intersection of linear halfspaces passing through 0.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Linearity.</strong>
    The partial sum $S_k(x) = \sum_{i=1}^k x_i$ can be written as the inner product $a_k^\top x$ where $a_k = (1, \dots, 1, 0, \dots, 0)^\top$ (k ones).</div>
    <div class="proof-step"><strong>Step 2: Halfspaces.</strong>
    The condition $S_k(x) \ge 0$ is equivalent to $a_k^\top x \ge 0$. This defines a closed halfspace passing through the origin.</div>
    <div class="proof-step"><strong>Step 3: Intersection.</strong>
    $C = \bigcap_{k=1}^n \{ x \mid a_k^\top x \ge 0 \}$.
    The intersection of any number of convex cones (and halfspaces through the origin are convex cones) is a convex cone. Since it is defined by a finite number of linear inequalities, it is a polyhedral cone.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.22 ‚Äî Perspectives</h3>
  <p>The perspective function $P: \mathbb{R}^{n+1} \to \mathbb{R}^n$ is $P(x, t) = x/t$ for $t > 0$. Prove that if $C \subseteq \mathbb{R}^n$ is convex, its inverse image under perspective $P^{-1}(C) = \{ (x,t) \mid x/t \in C, t > 0 \}$ is a convex cone.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Scaling:</b> $x/t \in C \iff x \in tC$. This relates to the conic hull.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Definition.</strong>
    Let $(x_1, t_1), (x_2, t_2) \in P^{-1}(C)$. This means $x_1/t_1 \in C$ and $x_2/t_2 \in C$.
    Let $\theta \in [0,1]$. We examine $z = \theta(x_1, t_1) + (1-\theta)(x_2, t_2)$.
    $z = (\theta x_1 + (1-\theta)x_2, \theta t_1 + (1-\theta)t_2)$.
    Let $x_{new} = \theta x_1 + (1-\theta)x_2$ and $t_{new} = \theta t_1 + (1-\theta)t_2$.</div>
    <div class="proof-step"><strong>Step 2: Check membership.</strong>
    We need to check if $x_{new}/t_{new} \in C$.
    $$ \frac{x_{new}}{t_{new}} = \frac{\theta t_1 (x_1/t_1) + (1-\theta)t_2 (x_2/t_2)}{t_{new}} $$
    Let $\lambda = \frac{\theta t_1}{t_{new}}$. Then $1-\lambda = \frac{(1-\theta)t_2}{t_{new}}$.
    The expression is $\lambda (x_1/t_1) + (1-\lambda)(x_2/t_2)$.
    This is a convex combination of two points in $C$. Since $C$ is convex, the result is in $C$.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.23 ‚Äî Linear-Fractional Functions (Detailed)</h3>
  <p>A function $f(x) = \frac{Ax + b}{c^\top x + d}$ is defined on $\{x \mid c^\top x + d > 0\}$. Show that $f$ preserves convexity of sets in the following sense: if $S$ is convex, the image $f(S)$ is convex.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Decomposition:</b> $f$ is a composition of an affine map and a perspective map.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Perspective projection.</strong>
    Consider the linear map $g(x) = (Ax+b, c^\top x + d) \in \mathbb{R}^{m+1}$.
    Since $S$ is convex, the image set $K = g(S) = \{ (Ax+b, c^\top x+d) \mid x \in S \}$ is convex (affine image).</div>
    <div class="proof-step"><strong>Step 2: Perspective map.</strong>
    The function $f(x)$ is exactly the perspective of $g(x)$: $P(y, t) = y/t$.
    So $f(S) = P(g(S))$.
    The perspective function preserves convexity of sets. As proved in P2.22, the inverse image of a convex set under the perspective map is convex. The forward image is also convex (it is the projection of the conic hull of the set lifted to one higher dimension).
    Thus $f(S)$ is convex.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.24 ‚Äî Converse Supporting Hyperplane</h3>
  <p>Prove that a closed set $S$ with non-empty interior is convex if and only if it has a supporting hyperplane at every boundary point.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Forward:</b> Convex $\implies$ Supporting HP (P2.15).</li><li><b>Reverse:</b> Requires showing that if $S$ is supported everywhere, it contains the segment between any two points.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Forward direction.</strong> Done in P2.15.</div>
    <div class="proof-step"><strong>Step 2: Reverse direction (Contradiction).</strong>
    Assume $S$ has supports everywhere but is non-convex.
    Then there exist $x, y \in S$ such that the segment $[x,y] \not\subseteq S$.
    Let $z \in [x,y] \setminus S$.
    Expand a ball around $z$ until it touches $S$. Or simpler: $S$ is the intersection of all its supporting halfspaces.
    The intersection of halfspaces is always convex.
    If $S$ equals this intersection, $S$ is convex.
    Since $S$ is closed and has supports at every boundary point, $S$ is indeed the intersection of these halfspaces.
    Thus $S$ is convex.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.25 ‚Äî 2D Cones (Wedges)</h3>
  <p>In $\mathbb{R}^2$, a cone generated by two vectors $u, v$ is $K = \{ \alpha u + \beta v \mid \alpha, \beta \ge 0 \}$. Show that the dual cone $K^*$ is generated by the two normal vectors to the bounding rays.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Geometry:</b> The dual cone boundaries are perpendicular to the primal cone boundaries.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Normal vectors.</strong>
    Let $n_u$ be such that $n_u^\perp u$ and $n_u^\top v > 0$.
    Let $n_v$ be such that $n_v^\perp v$ and $n_v^\top u > 0$.
    The dual cone is defined by $z^\top x \ge 0$.
    This requires $z^\top u \ge 0$ and $z^\top v \ge 0$.</div>
    <div class="proof-step"><strong>Step 2: Intersection of Halfspaces.</strong>
    $K^* = \{ z \mid u^\top z \ge 0 \} \cap \{ z \mid v^\top z \ge 0 \}$.
    This is the intersection of two halfplanes, forming another wedge.
    The edges of $K^*$ are orthogonal to the edges of $K$.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.26 ‚Äî Dual Cone Properties</h3>
  <p>Prove the following properties of dual cones: 1. $K^*$ is always closed and convex. 2. $K_1 \subseteq K_2 \implies K_2^* \subseteq K_1^*$.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Definition:</b> $K^*$ is an intersection of closed halfspaces.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>1. Closed and Convex.</strong>
    $K^* = \bigcap_{x \in K} \{ y \mid x^\top y \ge 0 \}$.
    For each fixed $x$, the set $\{ y \mid x^\top y \ge 0 \}$ is a closed halfspace (convex and closed).
    The intersection of any collection of closed convex sets is closed and convex.</div>
    <div class="proof-step"><strong>2. Order Reversal.</strong>
    Assume $K_1 \subseteq K_2$. Let $y \in K_2^*$.
    By definition, $y^\top x \ge 0$ for all $x \in K_2$.
    Since $K_1 \subseteq K_2$, this implies $y^\top x \ge 0$ for all $x \in K_1$.
    Therefore, $y \in K_1^*$.
    So $K_2^* \subseteq K_1^*$.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.27 ‚Äî Specific Cones</h3>
  <p>Find the dual of the monotone cone $K = \{ x \in \mathbb{R}^n \mid x_1 \le x_2 \le \dots \le x_n \}$.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Transformation:</b> Write $K$ as a linear map of the non-negative orthant.</li><li><b>Dual Formula:</b> $(Ax \ge 0)^* = \{ A^\top y \mid y \ge 0 \}$.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Difference Variables.</strong>
    We assume the standard monotone non-negative cone: $K = \{ x \in \mathbb{R}^n \mid 0 \le x_1 \le x_2 \le \dots \le x_n \}$.
    Let's define new variables to express $x$ as a non-negative sum.
    Let $y_1 = x_1$, $y_2 = x_2 - x_1$, ..., $y_n = x_n - x_{n-1}$.
    The constraints imply $y_i \ge 0$ for all $i$.
    We can reconstruct $x$ from $y$:
    $$ x_k = \sum_{i=1}^k y_i $$
    In matrix form, $x = Ly$, where $L$ is the lower triangular matrix of all ones.
    Thus $K = \{ Ly \mid y \ge 0 \} = L(\mathbb{R}^n_+)$.</div>
    <div class="proof-step"><strong>Step 2: Dual of Image.</strong>
    We use the property that $(L \mathbb{R}^n_+)^* = \{ z \mid L^\top z \in (\mathbb{R}^n_+)^* \} = \{ z \mid L^\top z \ge 0 \}$.
    $L^\top$ is the upper triangular matrix of ones.
    The condition $L^\top z \ge 0$ means that for each $i$:
    $$ (L^\top z)_i = \sum_{j=i}^n z_j \ge 0 $$
    So the dual cone is $\{ z \in \mathbb{R}^n \mid \sum_{j=i}^n z_j \ge 0, \forall i = 1, \dots, n \}$.
    This is the set of vectors with non-negative suffix sums.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.28 ‚Äî Geometric Mean Cone</h3>
  <p>The geometric mean cone is $K_{gm} = \{ (x,t) \in \mathbb{R}^n_+ \times \mathbb{R}_+ \mid (\prod x_i)^{1/n} \ge t \}$. Prove this is a convex cone.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>AM-GM:</b> Connects to arithmetic mean.</li><li><b>Function:</b> Epigraph of concave function $f(x) = (\prod x_i)^{1/n}$.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Log Transformation.</strong>
    Condition: $\frac{1}{n} \sum \log x_i \ge \log t$.
    The function $g(x) = (\prod x_i)^{1/n}$ is concave on $\mathbb{R}^n_+$.
    (Hessian check or reduction to log-sum).
    Since $g(x)$ is concave, its hypograph (set below it) is convex.
    The set $K$ is the hypograph $\{ (x,t) \mid t \le g(x) \}$.
    Thus $K$ is convex.</div>
    <div class="proof-step"><strong>Step 2: Conic property.</strong>
    $g(\alpha x) = (\prod \alpha x_i)^{1/n} = (\alpha^n \prod x_i)^{1/n} = \alpha g(x)$.
    Homogeneity ensures it is a cone.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.29 ‚Äî Schur Complement</h3>
  <p>Let $X = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}$. Prove that if $C \succ 0$, then $X \succeq 0$ if and only if the Schur complement $S = A - BC^{-1}B^\top \succeq 0$.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Factorization:</b> $X$ can be diagonalized by block elimination matrix.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Block Elimination.</strong>
    Consider $M = \begin{bmatrix} I & -BC^{-1} \\ 0 & I \end{bmatrix}$.
    Compute congruence transformation $M X M^\top$:
    $$ \begin{bmatrix} I & -BC^{-1} \\ 0 & I \end{bmatrix} \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix} \begin{bmatrix} I & 0 \\ -C^{-1}B^\top & I \end{bmatrix} $$
    $$ = \begin{bmatrix} A - BC^{-1}B^\top & 0 \\ 0 & C \end{bmatrix} = \begin{bmatrix} S & 0 \\ 0 & C \end{bmatrix} $$</div>
    <div class="proof-step"><strong>Step 2: Sylvester's Law of Inertia.</strong>
    Congruence preserves inertia (signs of eigenvalues).
    Since $C \succ 0$, $X \succeq 0$ iff $S \succeq 0$.</div>
  </div>
</div>

<div class="problem">
  <h3>P2.30 ‚Äî Epigraph of Matrix Norm</h3>
  <p>Let $f(X) = \|X\|_2$ (spectral norm) for $X \in \mathbb{R}^{m \times n}$. Express the epigraph $\{ (X, t) \mid \|X\|_2 \le t \}$ as a Linear Matrix Inequality (LMI).</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Singular Values:</b> $\|X\|_2 \le t \iff \sigma_{max}(X) \le t \iff X^\top X \preceq t^2 I$.</li><li><b>Schur Complement:</b> Convert quadratic to linear block.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Spectral Norm Definition.</strong>
    $\|X\|_2 \le t$ means $z^\top X^\top X z \le t^2 z^\top z$ for all $z$.
    Or $X^\top X \preceq t^2 I$.
    This implies $t^2 I - X^\top X \succeq 0$.
    Also need $t \ge 0$. So $t I \succeq 0$ (assuming $I$ identity size $m$ or $n$).</div>
    <div class="proof-step"><strong>Step 2: Apply Schur Complement.</strong>
    We want to write $t^2 I - X^\top I^{-1} X \succeq 0$ (roughly).
    Consider the matrix $M = \begin{bmatrix} tI_m & X \\ X^\top & tI_n \end{bmatrix}$.
    Schur complement with respect to $tI_m$ (if $t>0$):
    $S = tI_n - X^\top (tI_m)^{-1} X = tI_n - \frac{1}{t} X^\top X$.
    $S \succeq 0 \iff t^2 I_n - X^\top X \succeq 0$.
    Thus, the condition is equivalent to the LMI:
    $$ \begin{bmatrix} tI_m & X \\ X^\top & tI_n \end{bmatrix} \succeq 0 $$</div>
  </div>
</div>

<div class="problem">
  <h3>P2.31 ‚Äî Sum of k Largest Eigenvalues</h3>
  <p>Let $\lambda_1(X) \ge \dots \ge \lambda_n(X)$ be the eigenvalues of $X \in \mathbb{S}^n$. The function $f_k(X) = \sum_{i=1}^k \lambda_i(X)$ is convex. Provide its SDP representation.</p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Variational Form:</b> sum of k largest is max over orthogonal sets.</li><li><b>Dual:</b> $f_k(X) = \max \{ \text{tr}(XY) \mid 0 \preceq Y \preceq I, \text{tr}(Y)=k \}$.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Dual Representation.</strong>
    It is known (Ky Fan theorem) that the sum of $k$ largest eigenvalues is:
    $$ f_k(X) = \max_{Y} \{ \text{tr}(XY) \mid 0 \preceq Y \preceq I, \text{tr}(Y) = k \} $$
    This is a maximum of linear functions (in $X$), hence convex.</div>
    <div class="proof-step"><strong>Step 2: Primal SDP Form (Epigraph).</strong>
    We want $t \ge f_k(X)$. By strong duality of the expression above:
    Minimization form:
    $$ f_k(X) = \min_{Z, s} \quad k s + \text{tr}(Z) $$
    $$ \text{s.t.} \quad Z \succeq 0, \quad Z + sI \succeq X $$
    So the epigraph is defined by:
    There exist $Z \in \mathbb{S}^n, s \in \mathbb{R}$ such that:
    $$ k s + \text{tr}(Z) \le t $$
    $$ Z \succeq 0 $$
    $$ Z + sI - X \succeq 0 $$
    This is an SDP representation.</div>
  </div>
</div>

      </section>


    <section class="section-card" id="section-12">
      <h2>12. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required Reading:</strong> Boyd & Vandenberghe, Chapter 1 (Introduction) and Chapter 4 (Convex Optimization Problems).</li>
        <li><strong>Supplementary:</strong> Rockafellar, <em>Convex Analysis</em> (for theoretical background).</li>
        <li><strong>Solver Documentation:</strong> <a href="https://www.cvxpy.org/" target="_blank">CVXPY</a>, <a href="https://cvxr.com/cvx/" target="_blank">CVX</a>, or <a href="https://jump.dev/" target="_blank">JuMP</a>.</li>
      </ul>
    </section>
    </article>


    <footer class="site-footer">
      <div class="container">
        <p>¬© <span id="year"></a> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initConvexCombination } from './widgets/js/convex-combination.js';
    initConvexCombination('widget-convex-combination');
  </script>
  <script type="module">
    import { initOptimizationLandscape } from './widgets/js/optimization-landscape.js';
    initOptimizationLandscape('widget-optimization-landscape');
  </script>
  <script type="module">
    import { initConvergenceComparison } from './widgets/js/convergence-comparison.js';
    initConvergenceComparison('widget-convergence-comparison');
  </script>
  <script type="module">
    import { initProblemFlowchart } from './widgets/js/problem-flowchart.js';
    initProblemFlowchart('widget-problem-flowchart');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
