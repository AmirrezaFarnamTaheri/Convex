<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>04. Convex Optimization Problems: Standard Forms — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
</head>
<body>
  <!-- Header with navigation -->
  <header class="site-header">
    <div class="container header-inner">
      <div class="brand">
        <img src="../../static/assets/branding/logo.svg" class="logo" alt="logo" />
        <a href="../../index.html" style="text-decoration:none;color:inherit">
          <strong>Convex Optimization</strong>
        </a>
      </div>
      <nav class="nav">
        <a href="../../index.html">All Lectures</a>
        <a href="#widgets">Interactive</a>
        <a href="#readings">Readings</a>
      </nav>
    </div>
  </header>

  <!-- Main content -->
  <main class="container" style="padding: 32px 0 60px;">
    <!-- Lecture header -->
    <article class="card" style="margin-bottom: 32px;">
      <h1 style="margin-top: 0;">04. Convex Optimization Problems: Standard Forms</h1>
      <div class="meta">
        Date: 2025-11-11 · Duration: 90 min · Tags: standard-forms, classification, LP, QP, SOCP, SDP
      </div>

      <!-- Brief introduction -->
      <section style="margin-top: 16px;">
        <p><strong>Overview:</strong> This lecture covers convex optimization problem classes—LP, QP, SOCP, SDP—and the mathematical foundations for recognizing, classifying, and formulating convex problems. We develop the theoretical underpinnings of standard forms and establish the hierarchy of problem classes that governs modern optimization.</p>
        <p><strong>Prerequisites:</strong> <a href="../03-convex-functions/index.html">Lecture 03</a> (convex functions, first- and second-order conditions, common convex functions)</p>
      </section>
    </article>

    <!-- Learning objectives -->
    <section class="card" style="margin-bottom: 32px;">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul style="line-height: 1.8;">
        <li>Define and recognize the standard form of a convex optimization problem</li>
        <li>Classify problems into LP, QP, SOCP, and SDP categories with precise criteria</li>
        <li>Understand the containment hierarchy: LP ⊂ SOCP ⊂ SDP</li>
        <li>Formulate real-world problems using standard forms</li>
        <li>Apply problem reformulation techniques to convert non-standard forms</li>
        <li>Understand quasiconvex optimization and its relationship to convex optimization</li>
        <li>Make informed decisions about solver selection based on problem structure</li>
      </ul>
    </section>

    <!-- Section 1: Standard Form -->
    <section class="card" id="section-1">
      <h2>1. The Standard Form of Convex Optimization</h2>

      <h3>1.1 Definition</h3>
      <p>A <strong>convex optimization problem</strong> in <a href="#" class="definition-link">standard form</a> is:</p>

      <div style="padding: 16px; background: var(--panel); border-left: 4px solid var(--brand); margin: 16px 0;">
        <p style="margin: 0;">
          $
          \begin{aligned}
          \text{minimize} \quad & f_0(x) \\
          \text{subject to} \quad & f_i(x) \le 0, \quad i = 1, \dots, m \\
          & h_j(x) = 0, \quad j = 1, \dots, p
          \end{aligned}
          $
        </p>
      </div>

      <p>where:</p>
      <ul>
        <li>The variable is $x \in \mathbb{R}^n$</li>
        <li>$f_0: \mathbb{R}^n \to \mathbb{R}$ is the <strong>objective function</strong> (<a href="#" class="definition-link" data-term="convex function">convex</a>)</li>
        <li>$f_i: \mathbb{R}^n \to \mathbb{R}$, $i = 1, \dots, m$ are <strong>inequality constraint functions</strong> (all convex)</li>
        <li>$h_j: \mathbb{R}^n \to \mathbb{R}$, $j = 1, \dots, p$ are <strong>equality constraint functions</strong> (all affine, i.e., $h_j(x) = a_j^\top x - b_j$)</li>
      </ul>

      <h3>1.2 The Feasible Set</h3>
      <p>The <strong>feasible set</strong> (or constraint set) is:</p>
      <p style="text-align: center;">
        $
        \mathcal{F} = \{x \in \mathbb{R}^n \mid f_i(x) \le 0, \; i = 1, \dots, m; \; h_j(x) = 0, \; j = 1, \dots, p\}
        $
      </p>

      <div class="proof">
        <h4>Theorem: Convexity of Feasible Set</h4>
        <p><strong>Statement:</strong> If $f_i$ are convex and $h_j$ are affine, then $\mathcal{F}$ is a convex set.</p>

        <div class="proof-step">
          <strong>Proof:</strong> The feasible set $\mathcal{F}$ is the intersection of sublevel sets $\{x \mid f_i(x) \le 0\}$ (convex by Lecture 03) and affine sets $\{x \mid h_j(x) = 0\}$ (convex by definition). Since the intersection of convex sets is convex, $\mathcal{F}$ is convex.
        </div>
      </div>

      <h3>1.3 Optimal Value and Optimal Points</h3>
      <ul>
        <li>The <strong>optimal value</strong> is $p^* = \inf\{f_0(x) \mid x \in \mathcal{F}\}$</li>
        <li>$x^*$ is <strong>optimal</strong> (or a <strong>minimizer</strong>) if $x^* \in \mathcal{F}$ and $f_0(x^*) = p^*$</li>
        <li>$x$ is <strong>$\epsilon$-suboptimal</strong> if $x \in \mathcal{F}$ and $f_0(x) \le p^* + \epsilon$</li>
        <li>$x$ is <strong>locally optimal</strong> if there exists $R > 0$ such that $f_0(x) = \inf\{f_0(z) \mid z \in \mathcal{F}, \|z - x\|_2 \le R\}$</li>
      </ul>

      <div class="proof">
        <h4>Fundamental Property: Local = Global</h4>
        <p><strong>Statement:</strong> For convex optimization problems, any locally optimal point is globally optimal.</p>

        <div class="proof-step">
          <strong>Proof:</strong> This is a consequence of the convexity of $f_0$ and $\mathcal{F}$, as established in Lecture 01. If $x$ is locally optimal but not globally optimal, we can construct a line segment from $x$ to a better point that contradicts local optimality.
        </div>
      </div>

      <figure style="margin: 16px 0; text-align: center;">
        <img src="../../static/assets/topics/04-convex-opt-problems/linear-programming-feasible-region.svg" alt="An example of a feasible region in a Linear Program" style="max-width: 450px; height: auto;" />
        <figcaption style="font-size: 13px; color: var(--muted); margin-top: 8px;">
          A classic example of a feasible set for a Linear Program (LP). The region is a convex polygon defined by linear inequality constraints. The optimal solution occurs at a vertex of the polyhedron.
        </figcaption>
      </figure>
    </section>

    <!-- Section 2: Linear Programs (LP) -->
    <section class="card" id="section-2">
      <h2>2. Linear Programs (LP)</h2>

      <h3>2.1 Definition</h3>
      <p>A <strong>Linear Program (<a href="#" class="definition-link">LP</a>)</strong> has the form:</p>

      <div style="padding: 16px; background: var(--panel); border-left: 4px solid var(--brand); margin: 16px 0;">
        <p style="margin: 0;">
          $
          \begin{aligned}
          \text{minimize} \quad & c^\top x \\
          \text{subject to} \quad & Gx \preceq h \\
          & Ax = b
          \end{aligned}
          $
        </p>
      </div>

      <p>where:</p>
      <ul>
        <li>$x \in \mathbb{R}^n$ is the variable</li>
        <li>$c \in \mathbb{R}^n$ is the cost vector</li>
        <li>$G \in \mathbb{R}^{m \times n}$, $h \in \mathbb{R}^m$ define the inequality constraints</li>
        <li>$A \in \mathbb{R}^{p \times n}$, $b \in \mathbb{R}^p$ define the equality constraints</li>
        <li>$\preceq$ denotes componentwise inequality</li>
      </ul>

      <h3>2.2 Key Properties</h3>
      <ul>
        <li><strong>Feasible set:</strong> A polyhedron (intersection of halfspaces and hyperplanes)</li>
        <li><strong>Optimal solution:</strong> If it exists and the optimal value is finite, occurs at a vertex of the polyhedron</li>
        <li><strong>Complexity:</strong> Polynomial-time solvable (simplex method, interior-point methods)</li>
        <li><strong>Duality:</strong> Strong duality always holds (Lecture 05)</li>
      </ul>

      <h3>2.3 Standard LP Examples</h3>

      <h4>Example 2.1: Diet Problem</h4>
      <p>Given $n$ foods with costs $c_i$ and $m$ nutrients, where food $j$ contains $A_{ij}$ units of nutrient $i$, find the cheapest diet meeting minimum nutrient requirements $b_i$:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & \sum_{j=1}^n c_j x_j \\
        \text{subject to} \quad & \sum_{j=1}^n A_{ij} x_j \ge b_i, \quad i = 1, \dots, m \\
        & x_j \ge 0, \quad j = 1, \dots, n
        \end{aligned}
        $
      </p>
      <p>This is an LP: the objective is linear, constraints are linear inequalities.</p>

      <h4>Example 2.2: Chebyshev Approximation</h4>
      <p>Find $x \in \mathbb{R}^n$ minimizing $\|Ax - b\|_\infty$. This can be reformulated as:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & t \\
        \text{subject to} \quad & -t \mathbf{1} \preceq Ax - b \preceq t \mathbf{1}
        \end{aligned}
        $
      </p>
      <p>where $\mathbf{1}$ is the vector of ones. This is an LP in variables $(x, t)$.</p>

      <h3>2.4 Piecewise-Linear Minimization</h3>
      <p>Minimizing $f(x) = \max_{i=1,\dots,m} (a_i^T x + b_i)$ (a piecewise-linear convex function) can be reformulated as:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & t \\
        \text{subject to} \quad & a_i^T x + b_i \le t, \quad i = 1, \dots, m
        \end{aligned}
        $
      </p>
      <p>This transformation is fundamental: many convex problems can be reduced to LPs via epigraph reformulation.</p>
    </section>

    <!-- Section 3: Quadratic Programs (QP) -->
    <section class="card" id="section-3">
      <h2>3. Quadratic Programs (QP)</h2>

      <h3>3.1 Definition</h3>
      <p>A <strong>Quadratic Program (<a href="#" class="definition-link">QP</a>)</strong> has the form:</p>

      <div style="padding: 16px; background: var(--panel); border-left: 4px solid var(--brand); margin: 16px 0;">
        <p style="margin: 0;">
          $
          \begin{aligned}
          \text{minimize} \quad & \frac{1}{2} x^\top P x + q^\top x + r \\
          \text{subject to} \quad & Gx \preceq h \\
          & Ax = b
          \end{aligned}
          $
        </p>
      </div>

      <p>where $P \in \mathbb{S}_+^n$ (positive semidefinite), $q \in \mathbb{R}^n$, $r \in \mathbb{R}$.</p>

      <div class="proof">
        <h4>Convexity of QP</h4>
        <div class="proof-step">
          <strong>Objective convexity:</strong> Since $P \succeq 0$, the quadratic form $f_0(x) = \frac{1}{2} x^\top P x + q^\top x + r$ is convex. This follows from the second-order condition: $\nabla^2 f_0(x) = P \succeq 0$.
        </div>
        <div class="proof-step">
          <strong>Feasible set convexity:</strong> The constraints define a polyhedron, which is convex.
        </div>
        <div class="proof-step">
          <strong>Conclusion:</strong> A QP is a convex optimization problem.
        </div>
      </div>

      <h3>3.2 Key Properties</h3>
      <ul>
        <li><strong>Optimality:</strong> If $P \succ 0$ (positive definite), the problem is strictly convex and has a unique minimizer (if feasible)</li>
        <li><strong>KKT conditions:</strong> Necessary and sufficient for optimality (Lecture 05)</li>
        <li><strong>Solvers:</strong> Interior-point methods, active-set methods</li>
      </ul>

      <h3>3.3 Standard QP Examples</h3>

      <h4>Example 3.1: Least-Squares with Linear Inequality Constraints</h4>
      <p>Minimize $\|Ax - b\|_2^2$ subject to $Cx \preceq d$:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & x^\top A^\top A x - 2 b^\top A x + b^\top b \\
        \text{subject to} \quad & Cx \preceq d
        \end{aligned}
        $
      </p>
      <p>This is a QP with $P = 2A^\top A \succeq 0$ and $q = -2A^\top b$.</p>

      <h4>Example 3.2: Markowitz Portfolio Optimization</h4>
      <p>Given $n$ assets with expected returns $\mu \in \mathbb{R}^n$ and covariance matrix $\Sigma \in \mathbb{S}_{++}^n$, find a portfolio $x \in \mathbb{R}^n$ (where $x_i$ is the fraction invested in asset $i$) minimizing risk for a target return $r_{\text{target}}$:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & x^\top \Sigma x \\
        \text{subject to} \quad & \mu^\top x \ge r_{\text{target}} \\
        & \mathbf{1}^\top x = 1 \\
        & x \ge 0
        \end{aligned}
        $
      </p>
      <p>This is a QP with $P = 2\Sigma \succ 0$, capturing the fundamental risk-return tradeoff in finance.</p>

      <h3>3.4 Relationship to LP</h3>
      <p>Every LP is a QP (set $P = 0$). Thus: <strong>LP ⊂ QP</strong>.</p>
    </section>

    <!-- Section 4: Second-Order Cone Programs (SOCP) -->
    <section class="card" id="section-4">
      <h2>4. Second-Order Cone Programs (SOCP)</h2>

      <h3>4.1 Definition</h3>
      <p>A <strong>Second-Order Cone Program (<a href="#" class="definition-link" data-term="socp">SOCP</a>)</strong> has the form:</p>

      <div style="padding: 16px; background: var(--panel); border-left: 4px solid var(--brand); margin: 16px 0;">
        <p style="margin: 0;">
          $
          \begin{aligned}
          \text{minimize} \quad & f^\top x \\
          \text{subject to} \quad & \|A_i x + b_i\|_2 \le c_i^\top x + d_i, \quad i = 1, \dots, m \\
          & Fx = g
          \end{aligned}
          $
        </p>
      </div>

      <p>where $x \in \mathbb{R}^n$ is the variable. Each constraint $\|A_i x + b_i\|_2 \le c_i^T x + d_i$ is called a <strong>second-order cone constraint</strong>.</p>

      <h3>4.2 The Second-Order Cone (Lorentz Cone)</h3>
      <p>The <strong>second-order cone</strong> (or <strong>Lorentz cone</strong>) in $\mathbb{R}^{n+1}$ is:</p>
      <p style="text-align: center;">
        $
        \mathcal{L}^{n+1} = \{(x, t) \in \mathbb{R}^n \times \mathbb{R} \mid \|x\|_2 \le t\}
        $
      </p>

      <div class="proof">
        <h4>Convexity of the Second-Order Cone</h4>
        <div class="proof-step">
          <strong>Setup:</strong> Let $(x_1, t_1), (x_2, t_2) \in \mathcal{L}^{n+1}$ and $\theta \in [0, 1]$.
        </div>
        <div class="proof-step">
          <strong>Convex combination:</strong> We have $\|x_1\|_2 \le t_1$ and $\|x_2\|_2 \le t_2$. Consider:
          $
          \|\theta x_1 + (1-\theta) x_2\|_2 \le \theta \|x_1\|_2 + (1-\theta) \|x_2\|_2 \quad \text{(triangle inequality)}
          $
        </div>
        <div class="proof-step">
          <strong>Apply constraints:</strong>
          $
          \le \theta t_1 + (1-\theta) t_2
          $
        </div>
        <div class="proof-step">
          <strong>Conclusion:</strong> Thus $(\theta x_1 + (1-\theta) x_2, \theta t_1 + (1-\theta) t_2) \in \mathcal{L}^{n+1}$, so $\mathcal{L}^{n+1}$ is convex.
        </div>
      </div>

      <h3>4.3 Key Properties</h3>
      <ul>
        <li><strong>Generality:</strong> SOCP subsumes LP and QP</li>
        <li><strong>Solvers:</strong> Efficiently solved by interior-point methods</li>
        <li><strong>Applications:</strong> Robust optimization, robust least squares, $\ell_1$ and $\ell_\infty$ norm minimization</li>
      </ul>

      <h3>4.4 Standard SOCP Examples</h3>

      <h4>Example 4.1: Robust Least Squares</h4>
      <p>Consider the least-squares problem $\min \|Ax - b\|_2$ where $A$ is uncertain: $A \in \{A_0 + \sum_{i=1}^p u_i A_i \mid \|u\|_2 \le 1\}$. The <strong>robust</strong> version minimizes the worst-case residual:</p>
      <p style="text-align: center;">
        $
        \min_x \max_{\|u\|_2 \le 1} \left\| \left(A_0 + \sum_{i=1}^p u_i A_i\right) x - b \right\|_2
        $
      </p>

      <div class="proof-enhanced">
        <h4>Derivation (Unstructured Uncertainty)</h4>
        <div class="proof-step">
          <strong>Step 1: Define the uncertainty set.</strong>
          Assume the matrix $A$ is uncertain but bounded: $A = A_0 + \Delta$, where $\|\Delta\|_2 \le \rho$. We want to minimize the worst-case residual:
          $$ \min_x \max_{\|\Delta\|_2 \le \rho} \|(A_0 + \Delta)x - b\|_2 $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Expand the inner maximization.</strong>
          Let $r_0 = A_0 x - b$. The term inside the max is $\|r_0 + \Delta x\|_2$.
          By the triangle inequality:
          $$ \|r_0 + \Delta x\|_2 \le \|r_0\|_2 + \|\Delta x\|_2 \le \|r_0\|_2 + \|\Delta\|_2 \|x\|_2 \le \|r_0\|_2 + \rho \|x\|_2 $$
        </div>
        <div class="proof-step">
          <strong>Step 3: Show the bound is tight.</strong>
          We need to show there exists a $\Delta^*$ with $\|\Delta^*\|_2 \le \rho$ that achieves this bound.
          Choose $\Delta^* = \rho \frac{r_0}{\|r_0\|_2} \frac{x^\top}{\|x\|_2}$ (a rank-1 matrix).
          Then $\|\Delta^*\|_2 = \rho$, and $\Delta^* x = \rho \frac{r_0}{\|r_0\|_2} \|x\|_2$.
          The vector $\Delta^* x$ is parallel to $r_0$, so $\|r_0 + \Delta^* x\|_2 = \|r_0\|_2 + \|\Delta^* x\|_2 = \|r_0\|_2 + \rho \|x\|_2$.
        </div>
        <div class="proof-step">
          <strong>Step 4: Formulate as SOCP.</strong>
          The problem becomes $\min_x (\|A_0 x - b\|_2 + \rho \|x\|_2)$.
          We introduce slack variables $t_1, t_2$:
          $$ \min_{x, t_1, t_2} \quad t_1 + \rho t_2 $$
          $$ \text{s.t.} \quad \|A_0 x - b\|_2 \le t_1 $$
          $$ \quad \quad \|x\|_2 \le t_2 $$
          These are standard second-order cone constraints.
        </div>
      </div>

      <h4>Example 4.2: $\ell_1$ Norm Minimization via SOCP</h4>
      <p>The problem $\min \|x\|_1$ subject to $Ax = b$ can be written as:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & \mathbf{1}^\top y \\
        \text{subject to} \quad & -y \preceq x \preceq y \\
        & Ax = b
        \end{aligned}
        $
      </p>
      <p>which is an LP. However, for more complex norms, SOCP reformulations are essential.</p>

      <h3>4.5 Hierarchy: LP ⊂ QP ⊂ SOCP</h3>
      <ul>
        <li>Every LP is a QP (set $P = 0$)</li>
        <li>Every QP is an SOCP (quadratic constraints can be expressed as second-order cone constraints)</li>
      </ul>
    </section>

    <!-- Section 5: Semidefinite Programs (SDP) -->
    <section class="card" id="section-5">
      <h2>5. Semidefinite Programs (SDP)</h2>

      <h3>5.1 Definition</h3>
      <p>A <strong>Semidefinite Program (<a href="#" class="definition-link" data-term="sdp">SDP</a>)</strong> has the form:</p>

      <div style="padding: 16px; background: var(--panel); border-left: 4px solid var(--brand); margin: 16px 0;">
        <p style="margin: 0;">
          $
          \begin{aligned}
          \text{minimize} \quad & c^\top x \\
          \text{subject to} \quad & F(x) = F_0 + \sum_{i=1}^n x_i F_i \succeq 0 \\
          & Ax = b
          \end{aligned}
          $
        </p>
      </div>

      <p>where:</p>
      <ul>
        <li>$x \in \mathbb{R}^n$ is the variable</li>
        <li>$F_i \in \mathbb{S}^m$ (symmetric matrices) for $i = 0, \dots, n$</li>
        <li>$F(x) \succeq 0$ means $F(x)$ is positive semidefinite</li>
        <li>$A \in \mathbb{R}^{p \times n}$, $b \in \mathbb{R}^p$ define additional affine constraints</li>
      </ul>

      <h3>5.2 The PSD Cone</h3>
      <p>The constraint $F(x) \succeq 0$ means $F(x)$ lies in the cone of positive semidefinite matrices:</p>
      <p style="text-align: center;">
        $
        \mathbb{S}_+^m = \{X \in \mathbb{S}^m \mid X \succeq 0\}
        $
      </p>
      <p>Recall from Lecture 02 that $\mathbb{S}_+^m$ is a proper cone (convex, closed, pointed, solid).</p>

      <h3>5.3 Key Properties</h3>
      <ul>
        <li><strong>Generality:</strong> SDP subsumes LP, QP, and SOCP</li>
        <li><strong>Complexity:</strong> Polynomial-time solvable using interior-point methods</li>
        <li><strong>Duality:</strong> Strong duality typically holds; dual problem is also an SDP</li>
        <li><strong>Applications:</strong> Combinatorial optimization relaxations, control theory (LMI), matrix completion</li>
      </ul>

      <h3>5.4 Standard SDP Examples</h3>

      <h4>Example 5.1: Eigenvalue Minimization</h4>
      <p>Minimize the maximum eigenvalue of $A(x) = A_0 + \sum_{i=1}^n x_i A_i$:</p>
      <p style="text-align: center;">
        $
        \min_x \lambda_{\max}(A(x))
        $
      </p>
      <p>This is equivalent to the SDP:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & t \\
        \text{subject to} \quad & A(x) \preceq t I
        \end{aligned}
        $
      </p>
      <p>The constraint $A(x) \preceq tI$ is equivalent to $tI - A(x) \succeq 0$, which is an LMI (linear matrix inequality).</p>

      <h4>Example 5.2: Matrix Norm Minimization (Operator Norm)</h4>
      <p>Minimize the spectral norm (operator norm) of a matrix depending linearly on variables $x$: $\|A(x)\|_2$, where $A(x) = A_0 + \sum_{i=1}^n x_i A_i$.</p>

      <div class="proof-enhanced">
        <h4>Derivation of SDP Form</h4>
        <div class="proof-step">
          <strong>Step 1: Epigraph Form.</strong>
          The problem is equivalent to:
          $$ \text{minimize } t \quad \text{subject to } \|A(x)\|_2 \le t $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Singular Value Characterization.</strong>
          The spectral norm $\|A\|_2$ is the maximum singular value $\sigma_{\max}(A)$.
          The condition $\sigma_{\max}(A) \le t$ (for $t \ge 0$) is equivalent to saying all eigenvalues of $A^\top A$ are $\le t^2$:
          $$ A(x)^\top A(x) \preceq t^2 I $$
        </div>
        <div class="proof-step">
          <strong>Step 3: Schur Complement.</strong>
          We can rewrite the quadratic matrix inequality $t^2 I - A(x)^\top A(x) \succeq 0$ using the Schur complement.
          Recall that $\begin{bmatrix} A & B \\ B^\top & C \end{bmatrix} \succeq 0$ iff $C \succeq 0$ and $A - B C^{-1} B^\top \succeq 0$.
          We want to match $tI - A(x)^\top (tI)^{-1} A(x) \succeq 0$.
          Set $A_{block} = tI$, $B_{block} = A(x)^\top$, $C_{block} = tI$.
          Then the LMI is:
          $$ \begin{bmatrix} tI & A(x)^\top \\ A(x) & tI \end{bmatrix} \succeq 0 $$
        </div>
        <div class="proof-step">
          <strong>Step 4: Final SDP Formulation.</strong>
          $$
          \begin{aligned}
          \text{minimize} \quad & t \\
          \text{subject to} \quad & \begin{bmatrix} tI & A(x)^\top \\ A(x) & tI \end{bmatrix} \succeq 0
          \end{aligned}
          $$
          This is a standard SDP in variables $(x, t)$.
        </div>
      </div>

      <!-- Widget 5: SDP Visualizer -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Interactive: SDP Visualizer</h3>
        <p><strong>Purpose:</strong> 3D visualization of the positive semidefinite cone for $2 \times 2$ matrices.</p>
        <ul style="font-size: 14px; line-height: 1.6;">
          <li>Explore the geometry of $\mathbb{S}_+^2$ (a 3D cone in the space of symmetric matrices)</li>
          <li>Understand how SDP constraints restrict feasible matrices</li>
          <li>Connect eigenvalues to cone membership</li>
        </ul>
        <div id="widget-5" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <h3>5.5 The Hierarchy: LP ⊂ QP ⊂ SOCP ⊂ SDP</h3>
      <p>This inclusion hierarchy is strict:</p>
      <ul>
        <li>Every LP is a QP (take $P = 0$)</li>
        <li>Every QP can be formulated as an SOCP</li>
        <li>Every SOCP can be formulated as an SDP (the second-order cone can be represented via LMI)</li>
      </ul>

      <div class="proof">
        <h4>Embedding SOCP into SDP via Schur Complement</h4>
        <p>We want to show that the second-order cone constraint $\|y\|_2 \le t$ (where $y = Ax+b, t = c^\top x + d$) is equivalent to a Linear Matrix Inequality (LMI).</p>

        <div class="proof-step">
          <strong>Step 1: Schur Complement Lemma.</strong>
          Recall that for a block matrix $M = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}$ with $A \succ 0$, we have $M \succeq 0 \iff C - B^\top A^{-1} B \succeq 0$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Construct the matrix.</strong>
          Consider the matrix:
          $$
          M = \begin{bmatrix} tI & y \\ y^\top & t \end{bmatrix}
          $$
          Assume $t > 0$ (if $t=0$, we need $y=0$, which is handled by the limit or closure). Then $tI \succ 0$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Apply the Lemma.</strong>
          Identifying blocks: $A = tI$, $B = y$, $C = t$.
          The condition $M \succeq 0$ is equivalent to:
          $$ t - y^\top (tI)^{-1} y \ge 0 $$
          $$ t - \frac{1}{t} y^\top y \ge 0 $$
          Multiplying by $t$ (since $t > 0$):
          $$ t^2 - y^\top y \ge 0 \implies \|y\|_2^2 \le t^2 $$
          Since $t > 0$, this is equivalent to $\|y\|_2 \le t$.
        </div>

        <div class="proof-step">
          <strong>Step 4: Conclusion.</strong>
          The SOC constraint $\|Ax+b\|_2 \le c^\top x + d$ is equivalent to the LMI:
          $$
          \begin{bmatrix} (c^\top x + d) I & Ax + b \\ (Ax + b)^\top & c^\top x + d \end{bmatrix} \succeq 0
          $$
          Since this constraint is linear in the matrix variable (which depends linearly on $x$), an SOCP is a special case of an SDP.
        </div>
      </div>
    </section>

    <!-- Section 6: Problem Reformulation and Equivalence -->
    <section class="card" id="section-6">
      <h2>6. Problem Reformulation and Equivalence</h2>

      <h3>6.1 Equivalent Problems</h3>
      <p>Two optimization problems are <strong>equivalent</strong> if the solution of one can be readily obtained from the solution of the other, and vice versa.</p>

      <h4>Common Equivalence Transformations:</h4>
      <ul>
        <li><strong>Change of variables:</strong> Substitute $x = \phi(y)$ where $\phi$ is one-to-one</li>
        <li><strong>Slack variables:</strong> Introduce $s$ to turn $f(x) \le t$ into $f(x) + s = t$, $s \ge 0$</li>
        <li><strong>Epigraph form:</strong> Minimize $t$ subject to $f(x) \le t$ (instead of minimizing $f(x)$)</li>
        <li><strong>Eliminating equality constraints:</strong> Use $Ax = b$ to eliminate variables</li>
      </ul>

      <h3>6.2 Epigraph Reformulation</h3>
      <p>The problem $\min f_0(x)$ s.t. $x \in \mathcal{C}$ is equivalent to:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & t \\
        \text{subject to} \quad & f_0(x) \le t \\
        & x \in \mathcal{C}
        \end{aligned}
        $
      </p>
      <p>This transformation is crucial for reformulating problems with complex objectives into standard forms.</p>

      <h3>6.3 Example: Minimizing $\ell_\infty$ Norm</h3>
      <p>Minimize $\|Ax - b\|_\infty$ can be reformulated as an LP:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & t \\
        \text{subject to} \quad & -t \mathbf{1} \preceq Ax - b \preceq t \mathbf{1}
        \end{aligned}
        $
      </p>

      <h3>6.4 Eliminating Linear Equality Constraints</h3>
      <p>Given $\min f_0(x)$ s.t. $Ax = b$ and other constraints, if $\text{range}(A) = \mathbb{R}^p$, we can parameterize the solution space as:</p>
      <p style="text-align: center;">
        $
        x = A^\dagger b + (I - A^\dagger A) z
        $
      </p>
      <p>where $A^\dagger$ is the pseudoinverse and $z \in \mathbb{R}^n$ is a new variable. This reduces the problem dimension.</p>

      <!-- Widget 6: Problem Reformulation Tool -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Interactive: Problem Reformulation Tool</h3>
        <p><strong>Purpose:</strong> Learn how to reformulate non-standard problems into LP, QP, SOCP, or SDP forms.</p>
        <ul style="font-size: 14px; line-height: 1.6;">
          <li>Select a non-standard problem (e.g., $\ell_1$, $\ell_\infty$ minimization)</li>
          <li>See reformulation to standard form</li>
          <li>Understand epigraph and slack variable techniques</li>
        </ul>
        <div id="widget-6" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>
    </section>

    <!-- Section 7: Quasiconvex Optimization -->
    <section class="card" id="section-7">
      <h2>7. Quasiconvex Optimization</h2>

      <h3>7.1 Quasiconvex Functions</h3>
      <p>A function $f: \mathbb{R}^n \to \mathbb{R}$ is <strong>quasiconvex</strong> if its domain and all sublevel sets $\{x \mid f(x) \le \alpha\}$ are convex.</p>

      <p><strong>Examples:</strong></p>
      <ul>
        <li>$\log x$ on $\mathbb{R}_{++}$ (quasilinear: both quasiconvex and quasiconcave)</li>
        <li>$\lceil x \rceil$ (ceiling function)</li>
        <li>$x^3$ on $\mathbb{R}$ (not quasiconvex; sublevel sets are not convex)</li>
      </ul>

      <h3>7.2 Quasiconvex Optimization Problems</h3>
      <p>A problem is <strong>quasiconvex</strong> if the objective is quasiconvex and the feasible set is convex:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & f_0(x) \quad \text{(quasiconvex)} \\
        \text{subject to} \quad & f_i(x) \le 0 \quad \text{($f_i$ convex)} \\
        & Ax = b
        \end{aligned}
        $
      </p>

      <h3>7.3 Solution via Bisection (Convex Feasibility)</h3>
      <p>Quasiconvex optimization can be solved by solving a sequence of convex feasibility problems:</p>
      <ol>
        <li>Find interval $[l, u]$ containing $p^*$ (the optimal value)</li>
        <li>At each iteration, test if the sublevel set $\{x \in \mathcal{F} \mid f_0(x) \le t\}$ is nonempty (for $t = (l+u)/2$)</li>
        <li>Update $[l, u]$ based on feasibility</li>
        <li>Repeat until $u - l < \epsilon$</li>
      </ol>
      <p>Each feasibility check is a convex problem, so quasiconvex optimization reduces to convex optimization.</p>
    </section>

    <!-- Section 8: Disciplined Convex Programming (DCP) -->
    <section class="card" id="section-8">
      <h2>8. Disciplined Convex Programming (DCP)</h2>

      <h3>8.1 Motivation</h3>
      <p>While we can recognize many convex problems, it's often unclear whether a complex formulation is convex. <strong>Disciplined Convex Programming</strong> is a system of rules for constructing convex optimization problems from basic atoms.</p>

      <h3>8.2 DCP Rules</h3>
      <ul>
        <li><strong>Atoms:</strong> A set of basic convex/concave/affine functions with known curvature</li>
        <li><strong>Composition rules:</strong>
          <ul>
            <li>A convex function of a convex function is convex</li>
            <li>A convex function of a concave function is NOT necessarily convex</li>
            <li>A concave function of an affine function is concave</li>
          </ul>
        </li>
        <li><strong>Objective:</strong> Minimize a convex function or maximize a concave function</li>
        <li><strong>Constraints:</strong> Convex constraints of the form $f(x) \le 0$ where $f$ is convex</li>
      </ul>

      <h3>8.3 DCP in Practice: CVXPY</h3>
      <p>Tools like CVXPY verify DCP compliance and automatically transform problems to standard solver forms (LP, SOCP, SDP).</p>

      <!-- Widget 8: Solver Selection Guide -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Interactive: Solver Selection Guide</h3>
        <p><strong>Purpose:</strong> Decision tree tool for selecting the most appropriate solver (GLPK, CVXOPT, MOSEK, SeDuMi, etc.) based on problem structure and scale.</p>
        <ul style="font-size: 14px; line-height: 1.6;">
          <li>Input problem characteristics (size, sparsity, cone type)</li>
          <li>Receive solver recommendations with rationale</li>
          <li>Learn performance tradeoffs between solvers</li>
        </ul>
        <div id="widget-8" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>
    </section>

    <!-- SECTION 11: SUMMARY -->
    <section class="card" id="summary">
      <h2>11. Summary</h2>
      <p><strong>Overview:</strong> This lecture covers convex optimization problem classes—LP, QP, SOCP, SDP—and the mathematical foundations for recognizing, classifying, and formulating convex problems. We develop the theoretical underpinnings of standard forms and establish the hierarchy of problem classes that governs modern optimization.</p>
    </section>

    <!-- READINGS -->
    <section class="card" id="readings" style="margin-bottom: 32px;">
      <h2>12. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Boyd & Vandenberghe, Convex Optimization:</strong> Chapter 4 — Convex Optimization Problems</li>
        <li><strong>Nesterov, Introductory Lectures on Convex Optimization:</strong> Chapter 1 — Linear and Conic Programming</li>
        <li><strong>Ben-Tal & Nemirovski, Lectures on Modern Convex Optimization:</strong> Part I — Conic Programming</li>
        <li><strong>CVXPY Documentation:</strong> <a href="https://www.cvxpy.org/" target="_blank">https://www.cvxpy.org/</a> — DCP rules and examples</li>
      </ul>
    </section>

    <!-- SECTION 10: PROBLEM SET -->
    <section class="card" id="section-10">
      <h2>10. Problem Set & Solutions</h2>
      <p>These problems consolidate the lecture material and provide practice in problem classification, formulation, and reformulation.</p>

      <!-- Problem 4.1 -->
      <div class="problem">
        <h3>P4.1 — Classify and Reformulate: Minimizing Maximum Absolute Deviation</h3>
        <p>Consider the problem:</p>
        <p style="text-align: center;">
          $
          \begin{aligned}
          \text{minimize} \quad & \max_{i=1,\dots,m} |a_i^\top x - b_i|
          \end{aligned}
          $
        </p>
        <p>where $a_i \in \mathbb{R}^n$ and $b_i \in \mathbb{R}$.</p>
        <p><strong>(a)</strong> Is this a convex optimization problem? Justify your answer.</p>
        <p><strong>(b)</strong> Reformulate it as a standard LP.</p>

        <div class="solution">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>Part (a): Convexity.</strong> The function $f(x) = \max_{i=1,\dots,m} |a_i^\top x - b_i|$ is the pointwise maximum of functions $|a_i^\top x - b_i|$. Since $|a_i^\top x - b_i|$ is convex (it's the composition of the absolute value function with an affine function), and the pointwise maximum of convex functions is convex, $f(x)$ is convex. Thus, this is a convex optimization problem.
          </div>

          <div class="proof-step">
            <strong>Part (b): LP reformulation.</strong> We reformulate using epigraph form and absolute value splitting:
            $
            \begin{aligned}
            \text{minimize} \quad & t \\
            \text{subject to} \quad & -t \le a_i^\top x - b_i \le t, \quad i = 1, \dots, m
            \end{aligned}
            $
            This is equivalent to:
            $
            \begin{aligned}
            \text{minimize} \quad & t \\
            \text{subject to} \quad & a_i^\top x - b_i \le t, \quad i = 1, \dots, m \\
            & -a_i^\top x + b_i \le t, \quad i = 1, \dots, m
            \end{aligned}
            $
            This is an LP in variables $(x, t) \in \mathbb{R}^{n+1}$.
          </div>
        </div>
      </div>

      <!-- Problem 4.2 -->
      <div class="problem">
        <h3>P4.2 — QP Formulation: Constrained Least Squares</h3>
        <p>Consider the problem of finding $x \in \mathbb{R}^n$ that minimizes $\|Ax - b\|_2^2$ subject to $l \preceq x \preceq u$ (box constraints).</p>
        <p><strong>(a)</strong> Write this problem in standard QP form.</p>
        <p><strong>(b)</strong> Under what conditions on $A$ is the problem strictly convex (thus having a unique solution)?</p>

        <div class="solution">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>Part (a): QP formulation.</strong> Expand the objective:
            $
            \|Ax - b\|_2^2 = (Ax - b)^\top (Ax - b) = x^\top A^\top A x - 2 b^\top A x + b^\top b
            $
            The constant $b^\top b$ doesn't affect the minimizer, so we can write:
            $
            \begin{aligned}
            \text{minimize} \quad & x^\top (A^\top A) x - 2 (A^\top b)^\top x \\
            \text{subject to} \quad & x \succeq l \\
            & -x \succeq -u
            \end{aligned}
            $
            In standard QP form with $P = 2 A^\top A$, $q = -2 A^\top b$, and inequality constraints $x \ge l$, $x \le u$.
          </div>

          <div class="proof-step">
            <strong>Part (b): Strict convexity.</strong> The Hessian is $\nabla^2 f(x) = 2 A^\top A$. The problem is strictly convex if and only if $A^\top A \succ 0$, which occurs when $A$ has full column rank (i.e., $\text{rank}(A) = n$).
          </div>
        </div>
      </div>

      <!-- Problem 4.3 -->
      <div class="problem">
        <h3>P4.3 — SOCP Reformulation: $\ell_2$ Regularized Least Squares</h3>
        <p>Consider the problem:</p>
        <p style="text-align: center;">
          $
          \text{minimize} \quad \|Ax - b\|_2 + \lambda \|x\|_2
          $
        </p>
        <p>where $\lambda > 0$.</p>
        <p><strong>(a)</strong> Show that this is an SOCP by reformulating it in standard SOCP form.</p>
        <p><strong>(b)</strong> Compare this to Tikhonov regularization $\|Ax - b\|_2^2 + \lambda \|x\|_2^2$, which is a QP.</p>

        <div class="solution">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>Part (a): SOCP reformulation.</strong> Introduce auxiliary variables $t_1, t_2$ for the two norms:
            $
            \begin{aligned}
            \text{minimize} \quad & t_1 + \lambda t_2 \\
            \text{subject to} \quad & \|Ax - b\|_2 \le t_1 \\
            & \|x\|_2 \le t_2
            \end{aligned}
            $
            Both constraints are second-order cone constraints, so this is an SOCP in variables $(x, t_1, t_2)$.
          </div>

          <div class="proof-step">
            <strong>Part (b): Comparison to QP.</strong> Tikhonov regularization $\|Ax - b\|_2^2 + \lambda \|x\|_2^2$ is a QP (quadratic objective). The $\ell_2$ regularized version $\|Ax - b\|_2 + \lambda \|x\|_2$ is not a QP but an SOCP. Both are convex; the choice depends on application requirements (e.g., sparsity-promoting properties differ).
          </div>
        </div>
      </div>

      <!-- Problem 4.4 -->
      <div class="problem">
        <h3>P4.4 — SDP Example: Largest Eigenvalue Minimization</h3>
        <p>Consider the problem of minimizing $\lambda_{\max}(A_0 + x_1 A_1 + x_2 A_2)$ where $A_0, A_1, A_2 \in \mathbb{S}^n$ are given symmetric matrices.</p>
        <p><strong>(a)</strong> Formulate this as an SDP.</p>
        <p><strong>(b)</strong> Write the dual problem (you may use the general SDP duality results).</p>

        <div class="solution">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>Part (a): SDP formulation.</strong> We use the characterization $\lambda_{\max}(M) \le t \iff M \preceq tI$:
            $
            \begin{aligned}
            \text{minimize} \quad & t \\
            \text{subject to} \quad & A_0 + x_1 A_1 + x_2 A_2 \preceq t I
            \end{aligned}
            $
            Equivalently:
            $
            \begin{aligned}
            \text{minimize} \quad & t \\
            \text{subject to} \quad & t I - (A_0 + x_1 A_1 + x_2 A_2) \succeq 0
            \end{aligned}
            $
            This is an SDP in variables $(x_1, x_2, t)$.
          </div>

          <div class="proof-step">
            <strong>Part (b): Dual SDP.</strong> Using standard SDP duality (Lecture 05), the dual problem is:
            $
            \begin{aligned}
            \text{maximize} \quad & -\text{tr}(A_0 Z) \\
            \text{subject to} \quad & \text{tr}(A_i Z) = 0, \quad i = 1, 2 \\
            & \text{tr}(Z) = 1 \\
            & Z \succeq 0
            \end{aligned}
            $
            where $Z \in \mathbb{S}^n$ is the dual variable. This is also an SDP.
          </div>
        </div>
      </div>

      <!-- Problem 4.5 -->
      <div class="problem">
        <h3>P4.5 — Problem Classification: Portfolio Optimization with Risk Constraint</h3>
        <p>Consider a portfolio optimization problem with $n$ assets, expected returns $\mu \in \mathbb{R}^n$, and covariance matrix $\Sigma \in \mathbb{S}_{++}^n$:</p>
        <p style="text-align: center;">
          $
          \begin{aligned}
          \text{maximize} \quad & \mu^\top x \\
          \text{subject to} \quad & x^\top \Sigma x \le \sigma_{\max}^2 \\
          & \mathbf{1}^\top x = 1 \\
          & x \ge 0
          \end{aligned}
          $
        </p>
        <p><strong>(a)</strong> Classify this problem (LP, QP, SOCP, or SDP).</p>
        <p><strong>(b)</strong> Reformulate it to fit your classification.</p>

        <div class="solution">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>Part (a): Classification.</strong> The objective is linear (easy). The constraint $x^\top \Sigma x \le \sigma_{\max}^2$ is a quadratic constraint. Since we're maximizing a linear function (equivalent to minimizing $-\mu^\top x$), and we have a quadratic constraint, this is a <strong>Quadratically Constrained Quadratic Program (QCQP)</strong>, which is a subclass of SOCP (since $\Sigma \succ 0$).
          </div>

          <div class="proof-step">
            <strong>Part (b): SOCP reformulation.</strong> Since $\Sigma \succ 0$, we can write $\Sigma = L L^\top$ (Cholesky factorization). Then $x^\top \Sigma x = \|L^\top x\|_2^2$, so the constraint becomes:
            $
            \|L^\top x\|_2 \le \sigma_{\max}
            $
            The problem in standard SOCP form is:
            $
            \begin{aligned}
            \text{minimize} \quad & -\mu^\top x \\
            \text{subject to} \quad & \|L^\top x\|_2 \le \sigma_{\max} \\
            & \mathbf{1}^\top x = 1 \\
            & x \ge 0
            \end{aligned}
            $
          </div>
        </div>
      </div>

      <!-- Problem 4.6 -->
      <div class="problem">
        <h3>P4.6 — Equivalence: $\ell_1$ Minimization and LP</h3>
        <p>Show that the problem $\min \|x\|_1$ subject to $Ax = b$ is equivalent to an LP.</p>

        <div class="solution">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>Reformulation.</strong> Recall $\|x\|_1 = \sum_{i=1}^n |x_i|$. We introduce auxiliary variables $t \in \mathbb{R}^n$ and enforce $|x_i| \le t_i$ for all $i$:
            $
            \begin{aligned}
            \text{minimize} \quad & \mathbf{1}^\top t \\
            \text{subject to} \quad & -t \preceq x \preceq t \\
            & Ax = b
            \end{aligned}
            $
          </div>

          <div class="proof-step">
            <strong>Verification.</strong> The constraints $-t \preceq x \preceq t$ are equivalent to $-t_i \le x_i \le t_i$ for all $i$, which means $|x_i| \le t_i$. At optimality, we have $t_i = |x_i|$ (if $t_i > |x_i|$, we can decrease $t_i$ to reduce the objective). Thus $\mathbf{1}^\top t = \sum_i t_i = \sum_i |x_i| = \|x\|_1$.
          </div>

          <div class="proof-step">
            <strong>Conclusion.</strong> This is an LP in variables $(x, t) \in \mathbb{R}^{2n}$ with linear objective $\mathbf{1}^\top t$ and linear constraints.
          </div>
        </div>
      </div>

      <!-- Problem 4.7 -->
      <div class="problem">
        <h3>P4.7 — Quasiconvex Optimization: Bisection Algorithm</h3>
        <p>Consider minimizing $f(x) = \frac{x_1}{x_2}$ subject to $x_1 + x_2 \le 1$, $x_1, x_2 > 0$.</p>
        <p><strong>(a)</strong> Show that $f$ is quasiconvex (but not convex) on $\mathbb{R}_{++}^2$.</p>
        <p><strong>(b)</strong> Describe how to solve this problem using bisection over $t$ and convex feasibility problems.</p>

        <div class="solution">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>Part (a): Quasiconvexity.</strong> The sublevel set is:
            $
            S_\alpha = \left\{(x_1, x_2) \in \mathbb{R}_{++}^2 \mid \frac{x_1}{x_2} \le \alpha\right\} = \{(x_1, x_2) \mid x_1 \le \alpha x_2, \; x_1, x_2 > 0\}
            $
            This defines a convex cone for each $\alpha$ (it's a halfplane in $\mathbb{R}_{++}^2$). Thus $f$ is quasiconvex. To see it's not convex, note that $f$ is linear along rays from the origin, but its Hessian is not positive semidefinite everywhere (check by computing $\nabla^2 f$).
          </div>

          <div class="proof-step">
            <strong>Part (b): Bisection algorithm.</strong> We solve:
            $
            \min \frac{x_1}{x_2} \quad \text{s.t.} \quad x_1 + x_2 \le 1, \; x_1, x_2 > 0
            $
            using bisection over $t$. At each iteration, check feasibility of:
            $
            \begin{aligned}
            & \frac{x_1}{x_2} \le t \\
            & x_1 + x_2 \le 1 \\
            & x_1, x_2 > 0
            \end{aligned}
            $
            The first constraint is equivalent to $x_1 \le t x_2$ (a linear constraint!). So the feasibility problem is:
            $
            \begin{aligned}
            \text{find} \quad & x_1, x_2 \\
            \text{s.t.} \quad & x_1 - t x_2 \le 0 \\
            & x_1 + x_2 \le 1 \\
            & x_1, x_2 > 0
            \end{aligned}
            $
            This is an LP feasibility problem. We bisect on $t \in [l, u]$ until convergence.
          </div>
        </div>
      </div>

      <!-- Problem 4.8 -->
      <div class="problem">
        <h3>P4.8 — Minimum Enclosing Ellipsoid as SDP</h3>
        <p>Given points $x_1, \dots, x_m \in \mathbb{R}^n$, find the minimum-volume ellipsoid $\mathcal{E} = \{x \mid \|Ax - b\|_2 \le 1\}$ containing all points.</p>
        <p><strong>(a)</strong> Show that the volume of $\mathcal{E}$ is proportional to $\det(A^{-1})$.</p>
        <p><strong>(b)</strong> Formulate the problem as an SDP by using the objective $\log \det(A)$ (which is concave in $A$ for $A \succ 0$).</p>

        <div class="solution">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>Part (a): Volume formula.</strong> An ellipsoid $\mathcal{E} = \{Bz + c \mid \|z\|_2 \le 1\}$ has volume $V = \alpha_n \det(B)$, where $\alpha_n$ is the volume of the unit ball in $\mathbb{R}^n$. For $\mathcal{E} = \{x \mid \|Ax - b\|_2 \le 1\}$, we have $x = A^{-1} z + A^{-1} b$ where $\|z\|_2 \le 1$. Thus $B = A^{-1}$ and $V \propto \det(A^{-1})$.
          </div>

          <div class="proof-step">
            <strong>Part (b): SDP formulation.</strong> Minimizing $\det(A^{-1})$ is equivalent to maximizing $\det(A)$, or minimizing $-\log \det(A)$. The constraints are $\|Ax_i - b\|_2 \le 1$ for all $i$, which are second-order cone constraints. The problem is:
            $
            \begin{aligned}
            \text{minimize} \quad & -\log \det(A) \\
            \text{subject to} \quad & \|Ax_i - b\|_2 \le 1, \quad i = 1, \dots, m \\
            & A \succ 0
            \end{aligned}
            $
            The objective $-\log \det(A)$ is convex (since $\log \det$ is concave). Using the fact that $-\log \det(A) = \text{tr}(\log(A^{-1}))$ and epigraph formulations, this can be written as an SDP.
          </div>
        </div>
      </div>

      <!-- Problem 4.9 -->
      <div class="problem">
        <h3>P4.9 — Resource Allocation Formulation</h3>
        <p>Consider a resource allocation problem where we want to distribute a total budget $B$ among $n$ projects to maximize total utility. The utility of project $i$ is given by $U_i(x_i) = \alpha_i \log(1 + x_i)$, where $x_i \ge 0$ is the investment in project $i$ and $\alpha_i > 0$. There are also linear constraints $Ax \le b$ representing resource limits.</p>
        <p><strong>(a)</strong> Formulate this as a convex optimization problem.</p>
        <p><strong>(b)</strong> Verify that the objective is concave (so minimizing negative utility is convex).</p>

        <div class="solution">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>Part (a): Formulation.</strong>
            We want to maximize $\sum_{i=1}^n \alpha_i \log(1 + x_i)$ subject to $\sum x_i \le B$, $Ax \le b$, and $x \ge 0$.
            Standard convex form minimizes a convex function. Thus:
            $
            \begin{aligned}
            \text{minimize} \quad & -\sum_{i=1}^n \alpha_i \log(1 + x_i) \\
            \text{subject to} \quad & \mathbf{1}^\top x \le B \\
            & Ax \le b \\
            & -x \le 0
            \end{aligned}
            $
          </div>

          <div class="proof-step">
            <strong>Part (b): Convexity Verification.</strong>
            Consider $f_i(x_i) = -\alpha_i \log(1 + x_i)$.
            First derivative: $f_i'(x_i) = -\frac{\alpha_i}{1 + x_i}$.
            Second derivative: $f_i''(x_i) = \frac{\alpha_i}{(1 + x_i)^2}$.
            Since $\alpha_i > 0$ and $(1+x_i)^2 > 0$ for $x_i \ge 0$, we have $f_i''(x_i) > 0$.
            Thus, each term is convex. The sum of convex functions is convex.
            The constraints are all linear inequalities, which define a convex set.
            Therefore, this is a convex optimization problem.
          </div>
        </div>
      </div>

      <!-- Problem 4.10 -->
      <div class="problem">
        <h3>P4.10 — Standard Form Transformation: Linear-Fractional Programming</h3>
        <p>Consider the linear-fractional optimization problem:</p>
        $$
        \begin{aligned}
        \text{minimize} \quad & \frac{c^\top x + d}{e^\top x + f} \\
        \text{subject to} \quad & Ax \le b \\
        & e^\top x + f > 0
        \end{aligned}
        $$
        <p>This problem is quasiconvex but not convex. However, it can be transformed into a standard LP.</p>
        <p><strong>(a)</strong> Perform the Charnes-Cooper transformation by introducing variables $y = \frac{x}{e^\top x + f}$ and $z = \frac{1}{e^\top x + f}$.</p>
        <p><strong>(b)</strong> Write the resulting equivalent problem and verify it is an LP.</p>

        <div class="solution">
          <h4>Solution</h4>

          <div class="proof-step">
            <strong>Part (a): Variable Transformation.</strong>
            Let $z = \frac{1}{e^\top x + f}$. Since $e^\top x + f > 0$, we have $z > 0$.
            Let $y = zx$. Then $x = y/z$.
            Substitute into the objective:
            $$ \frac{c^\top x + d}{e^\top x + f} = \frac{c^\top (y/z) + d}{1/z} = c^\top y + d z $$
            Substitute into the definition of $z$:
            $$ e^\top x + f = \frac{1}{z} \implies e^\top (y/z) + f = \frac{1}{z} \implies e^\top y + f z = 1 $$
            Substitute into the constraints $Ax \le b$:
            $$ A(y/z) \le b \implies Ay \le bz \implies Ay - bz \le 0 $$
          </div>

          <div class="proof-step">
            <strong>Part (b): Equivalent LP.</strong>
            The transformed problem is:
            $$
            \begin{aligned}
            \text{minimize} \quad & c^\top y + d z \\
            \text{subject to} \quad & Ay - bz \le 0 \\
            & e^\top y + f z = 1 \\
            & z \ge 0
            \end{aligned}
            $$
            The constraint $z > 0$ is relaxed to $z \ge 0$ for standard LP form (usually handled by the fact that $z=0$ would imply unboundedness or infeasibility in the original problem context, or by $e^\top y = 1$ if $z=0$ which corresponds to "points at infinity").
            This is a Linear Program in variables $(y, z) \in \mathbb{R}^{n+1}$.
          </div>
        </div>
      </div>

      <!-- Problem 4.11 -->
      <div class="problem">
        <h3>P4.11 — Vector Optimization (Pareto Optimality)</h3>
        <p>A point $x^\star$ is <b>Pareto optimal</b> for the vector objective $f(x) = (f_1(x), \dots, f_k(x))$ if there is no feasible $y$ such that $f_i(y) \le f_i(x^\star)$ for all $i$ and $f_j(y) < f_j(x^\star)$ for at least one $j$.
        Prove that minimizing the weighted sum $\sum w_i f_i(x)$ with weights $w > 0$ yields a Pareto optimal point.</p>

        <div class="solution">
          <h4>Solution</h4>
          <div class="proof-step">
            <strong>Setup:</strong> Let $x^\star$ minimize $S(x) = \sum_{i=1}^k w_i f_i(x)$ over the feasible set $\mathcal{F}$. Assume for contradiction that $x^\star$ is not Pareto optimal.
          </div>
          <div class="proof-step">
            <strong>Contradiction:</strong> If $x^\star$ is not Pareto optimal, there exists a $y \in \mathcal{F}$ such that $f_i(y) \le f_i(x^\star)$ for all $i$ and $f_j(y) < f_j(x^\star)$ for some $j$.
          </div>
          <div class="proof-step">
            <strong>Weighted Sum:</strong> Since $w_i > 0$:
            $$ \sum_{i=1}^k w_i f_i(y) < \sum_{i=1}^k w_i f_i(x^\star) $$
            (Strict inequality holds because at least one term is strictly smaller and none are larger).
          </div>
          <div class="proof-step">
            <strong>Conclusion:</strong> This implies $S(y) < S(x^\star)$, contradicting the fact that $x^\star$ minimizes $S(x)$. Therefore, $x^\star$ must be Pareto optimal.
          </div>
        </div>
      </div>
    </section>
  </main>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">
        © <span id="year"></a> Convex Optimization Course ·
        <a href="../../README.md" style="color: var(--brand);">About</a>
      </p>
    </div>
  </footer>

  <!-- Load Pyodide for Python widgets (optional) -->
  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>

  <!-- Widget loaders -->
  <script type="module">
    import { initProblemRecognizer } from './widgets/js/problem-form-recognizer.js';
    initProblemRecognizer('widget-1');
  </script>
  <script type="module">
    import { initLPVisualizer } from './widgets/js/lp-visualizer.js';
    initLPVisualizer('widget-2');
  </script>
  <script type="module">
    import { initQPSandbox } from './widgets/js/qp-sandbox.js';
    initQPSandbox('widget-4');
  </script>
  <script type="module">
    import { initSDPVisualizer } from './widgets/js/sdp-visualizer.js';
    initSDPVisualizer('widget-5');
  </script>
  <script type="module">
    import { initProblemReformulationTool } from './widgets/js/reformulation-tool.js';
    initProblemReformulationTool('widget-6');
  </script>
  <script type="module">
    import { initSOCPExplorer } from './widgets/js/socp-explorer.js';
    initSOCPExplorer('widget-7');
  </script>
  <script type="module">
    import { initSolverGuide } from './widgets/js/solver-guide.js';
    initSolverGuide('widget-8');
  </script>

  <!-- Global utilities -->
  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/theme-switcher.js"></script>
<script src="../../static/js/toc.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
