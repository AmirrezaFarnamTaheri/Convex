<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>03. Convex Functions: Characterization & Operations â€” Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../02-convex-sets/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../04-convex-opt-problems/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <!-- Main content -->
  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>03. Convex Functions: Definitions, Characterizations, and Operations</h1>
      <div class="lecture-meta">
        <span>Date: 2025-11-04</a>
        <span>Duration: 90 min</a>
        <span>Tags: functions, theory, optimization, characterization</a>
      </div>
      <div class="lecture-summary">
        <p><strong>Overview:</strong> This lecture develops the theory of convex functions, which form the foundation for convex optimization. We present multiple equivalent characterizations (Jensen's inequality, epigraph, first-order conditions, second-order conditions), prove key properties, and establish operations that preserve convexity. The lecture culminates with a toolkit for recognizing and constructing convex functions in practice.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-primer/index.html">Lecture 00: Linear Algebra</a> (gradients, Hessians, eigenvalues) and <a href="../02-convex-sets/index.html">Lecture 02: Convex Sets</a> (convex sets, epigraphs, operations).</p>
        <p><strong>Forward Connections:</strong> Convex functions are the building blocks of convex optimization problems. First-order and second-order conditions lead directly to optimality conditions (Lecture 04-05). Operations preserving convexity enable sophisticated modeling (Lecture 06-07).</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <ul>
        <li><b>Understand Multiple Characterizations:</b> State and prove equivalences between Jensen's inequality, epigraph convexity, first-order conditions, and second-order conditions.</li>
        <li><b>Apply First-Order Conditions:</b> Use the tangent line property to verify convexity and derive inequalities.</li>
        <li><b>Use Second-Order Conditions:</b> Verify convexity via Hessian positive semidefiniteness.</li>
        <li><b>Understand Strong Convexity:</b> Define strong convexity and explain its importance for convergence rates and uniqueness.</li>
        <li><b>Recognize Convexity-Preserving Operations:</b> Apply nonnegative combinations, compositions, pointwise maxima, and perspective operations to construct complex convex functions.</li>
        <li><b>Understand the Conjugate Function:</b> Define the convex conjugate (Fenchel conjugate) and understand its role in duality theory.</li>
        <li><b>Apply Quasi-convexity:</b> Distinguish convex from quasi-convex functions and understand when quasi-convexity suffices for optimization.</li>
      </ul>
    </section>

    <article>
      <section class="section-card" id="section-1">
        <h2>1. Definition and Basic Properties</h2>

        <h3>1.1 Convex Functions: The Core Definition</h3>
        <p>A function $f: \mathbb{R}^n \to \mathbb{R}$ is <a href="#" class="definition-link" data-term="convex function">convex</a> if its domain $\mathrm{dom}\, f$ is a convex set (see <a href="../02-convex-sets/index.html">Lecture 02</a>), and for all $x, y \in \mathrm{dom}\, f$ and all $\theta \in [0, 1]$:</p>
        $$
        \boxed{ f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) }
        \tag{1}
        $$
        <p>This basic inequality states that the function value at a convex combination of points is at most the corresponding convex combination of function values. Geometrically, the <b>chord</b> connecting any two points $(x, f(x))$ and $(y, f(y))$ on the graph lies <b>above</b> the graph.</p>

        <div class="proof-box">
          <h4>Proof: Convexity of Norms</h4>
          <p>Any norm $\|\cdot\|$ is a convex function.</p>
          <div class="proof-step">
            <strong>Step 1: Triangle Inequality.</strong>
            Let $f(x) = \|x\|$. Take $x, y \in \mathbb{R}^n$ and $\theta \in [0, 1]$.
            We compute $f(\theta x + (1-\theta)y) = \|\theta x + (1-\theta)y\|$.
          </div>
          <div class="proof-step">
            <strong>Step 2: Homogeneity.</strong>
            By the triangle inequality:
            $$ \|\theta x + (1-\theta)y\| \le \|\theta x\| + \|(1-\theta)y\| $$
            By absolute homogeneity ($|\alpha| = \alpha$ for $\alpha \ge 0$):
            $$ = \theta \|x\| + (1-\theta)\|y\| = \theta f(x) + (1-\theta)f(y) $$
          </div>
          <div class="proof-step">
            <strong>Conclusion:</strong>
            $f$ satisfies the convexity definition. Geometrically, the "bowl" of a norm never dips downward; it is always "V"-shaped or "U"-shaped.
          </div>
        </div>

        <div class="insight">
          <h4>ðŸ’¡ Geometric Intuition</h4>
          <p>The chord connecting any two points on the graph of $f$ lies above (or on) the graph. Equivalently, $f$ "curves upward"â€”there are no "dents" or concave regions.</p>
        </div>

        <h3>1.2 Jensen's Inequality: The General Form</h3>
        <p>The basic definition involving two points generalizes to any number of points and even to integrals (expectations).</p>

        <h4>Finite Form</h4>
        <p>If $f$ is convex, $x_1, \dots, x_k \in \mathrm{dom}\, f$, and $\theta_1, \dots, \theta_k \ge 0$ with $\sum \theta_i = 1$, then:</p>
        $$
        f\left(\sum_{i=1}^k \theta_i x_i\right) \le \sum_{i=1}^k \theta_i f(x_i)
        $$
        <p><b>Proof Sketch (Induction):</b>
        <br>For $k=2$, this is the definition. Assume it holds for $k-1$.
        <br>Write $\sum_{i=1}^k \theta_i x_i = \theta_k x_k + (1-\theta_k) \sum_{i=1}^{k-1} \frac{\theta_i}{1-\theta_k} x_i$.
        <br>Apply the 2-point definition to combine $x_k$ and the aggregate term, then use the inductive hypothesis on the sum of $k-1$ terms.</p>

        <h4>Expectation Form</h4>
        <p>This generalizes to random variables. If $Z$ is a random variable taking values in $\mathrm{dom}\, f$, and $f$ is convex, then:</p>
        $$
        \boxed{ f(\mathbb{E}[Z]) \le \mathbb{E}[f(Z)] }
        $$
        <p>provided the expectations exist.
        <br><i>Interpretation:</i> For a convex cost function, the cost of the average scenario is less than or equal to the average cost of all scenarios. Uncertainty typically increases expected cost for convex functions.</p>

        <h3>1.3 Applications of Jensen's Inequality</h3>
        <p>Jensen's inequality is a power tool for deriving fundamental inequalities in analysis.</p>

        <div class="example">
          <h4>1. Arithmetic-Geometric Mean (AM-GM)</h4>
          <p>We derive the weighted AM-GM inequality directly from Jensen's inequality.</p>
          <div class="proof-step">
            <strong>Step 1: Convexity of $-\ln x$.</strong>
            Let $f(x) = -\ln x$ for $x > 0$. Then $f''(x) = 1/x^2 > 0$, so $f$ is convex.
          </div>
          <div class="proof-step">
            <strong>Step 2: Apply Jensen.</strong>
            For any $a, b > 0$ and $\theta \in [0, 1]$:
            $$ -\ln(\theta a + (1-\theta)b) \le -\theta \ln a - (1-\theta)\ln b $$
          </div>
          <div class="proof-step">
            <strong>Step 3: Rearrange.</strong>
            Multiply by $-1$ and exponentiate:
            $$ \ln(\theta a + (1-\theta)b) \ge \ln(a^\theta b^{1-\theta}) \implies \theta a + (1-\theta)b \ge a^\theta b^{1-\theta} $$
          </div>
        </div>

        <div class="example">
          <h4>2. From AM-GM to HÃ¶lder's Inequality</h4>
          <p>We build up to HÃ¶lder's inequality in two steps.</p>

          <p><b>Step A: Young's Inequality.</b> Let $p, q > 1$ with $1/p + 1/q = 1$. For $a, b \ge 0$:
          $$ ab \le \frac{a^p}{p} + \frac{b^q}{q} $$
          <i>Proof:</i> Apply weighted AM-GM with $\theta=1/p$, $u=a^p$, $v=b^q$:
          $$ \frac{1}{p} u + \frac{1}{q} v \ge u^{1/p} v^{1/q} = a b $$
          </p>

          <p><b>Step B: HÃ¶lder's Inequality.</b> For vectors $x, y$:
          $$ \sum |x_i y_i| \le \|x\|_p \|y\|_q $$
          <i>Proof:</i>
          <ol>
            <li>Normalize: $\hat{x} = x/\|x\|_p, \hat{y} = y/\|y\|_q$. Note $\sum |\hat{x}_i|^p = 1$.</li>
            <li>Apply Young's term-wise: $|\hat{x}_i \hat{y}_i| \le \frac{|\hat{x}_i|^p}{p} + \frac{|\hat{y}_i|^q}{q}$.</li>
            <li>Sum: $\sum |\hat{x}_i \hat{y}_i| \le \frac{1}{p}(1) + \frac{1}{q}(1) = 1$.</li>
            <li>Scale back: $\frac{1}{\|x\|_p \|y\|_q} \sum |x_i y_i| \le 1$.</li>
          </ol>
          </p>
        </div>

        <h3>1.4 Integral Characterization of Convexity</h3>
        <p>A powerful alternative definition of convexity involves integrals. This mirrors the "average value" property of harmonic functions but with an inequality.</p>

        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>A continuous function $f: \mathbb{R} \to \mathbb{R}$ is convex <b>if and only if</b> for all $x, y \in \mathbb{R}$:</p>
          $$ \int_0^1 f(x + t(y-x)) dt \le \frac{f(x) + f(y)}{2} $$
          <p>This states that the <b>average value</b> of the function along a segment is less than or equal to the <b>average of the values</b> at the endpoints.</p>
        </div>

        <div class="proof-box">
          <h4>Proof</h4>
          <div class="proof-step">
            <strong>($\Rightarrow$) Convex implies Inequality:</strong>
            <p>Let $g(t) = f(x + t(y-x))$ for $t \in [0, 1]$. Since $f$ is convex, $g$ is convex.
            By the definition of convexity, the graph of $g$ lies below the chord connecting $(0, g(0))$ and $(1, g(1))$:
            $$ g(t) \le (1-t)g(0) + t g(1) $$
            Integrating both sides from 0 to 1:
            $$ \int_0^1 g(t) dt \le g(0)\int_0^1 (1-t)dt + g(1)\int_0^1 t dt = \frac{1}{2}g(0) + \frac{1}{2}g(1) $$
            This proves the inequality.</p>
          </div>
          <div class="proof-step">
            <strong>($\Leftarrow$) Inequality implies Convex (Contrapositive):</strong>
            <p>Suppose $f$ is <b>not</b> convex. Then there exists a chord that lies <b>below</b> the graph at some point.
            <br>Specifically, there exist $x, y$ and the affine function $\ell(z)$ passing through $(x, f(x))$ and $(y, f(y))$ such that $f(z) > \ell(z)$ for some $z \in (x, y)$.
            <br>Let $h(z) = f(z) - \ell(z)$. Then $h(x)=h(y)=0$ but $h(z) > 0$ somewhere.
            <br>If $f$ is "strictly bumpy", the integral of the difference will be positive:
            $$ \int_0^1 f(x+t(y-x)) dt > \int_0^1 \ell(x+t(y-x)) dt $$
            The average value of the line $\ell$ is exactly the midpoint $\frac{\ell(x)+\ell(y)}{2} = \frac{f(x)+f(y)}{2}$.
            <br>Thus $\int_0^1 f(\cdot) > \frac{f(x)+f(y)}{2}$, violating the condition.</p>
          </div>
        </div>

        <h3>1.5 Strict and Strong Convexity</h3>

        <div class="subsection">
          <h4>Strictly Convex</h4>
          <p>A function $f$ is <a href="#" class="definition-link">strictly convex</a> if the inequality in Jensen's inequality is strict whenever $x \neq y$ and $\theta \in (0, 1)$:</p>
          $$
          f(\theta x + (1-\theta)y) < \theta f(x) + (1-\theta)f(y)
          $$
          <p><b>Implication:</b> Strictly convex functions have at most one minimizer (uniqueness of optimal solution).</p>
        </div>

        <div class="subsection">
          <h4>Strongly Convex</h4>
          <p>A function $f$ is <a href="#" class="definition-link">strongly convex</a> (with parameter $m > 0$) if for all $x, y \in \mathrm{dom}\, f$ and $\theta \in [0, 1]$:</p>
          $$
          f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) - \frac{m}{2} \theta(1-\theta) \|x - y\|_2^2
          $$
          <p>Equivalently, $f(x) - \frac{m}{2}\|x\|_2^2$ is convex.</p>
          <p><b>Implication:</b> Strong convexity provides a quadratic lower bound on the growth of the function, leading to fast convergence rates in optimization algorithms.</p>
        </div>

        <h3>1.5 Concave Functions</h3>
        <p>A function $f$ is <a href="#" class="definition-link">concave</a> if $-f$ is convex. Equivalently, the chord lies below the graph. All results for convex functions can be "flipped" for concave functions by negating.</p>

        <h3>1.6 Extended-Value Extension</h3>
        <p>We often extend $f$ to all of $\mathbb{R}^n$ by setting $f(x) = +\infty$ for $x \notin \mathrm{dom}\, f$. This allows us to write constraints implicitly: minimizing $f$ over $\mathbb{R}^n$ is equivalent to minimizing over $\mathrm{dom}\, f$.</p>

        <h3>1.7 Restriction to a Line</h3>
        <p>A function $f: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$ is convex if and only if its restriction to any line is convex. This is a powerful tool for proving convexity.</p>
        <div class="theorem-box">
          <h4>Theorem (Restriction to a Line)</h4>
          <p>$f$ is convex if and only if for all $x \in \mathrm{dom}\, f$ and $v \in \mathbb{R}^n$, the function $g(t) = f(x + tv)$ is convex (on its domain $\{t \mid x+tv \in \mathrm{dom}\, f\}$).</p>
        </div>
        <div class="proof-box">
          <h4>Proof</h4>
          <p><b>($\Rightarrow$)</b> If $f$ is convex, then for any $t_1, t_2$ and $\theta \in [0,1]$:
          $$ g(\theta t_1 + (1-\theta)t_2) = f(x + (\theta t_1 + (1-\theta)t_2)v) = f(\theta(x+t_1 v) + (1-\theta)(x+t_2 v)) $$
          $$ \le \theta f(x+t_1 v) + (1-\theta)f(x+t_2 v) = \theta g(t_1) + (1-\theta)g(t_2) $$
          So $g$ is convex.</p>
          <p><b>($\Leftarrow$)</b> If every restriction is convex, take any $x, y \in \mathrm{dom}\, f$ and $\theta \in [0,1]$. Define the line through $x$ and $y$ by $h(t) = f(x + t(y-x))$.
          Then $h(0) = f(x)$ and $h(1) = f(y)$. By convexity of $h$:
          $$ f(\theta x + (1-\theta)y) = f(x + (1-\theta)(y-x)) = h(1-\theta) $$
          $$ \le (1-(1-\theta))h(0) + (1-\theta)h(1) = \theta f(x) + (1-\theta)f(y) $$
          Thus $f$ is convex.</p>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Inspector: Understanding Convexity</h3>
          <p><b>Explore the Equivalences:</b> Convexity can be defined in multiple ways. This unified tool lets you toggle between different perspectives to see how they relate:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Jensen (Definition):</b> The chord between any two points lies above the graph.</li>
            <li><b>Epigraph (Set Theory):</b> The region above the graph is a convex set.</li>
            <li><b>Tangent (First-Order):</b> The tangent line is always a global underestimator (for differentiable functions).</li>
            <li><b>Quadratic Bound (Strong Convexity):</b> A quadratic bowl sits below the function, pushing it up.</li>
          </ul>
          <p><i>Note:</i> Select different functions to see how non-convex functions violate these conditions!</p>
          <div id="widget-convex-function-inspector" style="width: 100%; position: relative;"></div>
        </div>
      </section>

      <section class="section-card" id="section-2">
        <h2>2. Epigraph Characterization</h2>

        <h3>2.1 The Epigraph</h3>
        <p>The <a href="#" class="definition-link">epigraph</a> of a function $f: \mathbb{R}^n \to \mathbb{R}$ is the set of points lying on or above the graph:</p>
        $$
        \mathrm{epi}\, f = \{(x, t) \in \mathbb{R}^{n+1} \mid x \in \mathrm{dom}\, f, \ f(x) \le t\}
        $$
        <p>The epigraph "fills in" everything above the function's graph.</p>

        <h3>2.2 Convexity via Epigraph</h3>
        <div class="theorem-box">
          <h4>Theorem (Epigraph Characterization)</h4>
          <p>A function $f$ is convex <b>if and only if</b> its epigraph $\mathrm{epi}\, f$ is a convex set.</p>
        </div>

        <div class="proof-box">
          <h4>Proof</h4>

          <div class="proof-step">
            <strong>($\Rightarrow$) Convex function implies convex epigraph:</strong> Suppose $f$ is convex. Take any $(x, t), (y, s) \in \mathrm{epi}\, f$ and $\theta \in [0, 1]$. We must show $(\theta x + (1-\theta)y, \theta t + (1-\theta)s) \in \mathrm{epi}\, f$.
          </div>

          <div class="proof-step">
            <strong>Step 1:</strong> Since $(x, t) \in \mathrm{epi}\, f$ and $(y, s) \in \mathrm{epi}\, f$, we have $f(x) \le t$ and $f(y) \le s$.
          </div>

          <div class="proof-step">
            <strong>Step 2:</strong> By convexity of $f$:
            $$
            f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) \le \theta t + (1-\theta)s
            $$
          </div>

          <div class="proof-step">
            <strong>Step 3:</strong> Therefore, $(\theta x + (1-\theta)y, \theta t + (1-\theta)s) \in \mathrm{epi}\, f$, proving $\mathrm{epi}\, f$ is convex.
          </div>

          <div class="proof-step">
            <strong>($\Leftarrow$) Convex epigraph implies convex function:</strong> Suppose $\mathrm{epi}\, f$ is convex. Take any $x, y \in \mathrm{dom}\, f$ and $\theta \in [0, 1]$.
          </div>

          <div class="proof-step">
            <strong>Step 1:</strong> The points $(x, f(x))$ and $(y, f(y))$ lie in $\mathrm{epi}\, f$ (on the boundary, in fact).
          </div>

          <div class="proof-step">
            <strong>Step 2:</strong> By convexity of $\mathrm{epi}\, f$, the point $(\theta x + (1-\theta)y, \theta f(x) + (1-\theta)f(y))$ also lies in $\mathrm{epi}\, f$.
          </div>

          <div class="proof-step">
            <strong>Step 3:</strong> By definition of epigraph, this means:
            $$
            f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)
            $$
            proving $f$ is convex.
          </div>
        </div>

        <div class="insight">
          <h4>âš¡ Why This Matters</h4>
          <p>The epigraph characterization lets us translate between function convexity and set convexity. Many operations on functions (e.g., infimal convolution) can be understood as operations on their epigraphs (e.g., Minkowski sum).</p>
        </div>

        <h3>2.3 Convex Hull of a Function</h3>
        <p>Just as we can define the convex hull of a set, we can define the convex hull (or convex envelope) of a function $f: \mathbb{R}^n \to \mathbb{R}$ via its epigraph.</p>
        $$ g(x) = \inf \{ t \mid (x, t) \in \text{conv epi } f \} $$
        <p><b>Geometric Intuition:</b> Imagine "shrink-wrapping" the graph of $f$ from below. The function $g$ is the <b>greatest convex underestimator</b> of $f$. This concept is crucial in relaxation techniques for non-convex optimization (e.g., replacing a non-convex constraint with its convex hull).</p>

      </section>

      <section class="section-card" id="section-3">
        <h2>3. First-Order Conditions (Tangent Line Property)</h2>

        <h3>3.1 The First-Order Characterization</h3>
        <div class="theorem-box">
          <h4>Theorem (First-Order Condition for Convexity)</h4>
          <p>Suppose $f: \mathbb{R}^n \to \mathbb{R}$ is differentiable. Then $f$ is convex <b>if and only if</b> $\mathrm{dom}\, f$ is convex and for all $x, y \in \mathrm{dom}\, f$:</p>
          $$
          f(y) \ge f(x) + \nabla f(x)^\top (y - x)
          $$
          <p>This states that the <b>tangent line (or tangent hyperplane) at any point is a global underestimator</b> of the function.</p>
        </div>

        <div class="proof-box">
          <h4>Proof: Convexity $\iff$ First-Order Condition</h4>

          <div class="proof-step">
            <strong>Part 1: Convex $\Rightarrow$ Inequality</strong>
            <p>We use the idea "convex along every line".</p>
            <ul>
              <li><b>Step 1: Restrict to a line.</b> Fix two points $x, y \in \mathrm{dom}\, f$. Define direction $v = y-x$ and the 1-D function:
                $$ g(t) = f(x + tv), \quad t \in [0, 1] $$
              </li>
              <li><b>Step 2: Use 1-D convexity.</b> Since $f$ is convex, $g(t)$ is a convex function of a single variable. For a differentiable convex function $g: \mathbb{R} \to \mathbb{R}$, the graph lies above the tangent line:
                $$ g(b) \ge g(a) + g'(a)(b-a) $$
              </li>
              <li><b>Step 3: Apply at endpoints.</b> Set $a=0$ and $b=1$.
                $$ g(1) \ge g(0) + g'(0)(1-0) $$
              </li>
              <li><b>Step 4: Chain Rule.</b>
                <ul>
                  <li>$g(1) = f(x+v) = f(y)$.</li>
                  <li>$g(0) = f(x)$.</li>
                  <li>$g'(t) = \nabla f(x+tv)^\top v$, so $g'(0) = \nabla f(x)^\top (y-x)$.</li>
                </ul>
              </li>
              <li><b>Conclusion:</b> Substituting these back yields the inequality:
                $$ f(y) \ge f(x) + \nabla f(x)^\top (y-x) $$
              </li>
            </ul>
          </div>

          <div class="proof-step">
            <strong>Part 2: Inequality $\Rightarrow$ Convex</strong>
            <p>Assume the inequality holds for all $x, y$. Let $\theta \in [0, 1]$ and define $z = \theta x + (1-\theta)y$.</p>
            <ul>
              <li><b>Step 1: Apply at base point $z$.</b> Apply the inequality twice, once targeting $x$ and once targeting $y$:
                $$ f(x) \ge f(z) + \nabla f(z)^\top (x-z) \quad (1) $$
                $$ f(y) \ge f(z) + \nabla f(z)^\top (y-z) \quad (2) $$
              </li>
              <li><b>Step 2: Convex Combination.</b> Multiply (1) by $\theta$ and (2) by $(1-\theta)$ and sum them:
                $$
                \theta f(x) + (1-\theta)f(y) \ge \theta f(z) + (1-\theta)f(z) + \nabla f(z)^\top [\theta(x-z) + (1-\theta)(y-z)]
                $$
              </li>
              <li><b>Step 3: Vanishing Gradient Term.</b> Analyze the vector term inside the brackets:
                $$
                \theta(x-z) + (1-\theta)(y-z) = \theta x + (1-\theta)y - (\theta + 1-\theta)z = z - z = 0
                $$
                Thus, the gradient term disappears.
              </li>
              <li><b>Conclusion:</b> The inequality simplifies to:
                $$ \theta f(x) + (1-\theta)f(y) \ge f(z) = f(\theta x + (1-\theta)y) $$
                This is exactly the definition of convexity.
              </li>
            </ul>
          </div>
        </div>

        <div class="insight">
          <h4>ðŸ”‘ Key Takeaway</h4>
          <p>For differentiable functions, convexity is equivalent to: <b>the first-order Taylor approximation always underestimates the function</b>. This is an extremely useful characterization for analysis and algorithm design.</p>
        </div>

        <h3>3.2 Consequences and Applications</h3>

        <div class="example">
          <h4>Application 1: Proving a Function is Convex</h4>
          <p>Show that $f(x) = e^x$ is convex on $\mathbb{R}$.</p>
          <p><b>Solution:</b> We verify the first-order condition. For any $x, y \in \mathbb{R}$:</p>
          $$
          f(y) = e^y, \quad f(x) + f'(x)(y - x) = e^x + e^x(y - x) = e^x(1 + y - x)
          $$
          <p>We need to show $e^y \ge e^x(1 + y - x)$ for all $x, y$. Dividing by $e^x > 0$ and letting $t = y - x$:</p>
          $$
          e^t \ge 1 + t
          $$
          <p>This is a well-known inequality (follows from the Taylor series of $e^t$ with all positive terms). Therefore, $f(x) = e^x$ is convex.</p>
        </div>

        <div class="example">
          <h4>Application 2: Optimality Condition</h4>
          <p>If $f$ is convex and differentiable, then $x^*$ minimizes $f$ over a convex set $C$ if and only if:</p>
          $$
          \nabla f(x^*)^\top (y - x^*) \ge 0 \quad \forall y \in C
          $$
          <p>This is the <b>first-order optimality condition</b>. For unconstrained problems ($C = \mathbb{R}^n$), this reduces to $\nabla f(x^*) = 0$ (the gradient vanishes). This variational inequality is the basis for the <b>stationarity condition</b> in KKT theory (<a href="../05-duality/index.html">Lecture 05</a>).</p>
        </div>

      </section>

      <section class="section-card" id="section-4">
        <h2>4. Second-Order Conditions (Hessian Test)</h2>

        <h3>4.1 The Second-Order Characterization</h3>
        <div class="theorem-box">
          <h4>Theorem (Second-Order Condition for Convexity)</h4>
          <p>Suppose $f: \mathbb{R}^n \to \mathbb{R}$ is twice differentiable. Then $f$ is convex <b>if and only if</b> $\mathrm{dom}\, f$ is convex and for all $x \in \mathrm{dom}\, f$:</p>
          $$
          \nabla^2 f(x) \succeq 0
          $$
          <p>That is, the <b>Hessian matrix is positive semidefinite</b> at every point.</p>
        </div>

        <div class="proof-box">
          <h4>Proof: Convexity $\iff$ PSD Hessian</h4>

          <div class="proof-step">
            <strong>Part 1: Convex $\Rightarrow$ PSD</strong>
            <p>Fix any point $x \in \mathrm{dom}\, f$ and an arbitrary direction $v \in \mathbb{R}^n$. Define the restriction $g(t) = f(x+tv)$.</p>
            <ul>
              <li><b>Step 1: Use convexity of restriction.</b> Since $f$ is convex, $g(t)$ is a convex function of one variable. Thus, its second derivative must be non-negative everywhere: $g''(t) \ge 0$.</li>
              <li><b>Step 2: Directional Second Derivative.</b> Compute $g''(0)$ using the chain rule:
                $$ g'(t) = \nabla f(x+tv)^\top v $$
                $$ g''(t) = v^\top \nabla^2 f(x+tv) v $$
              </li>
              <li><b>Conclusion:</b> At $t=0$, we have $g''(0) = v^\top \nabla^2 f(x) v \ge 0$. Since this holds for <i>any</i> direction $v$, the Hessian $\nabla^2 f(x)$ is positive semidefinite by definition.</li>
            </ul>
          </div>

          <div class="proof-step">
            <strong>Part 2: PSD $\Rightarrow$ Convex</strong>
            <p>Assume $\nabla^2 f(x) \succeq 0$ for all $x$. Fix any two points $x, y$ and let $v = y-x$. Consider $g(t) = f(x+tv)$ on the interval $[0, 1]$.</p>
            <ul>
              <li><b>Step 1: Check curvature along the line.</b> For any $t \in [0, 1]$, the second derivative is:
                $$ g''(t) = v^\top \nabla^2 f(x+tv) v $$
              </li>
              <li><b>Step 2: Apply PSD condition.</b> Since the Hessian is PSD at the point $x+tv$, we know $v^\top \nabla^2 f(x+tv) v \ge 0$. Thus $g''(t) \ge 0$ for all $t$.</li>
              <li><b>Step 3: 1D Convexity.</b> A twice-differentiable function with a non-negative second derivative is convex. So $g(t)$ is convex.</li>
              <li><b>Conclusion:</b> Since the restriction of $f$ to <i>every</i> line segment is convex, the function $f$ is convex.</li>
            </ul>
          </div>
        </div>

        <h3>4.2 Strict and Strong Convexity via Hessian</h3>

        <div class="subsection">
          <h4>Strictly Convex</h4>
          <p>If $\nabla^2 f(x) \succ 0$ (positive definite) for all $x \in \mathrm{dom}\, f$, then $f$ is <b>strictly convex</b>.</p>
          <p><b>Note:</b> The converse is false! $f(x) = x^4$ is strictly convex but $f''(0) = 0$.</p>
        </div>

        <div class="subsection">
          <h4>Strongly Convex</h4>
          <p>If $\nabla^2 f(x) \succeq mI$ for some $m > 0$ and all $x \in \mathrm{dom}\, f$, then $f$ is <b>$m$-strongly convex</b>.</p>
          <p>Equivalently, the minimum eigenvalue of the Hessian is at least $m$ everywhere.</p>
        </div>

        <h3>4.3 Practical Verification</h3>

        <div class="proof-box">
          <h4>Example 1: Quadratic Function $f(x) = \|Ax - b\|_2^2$</h4>
          <p>We derive the Hessian explicitly to show convexity.</p>
          <div class="proof-step">
            <strong>Step 1: Expand.</strong>
            $$ f(x) = (Ax-b)^\top (Ax-b) = x^\top A^\top A x - 2b^\top A x + b^\top b $$
          </div>
          <div class="proof-step">
            <strong>Step 2: Gradient.</strong>
            $$ \nabla f(x) = 2A^\top A x - 2A^\top b = 2A^\top (Ax - b) $$
          </div>
          <div class="proof-step">
            <strong>Step 3: Hessian.</strong>
            $$ \nabla^2 f(x) = 2A^\top A $$
            For any vector $v$, $v^\top (2A^\top A) v = 2(Av)^\top (Av) = 2\|Av\|_2^2 \ge 0$.
            Thus the Hessian is PSD everywhere, so least squares is convex.
          </div>
        </div>

        <div class="proof-box">
          <h4>Example 2: Log-Sum-Exp (Full Derivation)</h4>
          <p>Let $f(x) = \log \left(\sum_{k=1}^n e^{x_k}\right)$. We perform a detailed derivation to prove its Hessian is PSD.</p>
          <p>Define $z_k = e^{x_k}$ and $S = \sum_{k=1}^n z_k$. Then $f(x) = \log S$.</p>

          <div class="proof-step">
            <strong>Step 1: Gradient.</strong>
            Using the chain rule: $\frac{\partial f}{\partial x_i} = \frac{1}{S} \frac{\partial S}{\partial x_i} = \frac{1}{S} e^{x_i} = \frac{z_i}{S}$.
            <br>In vector form: $\nabla f(x) = \frac{1}{S} z$, which is the <b>softmax</b> vector (probability distribution).
          </div>

          <div class="proof-step">
            <strong>Step 2: Hessian (Element-wise).</strong>
            We differentiate $\frac{\partial f}{\partial x_i} = \frac{z_i}{S}$ with respect to $x_j$ using the quotient rule:
            $$
            \frac{\partial^2 f}{\partial x_j \partial x_i} = \frac{\partial}{\partial x_j} \left( z_i S^{-1} \right)
            = \frac{\partial z_i}{\partial x_j} S^{-1} + z_i \frac{\partial S^{-1}}{\partial x_j}
            $$
            <ul>
              <li>$\frac{\partial z_i}{\partial x_j} = z_i \delta_{ij}$ (since $x_j$ only affects $z_j$).</li>
              <li>$\frac{\partial S^{-1}}{\partial x_j} = -S^{-2} \frac{\partial S}{\partial x_j} = -S^{-2} z_j$.</li>
            </ul>
            Combining these:
            $$ \frac{\partial^2 f}{\partial x_j \partial x_i} = \frac{z_i \delta_{ij}}{S} - \frac{z_i z_j}{S^2} $$
            In matrix form:
            $$ \nabla^2 f(x) = \frac{1}{S}\mathrm{diag}(z) - \frac{1}{S^2} z z^\top $$
          </div>

          <div class="proof-step">
            <strong>Step 3: PSD Check via Cauchy-Schwarz.</strong>
            For any vector $v \in \mathbb{R}^n$, consider the quadratic form:
            $$
            v^\top \nabla^2 f(x) v = \frac{1}{S} v^\top \mathrm{diag}(z) v - \frac{1}{S^2} v^\top (z z^\top) v
            $$
            $$ = \frac{1}{S} \sum_i z_i v_i^2 - \frac{1}{S^2} \left(\sum_i z_i v_i\right)^2 $$
            Multiply by $S^2$ (which is positive) to clear denominators:
            $$ S^2 (v^\top \nabla^2 f(x) v) = S \sum_i z_i v_i^2 - \left(\sum_i z_i v_i\right)^2 $$
            Recall $S = \sum z_i$. We use Cauchy-Schwarz with vectors $a_i = \sqrt{z_i}v_i$ and $b_i = \sqrt{z_i}$:
            $$ \left(\sum z_i v_i\right)^2 = \left(\sum (\sqrt{z_i}v_i)(\sqrt{z_i})\right)^2 \le \left(\sum z_i v_i^2\right) \left(\sum z_i\right) = \left(\sum z_i v_i^2\right) S $$
            Rearranging gives $S \sum z_i v_i^2 - (\sum z_i v_i)^2 \ge 0$.
            <br>Thus $v^\top \nabla^2 f(x) v \ge 0$ for all $v$, so the Hessian is PSD.
          </div>
        </div>

        <div class="proof-box">
          <h4>Proof: Concavity of Log-Determinant</h4>
          <p>The function $f(X) = \log \det X$ is <b>concave</b> on $\mathbb{S}^n_{++}$. We use the "restriction to a line" technique to prove this formally.</p>

          <div class="proof-step">
            <strong>Step 1: Restrict to a line.</strong>
            Let $X \in \mathbb{S}^n_{++}$ and $V \in \mathbb{S}^n$. Consider the line $Z(t) = X + tV$. The domain is $\{t \mid X+tV \succ 0\}$. Define $g(t) = \log \det(X + tV)$.
          </div>

          <div class="proof-step">
            <strong>Step 2: Factor out $X^{1/2}$.</strong>
            Since $X \succ 0$, $X^{1/2}$ exists. Write:
            $$ X + tV = X^{1/2} (I + t X^{-1/2} V X^{-1/2}) X^{1/2} $$
            Let $Y = X^{-1/2} V X^{-1/2}$. Note that $Y$ is symmetric.
            $$ \det(X + tV) = \det(X^{1/2}) \det(I + tY) \det(X^{1/2}) = \det(X) \det(I + tY) $$
            $$ g(t) = \log \det X + \log \det(I + tY) $$
          </div>

          <div class="proof-step">
            <strong>Step 3: Diagonalize.</strong>
            Let $\lambda_1, \dots, \lambda_n$ be the eigenvalues of the symmetric matrix $Y$. Then the eigenvalues of $I + tY$ are $1 + t\lambda_i$.
            $$ \det(I + tY) = \prod_{i=1}^n (1 + t\lambda_i) $$
            $$ g(t) = \log \det X + \sum_{i=1}^n \log(1 + t\lambda_i) $$
          </div>

          <div class="proof-step">
            <strong>Step 4: Check derivatives.</strong>
            $$ g'(t) = \sum_{i=1}^n \frac{\lambda_i}{1 + t\lambda_i} $$
            $$ g''(t) = -\sum_{i=1}^n \frac{\lambda_i^2}{(1 + t\lambda_i)^2} $$
            Since squares are non-negative, $g''(t) \le 0$ for all valid $t$. Thus $g(t)$ is concave.
          </div>

          <div class="proof-step">
            <strong>Conclusion:</strong> Since the restriction to any line is concave, $f(X) = \log \det X$ is concave.
          </div>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Heatmap: Hessian Eigenvalue Analyzer</h3>
          <p><b>Visualize Convexity Through the Hessian:</b> For twice-differentiable functions, convexity is determined by the Hessian being positive semidefinite (PSD). This tool provides a visual test:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Choose a 2D function:</b> Select from various functions (quadratic, Rosenbrock, etc.)</li>
            <li><b>Heatmap display:</b> The color at each point shows the minimum eigenvalue of the Hessian $\nabla^2 f(x)$</li>
            <li><b>Interactive Probe:</b> Hover over the heatmap to see the local quadratic approximation. Observe how the local "bowl" or "saddle" shape relates to the Hessian eigenvalues.</li>
            <li><b>Interpret colors:</b>
              <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
                <li><b>Blue/Green (positive):</b> PSD Hessianâ€”locally convex (bowl)</li>
                <li><b>Red (negative):</b> Not PSDâ€”locally concave or saddle point</li>
              </ul>
            </li>
            <li><b>Global convexity:</b> A function is convex if the entire region is blue/green (no red)</li>
          </ul>
          <p><i>Practical insight:</i> This is exactly how numerical optimization libraries verify convexity in practiceâ€”by checking that all eigenvalues of the Hessian are non-negative!</p>
          <div id="widget-hessian-heatmap" style="width: 100%; height: 450px; position: relative;"></div>
        </div>

                <h3>4.4 Log-Convex and Log-Concave Functions</h3>

        <figure style="text-align: center;">
          <img src="assets/log_concave_gaussian.png"
               alt="Gaussian density vs its log"
               style="max-width: 60%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
          <figcaption><i>Figure 4.1:</i> The Gaussian density function (top) is bell-shaped and not concave. However, its logarithm (bottom) is a downward-opening parabola, which is concave. Thus, the Gaussian is log-concave.</figcaption>
        </figure>

        <h4>4.4.1 Definitions</h4>
        <p>A function $f: \mathbb{R}^n \to \mathbb{R}_{++}$ is <b>log-convex</b> if $\log f$ is convex. Equivalently:</p>
        $$ f(\theta x + (1-\theta)y) \le f(x)^\theta f(y)^{1-\theta} $$
        <p>It is <b>log-concave</b> if $\log f$ is concave. Equivalently:</p>
        $$ f(\theta x + (1-\theta)y) \ge f(x)^\theta f(y)^{1-\theta} $$

        <div class="insight">
          <h4>Key Examples</h4>
          <ul>
            <li><b>Affine:</b> $f(x) = a^\top x + b$ is log-concave on $\{x \mid a^\top x + b > 0\}$.</li>
            <li><b>Powers:</b> $x^a$ on $\mathbb{R}_{++}$ is log-convex if $a \le 0$, and log-concave if $a \ge 0$.</li>
            <li><b>Gaussian Density:</b> $f(x) \propto e^{-\frac{1}{2}(x-\mu)^\top \Sigma^{-1}(x-\mu)}$ is log-concave.</li>
            <li><b>Determinant:</b> $\det(X)$ is log-concave on $\mathbb{S}^n_{++}$ (since $\log \det$ is concave).</li>
          </ul>
        </div>

        <h4>(a) Uniform on a Convex Set</h4>
        <p>Let $C \subseteq \mathbb{R}^n$ be a convex set. The uniform density function:
        $$ f(x) = \begin{cases} 1/\alpha & x \in C \\ 0 & x \notin C \end{cases} $$
        is log-concave.
        <br><i>Proof:</i> $\log f(x)$ takes value $-\log \alpha$ on $C$ and $-\infty$ outside.
        For $x, y \in C$ and $\theta \in [0, 1]$, the point $z = \theta x + (1-\theta)y \in C$ (by convexity of $C$).
        Thus $\log f(z) = -\log \alpha = \theta (-\log \alpha) + (1-\theta) (-\log \alpha) = \theta \log f(x) + (1-\theta) \log f(y)$.
        If either point is outside $C$, the condition holds trivially ($-\infty \le \dots$).</p>

        <h4>(b) Wishart Distribution</h4>
        <p>The Wishart density on positive definite matrices $X \in \mathbb{S}^n_{++}$ is (up to normalization):
        $$ f(X) \propto (\det X)^{(p-n-1)/2} \exp\left(-\frac{1}{2} \mathrm{tr}(\Sigma^{-1}X)\right) $$
        Taking the log:
        $$ \log f(X) = c + \frac{p-n-1}{2} \log \det X - \frac{1}{2} \mathrm{tr}(\Sigma^{-1}X) $$
        <ul>
            <li>The term $\log \det X$ is <b>concave</b> (as proven in Section 4.3).</li>
            <li>The term $-\mathrm{tr}(\Sigma^{-1}X)$ is <b>linear</b> (hence concave).</li>
        </ul>
        Since the sum of concave functions is concave, $\log f$ is concave, so the Wishart distribution is log-concave.</p>

        <h4>4.4.2 Hessian Condition</h4>
        <p>Assume $f > 0$ and $f$ is twice differentiable. We derive the condition for log-concavity.</p>
        <div class="proof-box">
          <h4>Derivation</h4>
          <p>Let $g(x) = \log f(x)$. By the chain rule:
          $$ \nabla g(x) = \frac{\nabla f(x)}{f(x)} $$
          The Hessian of $g$ is:
          $$ \nabla^2 g(x) = \frac{f(x)\nabla^2 f(x) - \nabla f(x)\nabla f(x)^\top}{f(x)^2} $$
          For $f$ to be log-concave, we require $\nabla^2 g(x) \preceq 0$ (negative semidefinite). Since $f(x)^2 > 0$, this is equivalent to the numerator being negative semidefinite:
          $$ f(x)\nabla^2 f(x) - \nabla f(x)\nabla f(x)^\top \preceq 0 $$
          Rearranging terms yields the condition:
          $$ \boxed{ f(x) \nabla^2 f(x) \preceq \nabla f(x) \nabla f(x)^\top } $$
          For log-convexity, the inequality is reversed.</p>
        </div>

        <h4>4.4.3 Integration Rules (PrÃ©kopa-Leindler)</h4>
        <p>A powerful property of log-concave functions is that they are closed under integration.</p>

        <div class="theorem-box">
          <h4>Theorem (Integration Preserves Log-Convexity)</h4>
          <p>If $f(x, y)$ is <b>log-convex</b> in $x$ for each fixed $y$, then $g(x) = \int f(x, y) dy$ is log-convex.</p>
          <p><i>Example:</i> The Laplace transform $P(z) = \int p(x) e^{-z^\top x} dx$ is log-convex in $z$, because the integrand $e^{-z^\top x}$ is log-linear (hence log-convex) in $z$.</p>
        </div>

        <div class="theorem-box">
          <h4>Theorem (Marginals Preserves Log-Concavity)</h4>
          <p>If $f(x, y)$ is <b>log-concave jointly</b> in $(x, y)$, then the marginal function $g(x) = \int f(x, y) dy$ is log-concave.</p>
          <p>This implies that marginals of log-concave distributions (like the multivariate normal) are log-concave.</p>
        </div>

        <div class="theorem-box">
          <h4>Theorem (Convolution)</h4>
          <p>If $f$ and $g$ are log-concave functions on $\mathbb{R}^n$, then their convolution $(f * g)(x) = \int f(x-y)g(y) dy$ is log-concave.</p>
          <p><i>Probability Implication:</i> The sum of independent log-concave random variables is log-concave.</p>
        </div>

        <h4>4.4.4 Probability Examples</h4>
        <div class="example">
          <h4>1. Hitting a Convex Set</h4>
          <p>Let $C \subseteq \mathbb{R}^n$ be a convex set and $w$ be a random variable with log-concave density $p(w)$. The probability that $x + w \in C$ is a log-concave function of $x$.</p>
          <p><i>Proof:</i> $f(x) = \mathbb{P}(x+w \in C) = \int \mathbf{1}_C(x+w) p(w) dw = \int \mathbf{1}_C(u) p(u-x) du$. The integrand is the product of log-concave functions (indicator of convex set and shifted density), so it is jointly log-concave. By the marginalization theorem, $f(x)$ is log-concave.</p>
        </div>

        <div class="example">
          <h4>2. Cumulative Distribution Function (CDF)</h4>
          <p>If a density $p(t)$ is log-concave, its CDF $F(x) = \int_{-\infty}^x p(t) dt$ is log-concave.
          <br>For multivariate $x$, $F(x) = \mathbb{P}(w \preceq x)$ is log-concave. This relies on the fact that the indicator function of the orthant $\{w \mid w \preceq x\}$ is log-concave.</p>
        </div>
      </section>

      <section class="section-card" id="section-5">
        <h2>5. Operations Preserving Convexity</h2>

        <p>One of the most powerful aspects of convex analysis is that complex convex functions can be built from simpler ones using operations that preserve convexity. This enables sophisticated modeling without sacrificing tractability.</p>

        <h3>5.1 Nonnegative Weighted Sum</h3>
        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>If $f_1, \ldots, f_m$ are convex functions and $w_1, \ldots, w_m \ge 0$, then:</p>
          $$
          f(x) = \sum_{i=1}^m w_i f_i(x)
          $$
          <p>is convex. This extends to infinite sums and integrals (provided they converge).</p>
        </div>

        <div class="proof-box">
          <h4>Proof</h4>
          <p>For any $x, y$ and $\theta \in [0, 1]$:</p>
          $$
          \begin{aligned}
          f(\theta x + (1-\theta)y) &= \sum_{i=1}^m w_i f_i(\theta x + (1-\theta)y) \\
          &\le \sum_{i=1}^m w_i \big( \theta f_i(x) + (1-\theta)f_i(y) \big) \quad \text{(convexity of } f_i, \ w_i \ge 0\text{)} \\
          &= \theta \sum_{i=1}^m w_i f_i(x) + (1-\theta) \sum_{i=1}^m w_i f_i(y) \\
          &= \theta f(x) + (1-\theta) f(y)
          \end{aligned}
          $$
        </div>

        <h3>5.2 Pointwise Maximum of Convex Functions</h3>
        <p>Let $f_1, \dots, f_m: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$ be convex functions. Define their <b>pointwise maximum</b>:</p>
        $$
        f(x) = \max_{i=1,\dots,m} f_i(x)
        $$

        <div class="theorem-box">
          <h4>Claim</h4>
          <p>The pointwise maximum function $f$ is convex.</p>
        </div>

        <div class="insight">
          <h4>Intuition</h4>
          <p>Take several convex curves; at each $x$ keep the one that lies highest. The upper envelope still bends "upwards"; you never get a concave dip by taking a max of convex curves. Geometrically, the epigraph is the intersection of epigraphs.</p>
        </div>

        <div class="proof-box">
          <h4>Algebraic Proof</h4>
          <div class="proof-step">
            <strong>Step 1: Definition.</strong>
            Let $x, y \in \mathbb{R}^n$ and $0 \le \theta \le 1$. We need to show $f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta) f(y)$.
          </div>
          <div class="proof-step">
            <strong>Step 2: Convexity of components.</strong>
            For each fixed $i$, convexity of $f_i$ gives:
            $$ f_i(\theta x + (1-\theta)y) \le \theta f_i(x) + (1-\theta) f_i(y) $$
          </div>
          <div class="proof-step">
            <strong>Step 3: Max over both sides.</strong>
            $$ \max_i f_i(\theta x + (1-\theta)y) \le \max_i \big( \theta f_i(x) + (1-\theta) f_i(y) \big) $$
          </div>
          <div class="proof-step">
            <strong>Step 4: Inequality for max.</strong>
            Use the inequality $\max_i (\theta a_i + (1-\theta)b_i) \le \theta \max_i a_i + (1-\theta)\max_i b_i$.
            $$ \max_i \big( \theta f_i(x) + (1-\theta) f_i(y) \big) \le \theta \max_i f_i(x) + (1-\theta)\max_i f_i(y) = \theta f(x) + (1-\theta) f(y) $$
          </div>
          <div class="proof-step">
            <strong>Conclusion:</strong> $f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta) f(y)$.
          </div>
        </div>

        <div class="proof-box">
          <h4>Geometric Proof (Epigraph)</h4>
          <p>The epigraph of the max is the intersection of the epigraphs:
          $$ \mathrm{epi}\, f = \bigcap_{i=1}^m \mathrm{epi}\, f_i $$
          Since a point $(x,t)$ satisfies $t \ge \max_i f_i(x)$ if and only if $t \ge f_i(x)$ for all $i$.
          The intersection of convex sets is convex, so $\mathrm{epi}\, f$ is convex.</p>
        </div>

        <h4>Examples of Pointwise Maxima</h4>
        <div class="example">
          <h4>1. Piecewise-Linear (Max of Affine Functions)</h4>
          <p>Let $\ell_i(x) = a_i^\top x + b_i$ for $i=1,\dots,m$. Define:</p>
          $$ f(x) = \max_{1\le i\le m} (a_i^\top x + b_i) $$
          <p>Each $\ell_i$ is convex, so $f$ is convex. Geometrically, this is a polyhedral convex function (its epigraph is a polyhedron).</p>
        </div>

        <div class="example">
          <h4>2. Sum of the $r$ Largest Components</h4>
          <p>For $x \in \mathbb{R}^n$, let $x_{[1]} \ge x_{[2]} \ge \cdots \ge x_{[n]}$ denote components sorted in descending order. Define:</p>
          $$ f(x) = \sum_{k=1}^r x_{[k]} $$
          <p>This function is convex. It can be represented as the maximum over all subsets of size $r$:</p>
          $$ f(x) = \max \left\{ \sum_{j=1}^r x_{i_j} \mid 1 \le i_1 < \cdots < i_r \le n \right\} $$
          <p>Since the sum of any specific subset of coordinates is a linear function $x \mapsto c^\top x$ (where $c$ has 1s at chosen indices), $f$ is the pointwise maximum of linear functions, hence convex. This is known as the <b>Ky Fan norm</b>.</p>
        </div>

        <h3>5.3 Pointwise Supremum over an Index Set</h3>
        <p>The "max of finitely many" generalizes to the supremum over an arbitrary index set $\mathcal{A}$.</p>

        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>Let $\mathcal{A}$ be any index set. For each $y \in \mathcal{A}$, let $f(\cdot, y)$ be convex in $x$. Define:</p>
          $$ g(x) = \sup_{y \in \mathcal{A}} f(x, y) $$
          <p>Then $g(x)$ is convex.</p>
        </div>

        <div class="proof-box">
          <h4>Proof</h4>
          <p>Let $x_1, x_2 \in \mathbb{R}^n$ and $\theta \in [0, 1]$.
          $$ g(\theta x_1 + (1-\theta)x_2) = \sup_y f(\theta x_1 + (1-\theta)x_2, y) $$
          $$ \le \sup_y \big( \theta f(x_1, y) + (1-\theta) f(x_2, y) \big) $$
          $$ \le \theta \sup_y f(x_1, y) + (1-\theta) \sup_y f(x_2, y) = \theta g(x_1) + (1-\theta) g(x_2) $$
          Geometrically, $\mathrm{epi}\, g = \bigcap_{y \in \mathcal{A}} \mathrm{epi}\, f(\cdot, y)$, which is an intersection of convex sets.</p>
        </div>

        <h4>Examples of Pointwise Supremum</h4>
        <div class="example">
          <h4>1. Support Function</h4>
          <p>For any set $C \subset \mathbb{R}^n$, the support function $S_C(x) = \sup_{y \in C} y^\top x$ is the pointwise supremum of linear functions $x \mapsto y^\top x$ indexed by $y \in C$. Thus $S_C$ is always convex, regardless of whether $C$ is convex.</p>
        </div>

        <div class="example">
          <h4>2. Distance to Farthest Point</h4>
          <p>Let $C \subset \mathbb{R}^n$. The function $f(x) = \sup_{y \in C} \|x - y\|$ (distance to the farthest point in $C$) is convex, because for each fixed $y$, $x \mapsto \|x - y\|$ is convex.</p>
        </div>

        <div class="example">
          <h4>3. Maximum Eigenvalue</h4>
          <p>For $X \in \mathbb{S}^n$, the maximum eigenvalue $\lambda_{\max}(X)$ is convex. By the Rayleigh-Ritz theorem:</p>
          $$ \lambda_{\max}(X) = \sup_{\|y\|_2=1} y^\top X y $$
          <p>For each fixed unit vector $y$, the map $X \mapsto y^\top X y$ is linear in $X$. Thus $\lambda_{\max}$ is the supremum of linear functions.</p>
        </div>

        <h3>5.4 Partial Minimization</h3>
        <p>We now look at minimizing over some coordinates: $g(x) = \inf_{y \in C} f(x, y)$.</p>

        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>If $f(x, y)$ is <b>jointly convex</b> in $(x, y)$ and $C$ is a convex set, then:</p>
          $$ g(x) = \inf_{y \in C} f(x, y) $$
          <p>is convex (provided $g(x) > -\infty$).</p>
        </div>

        <div class="insight">
          <h4>Geometric Picture</h4>
          <p>The epigraph of $g$ is the <b>projection</b> of the epigraph of $f$ onto the $(x, t)$ space (projecting out $y$). Since the projection of a convex set is convex, $g$ is convex.</p>
        </div>

        <div class="proof-box">
          <h4>Algebraic Proof</h4>
          <p>Let $x_1, x_2$ and $\theta \in [0, 1]$. We want $g(x_\theta) \le \theta g(x_1) + (1-\theta)g(x_2)$.
          <br>Write the infimum redundantly: $g(x_\theta) = \inf_{y_1, y_2 \in C} f(x_\theta, \theta y_1 + (1-\theta)y_2)$.
          <br>By joint convexity: $f(x_\theta, \theta y_1 + (1-\theta)y_2) \le \theta f(x_1, y_1) + (1-\theta) f(x_2, y_2)$.
          <br>Taking infimum over $y_1, y_2$:
          $$ g(x_\theta) \le \theta \inf_{y_1} f(x_1, y_1) + (1-\theta) \inf_{y_2} f(x_2, y_2) = \theta g(x_1) + (1-\theta) g(x_2) $$
          </p>
        </div>

        <h4>Examples of Partial Minimization</h4>
        <div class="example">
          <h4>1. Distance to a Convex Set</h4>
          <p>For a convex set $S$, $d(x, S) = \inf_{y \in S} \|x - y\|$.
          <br>Here $f(x, y) = \|x - y\|$ is convex in $(x, y)$ and $S$ is convex. Thus $d(x, S)$ is convex.</p>
        </div>

        <div class="example">
          <h4>2. Schur Complement</h4>
          <p>Let $f(x, y) = x^\top A x + 2x^\top B y + y^\top C y$ with $C \succ 0$.
          <br>We perform partial minimization to eliminate $y$. The gradient w.r.t $y$ is $\nabla_y f = 2Cy + 2B^\top x$. Setting to zero gives $y^* = -C^{-1}B^\top x$.
          <br>Substituting back:
          $$ g(x) = f(x, y^*) = x^\top A x + 2x^\top B (-C^{-1}B^\top x) + (-C^{-1}B^\top x)^\top C (-C^{-1}B^\top x) $$
          $$ = x^\top A x - 2x^\top B C^{-1} B^\top x + x^\top B C^{-1} B^\top x = x^\top (A - B C^{-1} B^\top) x $$
          Since partial minimization preserves convexity, if the original block matrix is PSD, then the Schur complement $A - B C^{-1} B^\top$ is also PSD.</p>
        </div>

        <h3>5.5 Composition Rules</h3>

        <div class="theorem-box">
          <h4>1. Affine Composition</h4>
          <p>If $f: \mathbb{R}^k \to \mathbb{R}$ is convex, and $x \mapsto Ax + b$ is an affine map, then the composition $g(x) = f(Ax + b)$ is convex.</p>
          <p><b>Proof:</b>
          $$
          \begin{aligned}
          g(\theta x + (1-\theta)y) &= f(A(\theta x + (1-\theta)y) + b) \\
          &= f(\theta(Ax+b) + (1-\theta)(Ay+b)) \\
          &\le \theta f(Ax+b) + (1-\theta)f(Ay+b) \\
          &= \theta g(x) + (1-\theta)g(y)
          \end{aligned}
          $$
          </p>
          <p><b>Examples:</b></p>
          <ul>
            <li><b>Log-Barrier:</b> $f(x) = -\sum \log(b_i - a_i^\top x)$. Inner: affine $b-a^\top x$. Outer: $-\log(\cdot)$ convex. Sum of convex is convex.</li>
            <li><b>Norm of Affine:</b> $\|Ax - b\|$ is convex.</li>
          </ul>
        </div>

        <div class="theorem-box">
          <h4>2. Scalar Composition Rule ($f=h\circ g$) in Depth</h4>
          <p>Let $g: \mathbb{R}^n \to \mathbb{R}$ and $h: \mathbb{R} \to \mathbb{R}$. Then $f(x) = h(g(x))$ is convex if:</p>
          <ol>
            <li>$g$ is <b>convex</b>, and $h$ is <b>convex</b> and <b>non-decreasing</b>.</li>
            <li>$g$ is <b>concave</b>, and $h$ is <b>convex</b> and <b>non-increasing</b>.</li>
            <li>$g$ is <b>affine</b>, and $h$ is <b>convex</b> (monotonicity not required).</li>
          </ol>

          <div style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center; margin: 20px 0;">
            <figure style="flex: 1; min-width: 300px; text-align: center;">
              <img src="assets/convex_composition_case1.png"
                   alt="Composition Case 1: Convex inner + Convex Increasing outer"
                   style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 10px; background: white;" />
              <figcaption><i>Figure 5.1:</i> Case 1. The convex inner function "curves up". The increasing outer function preserves this order, and its own convexity amplifies the curvature.</figcaption>
            </figure>
            <figure style="flex: 1; min-width: 300px; text-align: center;">
              <img src="assets/convex_composition_case2.png"
                   alt="Composition Case 2: Concave inner + Convex Decreasing outer"
                   style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 10px; background: white;" />
              <figcaption><i>Figure 5.2:</i> Case 2. The concave inner function "curves down". The decreasing outer function flips this to "curve up", and its own convexity reinforces the result.</figcaption>
            </figure>
          </div>

          <div class="proof-box">
            <h4>Proof Step-by-Step (Convex + Non-decreasing)</h4>
            <p>Assume $g$ is convex, $h$ is convex, and $h$ is non-decreasing.</p>

            <div class="proof-step">
              <strong>Step 1: Use convexity of inner function $g$.</strong>
              Take $x, y \in \mathrm{dom}\, f$ and $\theta \in [0, 1]$.
              $$ g(\theta x + (1-\theta)y) \le \theta g(x) + (1-\theta)g(y) $$
            </div>

            <div class="proof-step">
              <strong>Step 2: Apply monotone outer function $h$.</strong>
              Since $h$ is non-decreasing, applying it to both sides preserves the inequality:
              $$ h(g(\theta x + (1-\theta)y)) \le h(\theta g(x) + (1-\theta)g(y)) $$
              The LHS is exactly $f(\theta x + (1-\theta)y)$.
            </div>

            <div class="proof-step">
              <strong>Step 3: Use convexity of outer function $h$.</strong>
              Let $u = g(x)$ and $v = g(y)$. By convexity of $h$:
              $$ h(\theta u + (1-\theta)v) \le \theta h(u) + (1-\theta)h(v) $$
              Substituting $u, v$ back:
              $$ h(\theta g(x) + (1-\theta)g(y)) \le \theta h(g(x)) + (1-\theta)h(g(y)) = \theta f(x) + (1-\theta)f(y) $$
            </div>

            <div class="proof-step">
              <strong>Step 4: Combine.</strong>
              Chaining Step 2 and Step 3:
              $$ f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) $$
              Thus $f$ is convex.
            </div>
          </div>
        </div>

        <div class="example">
          <h4>Example: Log-Log-Sum-Exp</h4>
          <p>Consider $f(x) = -\log(-\log(\sum e^{a_i^\top x + b_i}))$.</p>
          <ul>
            <li>Inner: $g(x) = \log(\sum e^{y_i})$ is Convex (Log-Sum-Exp).</li>
            <li>Outer: $h(u) = -\log(-u)$ is Convex and Non-decreasing (for $u < 0$).</li>
            <li>Result: Convex.</li>
          </ul>
        </div>

        <div class="theorem-box">
          <h4>3. Vector Composition Rule ($f=h\circ g$) in Depth</h4>
          <p>Let $g: \mathbb{R}^n \to \mathbb{R}^k$ and $h: \mathbb{R}^k \to \mathbb{R}$. Then $f(x) = h(g_1(x), \dots, g_k(x))$ is convex if $h$ is convex and for each $i \in \{1, \dots, k\}$:</p>
          <ul>
            <li>$g_i$ is <b>convex</b> and $h$ is <b>non-decreasing</b> in the $i$-th argument.</li>
            <li>$g_i$ is <b>concave</b> and $h$ is <b>non-increasing</b> in the $i$-th argument.</li>
            <li>$g_i$ is <b>affine</b> (monotonicity in $i$-th argument not required).</li>
          </ul>

          <div class="proof-box">
            <h4>Rigorous Proof (Convex + Non-decreasing Case)</h4>
            <p>We prove the case where each $g_i$ is convex and $h$ is convex and non-decreasing in each coordinate.</p>
            <div class="proof-step">
              <strong>Step 1: Vector Jensen Inequality.</strong>
              Since each $g_i$ is convex, for $\theta \in [0,1]$:
              $$ g(\theta x + (1-\theta)y) \le \theta g(x) + (1-\theta)g(y) \quad \text{(componentwise)} $$
            </div>
            <div class="proof-step">
              <strong>Step 2: Monotonicity.</strong>
              Since $h$ is non-decreasing in every coordinate, applying it preserves the inequality:
              $$ h(g(\theta x + (1-\theta)y)) \le h(\theta g(x) + (1-\theta)g(y)) $$
            </div>
            <div class="proof-step">
              <strong>Step 3: Convexity of Outer Function.</strong>
              By convexity of $h$:
              $$ h(\theta g(x) + (1-\theta)g(y)) \le \theta h(g(x)) + (1-\theta) h(g(y)) $$
            </div>
            <div class="proof-step">
              <strong>Conclusion.</strong> Combining these:
              $$ f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) $$
            </div>
          </div>
        </div>

        <div class="example">
          <h4>Example 1: Sum of Logs (Concave)</h4>
          <p>Let $u(x) = \sum_{i=1}^m \log g_i(x)$ where each $g_i$ is concave and positive.</p>
          <ul>
            <li>Inner map: $g(x) = (g_1(x), \dots, g_m(x))$ is concave.</li>
            <li>Outer map: $h(y) = \sum \log y_i$. Since $\log$ is concave and increasing, $h$ is concave and increasing.</li>
            <li>Result: $u$ is concave.</li>
          </ul>
        </div>

        <div class="example">
          <h4>Example 2: Log-Sum-Exp of Convex Functions</h4>
          <p>Let $f(x) = \log\left(\sum_{i=1}^m \exp(g_i(x))\right)$ where each $g_i$ is convex.</p>
          <ul>
            <li>Inner map: $g(x)$ is convex.</li>
            <li>Outer map: $h(y) = \log(\sum e^{y_i})$ (Log-Sum-Exp).</li>
            <li>Properties of $h$: It is convex and <b>non-decreasing</b> in each argument (derivative is softmax probability > 0).</li>
            <li>Result: $f$ is convex.</li>
          </ul>
        </div>

        <h3>5.6 Perspective Function</h3>
        <div class="theorem-box">
          <h4>Definition and Property</h4>
          <p>If $f: \mathbb{R}^n \to \mathbb{R}$ is convex, then its <b>perspective</b> $g: \mathbb{R}^{n+1} \to \mathbb{R}$ defined by:</p>
          $$
          g(x, t) = t f(x/t), \quad t > 0
          $$
          <p>is convex on $\mathrm{dom}\, g = \{(x, t) \mid x/t \in \mathrm{dom}\, f, \ t > 0\}$. The perspective function preserves convexity.</p>
          <p><b>Geometric Intuition (Cone over Epigraph):</b></p>
          <p>The epigraph of the perspective function is related to the conic hull of the epigraph of $f$. Specifically, if we take the cone generated by $\text{epi}(f)$ in $\mathbb{R}^{n+2}$ and slice it at $t$, we recover the epigraph of the scaled function. Since the conic hull of a convex set is convex, and slicing preserves convexity, the perspective function is convex.</p>
        </div>

        <div class="proof-box">
          <h4>Inequality Proof of Convexity</h4>
          <div class="proof-step">
            <strong>Step 1: Setup and Goal.</strong>
            Take $(x_1, t_1), (x_2, t_2) \in \mathrm{dom}\, g$ (so $t_1, t_2 > 0$) and $\theta \in [0, 1]$.
            We want to show:
            $$ g(\theta x_1 + (1-\theta)x_2, \theta t_1 + (1-\theta)t_2) \le \theta g(x_1, t_1) + (1-\theta)g(x_2, t_2) $$
            The RHS is $\theta t_1 f(x_1/t_1) + (1-\theta)t_2 f(x_2/t_2)$.
          </div>

          <div class="proof-step">
            <strong>Step 2: Factor out the common scalar $T$.</strong>
            Let $T = \theta t_1 + (1-\theta)t_2$. Since $t_1, t_2 > 0$, $T > 0$.
            The LHS is $T f\left( \frac{\theta x_1 + (1-\theta)x_2}{T} \right)$.
            We rewrite the argument of $f$ to identify a convex combination:
            $$ \frac{\theta x_1 + (1-\theta)x_2}{T} = \frac{\theta t_1}{T} \frac{x_1}{t_1} + \frac{(1-\theta)t_2}{T} \frac{x_2}{t_2} $$
          </div>

          <div class="proof-step">
            <strong>Step 3: Define weights and apply convexity.</strong>
            Define $\mu = \frac{\theta t_1}{T}$. Then $1 - \mu = \frac{(1-\theta)t_2}{T}$. Note that $\mu \in [0, 1]$.
            By convexity of $f$:
            $$ f\left( \mu \frac{x_1}{t_1} + (1-\mu) \frac{x_2}{t_2} \right) \le \mu f(x_1/t_1) + (1-\mu) f(x_2/t_2) $$
          </div>

          <div class="proof-step">
            <strong>Step 4: Scale by $T$ to finish.</strong>
            Multiply the inequality by $T > 0$:
            $$ T f\left( \frac{\theta x_1 + (1-\theta)x_2}{T} \right) \le T \mu f(x_1/t_1) + T(1-\mu) f(x_2/t_2) $$
            Substitute back $T\mu = \theta t_1$ and $T(1-\mu) = (1-\theta)t_2$:
            $$ g(\theta x_1 + (1-\theta)x_2, T) \le \theta t_1 f(x_1/t_1) + (1-\theta) t_2 f(x_2/t_2) $$
            This matches the definition of convexity for $g$.
          </div>
        </div>

        <div class="example">
          <h4>Example: Relative Entropy</h4>
          <p>Let $f(u) = -\log u$, which is convex. Its perspective is:
          $$ g(x, t) = t f(x/t) = -t \log(x/t) = t \log t - t \log x $$
          This is related to the relative entropy function $x \log(x/y)$, which is jointly convex.</p>
        </div>

        <div class="example">
          <h4>Example: Power-over-Linear</h4>
          <p>The function $f(x, t) = \frac{\|x\|_p^p}{t^{p-1}}$ for $p > 1, t > 0$ is convex.
          <br><i>Derivation:</i> Factor out $t$: $f(x,t) = t \frac{\|x\|_p^p}{t^p} = t \|x/t\|_p^p$. This is the perspective of the convex function $h(u) = \|u\|_p^p$.</p>
        </div>

        <div class="example">
          <h4>Example: Quadratic-over-Linear</h4>
          <p>The function $f(x, y) = \frac{x^2}{y}$ for $y > 0$ is convex (Perspective of $x^2$).
          <br>More generally, $f(x) = \frac{\|Ax+b\|_2^2}{c^\top x + d}$ is convex (Affine composition of perspective).</p>
        </div>

        <h3>5.7 Power Functions and Homogeneity</h3>

        <p>Power functions are a rich source of examples and inequalities.</p>

        <h4>Convexity of $x^p$</h4>
        <p>The function $f(x) = x^p$ on $\mathbb{R}_{++}$ is:</p>
        <ul>
            <li><b>Convex</b> for $p \ge 1$ or $p \le 0$ ($f''(x) = p(p-1)x^{p-2} \ge 0$).</li>
            <li><b>Concave</b> for $0 \le p \le 1$ ($f''(x) \le 0$).</li>
        </ul>

        <h4>Homogeneity and Inequalities</h4>
        <p>For $p \ge 1$, $f(x) = x^p$ is convex. Using the tangent line inequality at $x=1$ ($f(x) \ge f(1) + f'(1)(x-1)$):</p>
        $$ x^p \ge 1 + p(x-1) $$
        <p>This can be used to prove that for $x, y \ge 0$ and $p \ge 1$:</p>
        $$ \boxed{ x^p + y^p \ge x + y \quad \text{for } x,y \in [0,1] } $$
        <p>Also, due to homogeneity ($(\lambda x)^p = \lambda^p x^p$), we can scale inequalities. For example, the function $f(x, y) = \frac{x^2}{y}$ arises from the homogeneity of quadratics.</p>

        <h3>5.8 What Doesn't Preserve Convexity?</h3>
        <p>A common pitfall is to assume the <b>minimum</b> of convex functions is convex. It is generally <b>not</b>.</p>
        <div class="insight">
          <h4>Counterexample</h4>
          <p>Let $f_1(x) = (x-1)^2$ and $f_2(x) = (x+1)^2$. Both are convex. Their minimum $g(x) = \min((x-1)^2, (x+1)^2)$ is "W"-shaped, with a local maximum at $x=0$, clearly nonconvex.</p>
        </div>
        <p>Similarly, minimizing over a <b>nonconvex</b> set usually breaks convexity. For example, the distance to a nonconvex set is generally nonconvex.</p>

        <h3>5.9 Summary: Mental Checklist</h3>
        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px;">
          <h4 style="margin-top: 0;">Quick Recognition Guide</h4>
          <ul style="margin-top: 0.5rem;">
            <li><b>Pointwise Max / Sup:</b> "Upper envelope". Intersection of epigraphs. (e.g., $\lambda_{\max}$, support function).</li>
            <li><b>Partial Minimization:</b> "Projection". Requires joint convexity. (e.g., distance to convex set, Schur complement).</li>
            <li><b>Composition:</b> Check curvature ("bowl vs dome") and monotonicity.</li>
            <li><b>Perspective:</b> Scales the domain and function value together; preserves convexity.</li>
          </ul>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Builder: Operations Preserving Convexity</h3>
          <p><b>Build Complex Convex Functions from Simple Ones:</b> This interactive tool lets you combine convex building blocks using convexity-preserving operations:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Start with basic functions:</b> $x^2$, $|x|$, $e^x$, $-\log(x)$, etc.</li>
            <li><b>Apply operations:</b>
              <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
                <li>Nonnegative weighted sum: $w_1 f_1 + w_2 f_2$ (sliders for weights)</li>
                <li>Pointwise maximum: $\max\{f_1, f_2, \ldots\}$</li>
                <li>Composition: $h(g(x))$ with appropriate monotonicity</li>
                <li>Affine transformations: $f(Ax + b)$</li>
              </ul>
            </li>
            <li><b>Visualize the result:</b> See the graph of the combined function</li>
            <li><b>Verify convexity:</b> Test via Jensen's inequality and Hessian eigenvalues</li>
            <li><b>Export formula:</b> Get the mathematical expression for the constructed function</li>
          </ul>
          <p><i>Modeling power:</i> Almost every convex function you'll encounter in practice can be built using these operations! This is the foundation of disciplined convex programming (DCP) in tools like CVXPY.</p>
          <div id="widget-operations-preserving" style="width: 100%; height: 500px; position: relative;"></div>
        </div>
      </section>
<section class="section-card" id="section-6">
        <h2>6. Strong Convexity and Smoothness</h2>

        <h3>6.1 Strong Convexity: Definition and Properties</h3>
        <p>A function $f$ is <b>$m$-strongly convex</b> (with parameter $m > 0$) if for all $x, y$ and $\theta \in [0, 1]$:</p>
        $$
        f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) - \frac{m}{2} \theta(1-\theta) \|x - y\|_2^2
        $$
        <p>Equivalently, $f(x) - \frac{m}{2}\|x\|_2^2$ is convex.</p>

        <h3>6.2 Characterizations of Strong Convexity</h3>

        <div class="theorem-box">
          <h4>Equivalent Conditions (for differentiable $f$)</h4>
          <p>The following are equivalent:</p>
          <ol>
            <li>$f$ is $m$-strongly convex.</li>
            <li><b>First-order with quadratic term:</b> $f(y) \ge f(x) + \nabla f(x)^\top (y - x) + \frac{m}{2}\|y - x\|_2^2$</li>
            <li><b>Hessian bound:</b> (If $f$ is twice differentiable) $\nabla^2 f(x) \succeq mI$ for all $x$.</li>
            <li><b>Gradient monotonicity:</b> $(\nabla f(y) - \nabla f(x))^\top (y - x) \ge m\|y - x\|_2^2$</li>
          </ol>
        </div>

        <h3>6.3 Implications for Optimization</h3>

        <div class="insight">
          <h4>Why Strong Convexity is Important</h4>
          <ul>
            <li><b>Unique minimizer:</b> An $m$-strongly convex function has at most one minimizer.</li>
            <li><b>Linear convergence:</b> Gradient descent on $m$-strongly convex functions converges at a geometric rate: $f(x_k) - f(x^*) \le (1 - m/L)^k (f(x_0) - f(x^*))$ where $L$ is the Lipschitz constant of the gradient.</li>
            <li><b>Condition number:</b> The ratio $\kappa = L/m$ (condition number) determines convergence speed.</li>
          </ul>
        </div>

        <h3>6.4 Smoothness (Lipschitz Gradient)</h3>
        <p>A differentiable function $f$ has <b>$L$-Lipschitz continuous gradient</b> (is <b>$L$-smooth</b>) if:</p>
        $$
        \|\nabla f(y) - \nabla f(x)\|_2 \le L \|y - x\|_2 \quad \forall x, y
        $$
        <p>Equivalently (for twice differentiable $f$): $\nabla^2 f(x) \preceq LI$ for all $x$.</p>

        <div class="example">
          <h4>Example: Quadratic Functions</h4>
          <p>For $f(x) = \frac{1}{2} x^\top Q x + b^\top x$ where $mI \preceq Q \preceq LI$:</p>
          <ul>
            <li>$f$ is $m$-strongly convex.</li>
            <li>$f$ has $L$-Lipschitz gradient.</li>
            <li>The condition number is $\kappa = L/m = \lambda_{\max}(Q) / \lambda_{\min}(Q)$.</li>
          </ul>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Explorer: Strong Convexity and Convergence</h3>
          <p><b>See How Strong Convexity Accelerates Optimization:</b> Strong convexity provides a quadratic lower bound on the function, leading to provably fast convergence. This visualization demonstrates:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Adjust $m$ (strong convexity parameter):</b> Use a slider to change the strength of convexity</li>
            <li><b>Compare bounds:</b>
              <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
                <li>Convex: $f(y) \ge f(x) + \nabla f(x)^\top (y-x)$ (tangent line)</li>
                <li>Strongly convex: $f(y) \ge f(x) + \nabla f(x)^\top (y-x) + \frac{m}{2}\|y-x\|_2^2$ (quadratic lower bound)</li>
              </ul>
            </li>
            <li><b>Watch gradient descent:</b> Run the algorithm and observe how larger $m$ leads to faster convergence</li>
            <li><b>Visualize condition number:</b> See how $\kappa = L/m$ determines the convergence rate</li>
            <li><b>Compare with non-strongly convex:</b> See the dramatic difference in convergence speed</li>
          </ul>
          <p><i>Practical impact:</i> This is why adding $\ell_2$ regularization ($\lambda \|x\|_2^2$) not only prevents overfitting but also speeds up optimizationâ€”it makes the problem strongly convex!</p>
          <div id="widget-strong-convexity" style="width: 100%; height: 550px; position: relative;"></div>
        </div>
      </section>

            <section class="section-card" id="section-7">
        <h2>7. The Convex Conjugate (Fenchel Conjugate)</h2>

        <h3>7.1 Definition and Geometric Intuition</h3>
        <p>The <a href="#" class="definition-link" data-term="convex conjugate">convex conjugate</a> (or <b>Fenchel-Legendre transform</b>) of a function $f: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$ is defined as:</p>
        $$
        \boxed{f^*(y) = \sup_{x \in \mathrm{dom}\, f} \left( y^\top x - f(x) \right)}
        $$
        <p>Here, $y \in \mathbb{R}^n$ acts as a "slope" vector. The value $f^*(y)$ answers the question: <b>"What is the maximum gap between the linear function $y^\top x$ and $f(x)$?"</b></p>

        <div class="insight">
          <h4>ðŸ’¡ Geometric Interpretation: The Supporting Hyperplane</h4>
          <p>Consider the affine function $h(x) = y^\top x - c$. We want to find the smallest vertical shift $c$ such that the hyperplane lies entirely below the graph of $f$, i.e., $y^\top x - c \le f(x)$ for all $x$.
          <br>Rearranging, we need $c \ge y^\top x - f(x)$. The smallest valid $c$ is $\sup_x (y^\top x - f(x))$, which is exactly $f^*(y)$.
          <br>Thus, $-f^*(y)$ is the intercept of the supporting hyperplane with slope $y$.</p>
        </div>

        <div style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center; margin: 20px 0;">
          <figure style="flex: 1; min-width: 300px; text-align: center;">
            <img src="assets/conjugate_definition_gap.png"
                 alt="Conjugate as maximum vertical gap between line yx and function f(x)"
                 style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 10px; background: white;" />
            <figcaption><i>Figure 7.1:</i> <b>The Maximum Gap View.</b> For a fixed slope $y$, we look for the point $x^*$ that maximizes the distance $yx - f(x)$. The value of this maximum distance is $f^*(y)$.</figcaption>
          </figure>
          <figure style="flex: 1; min-width: 300px; text-align: center;">
            <img src="assets/conjugate_supporting_line.png"
                 alt="Conjugate determines the intercept of the supporting line"
                 style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 10px; background: white;" />
            <figcaption><i>Figure 7.2:</i> <b>The Supporting Line View.</b> The line $y^\top x - f^*(y)$ supports the epigraph of $f$. $f^*(y)$ encodes the vertical offset required.</figcaption>
          </figure>
        </div>

        <h3>7.2 Convexity of the Conjugate</h3>
        <p><b>Theorem:</b> $f^*$ is convex, <i>even if $f$ is not</i>.</p>
        <div class="proof-box">
          <h4>Proof</h4>
          <p>For each fixed $x$, the function $g_x(y) = y^\top x - f(x)$ is affine in $y$ (a plane).
          The conjugate is the pointwise supremum of these affine functions:
          $$ f^*(y) = \sup_{x} g_x(y) $$
          Since the supremum of any collection of convex functions is convex, $f^*$ is convex.</p>
        </div>

        <h3>7.3 Fenchel's Inequality</h3>
        <p>From the definition, for any $x$ and $y$:</p>
        $$ f^*(y) \ge y^\top x - f(x) \implies \boxed{ f(x) + f^*(y) \ge y^\top x } $$
        <p>This is the <b>Fenchel-Young inequality</b>. Equality holds if and only if $y \in \partial f(x)$ (i.e., $y$ is a subgradient of $f$ at $x$). Ideally, $y = \nabla f(x)$.</p>

        <h3>7.4 Legendre Transform (Differentiable Case)</h3>
        <p>If $f$ is convex and differentiable, we can find the supremum by setting the gradient to zero.</p>
        <p>Let $\phi_y(x) = y^\top x - f(x)$. To maximize, set $\nabla_x \phi_y(x) = y - \nabla f(x) = 0$.
        Thus, the optimizer $x^*$ satisfies $\nabla f(x^*) = y$.
        $$ \boxed{ f^*(y) = y^\top x^* - f(x^*) \quad \text{where } y = \nabla f(x^*) } $$
        This is the classical <b>Legendre Transform</b>. It pairs the variable $x$ with its dual variable (gradient) $y$.
        <br><i>Identity:</i> If $f$ is strictly convex, $\nabla f^*$ is the inverse of $\nabla f$, i.e., $\nabla f^*(y) = (\nabla f)^{-1}(y) = x^*$.</p>

        <h3>7.5 1D Examples: Zero to Hero</h3>
        <p>Let's compute the conjugates of fundamental 1D functions. These building blocks appear everywhere.</p>

        <div class="example">
          <h4>1. Affine Function: $f(x) = ax + b$</h4>
          <p>$$ f^*(y) = \sup_x (yx - (ax+b)) = \sup_x ((y-a)x - b) $$
          <ul>
            <li>If $y \ne a$: The term $(y-a)x$ can go to $+\infty$ (choose $x \to \pm \infty$ depending on sign). So $f^*(y) = \infty$.</li>
            <li>If $y = a$: The expression is constant $-b$.</li>
          </ul>
          $$ f^*(y) = \begin{cases} -b & y=a \\ \infty & y \ne a \end{cases} $$
          This is the indicator function of the singleton set $\{a\}$ shifted by $-b$.</p>
        </div>

        <div class="example">
          <h4>2. Negative Log: $f(x) = -\log x$ on $(0, \infty)$</h4>
          <p>Maximize $g(x) = yx - (-\log x) = yx + \log x$.
          <br>Derivative: $g'(x) = y + 1/x = 0 \implies x^* = -1/y$.
          <br>For $x^* > 0$, we need $y < 0$. If $y \ge 0$, $yx + \log x \to \infty$ as $x \to \infty$.
          <br>Substitute $x^* = -1/y$:
          $$ f^*(y) = y(-1/y) + \log(-1/y) = -1 - \log(-y) \quad \text{for } y < 0 $$</p>
        </div>

        <div class="example">
          <h4>3. Exponential: $f(x) = e^x$</h4>
          <p>Maximize $g(x) = yx - e^x$.
          <br>Derivative: $g'(x) = y - e^x = 0 \implies e^x = y \implies x^* = \log y$.
          <br>Valid only for $y > 0$.
          <br>Substitute $x^*$: $f^*(y) = y \log y - e^{\log y} = y \log y - y$.
          <br><i>Boundary cases:</i>
          <ul>
            <li>$y=0$: $\sup_x (0 - e^x) = 0$. (Consistent with limit $y \log y \to 0$).</li>
            <li>$y < 0$: $yx - e^x \to \infty$ as $x \to -\infty$.</li>
          </ul>
          Result: $f^*(y) = y \log y - y$ for $y \ge 0$ (with $0 \log 0 = 0$).</p>
        </div>

        <div class="example">
          <h4>4. Reciprocal: $f(x) = 1/x$ on $(0, \infty)$</h4>
          <p>Maximize $g(x) = yx - 1/x$.
          <br>Derivative: $y + 1/x^2 = 0 \implies x^2 = -1/y$.
          <br>Requires $y < 0$. Then $x^* = \sqrt{-1/y}$.
          <br>Substitute:
          $$ f^*(y) = y\sqrt{-1/y} - \sqrt{-y} = -\sqrt{-y} - \sqrt{-y} = -2\sqrt{-y} \quad \text{for } y \le 0 $$</p>
        </div>

        <h3>7.6 Matrix and Vector Examples</h3>

        <div class="example">
          <h4>1. Quadratic: $f(x) = \frac{1}{2} x^\top Q x$ ($Q \succ 0$)</h4>
          <p>Maximize $y^\top x - \frac{1}{2} x^\top Q x$.
          <br>Gradient: $y - Qx = 0 \implies x^* = Q^{-1}y$.
          <br>Substitute:
          $$ f^*(y) = y^\top Q^{-1} y - \frac{1}{2} (Q^{-1}y)^\top Q (Q^{-1}y) = y^\top Q^{-1} y - \frac{1}{2} y^\top Q^{-1} y = \frac{1}{2} y^\top Q^{-1} y $$
          A quadratic conjugates to a quadratic with the inverse matrix.</p>
        </div>

        <div class="example">
          <h4>2. Log-Determinant: $f(X) = -\log \det X$ on $\mathbb{S}^n_{++}$</h4>
          <p>Maximize $\mathrm{tr}(Y X) + \log \det X$.
          <br>Gradient: $Y + X^{-1} = 0 \implies X^* = -Y^{-1}$.
          <br>Requires $Y \prec 0$ (negative definite).
          <br>Substitute:
          $$ f^*(Y) = \mathrm{tr}(Y(-Y^{-1})) + \log \det (-Y^{-1}) = \mathrm{tr}(-I) - \log \det(-Y) = -n - \log \det(-Y) $$
          Domain is $\mathbb{S}^n_{--}$.</p>
        </div>

        <div class="example">
          <h4>3. Norm: $f(x) = \|x\|$</h4>
          <p>$$ f^*(y) = \sup_x (y^\top x - \|x\|) $$
          If $\|y\|_* > 1$ (dual norm), align $x$ with $y$ and scale to infinity: supremum is $\infty$.
          If $\|y\|_* \le 1$, then $y^\top x \le \|x\|\|y\|_* \le \|x\|$, so $y^\top x - \|x\| \le 0$. Max is 0 (at $x=0$).
          $$ f^*(y) = I_{B_*}(y) = \begin{cases} 0 & \|y\|_* \le 1 \\ \infty & \text{otherwise} \end{cases} $$
          Conjugate of a norm is the indicator of the dual unit ball.</p>
        </div>

        <div class="example">
          <h4>4. Indicator of a Set: $f(x) = I_S(x)$</h4>
          <p>Let $I_S(x) = 0$ if $x \in S$, and $+\infty$ otherwise.
          $$ f^*(y) = \sup_x (y^\top x - I_S(x)) = \sup_{x \in S} (y^\top x) $$
          This is exactly the <b>support function</b> $\sigma_S(y)$ of the set $S$.
          <br><i>Connection:</i> Since $f^{**}$ is the closed convex hull of $f$, the biconjugate of the indicator $I_S$ is the indicator of the closed convex hull of $S$. This proves that the support function encodes the convex hull of a set.</p>
        </div>

        <h3>7.7 Calculus Rules</h3>
        <p>Knowing how conjugates transform allows us to derive duals for complex problems.</p>
        <table class="data-table">
          <thead>
            <tr><th>Function $g(x)$</th><th>Conjugate $g^*(y)$</th></tr>
          </thead>
          <tbody>
            <tr><td>$a f(x)$ ($a>0$)</td><td>$a f^*(y/a)$</td></tr>
            <tr><td>$f(Ax)$ ($A$ invertible)</td><td>$f^*(A^{-\top}y)$</td></tr>
            <tr><td>$f(x) + b$</td><td>$f^*(y) - b$</td></tr>
            <tr><td>$f(x - b)$</td><td>$f^*(y) + b^\top y$</td></tr>
            <tr><td>$f_1(x_1) + f_2(x_2)$</td><td>$f_1^*(y_1) + f_2^*(y_2)$ (Separable sum)</td></tr>
          </tbody>
        </table>

        <h3>7.8 Economic Interpretation</h3>
        <p>Let $f(x)$ be the <b>cost</b> to produce $x$ units of a good.
        Let $y$ be the market <b>price</b> per unit.
        <br>The <b>profit</b> maximizing producer solves:
        $$ \max_x (\text{Revenue} - \text{Cost}) = \max_x (y^\top x - f(x)) $$
        This maximum profit is exactly the conjugate $f^*(y)$.
        <br>Thus, the conjugate function maps prices to optimal profit.</p>

        <h3>7.9 The Biconjugate and Convex Hull</h3>
        <p>The <b>biconjugate</b> $f^{**} = (f^*)^*$ is the convex conjugate of the conjugate.
        <br><b>Theorem:</b> $f^{**}$ is the <b>closed convex hull</b> of $f$ (the largest closed convex function below $f$).
        <br>If $f$ is already closed and convex, then $f^{**} = f$. This duality is the foundation of the primal-dual relationship in optimization.</p>
      </section>


          <section class="section-card" id="section-8">
        <h2>8. Quasi-Convex Functions</h2>

        <h3>8.1 Definition via Sublevel Sets</h3>
        <p>A function $f: \mathbb{R}^n \to \mathbb{R}$ is <a href="#" class="definition-link">quasi-convex</a> if its domain is convex and all its <b>sublevel sets</b> are convex:</p>
        $$
        S_\alpha = \{x \in \mathrm{dom}\, f \mid f(x) \le \alpha\}
        $$
        <p>is convex for all $\alpha \in \mathbb{R}$.</p>
        <p><i>Intuition:</i> The function can have flat spots or kinks, and it doesn't need to curve "up" like a bowl, but it must be <b>unimodal</b> (no local minima that aren't global). It describes a "valley" structure.</p>

        <h3>8.2 Modified Jensen's Inequality</h3>
        <p>$f$ is quasi-convex if and only if for all $x, y \in \mathrm{dom}\, f$ and $\theta \in [0, 1]$:</p>
        $$
        f(\theta x + (1-\theta)y) \le \max\{f(x), f(y)\}
        $$
        <p>This condition says that the value on a segment never exceeds the larger of the two endpoints.</p>

        <h3>8.3 First-Order Condition</h3>
        <p>If $f$ is differentiable, it is quasi-convex if and only if:</p>
        $$ f(y) \le f(x) \implies \nabla f(x)^\top (y - x) \le 0 $$
        <p>Geometrically: The gradient at $x$ defines a supporting hyperplane to the sublevel set $S_{f(x)}$. If $y$ has a lower value, it must lie on the "downhill" side of the gradient.</p>

        <h3>8.4 Second-Order Condition</h3>
        <p>For a twice-differentiable quasi-convex function, the curvature must be non-negative in directions tangent to the level sets.</p>
        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>If $f$ is quasi-convex, then for all $x$ and vectors $y$ such that $y^\top \nabla f(x) = 0$ (tangent to level set), we must have:</p>
          $$ y^\top \nabla^2 f(x) y \ge 0 $$
          <p><i>Note:</i> This is weaker than convexity ($\nabla^2 f \succeq 0$). We only need PSD curvature on the subspace orthogonal to the gradient. We allow negative curvature in the "uphill/downhill" direction (e.g., bell curve tails).</p>
        </div>

        <h3>8.5 Operations Preserving Quasi-Convexity</h3>
        <p>Just like convex functions, we can build quasi-convex functions from simpler components. The calculus of quasi-convexity is simpler but less powerful.</p>

        <div class="theorem-box">
          <h4>1. Pointwise Maximum</h4>
          <p>If $f_1, \ldots, f_m$ are quasi-convex, then $f(x) = \max_i f_i(x)$ is quasi-convex.</p>
          <p><b>Proof:</b> The sublevel set of the maximum is the intersection of the sublevel sets:
          $$ \{x \mid \max_i f_i(x) \le \alpha\} = \bigcap_i \{x \mid f_i(x) \le \alpha\} $$
          Since the intersection of convex sets is convex, the resulting sublevel set is convex.</p>
        </div>

        <div class="theorem-box">
          <h4>2. Composition</h4>
          <p>If $g: \mathbb{R}^n \to \mathbb{R}$ is quasi-convex and $h: \mathbb{R} \to \mathbb{R}$ is <b>non-decreasing</b>, then $f(x) = h(g(x))$ is quasi-convex.</p>
          <p><b>Proof:</b>
          $$ \{x \mid h(g(x)) \le \alpha\} = \{x \mid g(x) \le \sup\{t \mid h(t) \le \alpha\}\} $$
          This is just a sublevel set of $g$ (for some level $\beta$), which is convex.</p>
          <p><i>Example:</i> If $g(x)$ is convex (hence quasi-convex), then $\sqrt{g(x)}$ (for $g \ge 0$) is quasi-convex.</p>
        </div>

        <div class="theorem-box">
          <h4>3. Minimization?</h4>
          <p>Unlike convex functions, the partial minimization of a quasi-convex function is <b>not</b> generally quasi-convex. However, minimizing a quasi-convex function over a convex set preserves quasi-convexity in the remaining parameters under specific conditions (e.g., if $f(x, y)$ is joint quasi-convex in $y$ for fixed $x$? No, this is tricky. Stick to the basics: Max and Composition are the reliable rules).</p>
        </div>

        <h3>8.6 Examples</h3>

        <div class="example">
          <h4>1. Length of a Vector (Support Size)</h4>
          <p>Define $f(x) = \max \{i \mid x_i \ne 0\}$ (index of last nonzero element).
          <br>Sublevel set $S_k = \{x \mid x_{k+1} = \dots = x_n = 0\}$. This is a linear subspace, hence convex. Thus $f$ is quasi-convex.</p>
        </div>

        <div class="example">
          <h4>2. Linear-Fractional Function</h4>
          <p>$f(x) = \frac{a^\top x + b}{c^\top x + d}$ on $\{x \mid c^\top x + d > 0\}$.
          <br>Sublevel set $f(x) \le \alpha \iff a^\top x + b \le \alpha(c^\top x + d)$. This is a linear inequality (halfspace), so $f$ is quasi-convex (and quasi-concave).</p>
        </div>

        <div class="example">
          <h4>3. Distance Ratio</h4>
          <p>$f(x) = \frac{\|x-a\|_2}{\|x-b\|_2}$ on $\{x \mid \|x-a\| \le \|x-b\|\}$.
          <br>Condition $f(x) \le \alpha$ (for $\alpha \le 1$):
          $$ \|x-a\|^2 \le \alpha^2 \|x-b\|^2 \iff x^\top x - 2a^\top x + \|a\|^2 \le \alpha^2 (x^\top x - 2b^\top x + \|b\|^2) $$
          $$ (1-\alpha^2) x^\top x + 2(\alpha^2 b - a)^\top x + C \le 0 $$
          For $\alpha \le 1$, the coefficient of $x^\top x$ is non-negative, so this defines a convex quadratic set (ball or halfspace). Thus quasi-convex.</p>
        </div>

        <div class="example">
          <h4>4. Internal Rate of Return (IRR)</h4>
          <p>Let $c = (c_0, \dots, c_n)$ be cash flows ($c_0 < 0$). The IRR is the rate $r$ satisfying $\sum c_i (1+r)^{-i} = 0$.
          <br>We view $\text{IRR}(c)$ as a function of the cash flows $c$.
          <br>Condition $\text{IRR}(c) \ge R \iff \sum c_i (1+R)^{-i} \ge 0$ (assuming standard investment profile).
          <br>This is a linear inequality in $c$. The superlevel sets are halfspaces (convex). Thus, IRR is a <b>quasi-concave</b> function of the cash flows.</p>
        </div>

        <div class="example">
          <h4>5. Bilinear Function</h4>
          <p>$f(x, y) = xy$ on $\mathbb{R}^2_{++}$ is quasi-concave.
          <br>Superlevel set $xy \ge \alpha \iff y \ge \alpha/x$. The region above the hyperbola $y=1/x$ is convex.</p>
        </div>

      </section>


    <section class="section-card" id="section-9">
      <h2>9. The Convex Function Toolkit: Canonical Examples</h2>

      <p>This section provides a "deep dive" into a few fundamental convex functions. Mastering the properties of these functions (and how to prove them) is essential for recognizing convexity in more complex applications.</p>

      <h3>9.1 Log-Sum-Exp</h3>
      <p>The function $f(x) = \log\left(\sum_{i=1}^n e^{x_i}\right)$ is the "soft max" function.</p>

      <h4>(a) Soft-Max Bounds</h4>
      <p>Let $M = \max_i x_i$. We have:
      $$ \max_i x_i \le f(x) \le \max_i x_i + \log n $$
      <i>Proof:</i> Factor out $e^M$: $\log(\sum e^{x_i}) = \log(e^M \sum e^{x_i-M}) = M + \log(\sum e^{x_i-M})$. Since $x_i \le M$, $e^{x_i-M} \le 1$. The sum has one term equal to 1, and $n-1$ terms $\le 1$. Thus $1 \le \sum e^{x_i-M} \le n$. Taking logs gives $0 \le \log(\sum) \le \log n$.</p>

      <h4>(b) Gradient and Hessian</h4>
      <p>The gradient is the softmax vector $p$ where $p_i = e^{x_i} / \sum e^{x_k}$.
      $$ \nabla f(x) = p, \quad \sum p_i = 1, \ p_i > 0 $$
      The Hessian is the covariance matrix of a distribution $p$:
      $$ \nabla^2 f(x) = \mathrm{diag}(p) - pp^\top $$</p>

      <h4>(c) Convexity via Variance</h4>
      <p>For any vector $v$, the quadratic form is:
      $$ v^\top \nabla^2 f(x) v = \sum p_i v_i^2 - (\sum p_i v_i)^2 = \mathbb{E}_p[V^2] - (\mathbb{E}_p[V])^2 = \mathrm{Var}_p(V) \ge 0 $$
      Since variance is always non-negative, the Hessian is PSD.</p>

      <h3>9.2 Geometric Mean</h3>
      <p>The function $G(x) = (\prod_{i=1}^n x_i)^{1/n}$ is concave on $\mathbb{R}^n_{++}$.</p>

      <h4>Proof via Homogeneity + Log-Concavity</h4>
      <p>This is a powerful general technique. A function $f > 0$ is concave if it is 1-homogeneous ($f(tx)=tf(x)$) and log-concave ($\log f$ is concave).</p>
      <div class="proof-box">
        <h4>Lemma: 1-Homogeneous + Log-Concave $\implies$ Concave</h4>
        <p><b>Step 1: Superadditivity.</b> We show $f(x+y) \ge f(x) + f(y)$.
        Let $u = x/f(x)$ and $v = y/f(y)$, so $f(u)=f(v)=1$.
        Consider the convex combination with $\lambda = \frac{f(x)}{f(x)+f(y)}$.
        $$ \lambda u + (1-\lambda)v = \frac{x+y}{f(x)+f(y)} $$
        By log-concavity, $f(\lambda u + (1-\lambda)v) \ge f(u)^\lambda f(v)^{1-\lambda} = 1$.
        By homogeneity, $f(\frac{x+y}{f(x)+f(y)}) = \frac{f(x+y)}{f(x)+f(y)} \ge 1 \implies f(x+y) \ge f(x)+f(y)$.</p>
        <p><b>Step 2: Concavity.</b>
        $f(\theta x + (1-\theta)y) \ge f(\theta x) + f((1-\theta)y) = \theta f(x) + (1-\theta)f(y)$.</p>
      </div>
      <p>Since $G(x)$ is 1-homogeneous and $\log G(x) = \frac{1}{n}\sum \log x_i$ is concave (sum of concave logs), $G(x)$ is concave.</p>

      <h3>9.3 Log-Determinant</h3>
      <p>The function $f(X) = \log \det X$ is concave on $\mathbb{S}^n_{++}$.</p>
      <p><b>Proof via Line Restriction:</b> Restrict to $Z(t) = X + tV$ ($X \succ 0$).
      $$ g(t) = \log \det (X + tV) = \log \det (X^{1/2}(I + tX^{-1/2}VX^{-1/2})X^{1/2}) $$
      Let eigenvalues of symmetric $X^{-1/2}VX^{-1/2}$ be $\lambda_i$.
      $$ g(t) = \log \det X + \sum \log(1 + t\lambda_i) $$
      $$ g''(t) = -\sum \frac{\lambda_i^2}{(1+t\lambda_i)^2} \le 0 $$
      Since the second derivative is non-positive along any line, $f$ is concave.</p>

      <h3>9.4 Geometric vs Arithmetic Mean Cone</h3>
      <p>The set $K_\alpha = \{x \in \mathbb{R}^n_{++} \mid G(x) \ge \alpha A(x)\}$ for $\alpha \in [0, 1]$ is a convex cone.</p>
      <p><b>Proof:</b> $G(x)$ is concave and $A(x)$ is linear (hence concave). Thus $h(x) = G(x) - \alpha A(x)$ is a concave function.
      The set is the 0-superlevel set of a concave function $\{x \mid h(x) \ge 0\}$, which is convex.
      Since both $G$ and $A$ are homogeneous, the set is closed under scaling, making it a cone.</p>

      <h3>9.5 Matrix-Fractional Function</h3>
      <p>The function $f(x, Y) = x^\top Y^{-1} x$ is defined for $x \in \mathbb{R}^n$ and $Y \in \mathbb{S}^n_{++}$. It is <b>jointly convex</b> in $x$ and $Y$.</p>

      <h4>Derivation: Epigraph via Schur Complement</h4>
      <p>The epigraph condition is $t \ge x^\top Y^{-1} x$ with $Y \succ 0$.
      <br>Using the <a href="../00-linear-algebra-primer/index.html#section-5">Schur Complement Lemma</a> (from Lecture 00), this scalar inequality is equivalent to the Linear Matrix Inequality (LMI):
      $$ \begin{bmatrix} Y & x \\ x^\top & t \end{bmatrix} \succeq 0, \quad Y \succ 0 $$
      Since the set of PSD matrices is convex, the epigraph of $f$ is the intersection of the PSD cone with a linear subspace (of block matrices). Thus, the epigraph is convex, so $f$ is convex.</p>

      <div class="insight">
        <h4>Alternative View: Variational Form</h4>
        <p>We can also express $f$ as a supremum of linear functions. For fixed $Y \succ 0$:
        $$ \sup_{z \in \mathbb{R}^n} (2z^\top x - z^\top Y z) = x^\top Y^{-1} x $$
        (The optimal $z$ is $Y^{-1}x$).
        <br>Thus $f(x, Y) = \sup_z (2z^\top x - \mathrm{tr}(zz^\top Y))$.
        <br>The term inside the supremum is linear in $x$ and linear in $Y$. The pointwise supremum of linear functions is convex.</p>
      </div>
    </section>

    <section class="section-card" id="section-10">
      <h2>10. Review & Cheat Sheet</h2>

      <h3>Common Convex Functions Reference</h3>


        <p>This table provides a quick reference for recognizing convex functions. Memorize these patternsâ€”they appear constantly in optimization.</p>

        <table class="data-table" style="width: 100%; margin-top: 16px;">
          <thead>
            <tr>
              <th>Function</th>
              <th>Domain</th>
              <th>Notes</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>$x^p$</td>
              <td>$\mathbb{R}_+$ if $p \ge 1$ or $p \le 0$</td>
              <td>Convex on $\mathbb{R}_+$ for $p \ge 1$ or $p \le 0$; concave for $0 \le p \le 1$</td>
            </tr>
            <tr>
              <td>$e^{ax}$</td>
              <td>$\mathbb{R}$</td>
              <td>Convex for any $a \in \mathbb{R}$</td>
            </tr>
            <tr>
              <td>$-\log(x)$</td>
              <td>$\mathbb{R}_{++}$</td>
              <td>Convex; $\log(x)$ is concave</td>
            </tr>
            <tr>
              <td>$x \log(x)$</td>
              <td>$\mathbb{R}_{++}$</td>
              <td>Convex (negative entropy term)</td>
            </tr>
            <tr>
              <td>$\|x\|_p$</td>
              <td>$\mathbb{R}^n$</td>
              <td>Convex for $p \ge 1$; not a norm for $p < 1$</td>
            </tr>
            <tr>
              <td>$\|x\|_2^2 = x^\top x$</td>
              <td>$\mathbb{R}^n$</td>
              <td>Convex (quadratic with PSD Hessian $2I$)</td>
            </tr>
            <tr>
              <td>$x^\top A x$</td>
              <td>$\mathbb{R}^n$</td>
              <td>Convex if $A \succeq 0$</td>
            </tr>
            <tr>
              <td>$\log(\sum_{i=1}^n e^{x_i})$</td>
              <td>$\mathbb{R}^n$</td>
              <td>Log-sum-exp: smooth approximation to $\max\{x_1, \ldots, x_n\}$</td>
            </tr>
            <tr>
              <td>$-\log(\det(X))$</td>
              <td>$\mathbb{S}^n_{++}$</td>
              <td>Negative log-determinant: convex on PD matrices</td>
            </tr>
            <tr>
              <td>$\lambda_{\max}(X)$</td>
              <td>$\mathbb{S}^n$</td>
              <td>Maximum eigenvalue: convex (supremum of linear functions)</td>
            </tr>
            <tr>
              <td>$\mathrm{tr}(X^p)$</td>
              <td>$\mathbb{S}^n_+$</td>
              <td>Convex for $p \ge 1$ or $p \le 0$ on PSD matrices</td>
            </tr>
          </tbody>
        </table>

        <h3>9.1 Operations and Their Effects</h3>
        <table class="data-table" style="width: 100%; margin-top: 16px;">
          <thead>
            <tr>
              <th>Operation</th>
              <th>Preserves Convexity?</th>
              <th>Conditions</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Nonnegative weighted sum</td>
              <td>âœ… Yes</td>
              <td>Always</td>
            </tr>
            <tr>
              <td>Pointwise maximum/supremum</td>
              <td>âœ… Yes</td>
              <td>Always</td>
            </tr>
            <tr>
              <td>Composition $h(g(x))$</td>
              <td>âœ… Yes</td>
              <td>If $g$ convex, $h$ convex non-decreasing</td>
            </tr>
            <tr>
              <td>Affine transformation $f(Ax+b)$</td>
              <td>âœ… Yes</td>
              <td>Always</td>
            </tr>
            <tr>
              <td>Minimization $\inf_y f(x,y)$</td>
              <td>âœ… Yes</td>
              <td>If $f$ convex in $(x,y)$</td>
            </tr>
            <tr>
              <td>Perspective</td>
              <td>âœ… Yes</td>
              <td>Always</td>
            </tr>
            <tr>
              <td>Product $f(x) \cdot g(x)$</td>
              <td>âŒ No</td>
              <td>Generally not convex</td>
            </tr>
            <tr>
              <td>Pointwise minimum</td>
              <td>âŒ No</td>
              <td>Not preserved (but concave functions preserved)</td>
            </tr>
          </tbody>
        </table>
    </section>



    <section class="section-card" id="section-11">
      <h2><i data-feather="edit-3"></i> 11. Exercises</h2>

<div class="problem">
  <h3>P3.1 â€” Verify Convexity Using First-Order Conditions</h3>
  <p>Prove that $f(x) = e^x$ is convex on $\mathbb{R}$ using the first-order condition.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Tangent Underestimator:</b> The first-order condition $\nabla f(x)$ defines a global linear underestimator: $f(y) \ge f(x) + \nabla f(x)^\top (y-x)$. Geometrically, the tangent plane lies below the graph.</li>
            <li><b>Inequality Source:</b> Many famous inequalities (like $e^x \ge 1+x$) are simply statements that a specific convex function lies above its tangent line at a specific point.</li>
        </ul></div>
</div>
<div class="problem">
  <h3>P3.2 â€” Verify Convexity Using Second-Order Conditions</h3>
  <p>Show that $f(x) = \|x\|_2^2$ is convex on $\mathbb{R}^n$ using the Hessian test.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Curvature Test (Hessian):</b> The Hessian matrix $\nabla^2 f(x)$ captures the local curvature. If the curvature is "non-negative" (Positive Semidefinite) in every direction at every point, the function is globally convex ("bowl shape").</li>
            <li><b>Quadratic Flatness:</b> Quadratic functions have constant curvature (constant Hessian). If this constant curvature matrix $Q$ is PSD, the entire surface is a convex bowl.</li>
        </ul></div>
</div>
<div class="problem">
  <h3>P3.3 â€” Composition of Convex Functions</h3>
  <p>Let $f(x) = -\log(x)$ on $\mathbb{R}_{++}$ and $g(x) = e^x$ on $\mathbb{R}$. Is $h(x) = f(g(x)) = -\log(e^x) = -x$ convex?</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Composition Rule Limitations:</b> The standard rules (e.g., Convex + Increasing) are sufficient but not necessary. A composition can be convex even if the rules don't apply.</li>
            <li><b>Direct Verification:</b> When composition rules are inconclusive (e.g., Convex + Decreasing), one must fall back to the definition or derivative tests. In this case, algebraic cancellation reveals a linear (hence convex) structure.</li>
        </ul></div>
</div>
<div class="problem">
  <h3>P3.4 â€” Maximum of Convex Functions</h3>
  <p>Prove that if $f_1, f_2: \mathbb{R}^n \to \mathbb{R}$ are convex, then $f(x) = \max\{f_1(x), f_2(x)\}$ is convex.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Pointwise Maximum Property:</b> The maximum of any collection of convex functions is always convex. This operation preserves the "upward curving" nature.</li>
            <li><b>Epigraph Intersection:</b> Geometrically, the epigraph of the maximum function $f = \max(f_1, f_2)$ is the intersection of the epigraphs of $f_1$ and $f_2$. Since the intersection of convex sets is convex, the result is convex.</li>
        </ul></div>
</div>
<div class="problem">
  <h3>P3.5 â€” Strong Convexity</h3>
  <p>Show that $f(x) = \frac{1}{2}x^\top Q x$ is $\lambda_{\min}(Q)$-strongly convex if $Q \succ 0$.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Strong Convexity Definition:</b> Requires a uniform minimum curvature $m > 0$. The function must curve upward at least as fast as a quadratic parabola $m x^2$.</li>
            <li><b>Quadratic Lower Bound:</b> Strong convexity implies $f(y) \ge f(x) + \nabla f(x)^\top(y-x) + \frac{m}{2}\|y-x\|^2$, providing a tighter bound than simple convexity.</li>
            <li><b>Matrix Inequality:</b> For Hessians, $Q \succeq mI$ means the smallest eigenvalue $\lambda_{\min}(Q)$ is at least $m$.</li>
        </ul></div>
</div>
<div class="problem">
  <h3>P3.6 â€” Conjugate Function</h3>
  <p>Compute the conjugate of $f(x) = \frac{1}{2}\|x\|_2^2$.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Legendre Transform Calculation:</b> For differentiable strictly convex functions, the conjugate $f^*(y)$ is found by maximizing $y^\top x - f(x)$. The maximum occurs where the gradient matches the slope: $\nabla f(x) = y$.</li>
            <li><b>Self-Conjugacy of Quadratics:</b> The quadratic function $\frac{1}{2}\|x\|^2$ is the unique function (up to scaling) that is its own convex conjugate. This mirrors the Fourier transform property of Gaussians, establishing a fundamental link between Euclidean geometry and duality.</li>
        </ul></div>
</div>
<div class="problem">
  <h3>P3.7 â€” Quasi-Convexity</h3>
  <p>Show that $f(x) = \lceil x \rceil$ (ceiling function) is quasi-convex but not convex.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Quasi-Convexity Definition:</b> A function is quasi-convex if all its sublevel sets $S_\alpha = \{x \mid f(x) \le \alpha\}$ are convex sets. This is a weaker condition than convexity.</li>
            <li><b>Monotonicity and Steps:</b> Any monotonic function on $\mathbb{R}$ (even discontinuous ones like the ceiling function) is both quasi-convex and quasi-concave (quasilinear).</li>
            <li><b>Jensen Failure:</b> Quasi-convexity does not satisfy Jensen's inequality ($f(\text{avg}) \le \text{avg}(f)$). It only satisfies the weaker condition $f(\text{avg}) \le \max(f(a), f(b))$.</li>
        </ul></div>
</div>
<div class="problem">
  <h3>P3.8 â€” Softmax Convexity</h3>
  <p>The <b>Softmax</b> function, or Log-Sum-Exp function, is defined as $f(x) = \log\left(\sum_{i=1}^n e^{x_i}\right)$.
  <ol type="a">
    <li>Prove the bounds: $\max_i x_i \le f(x) \le \max_i x_i + \log n$.</li>
    <li>Prove that $f(x)$ is convex by interpreting the Hessian quadratic form $v^\top \nabla^2 f(x) v$ as a variance.</li>
  </ol></p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Smooth Max:</b> The bounds show that $f(x)$ approximates the maximum element within a small additive error ($\log n$). This makes it a differentiable surrogate for the non-smooth max function.</li>
            <li><b>Hessian as Covariance:</b> The Hessian of LSE is $\text{diag}(p) - pp^\top$, where $p$ is the softmax probability vector. The quadratic form $v^\top (\text{diag}(p) - pp^\top) v$ is exactly the variance of a random variable taking values $v_i$ with probabilities $p_i$. Since variance is non-negative, the Hessian is PSD.</li>
        </ul></div>

  <div class="solution-box">
    <h4>Solution</h4>

    <div class="proof-step">
      <strong>(a) Bounds:</strong>
      Let $M = \max_i x_i$. Then:
      $$ f(x) = \log\left(\sum e^{x_i}\right) = \log\left(e^M \sum e^{x_i - M}\right) = M + \log\left(\sum e^{x_i - M}\right) $$
      Since $x_i - M \le 0$, we have $e^{x_i - M} \le 1$. The sum contains one term equal to 1 (for the max) and $n-1$ terms $\le 1$.
      <br>Lower Bound: $\sum e^{x_i - M} \ge 1 \implies \log(\sum) \ge 0 \implies f(x) \ge M$.
      <br>Upper Bound: $\sum e^{x_i - M} \le n \implies \log(\sum) \le \log n \implies f(x) \le M + \log n$.
    </div>

    <div class="proof-step">
      <strong>(b) Convexity via Variance:</strong>
      First, compute the gradient $\nabla f(x)_i = \frac{e^{x_i}}{\sum e^{x_k}} = p_i$. Note $\sum p_i = 1, p_i > 0$.
      <br>The Hessian entries are $H_{ij} = p_i \delta_{ij} - p_i p_j$. Thus $H = \text{diag}(p) - pp^\top$.
      <br>For any vector $v$:
      $$ v^\top H v = \sum p_i v_i^2 - (p^\top v)^2 = \sum p_i v_i^2 - (\sum p_i v_i)^2 $$
      Let $Z$ be a discrete random variable taking value $v_i$ with probability $p_i$.
      Then $\mathbb{E}[Z] = \sum p_i v_i$ and $\mathbb{E}[Z^2] = \sum p_i v_i^2$.
      $$ v^\top H v = \mathbb{E}[Z^2] - (\mathbb{E}[Z])^2 = \mathrm{Var}(Z) \ge 0 $$
      Since the variance is always non-negative, $\nabla^2 f(x) \succeq 0$, so $f$ is convex.
    </div>
  </div>
</div>
<div class="problem">
  <h3>P3.9 â€” Concavity of the Geometric Mean</h3>
  <p>We prove that the geometric mean $G(x) = (\prod_{i=1}^n x_i)^{1/n}$ is concave on $\mathbb{R}_{++}^n$.</p>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Geometric Mean Concavity:</b> The geometric mean $G(x)$ is concave on the positive orthant. This geometric fact underpins the AM-GM inequality.</li>
            <li><b>Proof via Transformation:</b> Taking the logarithm transforms the geometric mean into the arithmetic mean of logs ($\frac{1}{n}\sum \log x_i$). Since $\log$ is concave and sum preserves concavity, $\log G(x)$ is concave. A log-concave homogeneous function is concave.</li>
        </ul>


<div class="solution-box">
            <h4>Solution</h4>

            <div class="proof-step">
              <strong>Method 1: Log-Concavity + Homogeneity (Rigorous)</strong>
              <p><b>Step 1: Check Properties.</b> $G(x)$ is homogeneous of degree 1 ($G(tx) = tG(x)$) and log-concave (since $\log G(x) = \frac{1}{n}\sum \log x_i$ is a sum of concave functions).</p>
              <p><b>Step 2: Superadditivity Lemma.</b> We prove that if $f$ is 1-homogeneous and log-concave, it is superadditive ($f(x+y) \ge f(x)+f(y)$), which implies concavity.
              <br>Let $u = x/f(x)$ and $v = y/f(y)$ so $f(u)=f(v)=1$.
              <br>Consider the convex combination with $\lambda = \frac{f(x)}{f(x)+f(y)}$:
              $$ \lambda u + (1-\lambda)v = \frac{x + y}{f(x)+f(y)} $$
              By log-concavity: $f(\lambda u + (1-\lambda)v) \ge f(u)^\lambda f(v)^{1-\lambda} = 1^\lambda 1^{1-\lambda} = 1$.
              <br>By homogeneity: $f(\frac{x+y}{f(x)+f(y)}) = \frac{f(x+y)}{f(x)+f(y)}$.
              <br>Thus $\frac{f(x+y)}{f(x)+f(y)} \ge 1 \implies f(x+y) \ge f(x) + f(y)$.</p>
              <p><b>Step 3: Concavity.</b> $f(\theta x + (1-\theta)y) \ge f(\theta x) + f((1-\theta)y) = \theta f(x) + (1-\theta)f(y)$. Thus $G$ is concave.</p></div><div class="proof-step">
              <strong>Method 2: Hessian Test</strong>
              <p>Let $f(x) = \log G(x)$. Then $\nabla f(x) = \frac{1}{n}(1/x_1, \dots, 1/x_n)$.</p>
              <p>$\nabla^2 f(x) = -\frac{1}{n} \text{diag}(1/x_1^2, \dots, 1/x_n^2)$.</p>
              <p>Since $G(x) = e^{f(x)}$, $\nabla^2 G(x) = G(x)(\nabla f \nabla f^\top + \nabla^2 f)$.</p>
              <p>For any $v$, $v^\top \nabla^2 G(x) v = \frac{G(x)}{n^2} [(\sum v_i/x_i)^2 - n \sum (v_i/x_i)^2]$.</p>
              <p>By Cauchy-Schwarz, $(\sum a_i \cdot 1)^2 \le n \sum a_i^2$. Letting $a_i = v_i/x_i$, we get $(\sum v_i/x_i)^2 \le n \sum (v_i/x_i)^2$. Thus the quadratic form is $\le 0$, so $\nabla^2 G \preceq 0$.</p></div></div></div>
</div>
<div class="problem">
  <h3>P3.10 â€” Geometric vs Arithmetic Mean Cone</h3>
  <p>Let $S_\alpha = \{x \in \mathbb{R}^n_+ \mid G(x) \ge \alpha A(x)\}$ for $\alpha \in [0, 1]$, where $A(x) = \frac{1}{n}\sum x_i$.</p>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Superlevel Sets of Concave Functions:</b> The set $\{x \mid g(x) \ge 0\}$ is convex if $g$ is a concave function (this is a standard convex set definition).</li>
            <li><b>Cone Property via Homogeneity:</b> If the defining functions of a set are homogeneous (scale linearly), the resulting set is a cone (invariant under scaling). The geometric mean cone combines these properties.</li>
        </ul>


<div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>Convexity:</strong>
              <p>We know $G(x)$ is concave and $A(x)$ is linear (hence concave). Thus $h(x) = G(x) - \alpha A(x)$ is concave (since $\alpha \ge 0$).</p>
              <p>$S_\alpha = \{x \mid h(x) \ge 0\}$ is the 0-superlevel set of a concave function, which is a convex set.</p></div><div class="proof-step">
              <strong>Cone Property:</strong>
              <p>Both $G(x)$ and $A(x)$ are homogeneous of degree 1. $G(\lambda x) = \lambda G(x)$ and $A(\lambda x) = \lambda A(x)$.</p>
              <p>If $G(x) \ge \alpha A(x)$, then $G(\lambda x) = \lambda G(x) \ge \lambda \alpha A(x) = \alpha A(\lambda x)$ for $\lambda \ge 0$. Thus $S_\alpha$ is a cone.</p></div><div class="proof-step">
              <strong>Interpretation:</strong> This is the set of vectors "not too far" from having equal components. $\alpha=1$ implies $x_1 = \dots = x_n$ (the ray $\mathbf{1}$). $\alpha=0$ is the whole orthant.</div></div></div>
</div>
<div class="problem">
  <h3>P3.11 â€” Matrix Fractional Function</h3>
  <p>Show that $f(x, Y) = x^\top Y^{-1} x$ is convex on $\mathbb{R}^n \times \mathbb{S}^n_{++}$.</p>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Joint Convexity:</b> The matrix fractional function $x^\top Y^{-1} x$ is convex in the pair $(x, Y)$ jointly, not just individually. This is a crucial property for optimizing over matrix inverses.</li>
            <li><b>Schur Complement Link:</b> The epigraph condition $x^\top Y^{-1} x \le t$ can be rewritten as the LMI $\begin{bmatrix} Y & x \\ x^\top & t \end{bmatrix} \succeq 0$ using the Schur complement, proving convexity directly via PSD cone properties.</li>
        </ul>


<div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>Method 1: Epigraph via Schur Complement</strong>
              <p>The epigraph condition is $x^\top Y^{-1} x \le t$ with $Y \succ 0$.</p>
              <p>Using the Schur complement lemma, this is equivalent to:</p>
              $$ \begin{bmatrix} Y & x \\ x^\top & t \end{bmatrix} \succeq 0, \quad Y \succ 0 $$
              <p>This is a Linear Matrix Inequality (LMI) in variables $(x, Y, t)$, which defines a convex set. Thus the epigraph is convex.</p></div><div class="proof-step">
              <strong>Method 2: Supremum of Affine Functions</strong>
              <p>For fixed $Y \succ 0$, maximize $g_z(x) = 2z^\top x - z^\top Y z$ over $z$. The max is at $z = Y^{-1}x$, value $x^\top Y^{-1} x$.</p>
              <p>So $f(x, Y) = \sup_z (2z^\top x - \mathrm{tr}(zz^\top Y))$.</p>
              <p>Inside the sup is a function linear in $x$ and linear in $Y$. The supremum of affine functions is convex.</p></div></div></div>
</div>
<div class="problem">
  <h3>P3.12 â€” Definition of Convexity on $\mathbb{R}$</h3>
  <p>Let $f: \mathbb{R} \to \mathbb{R}$ be convex. Prove the following properties:</p>
          <ol type="a">
            <li><b>Chord Property:</b> $f(x) \le \frac{b-x}{b-a}f(a) + \frac{x-a}{b-a}f(b)$ for $a < x < b$.</li>
            <li><b>Slope Monotonicity:</b> $\frac{f(x)-f(a)}{x-a} \le \frac{f(b)-f(a)}{b-a} \le \frac{f(b)-f(x)}{b-x}$.</li>
            <li><b>Derivative Monotonicity:</b> If differentiable, $f'(a) \le \frac{f(b)-f(a)}{b-a} \le f'(b)$.</li>
            <li><b>Second Derivative:</b> If twice differentiable, $f''(x) \ge 0$.</li>
          </ol>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Secant Slope Monotonicity:</b> For a convex function, the slope of the secant line connecting $(a, f(a))$ to $(x, f(x))$ is non-decreasing as $x$ increases.</li>
            <li><b>Geometric Meaning:</b> As you move to the right, the function gets steeper (or less steep downwards). This monotonicity of slopes allows defining derivatives almost everywhere and is equivalent to the non-negative second derivative condition $f''(x) \ge 0$.</li>
        </ul>


<div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>(a) Chord Property:</strong>
              <p>Write $x$ as a convex combination: $x = \lambda a + (1-\lambda)b$ where $\lambda = \frac{b-x}{b-a}$.</p>
              <p>By convexity: $f(x) \le \lambda f(a) + (1-\lambda) f(b)$, which matches the formula.</p></div><div class="proof-step">
              <strong>(b) Slope Monotonicity:</strong>
              <p>Rearrange (a): multiply by $b-a$, subtract $(b-a)f(a)$:</p>
              $$ (b-a)(f(x)-f(a)) \le (x-a)(f(b)-f(a)) \implies \frac{f(x)-f(a)}{x-a} \le \frac{f(b)-f(a)}{b-a} $$
              <p>The second inequality follows by symmetry or applying the first to the interval $[x, b]$ with point $x$.</p></div><div class="proof-step">
              <strong>(c) & (d) Derivatives:</strong>
              <p>Taking limit $x \to a$ in (b) gives $f'(a) \le \frac{f(b)-f(a)}{b-a}$. Taking limit $x \to b$ gives the other side.</p>
              <p>This implies $f'$ is non-decreasing ($f'(a) \le f'(b)$). A differentiable function with non-decreasing derivative has $f'' \ge 0$.</p></div></div></div>
</div>
<div class="problem">
  <h3>P3.13 â€” Integral Characterization</h3>
  <p>Show $f$ is convex iff $\int_0^1 f(x + \lambda(y-x)) d\lambda \le \frac{f(x)+f(y)}{2}$.</p>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Hermite-Hadamard Context:</b> This integral inequality is one side of the famous Hermite-Hadamard inequality, stating that the average value of a convex function over an interval is less than the average of the function values at the endpoints.</li>
            <li><b>Midpoint Convexity:</b> The integral condition essentially averages the midpoint convexity property over the domain. Midpoint convexity, combined with continuity (implied by integrability here), guarantees full convexity.</li>
        </ul>


<div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>($\Rightarrow$) Convex implies Inequality:</strong>
              <p>Let $\phi(\lambda) = f(x + \lambda(y-x))$. If $f$ is convex, $\phi$ is convex on $[0,1]$.</p>
              <p>Thus $\phi(\lambda) \le (1-\lambda)\phi(0) + \lambda\phi(1)$.</p>
              <p>Integrate: $\int_0^1 \phi(\lambda) d\lambda \le \phi(0)\int(1-\lambda) + \phi(1)\int \lambda = \frac{1}{2}\phi(0) + \frac{1}{2}\phi(1)$.</p></div><div class="proof-step">
              <strong>($\Leftarrow$) Inequality implies Convex:</strong>
              <p>Applying the inequality to any sub-segment implies the <b>midpoint convexity</b> property: $\int_a^b \phi \le \frac{b-a}{2}(\phi(a)+\phi(b))$ implies $\phi(\frac{a+b}{2}) \le \frac{\phi(a)+\phi(b)}{2}$ (by considering limits of integrals near midpoint).</p>
              <p>Midpoint convexity + continuity implies convexity.</p></div></div></div>
</div>
<div class="problem">
  <h3>P3.14 â€” Running Average</h3>
  <p>If $f: \mathbb{R} \to \mathbb{R}$ is convex with $f(0) \le 0$, show $F(x) = \frac{1}{x} \int_0^x f(t) dt$ is convex for $x > 0$.</p>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Convexity of Integral Transforms:</b> The running average $F(x)$ transforms a convex function $f$ into another convex function. This is a specific instance of an integral operator preserving convexity.</li>
            <li><b>Proof Strategy:</b> By changing variables ($t=sx$), we express $F(x)$ as an integral of scaled functions $f(sx)$. Since $f(sx)$ is convex in $x$ for fixed $s$, and the integral (sum) preserves convexity, the result follows.</li>
        </ul>


<div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>Change of Variables:</strong>
              <p>Let $t = sx$, so $dt = x ds$. Range $0 \to x$ becomes $0 \to 1$.</p>
              $$ F(x) = \frac{1}{x} \int_0^1 f(sx) x ds = \int_0^1 f(sx) ds $$</div><div class="proof-step">
              <strong>Convexity Check:</strong>
              <p>For fixed $s \in [0, 1]$, the function $g_s(x) = f(sx)$ is convex in $x$ (composition of convex $f$ with linear map).</p>
              <p>$F(x)$ is a non-negative weighted sum (integral) of convex functions $g_s(x)$, so $F(x)$ is convex.</p></div><div class="proof-step">
              <strong>Interpretation:</strong> This is an expectation: $F(x) = \mathbb{E}[f(Sx)]$ where $S \sim U[0,1]$. Expectation preserves convexity.</div></div></div>
</div>
    <div class="problem">
  <h3>P3.15 â€” Fenchel's Inequality and Biconjugate</h3>
  <p>Let $f(x) = \frac{1}{2}x^\top Q x$ for $Q \in \mathbb{S}^n_{++}$.
  <ol type="a">
    <li>Verify Fenchel's inequality $f(x) + f^*(y) \ge x^\top y$ for this specific function. Under what condition does equality hold?</li>
    <li>Compute the biconjugate $f^{**}(x)$ directly from $f^*(y)$ and show it equals $f(x)$.</li>
  </ol></p>

  <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Equality Condition:</b> Fenchel's inequality becomes an equality ($f(x) + f^*(y) = x^\top y$) if and only if $y$ is the gradient (or subgradient) of $f$ at $x$. For a quadratic, this means $y = Qx$.</li>
        <li><b>Fenchel-Moreau Theorem:</b> For a closed convex function like a quadratic, $f^{**} = f$. Computing the conjugate twice returns the original function, confirming its convexity and closedness.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>(a) Verification:</strong>
      $f(x) = \frac{1}{2}x^\top Q x$, and we know $f^*(y) = \frac{1}{2}y^\top Q^{-1} y$.
      Sum: $LHS = \frac{1}{2}x^\top Q x + \frac{1}{2}y^\top Q^{-1} y$.
      To check $LHS \ge x^\top y$, consider the vector $z = Q^{1/2}x - Q^{-1/2}y$.
      $\|z\|^2 = (Q^{1/2}x - Q^{-1/2}y)^\top (Q^{1/2}x - Q^{-1/2}y)$
      $= x^\top Q x - 2 x^\top Q^{1/2} Q^{-1/2} y + y^\top Q^{-1} y$
      $= 2 f(x) - 2 x^\top y + 2 f^*(y) \ge 0$.
      Thus $f(x) + f^*(y) \ge x^\top y$.
      Equality holds iff $z=0 \iff Q^{1/2}x = Q^{-1/2}y \iff Qx = y$.
    </div>
    <div class="proof-step">
      <strong>(b) Biconjugate:</strong>
      $f^{**}(x) = (f^*)^*(x) = \sup_y (x^\top y - f^*(y)) = \sup_y (x^\top y - \frac{1}{2}y^\top Q^{-1} y)$.
      This is the conjugate of a quadratic with matrix $Q^{-1}$.
      Using the formula for quadratic conjugate (inverse of the matrix):
      matrix is $(Q^{-1})^{-1} = Q$.
      Thus $f^{**}(x) = \frac{1}{2}x^\top Q x = f(x)$.
    </div>
  </div>
</div>
</section>



    <section class="section-card" id="section-12">
      <h2>12. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required Reading:</strong> Boyd & Vandenberghe, Chapter 3 (Convex Functions).</li>
        <li><strong>Supplementary:</strong> Rockafellar, <em>Convex Analysis</em>, Part II.</li>
        <li><strong>Interactive:</strong> <a href="#widget-convex-function-inspector">Convex Function Inspector</a>.</li>
      </ul>
    </section>


    <footer class="site-footer">
      <div class="container">
        <p>Â© <span id="year"></a> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/ui.js"></script>
<script src="../../static/js/toc.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initConvexFunctionInspector } from './widgets/js/convex-function-inspector.js';
    initConvexFunctionInspector('widget-convex-function-inspector');
  </script>
  <script type="module">
    import { initHessianHeatmap } from './widgets/js/hessian-heatmap.js';
    initHessianHeatmap('widget-hessian-heatmap');
  </script>
  <script type="module">
    import { initOperationsPreserving } from './widgets/js/operations-preserving.js';
    initOperationsPreserving('widget-operations-preserving');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
