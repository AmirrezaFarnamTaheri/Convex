<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>03. Convex Functions: Characterization & Operations â€” Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">
          <img src="../../static/assets/branding/logo.svg" alt="Logo" />
          <span>Convex Optimization</a>
        </a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../02-convex-sets/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../04-convex-opt-problems/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <!-- Main content -->
  <div class="lecture-container">
    <aside class="sidebar">
      <div id="toc-container">
        <h2>Table of Contents</h2>
        <nav id="toc"></nav>
      </div>
    </aside>
    <main class="lecture-content">
  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header card-v2">
      <h1>03. Convex Functions: Definitions, Characterizations, and Operations</h1>
      <div class="lecture-meta">
        <span>Date: 2025-11-04</a>
        <span>Duration: 90 min</a>
        <span>Tags: functions, theory, optimization, characterization</a>
      </div>
      <div class="lecture-summary">
        <p><strong>Overview:</strong> This lecture develops the theory of convex functions, which form the foundation for convex optimization. We present multiple equivalent characterizations (Jensen's inequality, epigraph, first-order conditions, second-order conditions), prove key properties, and establish operations that preserve convexity. The lecture culminates with a toolkit for recognizing and constructing convex functions in practice.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-primer/index.html">Lecture 00: Linear Algebra</a> (gradients, Hessians, eigenvalues) and <a href="../02-convex-sets/index.html">Lecture 02: Convex Sets</a> (convex sets, epigraphs, operations).</p>
        <p><strong>Forward Connections:</strong> Convex functions are the building blocks of convex optimization problems. First-order and second-order conditions lead directly to optimality conditions (Lecture 04-05). Operations preserving convexity enable sophisticated modeling (Lecture 06-07).</p>
      </div>
    </header>

    <section class="card-v2">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <ul>
        <li><b>Understand Multiple Characterizations:</b> State and prove equivalences between Jensen's inequality, epigraph convexity, first-order conditions, and second-order conditions.</li>
        <li><b>Apply First-Order Conditions:</b> Use the tangent line property to verify convexity and derive inequalities.</li>
        <li><b>Use Second-Order Conditions:</b> Verify convexity via Hessian positive semidefiniteness.</li>
        <li><b>Understand Strong Convexity:</b> Define strong convexity and explain its importance for convergence rates and uniqueness.</li>
        <li><b>Recognize Convexity-Preserving Operations:</b> Apply nonnegative combinations, compositions, pointwise maxima, and perspective operations to construct complex convex functions.</li>
        <li><b>Understand the Conjugate Function:</b> Define the convex conjugate (Fenchel conjugate) and understand its role in duality theory.</li>
        <li><b>Apply Quasi-convexity:</b> Distinguish convex from quasi-convex functions and understand when quasi-convexity suffices for optimization.</li>
      </ul>
    </section>

    <article>
      <section class="card-v2" id="section-1">
        <h2>1. Definition and Basic Properties</h2>

        <h3>1.1 Convex Functions: The Core Definition</h3>
        <p>A function $f: \mathbb{R}^n \to \mathbb{R}$ is <a href="#" class="definition-link" data-term="convex function">convex</a> if its domain $\mathrm{dom}\, f$ is a convex set, and for all $x, y \in \mathrm{dom}\, f$ and all $\theta \in [0, 1]$:</p>
        $$
        \boxed{ f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) }
        \tag{1}
        $$
        <p>This basic inequality states that the function value at a convex combination of points is at most the corresponding convex combination of function values. Geometrically, the <b>chord</b> connecting any two points $(x, f(x))$ and $(y, f(y))$ on the graph lies <b>above</b> the graph.</p>

        <div class="insight">
          <h4>ðŸ’¡ Geometric Intuition</h4>
          <p>The chord connecting any two points on the graph of $f$ lies above (or on) the graph. Equivalently, $f$ "curves upward"â€”there are no "dents" or concave regions.</p>
        </div>

        <h3>1.2 Jensen's Inequality: The General Form</h3>
        <p>The basic definition involving two points generalizes to any number of points and even to integrals (expectations).</p>

        <h4>Finite Form</h4>
        <p>If $f$ is convex, $x_1, \dots, x_k \in \mathrm{dom}\, f$, and $\theta_1, \dots, \theta_k \ge 0$ with $\sum \theta_i = 1$, then:</p>
        $$
        f\left(\sum_{i=1}^k \theta_i x_i\right) \le \sum_{i=1}^k \theta_i f(x_i)
        $$
        <p><b>Proof Sketch (Induction):</b>
        <br>For $k=2$, this is the definition. Assume it holds for $k-1$.
        <br>Write $\sum_{i=1}^k \theta_i x_i = \theta_k x_k + (1-\theta_k) \sum_{i=1}^{k-1} \frac{\theta_i}{1-\theta_k} x_i$.
        <br>Apply the 2-point definition to combine $x_k$ and the aggregate term, then use the inductive hypothesis on the sum of $k-1$ terms.</p>

        <h4>Expectation Form</h4>
        <p>This generalizes to random variables. If $Z$ is a random variable taking values in $\mathrm{dom}\, f$, and $f$ is convex, then:</p>
        $$
        \boxed{ f(\mathbb{E}[Z]) \le \mathbb{E}[f(Z)] }
        $$
        <p>provided the expectations exist.
        <br><i>Interpretation:</i> For a convex cost function, the cost of the average scenario is less than or equal to the average cost of all scenarios. Uncertainty typically increases expected cost for convex functions.</p>

        <h3>1.2 Strict and Strong Convexity</h3>

        <div class="subsection">
          <h4>Strictly Convex</h4>
          <p>A function $f$ is <a href="#" class="definition-link">strictly convex</a> if the inequality in Jensen's inequality is strict whenever $x \neq y$ and $\theta \in (0, 1)$:</p>
          $$
          f(\theta x + (1-\theta)y) < \theta f(x) + (1-\theta)f(y)
          $$
          <p><b>Implication:</b> Strictly convex functions have at most one minimizer (uniqueness of optimal solution).</p>
        </div>

        <div class="subsection">
          <h4>Strongly Convex</h4>
          <p>A function $f$ is <a href="#" class="definition-link">strongly convex</a> (with parameter $m > 0$) if for all $x, y \in \mathrm{dom}\, f$ and $\theta \in [0, 1]$:</p>
          $$
          f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) - \frac{m}{2} \theta(1-\theta) \|x - y\|_2^2
          $$
          <p>Equivalently, $f(x) - \frac{m}{2}\|x\|_2^2$ is convex.</p>
          <p><b>Implication:</b> Strong convexity provides a quadratic lower bound on the growth of the function, leading to fast convergence rates in optimization algorithms.</p>
        </div>

        <h3>1.3 Concave Functions</h3>
        <p>A function $f$ is <a href="#" class="definition-link">concave</a> if $-f$ is convex. Equivalently, the chord lies below the graph. All results for convex functions can be "flipped" for concave functions by negating.</p>

        <h3>1.4 Extended-Value Extension</h3>
        <p>We often extend $f$ to all of $\mathbb{R}^n$ by setting $f(x) = +\infty$ for $x \notin \mathrm{dom}\, f$. This allows us to write constraints implicitly: minimizing $f$ over $\mathbb{R}^n$ is equivalent to minimizing over $\mathrm{dom}\, f$.</p>

        <h3>1.5 Restriction to a Line</h3>
        <p>A function $f: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$ is convex if and only if its restriction to any line is convex. This is a powerful tool for proving convexity.</p>
        <div class="theorem-box">
          <h4>Theorem (Restriction to a Line)</h4>
          <p>$f$ is convex if and only if for all $x \in \mathrm{dom}\, f$ and $v \in \mathbb{R}^n$, the function $g(t) = f(x + tv)$ is convex (on its domain $\{t \mid x+tv \in \mathrm{dom}\, f\}$).</p>
        </div>
        <div class="proof-enhanced">
          <h4>Proof</h4>
          <p><b>($\Rightarrow$)</b> If $f$ is convex, then for any $t_1, t_2$ and $\theta \in [0,1]$:
          $$ g(\theta t_1 + (1-\theta)t_2) = f(x + (\theta t_1 + (1-\theta)t_2)v) = f(\theta(x+t_1 v) + (1-\theta)(x+t_2 v)) $$
          $$ \le \theta f(x+t_1 v) + (1-\theta)f(x+t_2 v) = \theta g(t_1) + (1-\theta)g(t_2) $$
          So $g$ is convex.</p>
          <p><b>($\Leftarrow$)</b> If every restriction is convex, take any $x, y \in \mathrm{dom}\, f$ and $\theta \in [0,1]$. Define the line through $x$ and $y$ by $h(t) = f(x + t(y-x))$.
          Then $h(0) = f(x)$ and $h(1) = f(y)$. By convexity of $h$:
          $$ f(\theta x + (1-\theta)y) = f(x + (1-\theta)(y-x)) = h(1-\theta) $$
          $$ \le (1-(1-\theta))h(0) + (1-\theta)h(1) = \theta f(x) + (1-\theta)f(y) $$
          Thus $f$ is convex.</p>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Inspector: Understanding Convexity</h3>
          <p><b>Explore the Equivalences:</b> Convexity can be defined in multiple ways. This unified tool lets you toggle between different perspectives to see how they relate:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Jensen (Definition):</b> The chord between any two points lies above the graph.</li>
            <li><b>Epigraph (Set Theory):</b> The region above the graph is a convex set.</li>
            <li><b>Tangent (First-Order):</b> The tangent line is always a global underestimator (for differentiable functions).</li>
            <li><b>Quadratic Bound (Strong Convexity):</b> A quadratic bowl sits below the function, pushing it up.</li>
          </ul>
          <p><i>Note:</i> Select different functions to see how non-convex functions violate these conditions!</p>
          <div id="widget-convex-function-inspector" style="width: 100%; position: relative;"></div>
        </div>
      </section>

      <section class="card-v2" id="section-2">
        <h2>2. Epigraph Characterization</h2>

        <h3>2.1 The Epigraph</h3>
        <p>The <a href="#" class="definition-link">epigraph</a> of a function $f: \mathbb{R}^n \to \mathbb{R}$ is the set of points lying on or above the graph:</p>
        $$
        \mathrm{epi}\, f = \{(x, t) \in \mathbb{R}^{n+1} \mid x \in \mathrm{dom}\, f, \ f(x) \le t\}
        $$
        <p>The epigraph "fills in" everything above the function's graph.</p>

        <h3>2.2 Convexity via Epigraph</h3>
        <div class="theorem-box">
          <h4>Theorem (Epigraph Characterization)</h4>
          <p>A function $f$ is convex <b>if and only if</b> its epigraph $\mathrm{epi}\, f$ is a convex set.</p>
        </div>

        <div class="proof-enhanced">
          <h4>Proof</h4>

          <div class="proof-step">
            <strong>($\Rightarrow$) Convex function implies convex epigraph:</strong> Suppose $f$ is convex. Take any $(x, t), (y, s) \in \mathrm{epi}\, f$ and $\theta \in [0, 1]$. We must show $(\theta x + (1-\theta)y, \theta t + (1-\theta)s) \in \mathrm{epi}\, f$.
          </div>

          <div class="proof-step">
            <strong>Step 1:</strong> Since $(x, t) \in \mathrm{epi}\, f$ and $(y, s) \in \mathrm{epi}\, f$, we have $f(x) \le t$ and $f(y) \le s$.
          </div>

          <div class="proof-step">
            <strong>Step 2:</strong> By convexity of $f$:
            $$
            f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) \le \theta t + (1-\theta)s
            $$
          </div>

          <div class="proof-step">
            <strong>Step 3:</strong> Therefore, $(\theta x + (1-\theta)y, \theta t + (1-\theta)s) \in \mathrm{epi}\, f$, proving $\mathrm{epi}\, f$ is convex.
          </div>

          <div class="proof-step">
            <strong>($\Leftarrow$) Convex epigraph implies convex function:</strong> Suppose $\mathrm{epi}\, f$ is convex. Take any $x, y \in \mathrm{dom}\, f$ and $\theta \in [0, 1]$.
          </div>

          <div class="proof-step">
            <strong>Step 1:</strong> The points $(x, f(x))$ and $(y, f(y))$ lie in $\mathrm{epi}\, f$ (on the boundary, in fact).
          </div>

          <div class="proof-step">
            <strong>Step 2:</strong> By convexity of $\mathrm{epi}\, f$, the point $(\theta x + (1-\theta)y, \theta f(x) + (1-\theta)f(y))$ also lies in $\mathrm{epi}\, f$.
          </div>

          <div class="proof-step">
            <strong>Step 3:</strong> By definition of epigraph, this means:
            $$
            f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)
            $$
            proving $f$ is convex.
          </div>
        </div>

        <div class="insight">
          <h4>âš¡ Why This Matters</h4>
          <p>The epigraph characterization lets us translate between function convexity and set convexity. Many operations on functions (e.g., infimal convolution) can be understood as operations on their epigraphs (e.g., Minkowski sum).</p>
        </div>

        <h3>2.3 Convex Hull of a Function</h3>
        <p>Just as we can define the convex hull of a set, we can define the convex hull (or convex envelope) of a function $f: \mathbb{R}^n \to \mathbb{R}$ via its epigraph.</p>
        $$ g(x) = \inf \{ t \mid (x, t) \in \text{conv epi } f \} $$
        <p><b>Geometric Intuition:</b> Imagine "shrink-wrapping" the graph of $f$ from below. The function $g$ is the <b>greatest convex underestimator</b> of $f$. This concept is crucial in relaxation techniques for non-convex optimization (e.g., replacing a non-convex constraint with its convex hull).</p>

      </section>

      <section class="card-v2" id="section-3">
        <h2>3. First-Order Conditions (Tangent Line Property)</h2>

        <h3>3.1 The First-Order Characterization</h3>
        <div class="theorem-box">
          <h4>Theorem (First-Order Condition for Convexity)</h4>
          <p>Suppose $f: \mathbb{R}^n \to \mathbb{R}$ is differentiable. Then $f$ is convex <b>if and only if</b> $\mathrm{dom}\, f$ is convex and for all $x, y \in \mathrm{dom}\, f$:</p>
          $$
          f(y) \ge f(x) + \nabla f(x)^\top (y - x)
          $$
          <p>This states that the <b>tangent line (or tangent hyperplane) at any point is a global underestimator</b> of the function.</p>
        </div>

        <div class="proof-enhanced">
          <h4>Proof</h4>

          <div class="proof-step">
            <strong>($\Rightarrow$) Convex implies first-order condition:</strong>
            <p>We use the "Restriction to a line" principle. Fix $x, y \in \mathrm{dom}\, f$ and define $v = y-x$. Consider the 1-D function $g(t) = f(x + tv)$ on $t \in [0, 1]$.</p>
            <p>Since $f$ is convex, $g$ is a convex function of one variable.</p>
          </div>

          <div class="proof-step">
            <strong>Step 1: 1D Convexity Inequality.</strong>
            For a differentiable convex function $g: \mathbb{R} \to \mathbb{R}$, the graph lies above the tangent line:
            $$ g(b) \ge g(a) + g'(a)(b-a) $$
            Set $a=0$ and $b=1$. Then:
            $$ g(1) \ge g(0) + g'(0)(1-0) $$
          </div>

          <div class="proof-step">
            <strong>Step 2: Translate back to $f$.</strong>
            <ul>
              <li>$g(1) = f(x + 1 \cdot v) = f(y)$</li>
              <li>$g(0) = f(x)$</li>
              <li>$g'(t) = \nabla f(x+tv)^\top v$, so $g'(0) = \nabla f(x)^\top (y-x)$</li>
            </ul>
          </div>

          <div class="proof-step">
            <strong>Step 3: Conclusion.</strong>
            Substituting these back into the inequality:
            $$ f(y) \ge f(x) + \nabla f(x)^\top (y-x) $$
          </div>

          <div class="proof-step">
            <strong>($\Leftarrow$) First-order condition implies convex:</strong>
            Suppose the first-order condition holds. Take $x, y \in \mathrm{dom}\, f$ and $\theta \in [0, 1]$. Let $z = \theta x + (1-\theta)y$.
          </div>

          <div class="proof-step">
            <strong>Step 1: Apply condition at $z$.</strong>
            Apply the first-order inequality at base point $z$ for targets $x$ and $y$:
            $$
            f(x) \ge f(z) + \nabla f(z)^\top (x - z) \quad (1)
            $$
            $$
            f(y) \ge f(z) + \nabla f(z)^\top (y - z) \quad (2)
            $$
          </div>

          <div class="proof-step">
            <strong>Step 2: Convex combination.</strong>
            Multiply (1) by $\theta$ and (2) by $(1-\theta)$ and sum them:
            $$
            \theta f(x) + (1-\theta)f(y) \ge f(z) + \nabla f(z)^\top \underbrace{(\theta(x - z) + (1-\theta)(y - z))}_{=0}
            $$
            The vector term vanishes because $z$ is the weighted average of $x$ and $y$.
          </div>

          <div class="proof-step">
            <strong>Step 3: Conclusion.</strong>
            $$
            \theta f(x) + (1-\theta)f(y) \ge f(z) = f(\theta x + (1-\theta)y)
            $$
            This is exactly the definition of convexity.
          </div>
        </div>

        <div class="insight">
          <h4>ðŸ”‘ Key Takeaway</h4>
          <p>For differentiable functions, convexity is equivalent to: <b>the first-order Taylor approximation always underestimates the function</b>. This is an extremely useful characterization for analysis and algorithm design.</p>
        </div>

        <h3>3.2 Consequences and Applications</h3>

        <div class="example">
          <h4>Application 1: Proving a Function is Convex</h4>
          <p>Show that $f(x) = e^x$ is convex on $\mathbb{R}$.</p>
          <p><b>Solution:</b> We verify the first-order condition. For any $x, y \in \mathbb{R}$:</p>
          $$
          f(y) = e^y, \quad f(x) + f'(x)(y - x) = e^x + e^x(y - x) = e^x(1 + y - x)
          $$
          <p>We need to show $e^y \ge e^x(1 + y - x)$ for all $x, y$. Dividing by $e^x > 0$ and letting $t = y - x$:</p>
          $$
          e^t \ge 1 + t
          $$
          <p>This is a well-known inequality (follows from the Taylor series of $e^t$ with all positive terms). Therefore, $f(x) = e^x$ is convex.</p>
        </div>

        <div class="example">
          <h4>Application 2: Optimality Condition</h4>
          <p>If $f$ is convex and differentiable, then $x^*$ minimizes $f$ over a convex set $C$ if and only if:</p>
          $$
          \nabla f(x^*)^\top (y - x^*) \ge 0 \quad \forall y \in C
          $$
          <p>This is the <b>first-order optimality condition</b>. For unconstrained problems ($C = \mathbb{R}^n$), this reduces to $\nabla f(x^*) = 0$ (the gradient vanishes).</p>
        </div>

      </section>

      <section class="card-v2" id="section-4">
        <h2>4. Second-Order Conditions (Hessian Test)</h2>

        <h3>4.1 The Second-Order Characterization</h3>
        <div class="theorem-box">
          <h4>Theorem (Second-Order Condition for Convexity)</h4>
          <p>Suppose $f: \mathbb{R}^n \to \mathbb{R}$ is twice differentiable. Then $f$ is convex <b>if and only if</b> $\mathrm{dom}\, f$ is convex and for all $x \in \mathrm{dom}\, f$:</p>
          $$
          \nabla^2 f(x) \succeq 0
          $$
          <p>That is, the <b>Hessian matrix is positive semidefinite</b> at every point.</p>
        </div>

        <div class="proof-enhanced">
          <h4>Proof: Second-Order Condition</h4>

          <div class="proof-step">
            <strong>Part 1: $(\Rightarrow)$ Convex implies PSD Hessian.</strong>
            <p>Suppose $f$ is convex. Fix any point $x$ and direction $v \in \mathbb{R}^n$. Define $g(t) = f(x + tv)$.</p>
            <ul>
              <li>Since $f$ is convex, its restriction $g$ is convex.</li>
              <li>For a twice differentiable function $g: \mathbb{R} \to \mathbb{R}$, convexity implies $g''(t) \ge 0$ for all $t$.</li>
              <li>By the chain rule, $g''(t) = v^\top \nabla^2 f(x+tv) v$.</li>
              <li>At $t=0$, we get $v^\top \nabla^2 f(x) v \ge 0$.</li>
            </ul>
            <p>Since this holds for any vector $v$, the matrix $\nabla^2 f(x)$ is positive semidefinite.</p>
          </div>

          <div class="proof-step">
            <strong>Part 2: $(\Leftarrow)$ PSD Hessian implies Convexity.</strong>
            <p>Suppose $\nabla^2 f(x) \succeq 0$ for all $x$. Fix $x, y$ and let $v = y-x$. Consider $g(t) = f(x+tv)$ for $t \in [0, 1]$.</p>
            <ul>
              <li>The second derivative is $g''(t) = v^\top \nabla^2 f(x+tv) v$.</li>
              <li>Since the Hessian is PSD everywhere, $g''(t) \ge 0$ for all $t$.</li>
              <li>A function with non-negative second derivative is convex.</li>
              <li>Therefore, $g$ is convex on $[0, 1]$.</li>
            </ul>
            <p>Since the restriction to every line segment is convex, $f$ is convex.</p>
          </div>
        </div>

        <h3>4.2 Strict and Strong Convexity via Hessian</h3>

        <div class="subsection">
          <h4>Strictly Convex</h4>
          <p>If $\nabla^2 f(x) \succ 0$ (positive definite) for all $x \in \mathrm{dom}\, f$, then $f$ is <b>strictly convex</b>.</p>
          <p><b>Note:</b> The converse is false! $f(x) = x^4$ is strictly convex but $f''(0) = 0$.</p>
        </div>

        <div class="subsection">
          <h4>Strongly Convex</h4>
          <p>If $\nabla^2 f(x) \succeq mI$ for some $m > 0$ and all $x \in \mathrm{dom}\, f$, then $f$ is <b>$m$-strongly convex</b>.</p>
          <p>Equivalently, the minimum eigenvalue of the Hessian is at least $m$ everywhere.</p>
        </div>

        <h3>4.3 Practical Verification</h3>

        <div class="example">
          <h4>Example 1: Quadratic Form</h4>
          <p>Let $f(x) = \frac{1}{2} x^\top Q x + b^\top x + c$ where $Q \in \mathbb{S}^n$. Then:</p>
          $$
          \nabla^2 f(x) = Q
          $$
          <p>Therefore, $f$ is convex if and only if $Q \succeq 0$.</p>
        </div>

        <div class="proof-enhanced">
          <h4>Example 2: Log-Sum-Exp (Full Derivation)</h4>
          <p>Let $f(x) = \log S(x)$ where $S(x) = \sum_{k=1}^n e^{x_k}$. We prove convexity by showing the Hessian is PSD.</p>

          <div class="proof-step">
            <strong>Step 1: Gradient.</strong>
            Using the chain rule: $\frac{\partial f}{\partial x_i} = \frac{1}{S} \frac{\partial S}{\partial x_i} = \frac{e^{x_i}}{S}$.
            Define the probability vector $z$ where $z_i = e^{x_i}/S$. Note $z_i > 0$ and $\sum z_i = 1$.
            Then $\nabla f(x) = z$.
          </div>

          <div class="proof-step">
            <strong>Step 2: Hessian.</strong>
            Differentiate $\frac{\partial f}{\partial x_i} = z_i$ with respect to $x_j$:
            $$ \frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial}{\partial x_j} \left(\frac{e^{x_i}}{S}\right) = \frac{S \cdot \delta_{ij} e^{x_i} - e^{x_i} e^{x_j}}{S^2} = \frac{e^{x_i}}{S}\delta_{ij} - \frac{e^{x_i}}{S}\frac{e^{x_j}}{S} $$
            $$ (\nabla^2 f(x))_{ij} = z_i \delta_{ij} - z_i z_j $$
            In matrix form: $\nabla^2 f(x) = \text{diag}(z) - zz^\top$.
          </div>

          <div class="proof-step">
            <strong>Step 3: PSD Check via Cauchy-Schwarz.</strong>
            For any $v \in \mathbb{R}^n$:
            $$ v^\top \nabla^2 f(x) v = \sum_i z_i v_i^2 - (z^\top v)^2 = \sum_i z_i v_i^2 - \left(\sum_i z_i v_i\right)^2 $$
            Let $a_i = \sqrt{z_i} v_i$ and $b_i = \sqrt{z_i}$.
            Then $\sum a_i^2 = \sum z_i v_i^2$, $\sum b_i^2 = \sum z_i = 1$, and $\sum a_i b_i = \sum z_i v_i$.
            By Cauchy-Schwarz $(\sum a_i b_i)^2 \le (\sum a_i^2)(\sum b_i^2)$:
            $$ \left(\sum z_i v_i\right)^2 \le \left(\sum z_i v_i^2\right) \cdot 1 $$
            Rearranging gives $\sum z_i v_i^2 - (\sum z_i v_i)^2 \ge 0$.
            Thus $v^\top \nabla^2 f(x) v \ge 0$, so $f$ is convex.
          </div>
        </div>

        <div class="proof-enhanced">
          <h4>Detailed Proof: Concavity of Log-Determinant</h4>
          <p>The function $f(X) = \log \det X$ is <b>concave</b> on $\mathbb{S}^n_{++}$. We use the "restriction to a line" technique to prove this rigorously.</p>

          <div class="proof-step">
            <strong>Step 1: Restrict to a line.</strong>
            Let $X \in \mathbb{S}^n_{++}$ and $V \in \mathbb{S}^n$. Consider the line $Z(t) = X + tV$. The domain is $\{t \mid X+tV \succ 0\}$. Define $g(t) = \log \det(X + tV)$.
          </div>

          <div class="proof-step">
            <strong>Step 2: Factor out $X^{1/2}$.</strong>
            Since $X \succ 0$, $X^{1/2}$ exists. Write:
            $$ X + tV = X^{1/2} (I + t X^{-1/2} V X^{-1/2}) X^{1/2} $$
            Let $Y = X^{-1/2} V X^{-1/2}$. Note that $Y$ is symmetric.
            $$ \det(X + tV) = \det(X^{1/2}) \det(I + tY) \det(X^{1/2}) = \det(X) \det(I + tY) $$
            $$ g(t) = \log \det X + \log \det(I + tY) $$
          </div>

          <div class="proof-step">
            <strong>Step 3: Diagonalize.</strong>
            Let $\lambda_1, \dots, \lambda_n$ be the eigenvalues of the symmetric matrix $Y$. Then the eigenvalues of $I + tY$ are $1 + t\lambda_i$.
            $$ \det(I + tY) = \prod_{i=1}^n (1 + t\lambda_i) $$
            $$ g(t) = \log \det X + \sum_{i=1}^n \log(1 + t\lambda_i) $$
          </div>

          <div class="proof-step">
            <strong>Step 4: Check derivatives.</strong>
            $$ g'(t) = \sum_{i=1}^n \frac{\lambda_i}{1 + t\lambda_i} $$
            $$ g''(t) = -\sum_{i=1}^n \frac{\lambda_i^2}{(1 + t\lambda_i)^2} $$
            Since squares are non-negative, $g''(t) \le 0$ for all valid $t$. Thus $g(t)$ is concave.
          </div>

          <div class="proof-step">
            <strong>Conclusion:</strong> Since the restriction to any line is concave, $f(X) = \log \det X$ is concave.
          </div>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Heatmap: Hessian Eigenvalue Analyzer</h3>
          <p><b>Visualize Convexity Through the Hessian:</b> For twice-differentiable functions, convexity is determined by the Hessian being positive semidefinite (PSD). This tool provides a visual test:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Choose a 2D function:</b> Select from various functions (quadratic, Rosenbrock, etc.)</li>
            <li><b>Heatmap display:</b> The color at each point shows the minimum eigenvalue of the Hessian $\nabla^2 f(x)$</li>
            <li><b>Interactive Probe:</b> Hover over the heatmap to see the local quadratic approximation. Observe how the local "bowl" or "saddle" shape relates to the Hessian eigenvalues.</li>
            <li><b>Interpret colors:</b>
              <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
                <li><b>Blue/Green (positive):</b> PSD Hessianâ€”locally convex (bowl)</li>
                <li><b>Red (negative):</b> Not PSDâ€”locally concave or saddle point</li>
              </ul>
            </li>
            <li><b>Global convexity:</b> A function is convex if the entire region is blue/green (no red)</li>
          </ul>
          <p><i>Practical insight:</i> This is exactly how numerical optimization libraries verify convexity in practiceâ€”by checking that all eigenvalues of the Hessian are non-negative!</p>
          <div id="widget-hessian-heatmap" style="width: 100%; height: 450px; position: relative;"></div>
        </div>

        <h3>4.4 Log-Convex and Log-Concave Functions</h3>

        <figure style="text-align: center;">
          <img src="assets/log_concave_gaussian.png"
               alt="Gaussian density vs its log"
               style="max-width: 60%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
          <figcaption><i>Figure 4.1:</i> The Gaussian density function (top) is bell-shaped and not concave. However, its logarithm (bottom) is a downward-opening parabola, which is concave. Thus, the Gaussian is log-concave.</figcaption>
        </figure>

        <h4>4.4.1 Definitions</h4>
        <p>A function $f: \mathbb{R}^n \to \mathbb{R}_{++}$ is <b>log-convex</b> if $\log f$ is convex. Equivalently:</p>
        $$ f(\theta x + (1-\theta)y) \le f(x)^\theta f(y)^{1-\theta} $$
        <p>It is <b>log-concave</b> if $\log f$ is concave. Equivalently:</p>
        $$ f(\theta x + (1-\theta)y) \ge f(x)^\theta f(y)^{1-\theta} $$

        <div class="insight">
          <h4>Key Examples</h4>
          <ul>
            <li><b>Affine:</b> $f(x) = a^\top x + b$ is log-concave on $\{x \mid a^\top x + b > 0\}$.</li>
            <li><b>Powers:</b> $x^a$ on $\mathbb{R}_{++}$ is log-convex if $a \le 0$, and log-concave if $a \ge 0$.</li>
            <li><b>Gaussian Density:</b> $f(x) \propto e^{-\frac{1}{2}(x-\mu)^\top \Sigma^{-1}(x-\mu)}$ is log-concave.</li>
            <li><b>Determinant:</b> $\det(X)$ is log-concave on $\mathbb{S}^n_{++}$ (since $\log \det$ is concave).</li>
          </ul>
        </div>

        <h4>4.4.2 Hessian Condition</h4>
        <p>Using the chain rule, $\nabla^2 \log f(x) = \frac{1}{f(x)}\nabla^2 f(x) - \frac{1}{f(x)^2}\nabla f(x)\nabla f(x)^\top$.</p>
        <p>Thus $f$ is log-concave if and only if:</p>
        $$ f(x) \nabla^2 f(x) \preceq \nabla f(x) \nabla f(x)^\top $$
        <p>(For log-convexity, the inequality is reversed).</p>

        <h4>4.4.3 Integration Preserves Log-Concavity</h4>
        <div class="theorem-box">
          <h4>Theorem (PrÃ©kopa)</h4>
          <p>If $f(x, y)$ is log-concave, then the marginal function $g(x) = \int f(x, y) dy$ is log-concave.</p>
          <p>This is a deep result with massive implications in probability. It implies that marginals of log-concave distributions (like the multivariate normal) are log-concave. It also implies that the convolution of log-concave functions is log-concave.</p>
        </div>
      </section>

      <section class="card-v2" id="section-5">
        <h2>5. Operations Preserving Convexity</h2>

        <p>One of the most powerful aspects of convex analysis is that complex convex functions can be built from simpler ones using operations that preserve convexity. This enables sophisticated modeling without sacrificing tractability.</p>

        <h3>5.1 Nonnegative Weighted Sum</h3>
        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>If $f_1, \ldots, f_m$ are convex functions and $w_1, \ldots, w_m \ge 0$, then:</p>
          $$
          f(x) = \sum_{i=1}^m w_i f_i(x)
          $$
          <p>is convex. This extends to infinite sums and integrals (provided they converge).</p>
        </div>

        <div class="proof-enhanced">
          <h4>Proof</h4>
          <p>For any $x, y$ and $\theta \in [0, 1]$:</p>
          $$
          \begin{aligned}
          f(\theta x + (1-\theta)y) &= \sum_{i=1}^m w_i f_i(\theta x + (1-\theta)y) \\
          &\le \sum_{i=1}^m w_i \big( \theta f_i(x) + (1-\theta)f_i(y) \big) \quad \text{(convexity of } f_i, \ w_i \ge 0\text{)} \\
          &= \theta \sum_{i=1}^m w_i f_i(x) + (1-\theta) \sum_{i=1}^m w_i f_i(y) \\
          &= \theta f(x) + (1-\theta) f(y)
          \end{aligned}
          $$
        </div>

        <h3>5.2 Pointwise Maximum of Convex Functions</h3>
        <p>Let $f_1, \dots, f_m: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$ be convex functions. Define their <b>pointwise maximum</b>:</p>
        $$
        f(x) = \max_{i=1,\dots,m} f_i(x)
        $$

        <div class="theorem-box">
          <h4>Claim</h4>
          <p>The pointwise maximum function $f$ is convex.</p>
        </div>

        <div class="insight">
          <h4>Intuition</h4>
          <p>Take several convex curves; at each $x$ keep the one that lies highest. The upper envelope still bends "upwards"; you never get a concave dip by taking a max of convex curves. Geometrically, the epigraph is the intersection of epigraphs.</p>
        </div>

        <div class="proof-enhanced">
          <h4>Algebraic Proof</h4>
          <div class="proof-step">
            <strong>Step 1: Definition.</strong>
            Let $x, y \in \mathbb{R}^n$ and $0 \le \theta \le 1$. We need to show $f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta) f(y)$.
          </div>
          <div class="proof-step">
            <strong>Step 2: Convexity of components.</strong>
            For each fixed $i$, convexity of $f_i$ gives:
            $$ f_i(\theta x + (1-\theta)y) \le \theta f_i(x) + (1-\theta) f_i(y) $$
          </div>
          <div class="proof-step">
            <strong>Step 3: Max over both sides.</strong>
            $$ \max_i f_i(\theta x + (1-\theta)y) \le \max_i \big( \theta f_i(x) + (1-\theta) f_i(y) \big) $$
          </div>
          <div class="proof-step">
            <strong>Step 4: Inequality for max.</strong>
            Use the inequality $\max_i (\theta a_i + (1-\theta)b_i) \le \theta \max_i a_i + (1-\theta)\max_i b_i$.
            $$ \max_i \big( \theta f_i(x) + (1-\theta) f_i(y) \big) \le \theta \max_i f_i(x) + (1-\theta)\max_i f_i(y) = \theta f(x) + (1-\theta) f(y) $$
          </div>
          <div class="proof-step">
            <strong>Conclusion:</strong> $f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta) f(y)$.
          </div>
        </div>

        <div class="proof-enhanced">
          <h4>Geometric Proof (Epigraph)</h4>
          <p>The epigraph of the max is the intersection of the epigraphs:
          $$ \mathrm{epi}\, f = \bigcap_{i=1}^m \mathrm{epi}\, f_i $$
          Since a point $(x,t)$ satisfies $t \ge \max_i f_i(x)$ if and only if $t \ge f_i(x)$ for all $i$.
          The intersection of convex sets is convex, so $\mathrm{epi}\, f$ is convex.</p>
        </div>

        <h4>Examples of Pointwise Maxima</h4>
        <div class="example">
          <h4>1. Piecewise-Linear (Max of Affine Functions)</h4>
          <p>Let $\ell_i(x) = a_i^\top x + b_i$ for $i=1,\dots,m$. Define:</p>
          $$ f(x) = \max_{1\le i\le m} (a_i^\top x + b_i) $$
          <p>Each $\ell_i$ is convex, so $f$ is convex. Geometrically, this is a polyhedral convex function (its epigraph is a polyhedron).</p>
        </div>

        <div class="example">
          <h4>2. Sum of the $r$ Largest Components</h4>
          <p>For $x \in \mathbb{R}^n$, let $x_{[1]} \ge x_{[2]} \ge \cdots \ge x_{[n]}$ denote components sorted in descending order. Define:</p>
          $$ f(x) = \sum_{k=1}^r x_{[k]} $$
          <p>This function is convex. It can be represented as the maximum over all subsets of size $r$:</p>
          $$ f(x) = \max \left\{ \sum_{j=1}^r x_{i_j} \mid 1 \le i_1 < \cdots < i_r \le n \right\} $$
          <p>Since the sum of any specific subset of coordinates is a linear function $x \mapsto c^\top x$ (where $c$ has 1s at chosen indices), $f$ is the pointwise maximum of linear functions, hence convex. This is known as the <b>Ky Fan norm</b>.</p>
        </div>

        <h3>5.3 Pointwise Supremum over an Index Set</h3>
        <p>The "max of finitely many" generalizes to the supremum over an arbitrary index set $\mathcal{A}$.</p>

        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>Let $\mathcal{A}$ be any index set. For each $y \in \mathcal{A}$, let $f(\cdot, y)$ be convex in $x$. Define:</p>
          $$ g(x) = \sup_{y \in \mathcal{A}} f(x, y) $$
          <p>Then $g(x)$ is convex.</p>
        </div>

        <div class="proof-enhanced">
          <h4>Proof</h4>
          <p>Let $x_1, x_2 \in \mathbb{R}^n$ and $\theta \in [0, 1]$.
          $$ g(\theta x_1 + (1-\theta)x_2) = \sup_y f(\theta x_1 + (1-\theta)x_2, y) $$
          $$ \le \sup_y \big( \theta f(x_1, y) + (1-\theta) f(x_2, y) \big) $$
          $$ \le \theta \sup_y f(x_1, y) + (1-\theta) \sup_y f(x_2, y) = \theta g(x_1) + (1-\theta) g(x_2) $$
          Geometrically, $\mathrm{epi}\, g = \bigcap_{y \in \mathcal{A}} \mathrm{epi}\, f(\cdot, y)$, which is an intersection of convex sets.</p>
        </div>

        <h4>Examples of Pointwise Supremum</h4>
        <div class="example">
          <h4>1. Support Function</h4>
          <p>For any set $C \subset \mathbb{R}^n$, the support function $S_C(x) = \sup_{y \in C} y^\top x$ is the pointwise supremum of linear functions $x \mapsto y^\top x$ indexed by $y \in C$. Thus $S_C$ is always convex, regardless of whether $C$ is convex.</p>
        </div>

        <div class="example">
          <h4>2. Distance to Farthest Point</h4>
          <p>Let $C \subset \mathbb{R}^n$. The function $f(x) = \sup_{y \in C} \|x - y\|$ (distance to the farthest point in $C$) is convex, because for each fixed $y$, $x \mapsto \|x - y\|$ is convex.</p>
        </div>

        <div class="example">
          <h4>3. Maximum Eigenvalue</h4>
          <p>For $X \in \mathbb{S}^n$, the maximum eigenvalue $\lambda_{\max}(X)$ is convex. By the Rayleigh-Ritz theorem:</p>
          $$ \lambda_{\max}(X) = \sup_{\|y\|_2=1} y^\top X y $$
          <p>For each fixed unit vector $y$, the map $X \mapsto y^\top X y$ is linear in $X$. Thus $\lambda_{\max}$ is the supremum of linear functions.</p>
        </div>

        <h3>5.4 Partial Minimization</h3>
        <p>We now look at minimizing over some coordinates: $g(x) = \inf_{y \in C} f(x, y)$.</p>

        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>If $f(x, y)$ is <b>jointly convex</b> in $(x, y)$ and $C$ is a convex set, then:</p>
          $$ g(x) = \inf_{y \in C} f(x, y) $$
          <p>is convex (provided $g(x) > -\infty$).</p>
        </div>

        <div class="insight">
          <h4>Geometric Picture</h4>
          <p>The epigraph of $g$ is the <b>projection</b> of the epigraph of $f$ onto the $(x, t)$ space (projecting out $y$). Since the projection of a convex set is convex, $g$ is convex.</p>
        </div>

        <div class="proof-enhanced">
          <h4>Algebraic Proof</h4>
          <p>Let $x_1, x_2$ and $\theta \in [0, 1]$. We want $g(x_\theta) \le \theta g(x_1) + (1-\theta)g(x_2)$.
          <br>Write the infimum redundantly: $g(x_\theta) = \inf_{y_1, y_2 \in C} f(x_\theta, \theta y_1 + (1-\theta)y_2)$.
          <br>By joint convexity: $f(x_\theta, \theta y_1 + (1-\theta)y_2) \le \theta f(x_1, y_1) + (1-\theta) f(x_2, y_2)$.
          <br>Taking infimum over $y_1, y_2$:
          $$ g(x_\theta) \le \theta \inf_{y_1} f(x_1, y_1) + (1-\theta) \inf_{y_2} f(x_2, y_2) = \theta g(x_1) + (1-\theta) g(x_2) $$
          </p>
        </div>

        <h4>Examples of Partial Minimization</h4>
        <div class="example">
          <h4>1. Distance to a Convex Set</h4>
          <p>For a convex set $S$, $d(x, S) = \inf_{y \in S} \|x - y\|$.
          <br>Here $f(x, y) = \|x - y\|$ is convex in $(x, y)$ and $S$ is convex. Thus $d(x, S)$ is convex.</p>
        </div>

        <div class="example">
          <h4>2. Schur Complement</h4>
          <p>Let $f(x, y) = x^\top A x + 2x^\top B y + y^\top C y$ with $C \succ 0$ and $\begin{bmatrix} A & B \\ B^\top & C \end{bmatrix} \succeq 0$.
          <br>Minimizing over $y$ yields $y^*(x) = -C^{-1}B^\top x$. Substituting back:
          $$ g(x) = \inf_y f(x, y) = x^\top (A - B C^{-1} B^\top) x $$
          Since partial minimization preserves convexity, $g(x)$ is convex, which implies the Schur complement $A - B C^{-1} B^\top \succeq 0$.
          </p>
        </div>

        <h3>5.5 Composition Rules</h3>

        <div class="theorem-box">
          <h4>1. Affine Composition</h4>
          <p>If $f: \mathbb{R}^k \to \mathbb{R}$ is convex, and $x \mapsto Ax + b$ is an affine map, then the composition $g(x) = f(Ax + b)$ is convex.</p>
          <p><b>Proof:</b>
          $$
          \begin{aligned}
          g(\theta x + (1-\theta)y) &= f(A(\theta x + (1-\theta)y) + b) \\
          &= f(\theta(Ax+b) + (1-\theta)(Ay+b)) \\
          &\le \theta f(Ax+b) + (1-\theta)f(Ay+b) \\
          &= \theta g(x) + (1-\theta)g(y)
          \end{aligned}
          $$
          </p>
          <p><b>Examples:</b></p>
          <ul>
            <li><b>Log-Barrier:</b> $f(x) = -\sum \log(b_i - a_i^\top x)$. Inner: affine $b-a^\top x$. Outer: $-\log(\cdot)$ convex. Sum of convex is convex.</li>
            <li><b>Norm of Affine:</b> $\|Ax - b\|$ is convex.</li>
          </ul>
        </div>

        <div class="theorem-box">
          <h4>2. Scalar Composition Rule ($f=h\circ g$) in Depth</h4>
          <p>Let $g: \mathbb{R}^n \to \mathbb{R}$ and $h: \mathbb{R} \to \mathbb{R}$. Then $f(x) = h(g(x))$ is convex if:</p>
          <ol>
            <li>$g$ is <b>convex</b>, and $h$ is <b>convex</b> and <b>non-decreasing</b>.</li>
            <li>$g$ is <b>concave</b>, and $h$ is <b>convex</b> and <b>non-increasing</b>.</li>
            <li>$g$ is <b>affine</b>, and $h$ is <b>convex</b> (monotonicity not required).</li>
          </ol>

          <div style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center; margin: 20px 0;">
            <figure style="flex: 1; min-width: 300px; text-align: center;">
              <img src="assets/convex_composition_case1.png"
                   alt="Composition Case 1: Convex inner + Convex Increasing outer"
                   style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 10px; background: white;" />
              <figcaption><i>Figure 5.1:</i> Case 1. The convex inner function "curves up". The increasing outer function preserves this order, and its own convexity amplifies the curvature.</figcaption>
            </figure>
            <figure style="flex: 1; min-width: 300px; text-align: center;">
              <img src="assets/convex_composition_case2.png"
                   alt="Composition Case 2: Concave inner + Convex Decreasing outer"
                   style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 10px; background: white;" />
              <figcaption><i>Figure 5.2:</i> Case 2. The concave inner function "curves down". The decreasing outer function flips this to "curve up", and its own convexity reinforces the result.</figcaption>
            </figure>
          </div>

          <div class="proof-enhanced">
            <h4>Detailed Proof of Case 1 (Convex + Non-decreasing)</h4>
            <div class="proof-step">
              <strong>Step 1: Domain Sanity.</strong>
              Let $x, y \in \mathrm{dom}\, f$. Since $h$ is convex, its domain is convex. Thus, the convex combination of $g(x)$ and $g(y)$ stays in $\mathrm{dom}\, h$.
            </div>
            <div class="proof-step">
              <strong>Step 2: Inner Convexity.</strong>
              By convexity of $g$:
              $$ g(\theta x + (1-\theta)y) \le \theta g(x) + (1-\theta)g(y) $$
              Let $u = g(\theta x + (1-\theta)y)$ and $v = \theta g(x) + (1-\theta)g(y)$. So $u \le v$.
            </div>
            <div class="proof-step">
              <strong>Step 3: Outer Monotonicity.</strong>
              Since $h$ is non-decreasing:
              $$ h(u) \le h(v) \implies h(g(\theta x + (1-\theta)y)) \le h(\theta g(x) + (1-\theta)g(y)) $$
            </div>
            <div class="proof-step">
              <strong>Step 4: Outer Convexity.</strong>
              By convexity of $h$:
              $$ h(\theta g(x) + (1-\theta)g(y)) \le \theta h(g(x)) + (1-\theta)h(g(y)) $$
            </div>
            <div class="proof-step">
              <strong>Step 5: Chain.</strong>
              $$ f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) $$
              Thus $f$ is convex.
            </div>
          </div>
        </div>

        <div class="example">
          <h4>Example: Log-Log-Sum-Exp</h4>
          <p>Consider $f(x) = -\log(-\log(\sum e^{a_i^\top x + b_i}))$.</p>
          <ul>
            <li>Inner: $g(x) = \log(\sum e^{y_i})$ is Convex (Log-Sum-Exp).</li>
            <li>Outer: $h(u) = -\log(-u)$ is Convex and Non-decreasing (for $u < 0$).</li>
            <li>Result: Convex.</li>
          </ul>
        </div>

        <div class="theorem-box">
          <h4>3. Vector Composition Rule ($f=h\circ g$) in Depth</h4>
          <p>Let $g: \mathbb{R}^n \to \mathbb{R}^k$ and $h: \mathbb{R}^k \to \mathbb{R}$. Then $f(x) = h(g_1(x), \dots, g_k(x))$ is convex if $h$ is convex and for each $i \in \{1, \dots, k\}$:</p>
          <ul>
            <li>$g_i$ is <b>convex</b> and $h$ is <b>non-decreasing</b> in the $i$-th argument.</li>
            <li>$g_i$ is <b>concave</b> and $h$ is <b>non-increasing</b> in the $i$-th argument.</li>
            <li>$g_i$ is <b>affine</b> (monotonicity in $i$-th argument not required).</li>
          </ul>

          <div class="proof-enhanced">
            <h4>Differential View (Intuition)</h4>
            <p>For $n=1$, the chain rule gives the second derivative:</p>
            $$ f''(x) = \underbrace{h''(g(x)) [g'(x)]^2}_{\ge 0 \text{ by } h \text{ convex}} + \underbrace{h'(g(x)) g''(x)}_{\text{sign depends on rule}} $$
            <ul>
              <li>If $g$ convex ($g'' \ge 0$) and $h$ increasing ($h' \ge 0$), the second term is $\ge 0$.</li>
              <li>If $g$ concave ($g'' \le 0$) and $h$ decreasing ($h' \le 0$), the product is $(-)(-) \ge 0$.</li>
            </ul>
            <p>This confirms the rule matches the condition $f''(x) \ge 0$.</p>
          </div>
        </div>

        <div class="example">
          <h4>Example: Geometric Mean</h4>
          <p>The function $f(x) = (\prod_{i=1}^k g_i(x))^{1/k}$ is concave if all $g_i(x)$ are non-negative and concave. (Primitive: Geometric mean is concave and non-decreasing).</p>
        </div>

        <h3>5.6 Perspective Function</h3>
        <div class="theorem-box">
          <h4>Definition and Property</h4>
          <p>If $f: \mathbb{R}^n \to \mathbb{R}$ is convex, then its <b>perspective</b> $g: \mathbb{R}^{n+1} \to \mathbb{R}$ defined by:</p>
          $$
          g(x, t) = t f(x/t), \quad t > 0
          $$
          <p>is convex on $\mathrm{dom}\, g = \{(x, t) \mid x/t \in \mathrm{dom}\, f, \ t > 0\}$. The perspective function preserves convexity.</p>
          <p><b>Geometric Intuition (Cone over Epigraph):</b></p>
          <p>The epigraph of the perspective function is related to the conic hull of the epigraph of $f$. Specifically, if we take the cone generated by $\text{epi}(f)$ in $\mathbb{R}^{n+2}$ and slice it at $t$, we recover the epigraph of the scaled function. Since the conic hull of a convex set is convex, and slicing preserves convexity, the perspective function is convex.</p>
        </div>

        <div class="proof-enhanced">
          <h4>Inequality Proof of Convexity</h4>
          <div class="proof-step">
            <strong>Step 1: Setup.</strong>
            Let $(x_1, t_1), (x_2, t_2) \in \text{dom } g$ and $\theta \in [0, 1]$. Let $(\bar{x}, \bar{t}) = \theta(x_1, t_1) + (1-\theta)(x_2, t_2)$.
            We need to show $g(\bar{x}, \bar{t}) \le \theta g(x_1, t_1) + (1-\theta) g(x_2, t_2)$.
          </div>
          <div class="proof-step">
            <strong>Step 2: Convex combination trick.</strong>
            Note that $\frac{\bar{x}}{\bar{t}} = \frac{\theta x_1 + (1-\theta)x_2}{\bar{t}} = \frac{\theta t_1}{\bar{t}}\frac{x_1}{t_1} + \frac{(1-\theta)t_2}{\bar{t}}\frac{x_2}{t_2}$.
            Let $\mu = \frac{\theta t_1}{\bar{t}}$. Then $1-\mu = \frac{(1-\theta)t_2}{\bar{t}}$. Thus $\frac{\bar{x}}{\bar{t}}$ is a convex combination of $\frac{x_1}{t_1}$ and $\frac{x_2}{t_2}$.
          </div>
          <div class="proof-step">
            <strong>Step 3: Apply convexity of $f$.</strong>
            $$ f\left(\frac{\bar{x}}{\bar{t}}\right) \le \mu f\left(\frac{x_1}{t_1}\right) + (1-\mu) f\left(\frac{x_2}{t_2}\right) $$
          </div>
          <div class="proof-step">
            <strong>Step 4: Scale back.</strong>
            Multiply by $\bar{t}$:
            $$ \bar{t} f\left(\frac{\bar{x}}{\bar{t}}\right) \le \bar{t}\mu f\left(\frac{x_1}{t_1}\right) + \bar{t}(1-\mu) f\left(\frac{x_2}{t_2}\right) $$
            $$ g(\bar{x}, \bar{t}) \le \theta t_1 f\left(\frac{x_1}{t_1}\right) + (1-\theta)t_2 f\left(\frac{x_2}{t_2}\right) = \theta g(x_1, t_1) + (1-\theta) g(x_2, t_2) $$
          </div>
        </div>

        <div class="example">
          <h4>Example: Power-over-Linear</h4>
          <p>The function $f(x, t) = \frac{\|x\|_p^p}{t^{p-1}}$ for $p > 1, t > 0$ is convex.
          <br><i>Derivation:</i> Factor out $t$: $f(x,t) = t \frac{\|x\|_p^p}{t^p} = t \|x/t\|_p^p$. This is the perspective of the convex function $h(u) = \|u\|_p^p$.</p>
        </div>

        <div class="example">
          <h4>Example: Quadratic-over-Linear</h4>
          <p>The function $f(x, y) = \frac{x^2}{y}$ for $y > 0$ is convex (Perspective of $x^2$).
          <br>More generally, $f(x) = \frac{\|Ax+b\|_2^2}{c^\top x + d}$ is convex (Affine composition of perspective).</p>
        </div>

        <h3>5.7 Power Functions and Homogeneity</h3>

        <p>Power functions are a rich source of examples and inequalities.</p>

        <h4>Convexity of $x^p$</h4>
        <p>The function $f(x) = x^p$ on $\mathbb{R}_{++}$ is:</p>
        <ul>
            <li><b>Convex</b> for $p \ge 1$ or $p \le 0$ ($f''(x) = p(p-1)x^{p-2} \ge 0$).</li>
            <li><b>Concave</b> for $0 \le p \le 1$ ($f''(x) \le 0$).</li>
        </ul>

        <h4>Homogeneity and Inequalities</h4>
        <p>For $p \ge 1$, $f(x) = x^p$ is convex. Using the tangent line inequality at $x=1$ ($f(x) \ge f(1) + f'(1)(x-1)$):</p>
        $$ x^p \ge 1 + p(x-1) $$
        <p>This can be used to prove that for $x, y \ge 0$ and $p \ge 1$:</p>
        $$ \boxed{ x^p + y^p \ge x + y \quad \text{for } x,y \in [0,1] } $$
        <p>Also, due to homogeneity ($(\lambda x)^p = \lambda^p x^p$), we can scale inequalities. For example, the function $f(x, y) = \frac{x^2}{y}$ arises from the homogeneity of quadratics.</p>

        <h3>5.8 What Doesn't Preserve Convexity?</h3>
        <p>A common pitfall is to assume the <b>minimum</b> of convex functions is convex. It is generally <b>not</b>.</p>
        <div class="insight">
          <h4>Counterexample</h4>
          <p>Let $f_1(x) = (x-1)^2$ and $f_2(x) = (x+1)^2$. Both are convex. Their minimum $g(x) = \min((x-1)^2, (x+1)^2)$ is "W"-shaped, with a local maximum at $x=0$, clearly nonconvex.</p>
        </div>
        <p>Similarly, minimizing over a <b>nonconvex</b> set usually breaks convexity. For example, the distance to a nonconvex set is generally nonconvex.</p>

        <h3>5.9 Summary: Mental Checklist</h3>
        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px;">
          <h4 style="margin-top: 0;">Quick Recognition Guide</h4>
          <ul style="margin-top: 0.5rem;">
            <li><b>Pointwise Max / Sup:</b> "Upper envelope". Intersection of epigraphs. (e.g., $\lambda_{\max}$, support function).</li>
            <li><b>Partial Minimization:</b> "Projection". Requires joint convexity. (e.g., distance to convex set, Schur complement).</li>
            <li><b>Composition:</b> Check curvature ("bowl vs dome") and monotonicity.</li>
            <li><b>Perspective:</b> Scales the domain and function value together; preserves convexity.</li>
          </ul>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Builder: Operations Preserving Convexity</h3>
          <p><b>Build Complex Convex Functions from Simple Ones:</b> This interactive tool lets you combine convex building blocks using convexity-preserving operations:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Start with basic functions:</b> $x^2$, $|x|$, $e^x$, $-\log(x)$, etc.</li>
            <li><b>Apply operations:</b>
              <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
                <li>Nonnegative weighted sum: $w_1 f_1 + w_2 f_2$ (sliders for weights)</li>
                <li>Pointwise maximum: $\max\{f_1, f_2, \ldots\}$</li>
                <li>Composition: $h(g(x))$ with appropriate monotonicity</li>
                <li>Affine transformations: $f(Ax + b)$</li>
              </ul>
            </li>
            <li><b>Visualize the result:</b> See the graph of the combined function</li>
            <li><b>Verify convexity:</b> Test via Jensen's inequality and Hessian eigenvalues</li>
            <li><b>Export formula:</b> Get the mathematical expression for the constructed function</li>
          </ul>
          <p><i>Modeling power:</i> Almost every convex function you'll encounter in practice can be built using these operations! This is the foundation of disciplined convex programming (DCP) in tools like CVXPY.</p>
          <div id="widget-operations-preserving" style="width: 100%; height: 500px; position: relative;"></div>
        </div>
      </section>
<section class="card-v2" id="section-6">
        <h2>6. Strong Convexity and Smoothness</h2>

        <h3>6.1 Strong Convexity: Definition and Properties</h3>
        <p>A function $f$ is <b>$m$-strongly convex</b> (with parameter $m > 0$) if for all $x, y$ and $\theta \in [0, 1]$:</p>
        $$
        f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) - \frac{m}{2} \theta(1-\theta) \|x - y\|_2^2
        $$
        <p>Equivalently, $f(x) - \frac{m}{2}\|x\|_2^2$ is convex.</p>

        <h3>6.2 Characterizations of Strong Convexity</h3>

        <div class="theorem-box">
          <h4>Equivalent Conditions (for differentiable $f$)</h4>
          <p>The following are equivalent:</p>
          <ol>
            <li>$f$ is $m$-strongly convex.</li>
            <li><b>First-order with quadratic term:</b> $f(y) \ge f(x) + \nabla f(x)^\top (y - x) + \frac{m}{2}\|y - x\|_2^2$</li>
            <li><b>Hessian bound:</b> (If $f$ is twice differentiable) $\nabla^2 f(x) \succeq mI$ for all $x$.</li>
            <li><b>Gradient monotonicity:</b> $(\nabla f(y) - \nabla f(x))^\top (y - x) \ge m\|y - x\|_2^2$</li>
          </ol>
        </div>

        <h3>6.3 Implications for Optimization</h3>

        <div class="insight">
          <h4>Why Strong Convexity is Important</h4>
          <ul>
            <li><b>Unique minimizer:</b> An $m$-strongly convex function has at most one minimizer.</li>
            <li><b>Linear convergence:</b> Gradient descent on $m$-strongly convex functions converges at a geometric rate: $f(x_k) - f(x^*) \le (1 - m/L)^k (f(x_0) - f(x^*))$ where $L$ is the Lipschitz constant of the gradient.</li>
            <li><b>Condition number:</b> The ratio $\kappa = L/m$ (condition number) determines convergence speed.</li>
          </ul>
        </div>

        <h3>6.4 Smoothness (Lipschitz Gradient)</h3>
        <p>A differentiable function $f$ has <b>$L$-Lipschitz continuous gradient</b> (is <b>$L$-smooth</b>) if:</p>
        $$
        \|\nabla f(y) - \nabla f(x)\|_2 \le L \|y - x\|_2 \quad \forall x, y
        $$
        <p>Equivalently (for twice differentiable $f$): $\nabla^2 f(x) \preceq LI$ for all $x$.</p>

        <div class="example">
          <h4>Example: Quadratic Functions</h4>
          <p>For $f(x) = \frac{1}{2} x^\top Q x + b^\top x$ where $mI \preceq Q \preceq LI$:</p>
          <ul>
            <li>$f$ is $m$-strongly convex.</li>
            <li>$f$ has $L$-Lipschitz gradient.</li>
            <li>The condition number is $\kappa = L/m = \lambda_{\max}(Q) / \lambda_{\min}(Q)$.</li>
          </ul>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Explorer: Strong Convexity and Convergence</h3>
          <p><b>See How Strong Convexity Accelerates Optimization:</b> Strong convexity provides a quadratic lower bound on the function, leading to provably fast convergence. This visualization demonstrates:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Adjust $m$ (strong convexity parameter):</b> Use a slider to change the strength of convexity</li>
            <li><b>Compare bounds:</b>
              <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
                <li>Convex: $f(y) \ge f(x) + \nabla f(x)^\top (y-x)$ (tangent line)</li>
                <li>Strongly convex: $f(y) \ge f(x) + \nabla f(x)^\top (y-x) + \frac{m}{2}\|y-x\|_2^2$ (quadratic lower bound)</li>
              </ul>
            </li>
            <li><b>Watch gradient descent:</b> Run the algorithm and observe how larger $m$ leads to faster convergence</li>
            <li><b>Visualize condition number:</b> See how $\kappa = L/m$ determines the convergence rate</li>
            <li><b>Compare with non-strongly convex:</b> See the dramatic difference in convergence speed</li>
          </ul>
          <p><i>Practical impact:</i> This is why adding $\ell_2$ regularization ($\lambda \|x\|_2^2$) not only prevents overfitting but also speeds up optimizationâ€”it makes the problem strongly convex!</p>
          <div id="widget-strong-convexity" style="width: 100%; height: 550px; position: relative;"></div>
        </div>
      </section>

      <section class="card-v2" id="section-7">
        <h2>7. The Convex Conjugate (Fenchel Conjugate)</h2>

        <h3>7.1 Definition and Geometric Intuition</h3>
        <p>The <a href="#" class="definition-link" data-term="convex conjugate">convex conjugate</a> (or <b>Fenchel-Legendre transform</b>) of a function $f: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$ is defined as:</p>
        $$
        \boxed{f^*(y) = \sup_{x \in \mathrm{dom}\, f} \left( y^\top x - f(x) \right)}
        $$
        <p>Here, $y \in \mathbb{R}^n$ acts as a "slope" vector. The value $f^*(y)$ answers the question: <b>"How well can the linear function $x \mapsto y^\top x$ beat the function $f(x)$?"</b></p>
        <p>For each fixed slope $y$, we define the function $\phi_y(x) = y^\top x - f(x)$. Then $f^*(y) = \sup_x \phi_y(x)$. If the linear function can beat $f$ <b>arbitrarily badly</b> (i.e., $\phi_y(x) \to +\infty$), then $f^*(y) = +\infty$. If it can never beat it substantially, you might get a finite or even negative number.</p>

        <div class="insight">
          <h4>ðŸ’¡ 1D Interpretation: Maximum Gap</h4>
          <p>For $n=1$, consider a fixed slope $y$. The term $yx - f(x)$ represents the vertical distance (gap) between the line $L(x) = yx$ (passing through origin) and the curve $f(x)$. The conjugate $f^*(y)$ is the maximum such gap.</p>
          <p>Geometrically, this corresponds to finding a line with slope $y$ that is tangent to $f(x)$. The "supporting line" equation is $y^\top x - f^*(y) \le f(x)$, meaning $-f^*(y)$ is the $y$-intercept of the supporting hyperplane with normal $(y, -1)$.</p>
          <p>The conjugate $f^*(y)$ encodes the <b>highest line of slope $y$ that stays below the function $f$</b>.</p>
        </div>

        <div style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center; margin: 20px 0;">
          <figure style="flex: 1; min-width: 300px; text-align: center;">
            <img src="assets/conjugate_definition_gap.png"
                 alt="Conjugate as maximum vertical gap between line yx and function f(x)"
                 style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 10px; background: white;" />
            <figcaption><i>Figure 7.1:</i> <b>The Maximum Gap View.</b> For a fixed slope $y$, we look for the point $x^*$ that maximizes the distance $yx - f(x)$. The value of this maximum distance is $f^*(y)$. A vertical arrow labeled "Supremum Value = $f^*(y)$" connects the line $yx$ down to the curve $f(x)$ at this optimal $x^*$.</figcaption>
          </figure>
          <figure style="flex: 1; min-width: 300px; text-align: center;">
            <img src="assets/conjugate_supporting_line.png"
                 alt="Conjugate determines the intercept of the supporting line"
                 style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 10px; background: white;" />
            <figcaption><i>Figure 7.2:</i> <b>The Supporting Line View.</b> The tangent line to $f(x)$ with slope $y$ has the equation $\ell(x) = yx - f^*(y)$. The line intersects the vertical y-axis at the value $-f^*(y)$. This visually demonstrates that $f^*(y)$ determines the vertical intercept (negative) of the supporting hyperplane with slope $y$.</figcaption>
          </figure>
        </div>

        <h3>7.2 Convexity of the Conjugate</h3>
        <p>A remarkable property of the conjugate is that <b>$f^*$ is always convex</b>, regardless of whether $f$ is convex, concave, or neither.</p>

        <div class="proof-enhanced">
          <h4>Proof 1: Supremum of Affine Functions</h4>
          <p>For any fixed $x$, the function $g_x(y) = y^\top x - f(x)$ is affine (linear plus a constant) in $y$.
          The conjugate is the pointwise supremum of this family of affine functions:
          $$ f^*(y) = \sup_{x \in \mathrm{dom}\, f} g_x(y) $$
          Since the supremum of any collection of convex functions is convex, $f^*$ is convex.</p>
          <p><b>Detailed Check:</b> Let $h(y) = \sup_i h_i(y)$ where each $h_i$ is convex. For any $y_1, y_2$ and $\theta \in [0, 1]$:</p>
          $$
          \begin{aligned}
          h(\theta y_1 + (1-\theta)y_2) &= \sup_i h_i(\theta y_1 + (1-\theta)y_2) \\
          &\le \sup_i (\theta h_i(y_1) + (1-\theta)h_i(y_2)) \\
          &\le \theta \sup_i h_i(y_1) + (1-\theta) \sup_i h_i(y_2) \\
          &= \theta h(y_1) + (1-\theta)h(y_2)
          \end{aligned}
          $$
          <p>Applying this with $h_i = g_x$ completes the proof.</p>
        </div>

        <div class="proof-enhanced">
          <h4>Proof 2: Epigraph Intersection</h4>
          <p>The epigraph of $f^*$ is the intersection of a family of halfspaces.
          $$ (y, t) \in \mathrm{epi}\, f^* \iff f^*(y) \le t \iff \sup_x (y^\top x - f(x)) \le t \iff y^\top x - f(x) \le t \ \forall x $$
          This defines an intersection of closed halfspaces $H_x = \{(y, t) \mid x^\top y - t \le f(x)\}$, which is always a convex set.</p>
        </div>

        <div style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center; margin: 20px 0;">
          <figure style="flex: 1; min-width: 300px; text-align: center;">
            <img src="assets/conjugate_convexity_affine.png"
                 alt="f* is the upper envelope of affine functions"
                 style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 10px; background: white;" />
            <figcaption><i>Figure 7.3:</i> The conjugate $f^*(y)$ (thick curve) is the upper envelope of the lines $x \cdot y - f(x)$. The function $f^*(y)$ is the upper envelope (pointwise supremum) of all these lines. The resulting upper envelope forms a convex "V" shape or curve, visually proving that the conjugate $f^*$ is always convex.</figcaption>
          </figure>
          <figure style="flex: 1; min-width: 300px; text-align: center;">
            <img src="assets/conjugate_epigraph_intersection.png"
                 alt="Epigraph of f* as intersection of halfspaces"
                 style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 10px; background: white;" />
            <figcaption><i>Figure 7.4:</i> In 3D space $(y_1, y_2, t)$, the epigraph of $f^*$ is formed by intersecting halfspaces defined by each $x$. The resulting solid is convex. This corresponds to the "intersection of halfspaces" proof of convexity.</figcaption>
          </figure>
        </div>

        <h3>7.3 Fenchel-Young Inequality</h3>
        <p>A fundamental result in convex analysis, analogous to the Cauchy-Schwarz inequality for vectors.</p>
        <div class="theorem-box">
          <h4>Theorem (Fenchel-Young)</h4>
          <p>For any function $f$ and any $x, y$:</p>
          $$
          f(x) + f^*(y) \ge y^\top x
          $$
          <p><b>Equality holds if and only if</b> $y \in \partial f(x)$ (i.e., $y$ is a subgradient of $f$ at $x$).</p>
        </div>
        <p>So knowing the conjugate essentially tells you about the subgradients of $f$, and vice versa. This is why conjugates are central in duality.</p>

        <figure style="text-align: center;">
          <img src="assets/conjugate_fenchel_young.png"
               alt="Geometric illustration of Fenchel-Young inequality"
               style="max-width: 60%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
          <figcaption><i>Figure 7.5:</i> <b>Fenchel-Young Geometry.</b> The inequality $f(x) \ge yx - f^*(y)$ says the function curve stays above the tangent line. They touch (equality) exactly when the slope $y$ matches the derivative $f'(x)$.</figcaption>
        </figure>

        <h3>7.4 Where is the Supremum Attained?</h3>
        <p>If $f$ is convex and differentiable, we can find $f^*(y)$ by maximizing $y^\top x - f(x)$.</p>
        <p>The first-order condition for the maximum at $x^*$ is:</p>
        $$ \nabla_x (y^\top x - f(x)) = y - \nabla f(x^*) = 0 \implies y = \nabla f(x^*) $$
        <p><b>Interpretation:</b> The slope $y$ corresponds to the gradient at the point $x^*$ where the gap is maximized. If $f$ is strictly convex, this mapping is invertible: $x^* = (\nabla f)^{-1}(y)$.</p>
        <p>In general (possibly nondifferentiable), we replace gradient with <b>subgradient</b>: $y \in \partial f(x^*)$ if and only if $x^*$ is a maximizer in the definition of $f^*(y)$.</p>

        <h3>7.5 Classical Examples</h3>

        <div class="example">
          <h4>1. Quadratic Function (Self-Conjugate)</h4>
          <p>Let $f(x) = \frac{1}{2}x^\top Q x$ with $Q \succ 0$. To find $f^*(y)$, maximize $y^\top x - \frac{1}{2}x^\top Q x$.</p>
          <p>Gradient is $y - Qx = 0 \implies x^* = Q^{-1}y$. Substitute back:</p>
          $$ f^*(y) = y^\top (Q^{-1}y) - \frac{1}{2}(Q^{-1}y)^\top Q (Q^{-1}y) = y^\top Q^{-1} y - \frac{1}{2}y^\top Q^{-1} y = \frac{1}{2}y^\top Q^{-1} y $$
          <p>If $Q=I$ (Euclidean norm squared), then $f^*(y) = \frac{1}{2}\|y\|_2^2$. This is the continuous analogue of "Fourier transform of a Gaussian is a Gaussian": nice things stay nice.</p>
        </div>

        <div class="example">
          <h4>2. Indicator Function $\leftrightarrow$ Support Function</h4>
          <p>Let $f(x) = \delta_C(x)$ (0 if $x \in C$, $+\infty$ else) for a convex set $C$.</p>
          $$ f^*(y) = \sup_x (y^\top x - \delta_C(x)) = \sup_{x \in C} y^\top x = \sigma_C(y) $$
          <p>The conjugate of the indicator function is the <b>support function</b> of the set $C$.</p>
          <p><b>Geometric meaning:</b> For each direction $y$, $\sigma_C(y)$ is how far you can go in that direction while staying inside $C$, measured in inner product units.</p>
          <figure style="text-align: center; margin-top: 1rem;">
            <img src="assets/conjugate_indicator_support.png" alt="Conjugate of indicator is support function" style="max-width: 60%; border: 1px solid var(--border); border-radius: 8px; padding: 10px;" />
            <figcaption><i>Figure 7.6:</i> The support function $\sigma_C(y)$ measures the extent of set $C$ in direction $y$. A supporting hyperplane (line) perpendicular to $y$ touches the boundary of $C$ at the furthest point in direction $y$.</figcaption>
          </figure>
        </div>

        <div class="example">
          <h4>3. Norm $\leftrightarrow$ Dual Norm Ball</h4>
          <p>Let $f(x) = \|x\|$ (any norm). Its conjugate is the indicator of the <b>dual unit ball</b>.</p>
          <p>Recall the dual norm: $\|y\|_* = \sup_{\|x\|\le 1} y^\top x$. Compute $f^*(y)$:</p>
          $$
          \begin{aligned}
          f^*(y) &= \sup_x \big( y^\top x - \|x\| \big) \\
          &= \sup_{t\ge 0} \sup_{\|u\|\le 1} \big( y^\top (t u) - \|t u\|\big) \quad \text{(write }x=tu\text{ with }\|u\|\le 1\text{)}\\
          &= \sup_{t\ge 0} t \left( \sup_{\|u\|\le 1} y^\top u - 1\right) \\
          &= \sup_{t\ge 0} t(\|y\|_* - 1).
          \end{aligned}
          $$
          <p>If $\|y\|_* > 1$, the term $\|y\|_* - 1 > 0$, so we can drive the supremum to $+\infty$ by taking $t \to \infty$. If $\|y\|_* \le 1$, the term is $\le 0$, so the maximum is attained at $t=0$, giving 0.</p>
          $$ f^*(y) = \begin{cases} 0 & \|y\|_* \le 1 \\ +\infty & \|y\|_* > 1 \end{cases} $$
          <figure style="text-align: center; margin-top: 1rem;">
            <img src="assets/conjugate_dual_norm_ball.png" alt="Conjugate of absolute value is indicator of interval [-1, 1]" style="max-width: 60%; border: 1px solid var(--border); border-radius: 8px; padding: 10px;" />
            <figcaption><i>Figure 7.7:</i> Example for $f(x)=|x|$ (L1 norm in 1D). The conjugate is the indicator of $[-1, 1]$ (L-infinity unit ball). Outside $[-1, 1]$, the function is undefined or $+\infty$.</figcaption>
          </figure>
        </div>

        <div class="example">
          <h4>4. Linear Function</h4>
          <p>Let $f(x) = a^\top x + b$. Then:</p>
          $$
          \begin{aligned}
          f^*(y) &= \sup_x (y^\top x - a^\top x - b) \\
          &= \sup_x ((y-a)^\top x - b)
          \end{aligned}
          $$
          <p>If $y \ne a$, we can make $(y-a)^\top x$ arbitrarily large. If $y=a$, the value is $-b$.</p>
          $$ f^*(y) = \begin{cases} -b & y=a \\ +\infty & y \ne a \end{cases} $$
          <p>This is the indicator of the single point $\{a\}$ plus the constant $-b$. Geometrically, all tangents to a linear function coincide with the function itself; any other slope gives no supporting line.</p>
        </div>

        <div class="example">
          <h4>5. Log-Sum-Exp $\leftrightarrow$ Negative Entropy</h4>
          <p>For $f(x) = \log(\sum e^{x_i})$, the conjugate is the negative entropy restricted to the probability simplex:</p>
          $$ f^*(y) = \sum_{i=1}^n y_i \log y_i \quad \text{on } \Delta_n = \{y \ge 0, \mathbf{1}^\top y = 1\} $$
          <p>See the detailed derivation in the problem set or previous sections.</p>
        </div>

        <h3>7.6 Biconjugate and Convexification</h3>
        <p>What happens if we apply the operation twice? The <b>biconjugate</b> $f^{**} = (f^*)^*$ relates back to $f$.</p>

        <div class="theorem-box">
          <h4>Theorem (Fenchel-Moreau)</h4>
          <p>The biconjugate $f^{**}$ is the <b>closed convex hull</b> of $f$. This means $f^{**}$ is the largest closed convex function such that $f^{**}(x) \le f(x)$ for all $x$.</p>
          <p><b>Corollary:</b> If $f$ is already closed and convex, then $f^{**} = f$.</p>
        </div>
        <p>So the conjugate is not just some random transform; doing it twice <b>projects</b> $f$ onto the space of closed convex functions. That's why the conjugate is the natural language of convex duality: you can take a possibly nonconvex primal, conjugate bits of it, and end up with a clean convex dual problem.</p>

        <figure style="text-align: center;">
          <img src="assets/conjugate_biconjugate.png"
               alt="Biconjugate as convex hull of non-convex function"
               style="max-width: 60%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
          <figcaption><i>Figure 7.8:</i> <b>Convexification.</b> For a non-convex function $W$-shaped function $f$, the biconjugate $f^{**}$ "fills in" the non-convex valley, creating the tightest possible convex underestimator.</figcaption>
        </figure>

        <h3>7.7 Summary Roadmap</h3>
        <p>When working with conjugates, remember these five pillars:</p>
        <ol>
          <li><b>Definition:</b> $f^*(y) = \sup_x (y^\top x - f(x))$ (Max gap).</li>
          <li><b>Geometry:</b> $f^*(y)$ is the vertical shift of the <b>highest line of slope $y$</b> that lies below $f$.</li>
          <li><b>Convexity:</b> $f^*$ is <i>always</i> convex (supremum of affine functions).</li>
          <li><b>Fenchel-Young:</b> $f(x) + f^*(y) \ge y^\top x$, with equality iff $y \in \partial f(x)$.</li>
          <li><b>Duality:</b> $f^{**}$ is the convex hull of $f$. This operation is the engine behind Lagrangian Duality.</li>
        </ol>
        <p>Once these are internalized, all the duality results in the later lectures become much easier: most dual problems are just "apply conjugate to the right pieces and rearrange".</p>

      </section>

      <section class="card-v2" id="section-8">
        <h2>8. Quasi-Convex Functions</h2>

        <h3>8.1 Definition via Sublevel Sets</h3>
        <p>A function $f: \mathbb{R}^n \to \mathbb{R}$ is <a href="#" class="definition-link">quasi-convex</a> if its domain is convex and all its <b>sublevel sets</b> are convex:</p>
        $$
        S_\alpha = \{x \in \mathrm{dom}\, f \mid f(x) \le \alpha\}
        $$
        <p>is convex for all $\alpha \in \mathbb{R}$.</p>

        <div class="proof-enhanced">
          <h4>Theorem: Convex $\implies$ Quasiconvex</h4>
          <p>If $f$ is convex, it is also quasiconvex.</p>
          <p><b>Proof:</b> Let $S_\alpha = \{x \mid f(x) \le \alpha\}$. Take $x, y \in S_\alpha$ and $\theta \in [0, 1]$.
          $$ f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) \le \theta \alpha + (1-\theta)\alpha = \alpha $$
          So the convex combination is in $S_\alpha$. Thus $S_\alpha$ is convex.</p>
        </div>

        <p><b>Converse is False:</b> A function can have convex sublevel sets without being convex.
        <br><i>Example:</i> $f(x) = \sqrt{|x|}$ on $\mathbb{R}$. Sublevel sets are intervals $[-\alpha^2, \alpha^2]$ (convex), but the function is "bell-shaped" (concave on $x>0$, not convex overall).</p>

        <p>Similarly, $f$ is <b>quasi-concave</b> if $-f$ is quasi-convex (its superlevel sets are convex), and <b>quasilinear</b> if it is both quasi-convex and quasi-concave.</p>

        <figure style="text-align: center;">
          <img src="assets/quasiconvex_sublevel.png"
               alt="Sublevel sets of a quasiconvex function"
               style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
          <figcaption><i>Figure 8.1:</i> A quasiconvex function may not be convex (it can be "bell-shaped" or flattened), but its sublevel sets $S_\alpha$ must always be convex sets.</figcaption>
        </figure>

        <h3>8.2 Modified Jensen's Inequality</h3>
        <p>$f$ is quasi-convex if and only if for all $x, y \in \mathrm{dom}\, f$ and $\theta \in [0, 1]$:</p>
        $$
        f(\theta x + (1-\theta)y) \le \max\{f(x), f(y)\}
        $$
        <p>This condition says that the value of the function on a segment never exceeds the larger of the two endpoint values. It forbids "bumps" in the middle of a valley.</p>

        <h3>8.3 First-Order Condition</h3>
        <p>If $f$ is differentiable, it is quasi-convex if and only if:</p>
        $$ f(y) \le f(x) \implies \nabla f(x)^\top (y - x) \le 0 $$
        <p>Geometrically, this means that if you look in a direction where the function value decreases (or stays same), the gradient must oppose that motion (make an obtuse angle).</p>

        <figure style="text-align: center;">
          <img src="assets/quasiconvex_gradient.png"
               alt="First-order condition for quasiconvexity"
               style="max-width: 60%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
          <figcaption><i>Figure 8.2:</i> The gradient $\nabla f(x)$ at a point on a level curve points "outward" from the sublevel set. Any vector $y-x$ leading to a lower value $f(y) \le f(x)$ forms an obtuse angle with the gradient.</figcaption>
        </figure>

        <h3>8.4 Examples in Detail</h3>

        <div class="example">
          <h4>Example 1: Linear-Fractional Function</h4>
          <p>The function $f(x) = \frac{a^\top x + b}{c^\top x + d}$ on $\{x \mid c^\top x + d > 0\}$ is quasilinear.</p>
          <p><i>Proof:</i> The sublevel set $S_\alpha = \{x \mid \frac{a^\top x + b}{c^\top x + d} \le \alpha\}$ is equivalent to the intersection of two halfspaces: $(a - \alpha c)^\top x + (b - \alpha d) \le 0$ and $c^\top x + d > 0$. The intersection of convex sets is convex.</p>
        </div>

        <div class="example">
          <h4>Example 2: Distance Ratio</h4>
          <p>The function $f(x) = \frac{\|x-a\|_2}{\|x-b\|_2}$ on $\{x \mid \|x-a\|_2 \le \|x-b\|_2\}$ is quasiconvex.</p>
          <p><i>Proof:</i> The condition $f(x) \le \tau$ becomes $\|x-a\|_2 \le \tau \|x-b\|_2$. For $\tau \ge 1$, this condition defines a convex set (related to Second Order Cones).</p>
        </div>

        <div class="example">
          <h4>Example 3: Internal Rate of Return (IRR)</h4>
          <p>Consider a cash flow $x = (x_0, \dots, x_n)$ with $x_0 < 0$ and $\sum x_i > 0$. The <b>Internal Rate of Return</b> $\text{IRR}(x)$ is the interest rate $r$ such that the present value is zero: $\sum x_i (1+r)^{-i} = 0$.</p>
          <p>We claim $\text{IRR}(x)$ is a <b>quasiconcave</b> function of the cash flow vector $x$.</p>
          <div class="proof-enhanced">
            <h4>Proof</h4>
            <p>The superlevel set $\{x \mid \text{IRR}(x) \ge R\}$ consists of cash flows whose present value at rate $R$ is non-negative (assuming standard investment profile). This condition is linear in $x$:
            $$ \sum_{i=0}^n x_i (1+R)^{-i} \ge 0 $$
            Since the superlevel sets are halfspaces (convex), the function is quasiconcave.</p>
          </div>
        </div>

        <h3>8.4 Quasi-Convex Optimization</h3>
        <p>Minimizing a quasi-convex function over a convex set is a <b>quasi-convex optimization problem</b>. These are harder than convex problems (local minima can exist), but still more tractable than general nonconvex problems.</p>
        <p><b>Solution method:</b> Bisection on the objective value, solving feasibility problems $\{x \mid f(x) \le t, x \in C\}$ for various $t$.</p>

      </section>

      <section class="card-v2" id="section-9">
        <h2>9. Common Convex Functions Reference</h2>

        <p>This table provides a quick reference for recognizing convex functions. Memorize these patternsâ€”they appear constantly in optimization.</p>

        <table class="data-table" style="width: 100%; margin-top: 16px;">
          <thead>
            <tr>
              <th>Function</th>
              <th>Domain</th>
              <th>Notes</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>$x^p$</td>
              <td>$\mathbb{R}_+$ if $p \ge 1$ or $p \le 0$</td>
              <td>Convex on $\mathbb{R}_+$ for $p \ge 1$ or $p \le 0$; concave for $0 \le p \le 1$</td>
            </tr>
            <tr>
              <td>$e^{ax}$</td>
              <td>$\mathbb{R}$</td>
              <td>Convex for any $a \in \mathbb{R}$</td>
            </tr>
            <tr>
              <td>$-\log(x)$</td>
              <td>$\mathbb{R}_{++}$</td>
              <td>Convex; $\log(x)$ is concave</td>
            </tr>
            <tr>
              <td>$x \log(x)$</td>
              <td>$\mathbb{R}_{++}$</td>
              <td>Convex (negative entropy term)</td>
            </tr>
            <tr>
              <td>$\|x\|_p$</td>
              <td>$\mathbb{R}^n$</td>
              <td>Convex for $p \ge 1$; not a norm for $p < 1$</td>
            </tr>
            <tr>
              <td>$\|x\|_2^2 = x^\top x$</td>
              <td>$\mathbb{R}^n$</td>
              <td>Convex (quadratic with PSD Hessian $2I$)</td>
            </tr>
            <tr>
              <td>$x^\top A x$</td>
              <td>$\mathbb{R}^n$</td>
              <td>Convex if $A \succeq 0$</td>
            </tr>
            <tr>
              <td>$\log(\sum_{i=1}^n e^{x_i})$</td>
              <td>$\mathbb{R}^n$</td>
              <td>Log-sum-exp: smooth approximation to $\max\{x_1, \ldots, x_n\}$</td>
            </tr>
            <tr>
              <td>$-\log(\det(X))$</td>
              <td>$\mathbb{S}^n_{++}$</td>
              <td>Negative log-determinant: convex on PD matrices</td>
            </tr>
            <tr>
              <td>$\lambda_{\max}(X)$</td>
              <td>$\mathbb{S}^n$</td>
              <td>Maximum eigenvalue: convex (supremum of linear functions)</td>
            </tr>
            <tr>
              <td>$\mathrm{tr}(X^p)$</td>
              <td>$\mathbb{S}^n_+$</td>
              <td>Convex for $p \ge 1$ or $p \le 0$ on PSD matrices</td>
            </tr>
          </tbody>
        </table>

        <h3>9.1 Operations and Their Effects</h3>
        <table class="data-table" style="width: 100%; margin-top: 16px;">
          <thead>
            <tr>
              <th>Operation</th>
              <th>Preserves Convexity?</th>
              <th>Conditions</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Nonnegative weighted sum</td>
              <td>âœ… Yes</td>
              <td>Always</td>
            </tr>
            <tr>
              <td>Pointwise maximum/supremum</td>
              <td>âœ… Yes</td>
              <td>Always</td>
            </tr>
            <tr>
              <td>Composition $h(g(x))$</td>
              <td>âœ… Yes</td>
              <td>If $g$ convex, $h$ convex non-decreasing</td>
            </tr>
            <tr>
              <td>Affine transformation $f(Ax+b)$</td>
              <td>âœ… Yes</td>
              <td>Always</td>
            </tr>
            <tr>
              <td>Minimization $\inf_y f(x,y)$</td>
              <td>âœ… Yes</td>
              <td>If $f$ convex in $(x,y)$</td>
            </tr>
            <tr>
              <td>Perspective</td>
              <td>âœ… Yes</td>
              <td>Always</td>
            </tr>
            <tr>
              <td>Product $f(x) \cdot g(x)$</td>
              <td>âŒ No</td>
              <td>Generally not convex</td>
            </tr>
            <tr>
              <td>Pointwise minimum</td>
              <td>âŒ No</td>
              <td>Not preserved (but concave functions preserved)</td>
            </tr>
          </tbody>
        </table>
      </section>

      <!-- SECTION 10: EXERCISES -->
      <section class="card-v2" id="section-10">
        <h2><i data-feather="edit-3"></i> 10. Exercises and Detailed Examples</h2>

        <div class="problem">
          <h3>P3.1 â€” Verify Convexity Using First-Order Conditions</h3>
          <p>Prove that $f(x) = e^x$ is convex on $\mathbb{R}$ using the first-order condition.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Tangent Underestimator:</b> The first-order condition says that the tangent line always lies below the function graph.</li>
            <li><b>Inequality Source:</b> Many famous inequalities (like $e^x \ge 1+x$) are simply statements that a convex function lies above its tangent.</li>
        </ul>
      </div>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Tangent Underestimator:</b> The first-order condition says that the tangent line always lies below the function graph.</li>
            <li><b>Inequality Source:</b> Many famous inequalities (like $e^x \ge 1+x$) are simply statements that a convex function lies above its tangent.</li>
        </ul>
      </div>

<div class="solution">
            <h4>Solution</h4>

            <div class="proof-step">
              <strong>Step 1: Compute gradient.</strong> For $f(x) = e^x$, we have $f'(x) = e^x$.
            </div>

            <div class="proof-step">
              <strong>Step 2: Apply first-order condition.</strong> We need to show $f(y) \ge f(x) + f'(x)(y - x)$ for all $x, y \in \mathbb{R}$.
              Substituting:
              $$
              e^y \ge e^x + e^x(y - x) = e^x(1 + y - x)
              $$
            </div>

            <div class="proof-step">
              <strong>Step 3: Simplify.</strong> Divide both sides by $e^x > 0$ and let $t = y - x$:
              $$
              e^t \ge 1 + t
              $$
            </div>

            <div class="proof-step">
              <strong>Step 4: Verify inequality.</strong> The inequality $e^t \ge 1 + t$ for all $t \in \mathbb{R}$ follows from the Taylor series:
              $$
              e^t = 1 + t + \frac{t^2}{2!} + \frac{t^3}{3!} + \cdots \ge 1 + t
              $$
              (all terms after $1 + t$ are non-negative for $t \ge 0$, and the inequality reverses then reverses back for $t < 0$ by inspecting the series).
            </div>

            <div class="proof-step">
              <strong>Conclusion:</strong> Therefore, $f(x) = e^x$ is convex.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.2 â€” Verify Convexity Using Second-Order Conditions</h3>
          <p>Show that $f(x) = \|x\|_2^2$ is convex on $\mathbb{R}^n$ using the Hessian test.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Curvature Test:</b> The Hessian matrix $\nabla^2 f(x)$ captures the local curvature. Positive semidefinite curvature everywhere ensures global convexity ("bowl shape").</li>
            <li><b>Constant Hessian:</b> Quadratic functions have constant curvature. If the curvature is "flat" ($2I$) everywhere, it's convex.</li>
        </ul>
      </div>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Curvature Test:</b> The Hessian matrix $\nabla^2 f(x)$ captures the local curvature. Positive semidefinite curvature everywhere ensures global convexity ("bowl shape").</li>
            <li><b>Constant Hessian:</b> Quadratic functions have constant curvature. If the curvature is "flat" ($2I$) everywhere, it's convex.</li>
        </ul>
      </div>

<div class="solution">
            <h4>Solution</h4>

            <div class="proof-step">
              <strong>Step 1: Compute gradient.</strong>
              $$
              f(x) = x^\top x \implies \nabla f(x) = 2x
              $$
            </div>

            <div class="proof-step">
              <strong>Step 2: Compute Hessian.</strong>
              $$
              \nabla^2 f(x) = 2I
              $$
              where $I$ is the $n \times n$ identity matrix.
            </div>

            <div class="proof-step">
              <strong>Step 3: Check PSD.</strong> The identity matrix $I$ is positive definite (all eigenvalues equal 1 > 0), so $2I$ is also positive definite, hence positive semidefinite.
            </div>

            <div class="proof-step">
              <strong>Conclusion:</strong> Since $\nabla^2 f(x) = 2I \succeq 0$ for all $x \in \mathbb{R}^n$, the function $f(x) = \|x\|_2^2$ is convex. In fact, it's strongly convex with parameter $m = 2$.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.3 â€” Composition of Convex Functions</h3>
          <p>Let $f(x) = -\log(x)$ on $\mathbb{R}_{++}$ and $g(x) = e^x$ on $\mathbb{R}$. Is $h(x) = f(g(x)) = -\log(e^x) = -x$ convex?</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Composition Rules:</b> The standard rules (Convex + Increasing, etc.) are sufficient but not necessary.</li>
            <li><b>Direct Check:</b> When rules fail (e.g., Convex + Decreasing), always fall back to the definition or computing derivatives. Here, cancellation made the result linear.</li>
        </ul>
      </div>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Composition Rules:</b> The standard rules (Convex + Increasing, etc.) are sufficient but not necessary.</li>
            <li><b>Direct Check:</b> When rules fail (e.g., Convex + Decreasing), always fall back to the definition or computing derivatives. Here, cancellation made the result linear.</li>
        </ul>
      </div>

<div class="solution">
            <h4>Solution</h4>

            <div class="proof-step">
              <strong>Analyze composition rule:</strong> We have $h(x) = f(g(x))$ where:
              <ul>
                <li>$g(x) = e^x$ is convex and increasing on $\mathbb{R}$.</li>
                <li>$f(x) = -\log(x)$ is convex on $\mathbb{R}_{++}$.</li>
              </ul>
            </div>

            <div class="proof-step">
              <strong>Check monotonicity:</strong> For the composition $h = f \circ g$ to be convex when both $f$ and $g$ are convex, we need $f$ to be <b>non-decreasing</b>. However, $f(x) = -\log(x)$ is <b>decreasing</b> (since $f'(x) = -1/x < 0$).
            </div>

            <div class="proof-step">
              <strong>Alternative: Direct verification.</strong> We can directly compute: $h(x) = -x$ is linear (hence both convex and concave). So yes, it's convex, but not strictly.
            </div>

            <div class="proof-step">
              <strong>Lesson:</strong> The composition rules are <i>sufficient</i> conditions, not necessary. The composition $f \circ g$ can still be convex even when the standard rules don't applyâ€”you need to verify directly.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.4 â€” Maximum of Convex Functions</h3>
          <p>Prove that if $f_1, f_2: \mathbb{R}^n \to \mathbb{R}$ are convex, then $f(x) = \max\{f_1(x), f_2(x)\}$ is convex.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Pointwise Maximum:</b> The maximum of any collection of convex functions is convex.</li>
            <li><b>Epigraph View:</b> The epigraph of the max is the intersection of the epigraphs of the individual functions. Intersection of convex sets is convex.</li>
        </ul>
      </div>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Pointwise Maximum:</b> The maximum of any collection of convex functions is convex.</li>
            <li><b>Epigraph View:</b> The epigraph of the max is the intersection of the epigraphs of the individual functions. Intersection of convex sets is convex.</li>
        </ul>
      </div>

<div class="solution">
            <h4>Solution</h4>

            <div class="proof-step">
              <strong>Setup:</strong> Take any $x, y \in \mathbb{R}^n$ and $\theta \in [0, 1]$. Let $z = \theta x + (1-\theta)y$.
            </div>

            <div class="proof-step">
              <strong>Step 1: Bound $f(z)$.</strong> By definition:
              $$
              f(z) = \max\{f_1(z), f_2(z)\}
              $$
            </div>

            <div class="proof-step">
              <strong>Step 2: Apply convexity of $f_1$ and $f_2$.</strong>
              $$
              f_1(z) \le \theta f_1(x) + (1-\theta)f_1(y) \le \theta \max\{f_1(x), f_2(x)\} + (1-\theta)\max\{f_1(y), f_2(y)\}
              $$
              $$
              = \theta f(x) + (1-\theta)f(y)
              $$
              Similarly, $f_2(z) \le \theta f(x) + (1-\theta)f(y)$.
            </div>

            <div class="proof-step">
              <strong>Step 3: Conclude.</strong> Since both $f_1(z)$ and $f_2(z)$ are at most $\theta f(x) + (1-\theta)f(y)$, their maximum is also:
              $$
              f(z) = \max\{f_1(z), f_2(z)\} \le \theta f(x) + (1-\theta)f(y)
              $$
              proving $f$ is convex.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.5 â€” Strong Convexity</h3>
          <p>Show that $f(x) = \frac{1}{2}x^\top Q x$ is $\lambda_{\min}(Q)$-strongly convex if $Q \succ 0$.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Strong Convexity:</b> Requires a uniform minimum curvature $m > 0$.</li>
            <li><b>Quadratic Bound:</b> $f(y) \ge f(x) + \nabla f(x)^\top(y-x) + \frac{m}{2}\|y-x\|^2$.</li>
            <li><b>Matrix Inequality:</b> $Q \succeq mI$ means $\lambda_{\min}(Q) \ge m$.</li>
        </ul>
      </div>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Strong Convexity:</b> Requires a uniform minimum curvature $m > 0$.</li>
            <li><b>Quadratic Bound:</b> $f(y) \ge f(x) + \nabla f(x)^\top(y-x) + \frac{m}{2}\|y-x\|^2$.</li>
            <li><b>Matrix Inequality:</b> $Q \succeq mI$ means $\lambda_{\min}(Q) \ge m$.</li>
        </ul>
      </div>

<div class="solution">
            <h4>Solution</h4>

            <div class="proof-step">
              <strong>Use Hessian characterization:</strong> We have:
              $$
              \nabla^2 f(x) = Q
              $$
            </div>

            <div class="proof-step">
              <strong>Check strong convexity condition:</strong> A function is $m$-strongly convex if $\nabla^2 f(x) \succeq mI$ for all $x$. Here, we need:
              $$
              Q \succeq mI
              $$
            </div>

            <div class="proof-step">
              <strong>Eigenvalue interpretation:</strong> The condition $Q \succeq mI$ is equivalent to saying all eigenvalues of $Q$ are at least $m$. Since $Q$ is symmetric, its eigenvalues are real. The smallest eigenvalue is $\lambda_{\min}(Q)$.
            </div>

            <div class="proof-step">
              <strong>Conclusion:</strong> Since all eigenvalues of $Q$ are at least $\lambda_{\min}(Q)$, we have $Q \succeq \lambda_{\min}(Q) I$. Therefore, $f$ is $\lambda_{\min}(Q)$-strongly convex.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.6 â€” Conjugate Function</h3>
          <p>Compute the conjugate of $f(x) = \frac{1}{2}\|x\|_2^2$.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Legendre Transform:</b> For differentiable strictly convex functions, the conjugate is obtained by solving $\nabla f(x) = y$ for $x$.</li>
            <li><b>Self-Conjugacy:</b> The quadratic function $\frac{1}{2}\|x\|^2$ is the unique function (up to scaling) that is its own convex conjugate. This mirrors the Fourier transform property of Gaussians.</li>
        </ul>
      </div>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Legendre Transform:</b> For differentiable strictly convex functions, the conjugate is obtained by solving $\nabla f(x) = y$ for $x$.</li>
            <li><b>Self-Conjugacy:</b> The quadratic function $\frac{1}{2}\|x\|^2$ is the unique function (up to scaling) that is its own convex conjugate. This mirrors the Fourier transform property of Gaussians.</li>
        </ul>
      </div>

<div class="solution">
            <h4>Solution</h4>

            <div class="proof-step">
              <strong>Definition:</strong> The conjugate is:
              $$
              f^*(y) = \sup_{x \in \mathbb{R}^n} \left( y^\top x - \frac{1}{2}\|x\|_2^2 \right)
              $$
            </div>

            <div class="proof-step">
              <strong>Find supremum:</strong> The function $g(x) = y^\top x - \frac{1}{2}\|x\|_2^2$ is concave in $x$ (Hessian is $-I \preceq 0$). The maximum is attained where:
              $$
              \nabla_x g(x) = y - x = 0 \implies x = y
              $$
            </div>

            <div class="proof-step">
              <strong>Compute supremum value:</strong> Substituting $x = y$:
              $$
              f^*(y) = y^\top y - \frac{1}{2}\|y\|_2^2 = \|y\|_2^2 - \frac{1}{2}\|y\|_2^2 = \frac{1}{2}\|y\|_2^2
              $$
            </div>

            <div class="proof-step">
              <strong>Conclusion:</strong> The conjugate of $f(x) = \frac{1}{2}\|x\|_2^2$ is $f^*(y) = \frac{1}{2}\|y\|_2^2$. The quadratic is <b>self-conjugate</b>!
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.7 â€” Quasi-Convexity</h3>
          <p>Show that $f(x) = \lceil x \rceil$ (ceiling function) is quasi-convex but not convex.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Sublevel Sets:</b> Quasi-convexity requires convex sublevel sets $S_\alpha$.</li>
            <li><b>Step Functions:</b> Monotonic functions (even discontinuous ones like ceiling) are quasi-convex (and quasi-concave).</li>
            <li><b>Jensen Failure:</b> Quasi-convexity does not satisfy the average value inequality (Jensen's).</li>
        </ul>
      </div>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Sublevel Sets:</b> Quasi-convexity requires convex sublevel sets $S_\alpha$.</li>
            <li><b>Step Functions:</b> Monotonic functions (even discontinuous ones like ceiling) are quasi-convex (and quasi-concave).</li>
            <li><b>Jensen Failure:</b> Quasi-convexity does not satisfy the average value inequality (Jensen's).</li>
        </ul>
      </div>

<div class="solution">
            <h4>Solution</h4>

            <div class="proof-step">
              <strong>Verify quasi-convexity:</strong> The sublevel sets are:
              $$
              S_\alpha = \{x \in \mathbb{R} \mid \lceil x \rceil \le \alpha\}
              $$
            </div>

            <div class="proof-step">
              <strong>Characterize sublevel sets:</strong> Since $\lceil x \rceil$ is the smallest integer $\ge x$, we have $\lceil x \rceil \le \alpha$ if and only if $x \le \lfloor\alpha\rfloor$. Thus:
              $$
              S_\alpha = (-\infty, \lfloor\alpha\rfloor]
              $$
              which is an interval (convex). Since all sublevel sets are convex, $f$ is quasi-convex.
            </div>

            <div class="proof-step">
              <strong>Show not convex:</strong> Convexity requires $f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)$ for all $x, y, \theta \in [0,1]$.
              Consider $x=0$ and $y=0.6$.
              <ul>
                <li>$f(0) = \lceil 0 \rceil = 0$</li>
                <li>$f(0.6) = \lceil 0.6 \rceil = 1$</li>
              </ul>
              Let $\theta = 0.5$, so the midpoint is $z = 0.5(0) + 0.5(0.6) = 0.3$.
              <ul>
                <li>Actual value: $f(0.3) = \lceil 0.3 \rceil = 1$</li>
                <li>Convex combination: $0.5 f(0) + 0.5 f(0.6) = 0.5(0) + 0.5(1) = 0.5$</li>
              </ul>
              Since $1 \not\le 0.5$, the inequality is violated. Therefore, the ceiling function is <b>not convex</b>.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.8 â€” Softmax Convexity</h3>
          <p>The <b>Softmax</b> function, or Log-Sum-Exp function, is defined as $f(x) = \log\left(\sum_{i=1}^n e^{x_i}\right)$. Prove that $f(x)$ is convex on $\mathbb{R}^n$.</p>

      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Log-Sum-Exp:</b> This is a smooth approximation of the max function ($\text{LSE}(x) \approx \max_i x_i$). Since max is convex, LSE is convex.</li>
            <li><b>Hessian Structure:</b> The Hessian is the covariance matrix of the softmax probability distribution. Covariance matrices are always PSD.</li>
        </ul>
      </div>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Log-Sum-Exp:</b> This is a smooth approximation of the max function ($\text{LSE}(x) \approx \max_i x_i$). Since max is convex, LSE is convex.</li>
            <li><b>Hessian Structure:</b> The Hessian is the covariance matrix of the softmax probability distribution. Covariance matrices are always PSD.</li>
        </ul>
      </div>

<div class="solution">
            <h4>Solution</h4>

            <div class="proof-step">
              <strong>Step 1: Compute Gradient.</strong>
              $$ \frac{\partial f}{\partial x_k} = \frac{1}{\sum_j e^{x_j}} \cdot e^{x_k} $$
              Let $z_k = \frac{e^{x_k}}{\sum_j e^{x_j}}$. Note that $z_k > 0$ and $\sum_k z_k = 1$ (it's a probability distribution).
              The gradient is $\nabla f(x) = z$.
            </div>

            <div class="proof-step">
              <strong>Step 2: Compute Hessian.</strong>
              $$ \frac{\partial^2 f}{\partial x_k \partial x_l} = \frac{\partial z_k}{\partial x_l} $$
              If $k=l$:
              $$ \frac{\partial z_k}{\partial x_k} = \frac{e^{x_k}(\sum e^{x_j}) - e^{x_k}e^{x_k}}{(\sum e^{x_j})^2} = z_k - z_k^2 = z_k(1-z_k) $$
              If $k \ne l$:
              $$ \frac{\partial z_k}{\partial x_l} = \frac{0 - e^{x_k}e^{x_l}}{(\sum e^{x_j})^2} = -z_k z_l $$
              Thus, the Hessian matrix is $\nabla^2 f(x) = \text{diag}(z) - zz^\top$.
            </div>

            <div class="proof-step">
              <strong>Step 3: Prove PSD.</strong>
              For any vector $v \in \mathbb{R}^n$:
              $$ v^\top (\text{diag}(z) - zz^\top) v = \sum_i z_i v_i^2 - (v^\top z)^2 $$
              Since $z_i > 0$ and $\sum z_i = 1$, we can interpret $z$ as probabilities. Then $\sum z_i v_i^2 = \mathbb{E}[V^2]$ and $(v^\top z)^2 = (\sum z_i v_i)^2 = (\mathbb{E}[V])^2$ where $V$ is a random variable taking value $v_i$ with probability $z_i$.
              $$ \mathbb{E}[V^2] - (\mathbb{E}[V])^2 = \text{Var}(V) \ge 0 $$
              Thus $v^\top \nabla^2 f(x) v \ge 0$, so the Hessian is positive semidefinite. Therefore, $f$ is convex.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.9 â€” Concavity of the Geometric Mean</h3>
          <p>We prove that the geometric mean $G(x) = (\prod_{i=1}^n x_i)^{1/n}$ is concave on $\mathbb{R}_{++}^n$.</p>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Geometric Mean:</b> $G(x)$ is concave on the positive orthant. This is related to the AM-GM inequality.</li>
            <li><b>Log-Concavity:</b> Logarithm of geometric mean is arithmetic mean of logs (concave). Homogeneity + log-concavity implies concavity.</li>
        </ul>
      </div>

<div class="proof-enhanced">
            <h4>Solution</h4>

            <div class="proof-step">
              <strong>Method 1: Log-Concavity + Homogeneity</strong>
              <p>Consider $f(x) = \log G(x) = \frac{1}{n} \sum_{i=1}^n \log x_i$. Since $\log x_i$ is concave, $f(x)$ is concave (sum of concave functions). Thus $G(x) = e^{f(x)}$ is log-concave.</p>
              <p>Also, $G(x)$ is homogeneous of degree 1: $G(\lambda x) = \lambda G(x)$ for $\lambda > 0$.</p>
              <p><b>Lemma:</b> A positive, log-concave, degree-1 homogeneous function is concave.</p>
              <p><i>Proof of Lemma:</i> By log-concavity, $f(\theta x + (1-\theta)y) \ge f(x)^\theta f(y)^{1-\theta}$. Normalize so $f(x)=f(y)=1$ (using homogeneity). Then $f(\text{convex combo}) \ge 1 = \theta(1) + (1-\theta)(1)$. Scaling back gives full concavity.</p>
            </div>

            <div class="proof-step">
              <strong>Method 2: Hessian Test</strong>
              <p>Let $f(x) = \log G(x)$. Then $\nabla f(x) = \frac{1}{n}(1/x_1, \dots, 1/x_n)$.</p>
              <p>$\nabla^2 f(x) = -\frac{1}{n} \text{diag}(1/x_1^2, \dots, 1/x_n^2)$.</p>
              <p>Since $G(x) = e^{f(x)}$, $\nabla^2 G(x) = G(x)(\nabla f \nabla f^\top + \nabla^2 f)$.</p>
              <p>For any $v$, $v^\top \nabla^2 G(x) v = \frac{G(x)}{n^2} [(\sum v_i/x_i)^2 - n \sum (v_i/x_i)^2]$.</p>
              <p>By Cauchy-Schwarz, $(\sum a_i \cdot 1)^2 \le n \sum a_i^2$. Letting $a_i = v_i/x_i$, we get $(\sum v_i/x_i)^2 \le n \sum (v_i/x_i)^2$. Thus the quadratic form is $\le 0$, so $\nabla^2 G \preceq 0$.</p>
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.10 â€” Geometric vs Arithmetic Mean Cone</h3>
          <p>Let $S_\alpha = \{x \in \mathbb{R}^n_+ \mid G(x) \ge \alpha A(x)\}$ for $\alpha \in [0, 1]$, where $A(x) = \frac{1}{n}\sum x_i$.</p>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Superlevel Set of Concave:</b> The set $\{x \mid g(x) \ge 0\}$ is convex if $g$ is concave.</li>
            <li><b>Homogeneity implies Cone:</b> If the defining functions are homogeneous, the resulting set is a cone (scale invariant).</li>
        </ul>
      </div>

<div class="proof-enhanced">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>Convexity:</strong>
              <p>We know $G(x)$ is concave and $A(x)$ is linear (hence concave). Thus $h(x) = G(x) - \alpha A(x)$ is concave (since $\alpha \ge 0$).</p>
              <p>$S_\alpha = \{x \mid h(x) \ge 0\}$ is the 0-superlevel set of a concave function, which is a convex set.</p>
            </div>
            <div class="proof-step">
              <strong>Cone Property:</strong>
              <p>Both $G(x)$ and $A(x)$ are homogeneous of degree 1. $G(\lambda x) = \lambda G(x)$ and $A(\lambda x) = \lambda A(x)$.</p>
              <p>If $G(x) \ge \alpha A(x)$, then $G(\lambda x) = \lambda G(x) \ge \lambda \alpha A(x) = \alpha A(\lambda x)$ for $\lambda \ge 0$. Thus $S_\alpha$ is a cone.</p>
            </div>
            <div class="proof-step">
              <strong>Interpretation:</strong> This is the set of vectors "not too far" from having equal components. $\alpha=1$ implies $x_1 = \dots = x_n$ (the ray $\mathbf{1}$). $\alpha=0$ is the whole orthant.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.11 â€” Matrix Fractional Function</h3>
          <p>Show that $f(x, Y) = x^\top Y^{-1} x$ is convex on $\mathbb{R}^n \times \mathbb{S}^n_{++}$.</p>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Joint Convexity:</b> The function is convex in $(x, Y)$ jointly, not just individually.</li>
            <li><b>Schur Complement:</b> The condition for epigraph convexity ($x^\top Y^{-1} x \le t$) converts directly to an LMI using Schur complements.</li>
        </ul>
      </div>

<div class="proof-enhanced">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>Method 1: Epigraph via Schur Complement</strong>
              <p>The epigraph condition is $x^\top Y^{-1} x \le t$ with $Y \succ 0$.</p>
              <p>Using the Schur complement lemma, this is equivalent to:</p>
              $$ \begin{bmatrix} Y & x \\ x^\top & t \end{bmatrix} \succeq 0, \quad Y \succ 0 $$
              <p>This is a Linear Matrix Inequality (LMI) in variables $(x, Y, t)$, which defines a convex set. Thus the epigraph is convex.</p>
            </div>
            <div class="proof-step">
              <strong>Method 2: Supremum of Affine Functions</strong>
              <p>For fixed $Y \succ 0$, maximize $g_z(x) = 2z^\top x - z^\top Y z$ over $z$. The max is at $z = Y^{-1}x$, value $x^\top Y^{-1} x$.</p>
              <p>So $f(x, Y) = \sup_z (2z^\top x - \mathrm{tr}(zz^\top Y))$.</p>
              <p>Inside the sup is a function linear in $x$ and linear in $Y$. The supremum of affine functions is convex.</p>
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.12 â€” Definition of Convexity on $\mathbb{R}$</h3>
          <p>Let $f: \mathbb{R} \to \mathbb{R}$ be convex. Prove the following properties:</p>
          <ol type="a">
            <li><b>Chord Property:</b> $f(x) \le \frac{b-x}{b-a}f(a) + \frac{x-a}{b-a}f(b)$ for $a < x < b$.</li>
            <li><b>Slope Monotonicity:</b> $\frac{f(x)-f(a)}{x-a} \le \frac{f(b)-f(a)}{b-a} \le \frac{f(b)-f(x)}{b-x}$.</li>
            <li><b>Derivative Monotonicity:</b> If differentiable, $f'(a) \le \frac{f(b)-f(a)}{b-a} \le f'(b)$.</li>
            <li><b>Second Derivative:</b> If twice differentiable, $f''(x) \ge 0$.</li>
          </ol>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Secant Slope:</b> For a convex function, the slope of the secant lines is non-decreasing.</li>
            <li><b>Geometric Meaning:</b> As you move to the right, the function gets steeper (or less steep downwards). This monotonicity of slopes is equivalent to the non-negative second derivative.</li>
        </ul>
      </div>

<div class="proof-enhanced">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>(a) Chord Property:</strong>
              <p>Write $x$ as a convex combination: $x = \lambda a + (1-\lambda)b$ where $\lambda = \frac{b-x}{b-a}$.</p>
              <p>By convexity: $f(x) \le \lambda f(a) + (1-\lambda) f(b)$, which matches the formula.</p>
            </div>
            <div class="proof-step">
              <strong>(b) Slope Monotonicity:</strong>
              <p>Rearrange (a): multiply by $b-a$, subtract $(b-a)f(a)$:</p>
              $$ (b-a)(f(x)-f(a)) \le (x-a)(f(b)-f(a)) \implies \frac{f(x)-f(a)}{x-a} \le \frac{f(b)-f(a)}{b-a} $$
              <p>The second inequality follows by symmetry or applying the first to the interval $[x, b]$ with point $x$.</p>
            </div>
            <div class="proof-step">
              <strong>(c) & (d) Derivatives:</strong>
              <p>Taking limit $x \to a$ in (b) gives $f'(a) \le \frac{f(b)-f(a)}{b-a}$. Taking limit $x \to b$ gives the other side.</p>
              <p>This implies $f'$ is non-decreasing ($f'(a) \le f'(b)$). A differentiable function with non-decreasing derivative has $f'' \ge 0$.</p>
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.13 â€” Integral Characterization</h3>
          <p>Show $f$ is convex iff $\int_0^1 f(x + \lambda(y-x)) d\lambda \le \frac{f(x)+f(y)}{2}$.</p>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Hermite-Hadamard Inequality:</b> This is the "right half" of the famous inequality: $f(\frac{a+b}{2}) \le \frac{1}{b-a}\int_a^b f(x)dx \le \frac{f(a)+f(b)}{2}$.</li>
            <li><b>Midpoint Convexity:</b> Convexity at the midpoint, combined with continuity, implies convexity everywhere.</li>
        </ul>
      </div>

<div class="proof-enhanced">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>($\Rightarrow$) Convex implies Inequality:</strong>
              <p>Let $\phi(\lambda) = f(x + \lambda(y-x))$. If $f$ is convex, $\phi$ is convex on $[0,1]$.</p>
              <p>Thus $\phi(\lambda) \le (1-\lambda)\phi(0) + \lambda\phi(1)$.</p>
              <p>Integrate: $\int_0^1 \phi(\lambda) d\lambda \le \phi(0)\int(1-\lambda) + \phi(1)\int \lambda = \frac{1}{2}\phi(0) + \frac{1}{2}\phi(1)$.</p>
            </div>
            <div class="proof-step">
              <strong>($\Leftarrow$) Inequality implies Convex:</strong>
              <p>Applying the inequality to any sub-segment implies the <b>midpoint convexity</b> property: $\int_a^b \phi \le \frac{b-a}{2}(\phi(a)+\phi(b))$ implies $\phi(\frac{a+b}{2}) \le \frac{\phi(a)+\phi(b)}{2}$ (by considering limits of integrals near midpoint).</p>
              <p>Midpoint convexity + continuity implies convexity.</p>
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.14 â€” Running Average</h3>
          <p>If $f: \mathbb{R} \to \mathbb{R}$ is convex with $f(0) \le 0$, show $F(x) = \frac{1}{x} \int_0^x f(t) dt$ is convex for $x > 0$.</p>


      <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Integral Transform:</b> The running average is a linear operator on functions.</li>
            <li><b>Perspective Argument:</b> The change of variables reveals $F(x)$ is an integral (sum) of perspective functions $f(sx) = f(x \cdot s)/1$, which preserves convexity. Actually, simple scaling $g_s(x)=f(sx)$ is enough.</li>
        </ul>
      </div>

<div class="proof-enhanced">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>Change of Variables:</strong>
              <p>Let $t = sx$, so $dt = x ds$. Range $0 \to x$ becomes $0 \to 1$.</p>
              $$ F(x) = \frac{1}{x} \int_0^1 f(sx) x ds = \int_0^1 f(sx) ds $$
            </div>
            <div class="proof-step">
              <strong>Convexity Check:</strong>
              <p>For fixed $s \in [0, 1]$, the function $g_s(x) = f(sx)$ is convex in $x$ (composition of convex $f$ with linear map).</p>
              <p>$F(x)$ is a non-negative weighted sum (integral) of convex functions $g_s(x)$, so $F(x)$ is convex.</p>
            </div>
            <div class="proof-step">
              <strong>Interpretation:</strong> This is an expectation: $F(x) = \mathbb{E}[f(Sx)]$ where $S \sim U[0,1]$. Expectation preserves convexity.
            </div>
          </div>
        </div>
      </section>

    </article>

    <!-- SECTION 11: SUMMARY -->
    <section class="card-v2" id="summary">
      <h2>11. Summary</h2>
      <p><strong>Overview:</strong> This lecture develops the theory of convex functions, which form the foundation for convex optimization. We present multiple equivalent characterizations (Jensen's inequality, epigraph, first-order conditions, second-order conditions), prove key properties, and establish operations that preserve convexity. The lecture culminates with a toolkit for recognizing and constructing convex functions in practice.</p>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>Â© <span id="year"></a> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/theme-switcher.js"></script>
<script src="../../static/js/toc.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initConvexFunctionInspector } from './widgets/js/convex-function-inspector.js';
    initConvexFunctionInspector('widget-convex-function-inspector');
  </script>
  <script type="module">
    import { initHessianHeatmap } from './widgets/js/hessian-heatmap.js';
    initHessianHeatmap('widget-hessian-heatmap');
  </script>
  <script type="module">
    import { initOperationsPreserving } from './widgets/js/operations-preserving.js';
    initOperationsPreserving('widget-operations-preserving');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
