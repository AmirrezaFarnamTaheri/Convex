<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>08. Convex Optimization Problems: Conic Programming — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
</head>
<body>
  <!-- Header with navigation -->
  <header class="site-header sticky">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../07-convex-problems-standard/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../09-duality/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>08. Convex Optimization Problems: Conic Programming</h1>
      <div class="lecture-meta">
        <span>Date: 2025-11-18</span>
        <span>Duration: 90 min</span>
        <span>Tags: standard-forms, classification, SOCP, SDP, conic</span>
      </div>
      <div class="lecture-summary">
        <p><strong>Overview:</strong> This lecture covers advanced convex optimization problem classes: Second-Order Cone Programs (SOCP) and Semidefinite Programs (SDP). We also explore quasiconvex optimization and Disciplined Convex Programming (DCP) frameworks.</p>
        <p><strong>Prerequisites:</strong> <a href="../07-convex-problems-standard/index.html">Lecture 07: Convex Problems Standard</a> (standard form, LP, QP) and <a href="../04-convex-sets-cones/index.html">Lecture 04: Convex Sets Cones</a> (cones, dual cones).</p>
        <p><strong>Forward Connections:</strong> Duality theory (<a href="../09-duality/index.html">Lecture 09</a>) provides optimality conditions for conic programs. These problem classes appear throughout applications (Lectures 10-12).</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li>Define and recognize SOCP and SDP problems</li>
        <li>Understand the containment hierarchy: LP ⊂ QP ⊂ SOCP ⊂ SDP</li>
        <li>Formulate problems using conic constraints</li>
        <li>Understand quasiconvex optimization and its relationship to convex optimization</li>
        <li>Apply DCP rules to construct convex optimization problems</li>
      </ul>
    </section>

    <article>
      <section class="section-card" id="section-1">
      <h2>1. Second-Order Cone Programs (SOCP)</h2>

      <h3>1.1 Definition</h3>
      <p>A <strong>Second-Order Cone Program (<a href="#" class="definition-link" data-term="socp">SOCP</a>)</strong> has the form:</p>

      <div style="padding: 16px; background: var(--panel); border-left: 4px solid var(--brand); margin: 16px 0;">
        <p style="margin: 0;">
          $
          \begin{aligned}
          \text{minimize} \quad & f^\top x \\
          \text{subject to} \quad & \|A_i x + b_i\|_2 \le c_i^\top x + d_i, \quad i = 1, \dots, m \\
          & Fx = g
          \end{aligned}
          $
        </p>
      </div>

      <p>where $x \in \mathbb{R}^n$ is the variable. Each constraint $\|A_i x + b_i\|_2 \le c_i^T x + d_i$ is called a <strong>second-order cone constraint</strong>.</p>

      <h3>1.2 The Second-Order Cone (Lorentz Cone)</h3>
      <p>The <strong>second-order cone</strong> (or <strong>Lorentz cone</strong>) in $\mathbb{R}^{n+1}$ is:</p>
      <p style="text-align: center;">
        $
        \mathcal{L}^{n+1} = \{(x, t) \in \mathbb{R}^n \times \mathbb{R} \mid \|x\|_2 \le t\}
        $
      </p>

      <div class="proof-box">
        <h4>Convexity of the Second-Order Cone</h4>
        <div class="proof-step">
          <strong>Setup:</strong> Let $(x_1, t_1), (x_2, t_2) \in \mathcal{L}^{n+1}$ and $\theta \in [0, 1]$.
        </div>
        <div class="proof-step">
          <strong>Convex combination:</strong> We have $\|x_1\|_2 \le t_1$ and $\|x_2\|_2 \le t_2$. Consider:
          $
          \|\theta x_1 + (1-\theta) x_2\|_2 \le \theta \|x_1\|_2 + (1-\theta) \|x_2\|_2 \quad \text{(triangle inequality)}
          $
        </div>
        <div class="proof-step">
          <strong>Apply constraints:</strong>
          $
          \le \theta t_1 + (1-\theta) t_2
          $
        </div>
        <div class="proof-step">
          <strong>Conclusion:</strong> Thus $(\theta x_1 + (1-\theta) x_2, \theta t_1 + (1-\theta) t_2) \in \mathcal{L}^{n+1}$, so $\mathcal{L}^{n+1}$ is convex.
        </div>
      </div>

      <h3>1.3 Key Properties</h3>
      <ul>
        <li><strong>Generality:</strong> SOCP subsumes LP and QP</li>
        <li><strong>Solvers:</strong> Efficiently solved by interior-point methods</li>
        <li><strong>Applications:</strong> Robust optimization, robust least squares, $\ell_1$ and $\ell_\infty$ norm minimization</li>
      </ul>

      <h3>1.4 Standard SOCP Examples</h3>

      <h4>Example 1.1: Robust Least Squares</h4>
      <p>Consider the least-squares problem where the matrix $A$ is subject to unstructured uncertainty bounded by a spectral norm $\|\Delta A\|_2 \le \rho$. The <strong>robust</strong> problem is:</p>
      <p style="text-align: center;">
        $
        \min_x \max_{\|\Delta A\|_2 \le \rho} \|(A + \Delta A)x - b\|_2
        $
      </p>

      <div class="proof-box">
        <h4>Derivation of Robust Least Squares SOCP (Unstructured Uncertainty)</h4>

        <div class="proof-step">
          <strong>Step 1: Formulate the Worst-Case Objective.</strong>
          Let $r(x) = \max_{\|\Delta A\|_2 \le \rho} \|(Ax - b) + \Delta A x\|_2$.
          We want to find the perturbation $\Delta A$ that maximizes the residual norm.
        </div>

        <div class="proof-step">
          <strong>Step 2: Triangle Inequality Upper Bound.</strong>
          Using the triangle inequality:
          $$ \|(Ax - b) + \Delta A x\|_2 \le \|Ax - b\|_2 + \|\Delta A x\|_2 $$
          Since $\|\Delta A x\|_2 \le \|\Delta A\|_2 \|x\|_2 \le \rho \|x\|_2$, we have the upper bound:
          $$ r(x) \le \|Ax - b\|_2 + \rho \|x\|_2 $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Construct Worst-Case Perturbation.</strong>
          We show the bound is tight. Let $u = Ax - b$.
          If $u=0$ or $x=0$, the bound is trivial. Assume $u \ne 0, x \ne 0$.
          Choose $\Delta A$ to be a rank-1 matrix that aligns $\Delta A x$ with $u$:
          $$ \Delta A = \rho \frac{u}{\|u\|_2} \frac{x^\top}{\|x\|_2} $$
          This matrix has spectral norm $\rho$ (product of unit vectors scaled by $\rho$).
          Applying it to $x$:
          $$ \Delta A x = \rho \frac{u}{\|u\|_2} \frac{x^\top x}{\|x\|_2} = \rho \frac{u}{\|u\|_2} \|x\|_2 $$
          This vector is a positive scalar multiple of $u$, so the norms add directly:
          $$ \|u + \Delta A x\|_2 = \|u\|_2 + \|\Delta A x\|_2 = \|Ax - b\|_2 + \rho \|x\|_2 $$
        </div>

        <div class="proof-step">
          <strong>Step 4: Formulate as SOCP.</strong>
          The problem minimizes the sum of two Euclidean norms: $\|Ax - b\|_2 + \rho \|x\|_2$.
          We introduce epigraph variables $t_1, t_2$ to move the norms to the constraints:
          $$
          \begin{aligned}
          \text{minimize} \quad & t_1 + \rho t_2 \\
          \text{subject to} \quad & \|Ax - b\|_2 \le t_1 \\
          & \|x\|_2 \le t_2
          \end{aligned}
          $$
          These are standard Second-Order Cone (SOC) constraints. Thus, Robust Least Squares is an SOCP.
        </div>
      </div>

      <h4>Example 1.2: Sum of Euclidean Norms (Fermat-Weber Problem)</h4>
      <p>Consider the problem of finding a point $x$ that minimizes the sum of Euclidean distances to a set of fixed points $p_1, \dots, p_k$:</p>
      <p style="text-align: center;">
        $
        \text{minimize} \quad \sum_{i=1}^k \|x - p_i\|_2
        $
      </p>
      <p>This arises in facility location (finding the optimal warehouse location). It is <strong>not</strong> an LP because the Euclidean norm is not polyhedral. We formulate it as an SOCP by introducing auxiliary variables $t_1, \dots, t_k$:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & \sum_{i=1}^k t_i \\
        \text{subject to} \quad & \|x - p_i\|_2 \le t_i, \quad i = 1, \dots, k
        \end{aligned}
        $
      </p>
      <p>The constraints $\|x - p_i\|_2 \le t_i$ are standard second-order cone constraints $(x - p_i, t_i) \in \mathcal{L}^{n+1}$.</p>

      <h3>1.5 Hierarchy: LP ⊂ QP ⊂ SOCP</h3>
      <p>The problem classes form a nested hierarchy of expressiveness:</p>
      <ul>
        <li><b>LP ⊂ QP:</b> A Linear Program is a Quadratic Program with a zero quadratic term ($P=0$).</li>
        <li><b>QP ⊂ SOCP:</b> A convex Quadratic Program can be cast as an SOCP. The quadratic constraint $x^\top P x + q^\top x + r \le 0$ (with $P = L L^\top$) is equivalent to $\|L^\top x + \frac{1}{2}L^{-1}q\|_2 \le \text{affine}(x)$, which is an SOC constraint.</li>
      </ul>
    </section>

    <!-- Section 2: Semidefinite Programs (SDP) -->
    <section class="section-card" id="section-2">
      <h2>2. Semidefinite Programs (SDP)</h2>

      <h3>2.1 Definition</h3>
      <p>A <strong>Semidefinite Program (<a href="#" class="definition-link" data-term="sdp">SDP</a>)</strong> has the form:</p>

      <div style="padding: 16px; background: var(--panel); border-left: 4px solid var(--brand); margin: 16px 0;">
        <p style="margin: 0;">
          $
          \begin{aligned}
          \text{minimize} \quad & c^\top x \\
          \text{subject to} \quad & F(x) = F_0 + \sum_{i=1}^n x_i F_i \succeq 0 \\
          & Ax = b
          \end{aligned}
          $
        </p>
      </div>

      <p>where:</p>
      <ul>
        <li>$x \in \mathbb{R}^n$ is the variable</li>
        <li>$F_i \in \mathbb{S}^m$ (symmetric matrices) for $i = 0, \dots, n$</li>
        <li>$F(x) \succeq 0$ means $F(x)$ is positive semidefinite</li>
        <li>$A \in \mathbb{R}^{p \times n}$, $b \in \mathbb{R}^p$ define additional affine constraints</li>
      </ul>

      <h3>2.2 The PSD Cone</h3>
      <p>The constraint $F(x) \succeq 0$ means $F(x)$ lies in the cone of positive semidefinite matrices:</p>
      <p style="text-align: center;">
        $
        \mathbb{S}_+^m = \{X \in \mathbb{S}^m \mid X \succeq 0\}
        $
      </p>
      <p>Recall from Lecture 04 that $\mathbb{S}_+^m$ is a proper cone (convex, closed, pointed, solid).</p>

      <h3>2.3 Key Properties</h3>
      <ul>
        <li><strong>Generality:</strong> SDP subsumes LP, QP, and SOCP</li>
        <li><strong>Complexity:</strong> Polynomial-time solvable using interior-point methods</li>
        <li><strong>Duality:</strong> Strong duality typically holds; dual problem is also an SDP</li>
        <li><strong>Applications:</strong> Combinatorial optimization relaxations, control theory (LMI), matrix completion</li>
      </ul>

      <h3>2.4 Standard SDP Examples</h3>

      <h4>Example 2.1: Eigenvalue Minimization</h4>
      <p>Minimize the maximum eigenvalue of $A(x) = A_0 + \sum_{i=1}^n x_i A_i$:</p>
      <p style="text-align: center;">
        $
        \min_x \lambda_{\max}(A(x))
        $
      </p>
      <p>This is equivalent to the SDP:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & t \\
        \text{subject to} \quad & A(x) \preceq t I
        \end{aligned}
        $
      </p>
      <p>The constraint $A(x) \preceq tI$ is equivalent to $tI - A(x) \succeq 0$, which is an LMI (linear matrix inequality).</p>

      <h4>Example 2.2: Matrix Norm Minimization (Operator Norm)</h4>
      <p>Minimize the spectral norm (operator norm) of a matrix depending linearly on variables $x$: $\|A(x)\|_2$, where $A(x) = A_0 + \sum_{i=1}^n x_i A_i$.</p>

      <div class="proof-box">
        <h4>Derivation of SDP Form</h4>
        <div class="proof-step">
          <strong>Step 1: Epigraph Form.</strong>
          The problem is equivalent to:
          $$ \text{minimize } t \quad \text{subject to } \|A(x)\|_2 \le t $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Singular Value Characterization.</strong>
          The spectral norm $\|A\|_2$ is the maximum singular value $\sigma_{\max}(A)$. Recall from <a href="../01-linear-algebra-advanced/index.html">Lecture 01</a> that the spectral norm is $\sqrt{\lambda_{\max}(A^\top A)}$.
          The condition $\sigma_{\max}(A) \le t$ (for $t \ge 0$) is equivalent to saying all eigenvalues of $A^\top A$ are $\le t^2$:
          $$ A(x)^\top A(x) \preceq t^2 I $$
        </div>
        <div class="proof-step">
          <strong>Step 3: Schur Complement.</strong>
          We can rewrite the quadratic matrix inequality $t^2 I - A(x)^\top A(x) \succeq 0$ using the Schur complement.
          Recall that $\begin{bmatrix} A & B \\ B^\top & C \end{bmatrix} \succeq 0$ iff $C \succeq 0$ and $A - B C^{-1} B^\top \succeq 0$.
          We want to match $tI - A(x)^\top (tI)^{-1} A(x) \succeq 0$.
          Set $A_{block} = tI$, $B_{block} = A(x)^\top$, $C_{block} = tI$.
          Then the LMI is:
          $$ \begin{bmatrix} tI & A(x)^\top \\ A(x) & tI \end{bmatrix} \succeq 0 $$
        </div>
        <div class="proof-step">
          <strong>Step 4: Final SDP Formulation.</strong>
          $$
          \begin{aligned}
          \text{minimize} \quad & t \\
          \text{subject to} \quad & \begin{bmatrix} tI & A(x)^\top \\ A(x) & tI \end{bmatrix} \succeq 0
          \end{aligned}
          $$
          This is a standard SDP in variables $(x, t)$.
        </div>
      </div>

      <!-- Widget 5: SDP Visualizer -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Interactive: SDP Visualizer</h3>
        <p><strong>Purpose:</strong> 3D visualization of the positive semidefinite cone for $2 \times 2$ matrices.</p>
        <ul style="font-size: 14px; line-height: 1.6;">
          <li>Explore the geometry of $\mathbb{S}_+^2$ (a 3D cone in the space of symmetric matrices)</li>
          <li>Understand how SDP constraints restrict feasible matrices</li>
          <li>Connect eigenvalues to cone membership</li>
        </ul>
        <div id="widget-5" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <h3>2.5 The Hierarchy: LP ⊂ QP ⊂ SOCP ⊂ SDP</h3>
      <p>This inclusion hierarchy is strict and fundamental to choosing the right solver:</p>
      <ul>
        <li><b>LP ⊂ QP:</b> A Linear Program is a Quadratic Program with a zero quadratic term ($P=0$).</li>
        <li><b>QP ⊂ SOCP:</b> A convex Quadratic Program can be cast as an SOCP. The quadratic constraint $x^\top P x + q^\top x + r \le 0$ (with $P = L L^\top$) is equivalent to $\|L^\top x + \frac{1}{2}L^{-1}q\|_2 \le \text{affine}(x)$, which is an SOC constraint.</li>
        <li><b>SOCP ⊂ SDP:</b> Every SOCP can be formulated as an SDP using the Schur complement lemma (as shown below).</li>
      </ul>
      <p><b>Why not always use SDP?</b> While SDP is the most general, its solvers ($O(n^6)$ complexity or similar) are significantly slower than dedicated LP/QP solvers ($O(n^3)$). Always use the most specific class possible.</p>

      <div class="proof-box">
        <h4>Embedding SOCP into SDP via Schur Complement</h4>
        <p>We want to show that the second-order cone constraint $\|y\|_2 \le t$ (where $y = Ax+b, t = c^\top x + d$) is equivalent to a Linear Matrix Inequality (LMI).</p>

        <div class="proof-step">
          <strong>Step 1: Schur Complement Lemma.</strong>
          Recall that for a block matrix $M = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}$ with $A \succ 0$, we have $M \succeq 0 \iff C - B^\top A^{-1} B \succeq 0$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Construct the matrix.</strong>
          Consider the matrix:
          $$
          M = \begin{bmatrix} tI & y \\ y^\top & t \end{bmatrix}
          $$
          Assume $t > 0$ (if $t=0$, we need $y=0$, which is handled by the limit or closure). Then $tI \succ 0$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Apply the Lemma.</strong>
          Identifying blocks: $A = tI$, $B = y$, $C = t$.
          The condition $M \succeq 0$ is equivalent to:
          $$ t - y^\top (tI)^{-1} y \ge 0 $$
          $$ t - \frac{1}{t} y^\top y \ge 0 $$
          Multiplying by $t$ (since $t > 0$):
          $$ t^2 - y^\top y \ge 0 \implies \|y\|_2^2 \le t^2 $$
          Since $t > 0$, this is equivalent to $\|y\|_2 \le t$.
        </div>

        <div class="proof-step">
          <strong>Step 4: Conclusion.</strong>
          The SOC constraint $\|Ax+b\|_2 \le c^\top x + d$ is equivalent to the LMI:
          $$
          \begin{bmatrix} (c^\top x + d) I & Ax + b \\ (Ax + b)^\top & c^\top x + d \end{bmatrix} \succeq 0
          $$
          Since this constraint is linear in the matrix variable (which depends linearly on $x$), an SOCP is a special case of an SDP.
        </div>
      </div>
    </section>

    <!-- Section 3: Problem Reformulation and Equivalence -->
    <section class="section-card" id="section-3">
      <h2>3. Problem Reformulation and Equivalence</h2>

      <h3>3.1 Equivalent Problems</h3>
      <p>Two optimization problems are <strong>equivalent</strong> if the solution of one can be readily obtained from the solution of the other, and vice versa.</p>

      <h4>Common Equivalence Transformations:</h4>
      <ul>
        <li><strong>Change of variables:</strong> Substitute $x = \phi(y)$ where $\phi$ is one-to-one</li>
        <li><strong>Slack variables:</strong> Introduce $s$ to turn $f(x) \le t$ into $f(x) + s = t$, $s \ge 0$</li>
        <li><strong>Epigraph form:</strong> Minimize $t$ subject to $f(x) \le t$ (instead of minimizing $f(x)$)</li>
        <li><strong>Eliminating equality constraints:</strong> Use $Ax = b$ to eliminate variables</li>
      </ul>

      <h3>3.2 Epigraph Reformulation</h3>
      <p>The problem $\min f_0(x)$ s.t. $x \in \mathcal{C}$ is equivalent to:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & t \\
        \text{subject to} \quad & f_0(x) \le t \\
        & x \in \mathcal{C}
        \end{aligned}
        $
      </p>
      <p>This transformation is crucial for reformulating problems with complex objectives into standard forms.</p>

      <h3>3.3 Example: Minimizing $\ell_\infty$ Norm</h3>
      <p>Minimize $\|Ax - b\|_\infty$ can be reformulated as an LP:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & t \\
        \text{subject to} \quad & -t \mathbf{1} \preceq Ax - b \preceq t \mathbf{1}
        \end{aligned}
        $
      </p>

      <h3>3.4 Eliminating Linear Equality Constraints</h3>
      <p>Given $\min f_0(x)$ s.t. $Ax = b$ and other constraints, if $\text{range}(A) = \mathbb{R}^p$, we can parameterize the solution space as:</p>
      <p style="text-align: center;">
        $
        x = A^\dagger b + (I - A^\dagger A) z
        $
      </p>
      <p>where $A^\dagger$ is the pseudoinverse and $z \in \mathbb{R}^n$ is a new variable. This reduces the problem dimension.</p>
    </section>

      <section class="section-card" id="section-4">
      <h2>4. Quasiconvex Optimization</h2>

      <h3>4.1 Quasiconvex Functions</h3>
      <p>A function $f: \mathbb{R}^n \to \mathbb{R}$ is <strong>quasiconvex</strong> if its domain and all sublevel sets $\{x \mid f(x) \le \alpha\}$ are convex.</p>

      <p><strong>Examples:</strong></p>
      <ul>
        <li>$\log x$ on $\mathbb{R}_{++}$ (quasilinear: both quasiconvex and quasiconcave)</li>
        <li>$\lceil x \rceil$ (ceiling function)</li>
        <li>$x^3$ on $\mathbb{R}$ (not quasiconvex; sublevel sets are not convex)</li>
      </ul>

      <h3>4.2 Quasiconvex Optimization Problems</h3>
      <p>A problem is <strong>quasiconvex</strong> if the objective is quasiconvex and the feasible set is convex:</p>
      <p style="text-align: center;">
        $
        \begin{aligned}
        \text{minimize} \quad & f_0(x) \quad \text{(quasiconvex)} \\
        \text{subject to} \quad & f_i(x) \le 0 \quad \text{($f_i$ convex)} \\
        & Ax = b
        \end{aligned}
        $
      </p>

      <h3>4.3 Solution via Bisection (Convex Feasibility)</h3>
      <p>Quasiconvex optimization can be solved by solving a sequence of convex feasibility problems:</p>
      <ol>
        <li>Find interval $[l, u]$ containing $p^*$ (the optimal value)</li>
        <li>At each iteration, test if the sublevel set $\{x \in \mathcal{F} \mid f_0(x) \le t\}$ is nonempty (for $t = (l+u)/2$)</li>
        <li>Update $[l, u]$ based on feasibility</li>
        <li>Repeat until $u - l < \epsilon$</li>
      </ol>
      <p>Each feasibility check is a convex problem, so quasiconvex optimization reduces to convex optimization.</p>
    </section>

    <!-- Section 5: Disciplined Convex Programming (DCP) -->
    <section class="section-card" id="section-5">
      <h2>5. Disciplined Convex Programming (DCP)</h2>

      <h3>5.1 Motivation</h3>
      <p>While we can recognize many convex problems, it's often unclear whether a complex formulation is convex. <strong>Disciplined Convex Programming</strong> is a system of rules for constructing convex optimization problems from basic atoms.</p>

      <h3>5.2 DCP Rules</h3>
      <ul>
        <li><strong>Atoms:</strong> A set of basic convex/concave/affine functions with known curvature</li>
        <li><strong>Composition rules:</strong>
          <ul>
            <li>A convex function of a convex function is convex</li>
            <li>A convex function of a concave function is NOT necessarily convex</li>
            <li>A concave function of an affine function is concave</li>
          </ul>
        </li>
        <li><strong>Objective:</strong> Minimize a convex function or maximize a concave function</li>
        <li><strong>Constraints:</strong> Convex constraints of the form $f(x) \le 0$ where $f$ is convex</li>
      </ul>

      <h3>5.3 DCP in Practice: CVXPY</h3>
      <p>Tools like CVXPY verify DCP compliance and automatically transform problems to standard solver forms (LP, SOCP, SDP).</p>

      <!-- Widget 8: Solver Selection Guide -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Interactive: Solver Selection Guide</h3>
        <p><strong>Purpose:</strong> Decision tree tool for selecting the most appropriate solver (GLPK, CVXOPT, MOSEK, SeDuMi, etc.) based on problem structure and scale.</p>
        <ul style="font-size: 14px; line-height: 1.6;">
          <li>Input problem characteristics (size, sparsity, cone type)</li>
          <li>Receive solver recommendations with rationale</li>
          <li>Learn performance tradeoffs between solvers</li>
        </ul>
        <div id="widget-8" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>
    </section>

    <section class="section-card" id="section-6">
      <h2>6. Review & Cheat Sheet</h2>

      <h3>Problem Classes Reference</h3>
      <table class="data-table" style="width: 100%; margin-bottom: 24px;">
        <thead>
          <tr>
            <th>Class</th>
            <th>Objective</th>
            <th>Constraints</th>
            <th>Key Property</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>LP</b></td>
            <td>Linear</td>
            <td>Linear inequalities ($Ax \le b$)</td>
            <td>Solution at vertex; simplex method.</td>
          </tr>
          <tr>
            <td><b>QP</b></td>
            <td>Convex Quadratic</td>
            <td>Linear inequalities</td>
            <td>Unique solution if $P \succ 0$.</td>
          </tr>
          <tr>
            <td><b>SOCP</b></td>
            <td>Linear</td>
            <td>Norm constraints ($\|Ax+b\|_2 \le c^\top x + d$)</td>
            <td>Robust optimization; Euclidean geometry.</td>
          </tr>
          <tr>
            <td><b>SDP</b></td>
            <td>Linear</td>
            <td>LMI ($F_0 + \sum x_i F_i \succeq 0$)</td>
            <td>Optimizes over matrices; relaxation power.</td>
          </tr>
        </tbody>
      </table>

      <h3>Hierarchy</h3>
      <p style="text-align: center; font-size: 1.2em;">
        <strong>LP ⊂ QP ⊂ SOCP ⊂ SDP</strong>
      </p>
      <ul>
        <li>Every LP is a QP (with $P=0$).</li>
        <li>Every QP is an SOCP (via Cholesky factorization of constraints).</li>
        <li>Every SOCP is an SDP (via Schur complement).</li>
      </ul>
    </section>

    <section class="section-card" id="section-7">
      <h2><i data-feather="edit-3"></i> 7. Exercises</h2>
      <p>These problems consolidate the lecture material and provide practice in problem classification, formulation, and reformulation.</p>

      <div class="problem">
        <h3>P8.1 — Classify and Reformulate: Minimizing Maximum Absolute Deviation</h3>
        <p>Consider the problem:</p>
        <p style="text-align: center;">
          $
          \begin{aligned}
          \text{minimize} \quad & \max_{i=1,\dots,m} |a_i^\top x - b_i|
          \end{aligned}
          $
        </p>
        <p>where $a_i \in \mathbb{R}^n$ and $b_i \in \mathbb{R}$.</p>
        <p><strong>(a)</strong> Is this a convex optimization problem? Justify your answer.</p>
        <p><strong>(b)</strong> Reformulate it as a standard LP.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            <strong>Part (a): Convexity.</strong> The function $f(x) = \max_{i=1,\dots,m} |a_i^\top x - b_i|$ is the pointwise maximum of functions $|a_i^\top x - b_i|$. Since $|a_i^\top x - b_i|$ is convex (it's the composition of the absolute value function with an affine function), and the pointwise maximum of convex functions is convex, $f(x)$ is convex. Thus, this is a convex optimization problem.
          </div>
          <div class="proof-step">
            <strong>Part (b): LP reformulation.</strong> We reformulate using epigraph form and absolute value splitting:
            $
            \begin{aligned}
            \text{minimize} \quad & t \\
            \text{subject to} \quad & -t \le a_i^\top x - b_i \le t, \quad i = 1, \dots, m
            \end{aligned}
            $
            This is equivalent to:
            $
            \begin{aligned}
            \text{minimize} \quad & t \\
            \text{subject to} \quad & a_i^\top x - b_i \le t, \quad i = 1, \dots, m \\
            & -a_i^\top x + b_i \le t, \quad i = 1, \dots, m
            \end{aligned}
            $
            This is an LP in variables $(x, t) \in \mathbb{R}^{n+1}$.
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P8.2 — QP Formulation: Constrained Least Squares</h3>
        <p>Consider the problem of finding $x \in \mathbb{R}^n$ that minimizes $\|Ax - b\|_2^2$ subject to $l \preceq x \preceq u$ (box constraints).</p>
        <p><strong>(a)</strong> Write this problem in standard QP form.</p>
        <p><strong>(b)</strong> Under what conditions on $A$ is the problem strictly convex (thus having a unique solution)?</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            <strong>Part (a): QP formulation.</strong> Expand the objective:
            $
            \|Ax - b\|_2^2 = (Ax - b)^\top (Ax - b) = x^\top A^\top A x - 2 b^\top A x + b^\top b
            $
            The constant $b^\top b$ doesn't affect the minimizer, so we can write:
            $
            \begin{aligned}
            \text{minimize} \quad & x^\top (A^\top A) x - 2 (A^\top b)^\top x \\
            \text{subject to} \quad & x \succeq l \\
            & -x \succeq -u
            \end{aligned}
            $
            In standard QP form with $P = 2 A^\top A$, $q = -2 A^\top b$, and inequality constraints $x \ge l$, $x \le u$.
          </div>
          <div class="proof-step">
            <strong>Part (b): Strict convexity.</strong> The Hessian is $\nabla^2 f(x) = 2 A^\top A$. The problem is strictly convex if and only if $A^\top A \succ 0$, which occurs when $A$ has full column rank (i.e., $\text{rank}(A) = n$).
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P8.3 — SOCP Reformulation: $\ell_2$ Regularized Least Squares</h3>
        <p>Consider the problem:</p>
        <p style="text-align: center;">
          $
          \text{minimize} \quad \|Ax - b\|_2 + \lambda \|x\|_2
          $
        </p>
        <p>where $\lambda > 0$.</p>
        <p><strong>(a)</strong> Show that this is an SOCP by reformulating it in standard SOCP form.</p>
        <p><strong>(b)</strong> Compare this to Tikhonov regularization $\|Ax - b\|_2^2 + \lambda \|x\|_2^2$, which is a QP.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            <strong>Part (a): SOCP reformulation.</strong> Introduce auxiliary variables $t_1, t_2$ for the two norms:
            $
            \begin{aligned}
            \text{minimize} \quad & t_1 + \lambda t_2 \\
            \text{subject to} \quad & \|Ax - b\|_2 \le t_1 \\
            & \|x\|_2 \le t_2
            \end{aligned}
            $
            Both constraints are second-order cone constraints, so this is an SOCP in variables $(x, t_1, t_2)$.
          </div>
          <div class="proof-step">
            <strong>Part (b): Comparison to QP.</strong> Tikhonov regularization $\|Ax - b\|_2^2 + \lambda \|x\|_2^2$ is a QP (quadratic objective). The $\ell_2$ regularized version $\|Ax - b\|_2 + \lambda \|x\|_2$ is not a QP but an SOCP. Both are convex; the choice depends on application requirements (e.g., sparsity-promoting properties differ).
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P8.4 — SDP Example: Largest Eigenvalue Minimization</h3>
        <p>Consider the problem of minimizing $\lambda_{\max}(A_0 + x_1 A_1 + x_2 A_2)$ where $A_0, A_1, A_2 \in \mathbb{S}^n$ are given symmetric matrices.</p>
        <p><strong>(a)</strong> Formulate this as an SDP.</p>
        <p><strong>(b)</strong> Write the dual problem (you may use the general SDP duality results).</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            <strong>Part (a): SDP formulation.</strong> We use the characterization $\lambda_{\max}(M) \le t \iff M \preceq tI$:
            $
            \begin{aligned}
            \text{minimize} \quad & t \\
            \text{subject to} \quad & A_0 + x_1 A_1 + x_2 A_2 \preceq t I
            \end{aligned}
            $
            Equivalently:
            $
            \begin{aligned}
            \text{minimize} \quad & t \\
            \text{subject to} \quad & t I - (A_0 + x_1 A_1 + x_2 A_2) \succeq 0
            \end{aligned}
            $
            This is an SDP in variables $(x_1, x_2, t)$.
          </div>
          <div class="proof-step">
            <strong>Part (b): Dual SDP.</strong> Using standard SDP duality (<a href="../09-duality/index.html">Lecture 09</a>), the dual problem is:
            $
            \begin{aligned}
            \text{maximize} \quad & -\text{tr}(A_0 Z) \\
            \text{subject to} \quad & \text{tr}(A_i Z) = 0, \quad i = 1, 2 \\
            & \text{tr}(Z) = 1 \\
            & Z \succeq 0
            \end{aligned}
            $
            where $Z \in \mathbb{S}^n$ is the dual variable. This is also an SDP.
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P8.5 — Problem Classification: Portfolio Optimization with Risk Constraint</h3>
        <p>Consider a portfolio optimization problem with $n$ assets, expected returns $\mu \in \mathbb{R}^n$, and covariance matrix $\Sigma \in \mathbb{S}_{++}^n$:</p>
        <p style="text-align: center;">
          $
          \begin{aligned}
          \text{maximize} \quad & \mu^\top x \\
          \text{subject to} \quad & x^\top \Sigma x \le \sigma_{\max}^2 \\
          & \mathbf{1}^\top x = 1 \\
          & x \ge 0
          \end{aligned}
          $
        </p>
        <p><strong>(a)</strong> Classify this problem (LP, QP, SOCP, or SDP).</p>
        <p><strong>(b)</strong> Reformulate it to fit your classification.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            <strong>Part (a): Classification.</strong> The objective is linear (easy). The constraint $x^\top \Sigma x \le \sigma_{\max}^2$ is a quadratic constraint. Since we're maximizing a linear function (equivalent to minimizing $-\mu^\top x$), and we have a quadratic constraint, this is a <strong>Quadratically Constrained Quadratic Program (QCQP)</strong>, which is a subclass of SOCP (since $\Sigma \succ 0$).
          </div>
          <div class="proof-step">
            <strong>Part (b): SOCP reformulation.</strong> Since $\Sigma \succ 0$, we can write $\Sigma = L L^\top$ (Cholesky factorization). Then $x^\top \Sigma x = \|L^\top x\|_2^2$, so the constraint becomes:
            $
            \|L^\top x\|_2 \le \sigma_{\max}
            $
            The problem in standard SOCP form is:
            $
            \begin{aligned}
            \text{minimize} \quad & -\mu^\top x \\
            \text{subject to} \quad & \|L^\top x\|_2 \le \sigma_{\max} \\
            & \mathbf{1}^\top x = 1 \\
            & x \ge 0
            \end{aligned}
            $
          </div>
        </div>
      </div>
    </section>

    <section class="section-card" id="section-8">
      <h2>8. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Boyd & Vandenberghe, Convex Optimization:</strong> Chapter 4 — Convex Optimization Problems</li>
        <li><strong>Nesterov, Introductory Lectures on Convex Optimization:</strong> Chapter 1 — Linear and Conic Programming</li>
        <li><strong>Ben-Tal & Nemirovski, Lectures on Modern Convex Optimization:</strong> Part I — Conic Programming</li>
        <li><strong>CVXPY Documentation:</strong> <a href="https://www.cvxpy.org/" target="_blank">https://www.cvxpy.org/</a> — DCP rules and examples</li>
      </ul>
    </section>
    </article>
  </main></div>

  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">
        © <span id="year"></span> Convex Optimization Course ·
        <a href="../../README.md" style="color: var(--brand);">About</a>
      </p>
    </div>
  </footer>

  <!-- Load Pyodide for Python widgets (optional) -->
  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>

  <!-- Widget loaders -->
  <script type="module">
    import { initProblemRecognizer } from './widgets/js/problem-form-recognizer.js';
    initProblemRecognizer('widget-1');
  </script>
  <script type="module">
    import { initLPVisualizer } from './widgets/js/lp-visualizer.js';
    initLPVisualizer('widget-2');
  </script>
  <script type="module">
    import { initQPSandbox } from './widgets/js/qp-sandbox.js';
    initQPSandbox('widget-4');
  </script>
  <script type="module">
    import { initSDPVisualizer } from './widgets/js/sdp-visualizer.js';
    initSDPVisualizer('widget-5');
  </script>
  <script type="module">
    import { initProblemReformulationTool } from './widgets/js/reformulation-tool.js';
    initProblemReformulationTool('widget-6');
  </script>
  <script type="module">
    import { initSOCPExplorer } from './widgets/js/socp-explorer.js';
    initSOCPExplorer('widget-7');
  </script>
  <script type="module">
    import { initSolverGuide } from './widgets/js/solver-guide.js';
    initSolverGuide('widget-8');
  </script>

  <!-- Global utilities -->
  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/ui.js"></script>
<script src="../../static/js/toc.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
    feather.replace();
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>