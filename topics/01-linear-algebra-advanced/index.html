<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>01. Linear Algebra Advanced — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.128/build/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../00-linear-algebra-basics/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../02-introduction/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>01. Linear Algebra Advanced</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, linear-algebra, svd, qr, conditioning</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture covers advanced linear algebra topics essential for robust numerical optimization. We explore matrix factorizations (QR, SVD) that provide stable alternatives to the normal equations, define the pseudoinverse for rank-deficient problems, and analyze the impact of condition numbers on algorithmic stability.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-basics/index.html">Lecture 00: Linear Algebra Basics</a> (Subspaces, Norms, Least Squares).</p>
        <p><strong>Forward Connections:</strong> The SVD is central to the analysis of the Nuclear Norm in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>. Condition numbers determine the convergence rates of First-Order Methods in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a>.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Factorize Matrices:</b> Compute and interpret the QR decomposition and Singular Value Decomposition (SVD).</li>
        <li><b>Solve Robust Least Squares:</b> Apply QR and SVD to solve least squares problems without forming $A^\top A$.</li>
        <li><b>Handle Rank Deficiency:</b> Use the Moore-Penrose pseudoinverse to find minimum-norm solutions.</li>
        <li><b>Analyze Stability:</b> Calculate condition numbers and estimate the sensitivity of linear systems to perturbations.</li>
        <li><b>Approximate Matrices:</b> Apply the Eckart-Young-Mirsky theorem for optimal low-rank matrix approximations (PCA).</li>
      </ul>
    </section>

    <!-- SECTION 1: QR DECOMPOSITION -->
    <section class="section-card" id="section-qr">
      <h2>1. The QR Decomposition</h2>

      <h3>Motivation: Numerical Stability</h3>
      <p>In Lecture 00, we solved the least squares problem $\min \|Ax - b\|_2$ using the normal equations $A^\top A x = A^\top b$. While theoretically correct, this approach is numerically hazardous. If $A$ has condition number $\kappa(A)$, then $A^\top A$ has condition number $\kappa(A)^2$. For ill-conditioned matrices, this squaring can result in a loss of precision that exceeds machine limits.</p>
      <p>The <b>QR decomposition</b> offers a solution that depends on $\kappa(A)$ rather than $\kappa(A)^2$, providing significantly better stability.</p>

      <h3>Definition and Existence</h3>
      <div class="theorem-box">
        <h4>Theorem: QR Factorization</h4>
        <p>Any matrix $A \in \mathbb{R}^{m \times n}$ with $m \ge n$ and linearly independent columns can be factored as:</p>
        $$ A = QR $$
        <p>where $Q \in \mathbb{R}^{m \times n}$ has orthonormal columns ($Q^\top Q = I_n$) and $R \in \mathbb{R}^{n \times n}$ is upper triangular with positive diagonal entries.</p>
      </div>

      <div class="proof-box">
        <h4>Construction via Gram-Schmidt</h4>
        <p>The existence of the QR decomposition follows directly from the Gram-Schmidt orthogonalization process applied to the columns $a_1, \dots, a_n$ of $A$.</p>

        <div class="proof-step">
          <strong>Step 1: Initialization.</strong>
          Let $u_1 = a_1$. Normalize to get the first vector: $q_1 = u_1 / \|u_1\|_2$.
          <br>Set $r_{11} = \|u_1\|_2$. Then $a_1 = r_{11} q_1$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Iteration ($k=2, \dots, n$).</strong>
          We project the current vector $a_k$ onto the subspace spanned by the previous orthonormal vectors $q_1, \dots, q_{k-1}$ and subtract this projection. This process, "Gram-Schmidt orthogonalization", ensures that the remaining vector $v_k$ is orthogonal to all previous $q_j$:
          $$ v_k = a_k - \sum_{j=1}^{k-1} \langle q_j, a_k \rangle q_j $$
          Here, $\langle q_j, a_k \rangle$ is the projection coefficient of $a_k$ onto $q_j$. Let $r_{jk} = \langle q_j, a_k \rangle$. These are the coefficients in the upper triangle of $R$.
          <br>Normalize the residual to get the next basis vector: $r_{kk} = \|v_k\|_2$ and $q_k = v_k / r_{kk}$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Matrix Form.</strong>
          Rearranging the definition of $v_k$, we express each column $a_k$ as a linear combination of $q_1, \dots, q_k$:
          $$ a_k = r_{1k} q_1 + r_{2k} q_2 + \dots + r_{kk} q_k $$
          In matrix notation, this is exactly $A = QR$, where $R$ is upper triangular because $a_k$ depends only on the first $k$ columns of $Q$.
        </div>
      </div>

      <h3>Solving Least Squares with QR</h3>
      <p>Given $A = QR$, the least squares objective simplifies due to the orthogonal invariance of the Euclidean norm. The standard efficient approach uses the normal equations:</p>
      $$ A^\top A x = A^\top b \iff (QR)^\top (QR) x = (QR)^\top b \iff R^\top Q^\top Q R x = R^\top Q^\top b $$
      <p>Since $Q^\top Q = I$, this simplifies to $R^\top R x = R^\top Q^\top b$. Since $R$ is invertible (full rank assumption), we multiply by $R^{-\top}$:
      $$ R x = Q^\top b $$
      This is a triangular system, easily solvable by <b>back substitution</b> in $O(n^2)$ time.</p>
    </section>

    <!-- SECTION 2: SVD -->
    <section class="section-card" id="section-svd">
      <h2>2. The Singular Value Decomposition (SVD)</h2>

      <h3>Geometric Intuition</h3>
      <p>The SVD provides the "diagonalization" of any matrix, even rectangular ones. It states that <b>every linear map</b> can be decomposed into a rotation, a scaling, and another rotation.</p>
      $$ A = U \Sigma V^\top $$
      <ul>
        <li><b>$V^\top$ (Rotation/Reflection):</b> Aligns the input basis with the "principal axes" of the data.</li>
        <li><b>$\Sigma$ (Stretch):</b> Scales the axes by the singular values $\sigma_i \ge 0$.</li>
        <li><b>$U$ (Rotation/Reflection):</b> Rotates the stretched axes to their final directions in the output space.</li>
      </ul>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-basics/matrix-multiplication.svg"
             alt="SVD Geometric Interpretation"
             style="max-width: 80%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 2.1:</i> SVD decomposes a transformation into Rotation ($V^\top$) $\to$ Stretch ($\Sigma$) $\to$ Rotation ($U$).</figcaption>
      </figure>

      <h3>Theorem: Existence of SVD</h3>
      <div class="theorem-box">
        <p>Every matrix $A \in \mathbb{R}^{m \times n}$ can be factored as $A = U \Sigma V^\top$, where:</p>
        <ul>
          <li>$U \in \mathbb{R}^{m \times m}$ is orthogonal ($U^\top U = I$).</li>
          <li>$V \in \mathbb{R}^{n \times n}$ is orthogonal ($V^\top V = I$).</li>
          <li>$\Sigma \in \mathbb{R}^{m \times n}$ is diagonal with entries $\sigma_1 \ge \sigma_2 \ge \dots \ge 0$.</li>
        </ul>
      </div>

      <div class="proof-box">
        <h4>Constructive Proof via Spectral Theorem</h4>
        <p>We construct the SVD using the eigendecomposition of the symmetric PSD matrix $A^\top A$.</p>

        <div class="proof-step">
          <strong>Step 1: Eigenvalues of $A^\top A$.</strong>
          Let $A^\top A = V \Lambda V^\top$ be the spectral decomposition, with $\lambda_1 \ge \dots \ge \lambda_n \ge 0$.
          Define the singular values $\sigma_i = \sqrt{\lambda_i}$. Let $r$ be the rank of $A$, so $\sigma_1, \dots, \sigma_r > 0$ and $\sigma_{r+1} = \dots = 0$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Construct $U$.</strong>
          For each non-zero singular value index $i = 1, \dots, r$, define the vector $u_i = \frac{1}{\sigma_i} A v_i$.
          <br><b>Verify Orthonormality:</b> We compute the inner product $u_i^\top u_j$ for any pair $i, j \in \{1, \dots, r\}$:
          $$ u_i^\top u_j = \left(\frac{1}{\sigma_i} A v_i\right)^\top \left(\frac{1}{\sigma_j} A v_j\right) = \frac{1}{\sigma_i \sigma_j} v_i^\top (A^\top A v_j) $$
          Substitute the eigenvalue equation $A^\top A v_j = \lambda_j v_j = \sigma_j^2 v_j$:
          $$ = \frac{1}{\sigma_i \sigma_j} v_i^\top (\sigma_j^2 v_j) = \frac{\sigma_j^2}{\sigma_i \sigma_j} (v_i^\top v_j) $$
          Since the eigenvectors $v_i$ are orthonormal, $v_i^\top v_j = \delta_{ij}$ (1 if $i=j$, 0 otherwise).
          <ul>
            <li>If $i \ne j$, the result is 0.</li>
            <li>If $i = j$, the result is $\frac{\sigma_i^2}{\sigma_i^2} \cdot 1 = 1$.</li>
          </ul>
          Thus, the vectors $u_1, \dots, u_r$ form an orthonormal set.
          <br><b>Completion:</b> Use the Gram-Schmidt process to extend this set to a full orthonormal basis $\{u_1, \dots, u_m\}$ for $\mathbb{R}^m$. The matrix $U$ is formed with these basis vectors as columns.
        </div>

        <div class="proof-step">
          <strong>Step 3: Verification.</strong>
          Since $\{v_1, \dots, v_n\}$ is a basis for $\mathbb{R}^n$, two linear maps are equal if they agree on all basis vectors. We check $A v_i$ against $U \Sigma V^\top v_i$:
          <ul>
            <li>If $i \le r$:
              $$ (U \Sigma V^\top) v_i = U \Sigma e_i = U (\sigma_i e_i) = \sigma_i u_i = \sigma_i \left(\frac{1}{\sigma_i} A v_i\right) = A v_i $$
            </li>
            <li>If $i > r$:
              $$ (U \Sigma V^\top) v_i = U \Sigma e_i = U (0) = 0 $$
              Also $A v_i = 0$ because $v_i$ is an eigenvector of $A^\top A$ with eigenvalue $\lambda_i = 0$, which implies $v_i \in \mathcal{N}(A^\top A) = \mathcal{N}(A)$.
            </li>
          </ul>
          Thus $A = U \Sigma V^\top$.
        </div>
      </div>

      <h3>Matrix Norms via SVD</h3>
      <p>The SVD provides the most natural characterization of matrix norms:</p>
      <ul>
        <li><b>Spectral Norm (Operator Norm):</b> $\|A\|_2 = \sigma_{\max}(A) = \sigma_1$. It measures the maximum gain.</li>
        <li><b>Frobenius Norm (Energy):</b> $\|A\|_F = \sqrt{\sum \sigma_i^2}$. It measures the total energy.</li>
        <li><b>Nuclear Norm (Trace Norm):</b> $\|A\|_* = \sum \sigma_i$. This is the convex envelope of the rank function on the unit ball, playing the same role for matrices that the $\ell_1$ norm plays for vectors (sparsity promotion).</li>
      </ul>

      <h3>Low-Rank Approximation (Eckart-Young-Mirsky)</h3>
      <p>The SVD allows us to find the best approximation of a matrix $A$ by a matrix of lower rank $k$. This is crucial for compression (keeping signal) and denoising (discarding noise).</p>
      <div class="theorem-box">
        <p>Let $A_k = \sum_{i=1}^k \sigma_i u_i v_i^\top$ be the truncated SVD. Then $A_k$ is the solution to:</p>
        $$ \min_{\text{rank}(B) \le k} \|A - B\| $$
        <p>Intuitively, by keeping the largest singular values, we capture the directions where the matrix has the most energy (action), minimizing the residual energy.</p>
        <p>The truncated SVD $A_k$ is the optimal approximation for both the spectral norm ($\|\cdot\|_2$) and the Frobenius norm ($\|\cdot\|_F$). The errors are:</p>
        <ul>
          <li>Spectral Error: $\|A - A_k\|_2 = \sigma_{k+1}$</li>
          <li>Frobenius Error: $\|A - A_k\|_F = \sqrt{\sum_{i=k+1}^r \sigma_i^2}$</li>
        </ul>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: SVD for Image Compression</h3>
        <p><b>The Power of Low-Rank Approximation:</b> An image can be viewed as a matrix of pixel intensities. By computing its SVD and keeping only the largest singular values, we can reconstruct the image with far less data.</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Concept:</b> $A \approx \sum_{i=1}^k \sigma_i u_i v_i^\top$.</li>
          <li><b>Observation:</b> Notice how the structure (large features) is captured by the first few modes, while details (and noise) reside in the smaller singular values.</li>
        </ul>
        <div id="widget-svd-approximator" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 3: PSEUDOINVERSE -->
    <section class="section-card" id="section-pseudoinverse">
      <h2>3. The Pseudoinverse and Condition Number</h2>

      <h3>The Moore-Penrose Pseudoinverse ($A^+$)</h3>
      <p>When $A$ is not square or not invertible, we cannot define $A^{-1}$. However, we can define a generalized inverse $A^+$ that "inverts" the action of $A$ on its range and sends the orthogonal complement to zero.</p>

      <p><b>Definition via SVD:</b> If $A = U \Sigma V^\top$, then:
      $$ A^+ = V \Sigma^+ U^\top $$
      where $\Sigma^+$ is the $n \times m$ matrix obtained by transposing $\Sigma$ and inverting the non-zero singular values:
      $$
      \Sigma^+ = \begin{bmatrix}
      1/\sigma_1 & & & 0 & \dots \\
      & \ddots & & \vdots & \\
      & & 1/\sigma_r & 0 & \dots \\
      0 & \dots & 0 & 0 & \dots \\
      \vdots & & \vdots & \vdots & \ddots
      \end{bmatrix}
      $$
      (The non-zero block is diagonal with entries $1/\sigma_i$; all other entries are zero).</p>

      <h4>Properties and Applications</h4>
      <ul>
        <li><b>Minimum Norm Solution:</b> For any system $Ax=b$ (or the least squares problem), the vector $x^\star = A^+ b$ is the solution that has the <b>minimum Euclidean norm</b>.
        <br><i>Why?</i> The general solution is $x = A^+ b + (I - A^+ A)z$ for arbitrary $z$. The first term $A^+ b$ lies in the row space $\mathcal{R}(A^\top)$ (image of $V$), while the second term projects onto the nullspace $\mathcal{N}(A)$. Since these subspaces are orthogonal, $\|x\|^2 = \|A^+ b\|^2 + \|(I - A^+ A)z\|^2$, which is minimized when the second term is zero (since norms are non-negative).</li>
        <li><b>Projectors:</b> $AA^+$ is the orthogonal projector onto the column space $\mathcal{R}(A)$. $A^+A$ is the orthogonal projector onto the row space $\mathcal{R}(A^\top)$.</li>
      </ul>

      <h3>Condition Number</h3>
      <p>The condition number $\kappa(A)$ measures the sensitivity of the solution of a linear system to perturbations in the data.</p>
      $$ \kappa(A) = \|A\|_2 \|A^+\|_2 = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} $$
      <p>where $\sigma_{\min}$ is the smallest <i>non-zero</i> singular value.</p>

      <ul>
        <li><b>$\kappa \approx 1$:</b> Well-conditioned. Errors are not amplified. Orthogonal matrices have $\kappa = 1$.</li>
        <li><b>$\kappa \gg 1$:</b> Ill-conditioned. Small errors in $b$ can lead to massive errors in $x$.</li>
      </ul>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Condition Number & Stability</h3>
        <p><b>Visualize Ill-Conditioning:</b> Compare solving $Ax=b$ for a well-conditioned matrix versus an ill-conditioned one. In the ill-conditioned case, the "ellipse" of the matrix is very skinny. A small change in the target vector $b$ (moving the target slightly off the major axis) requires a massive change in $x$ to compensate.</p>
        <div id="widget-condition-number" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 4: REVIEW -->
    <section class="section-card" id="section-review">
      <h2>4. Review & Cheat Sheet</h2>

      <table class="data-table">
        <thead>
          <tr>
            <th>Decomposition</th>
            <th>Formula</th>
            <th>Components</th>
            <th>Primary Use</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>QR</b></td>
            <td>$A = QR$</td>
            <td>$Q$ orthonormal, $R$ upper triangular</td>
            <td>Least squares, Gram-Schmidt</td>
          </tr>
          <tr>
            <td><b>SVD</b></td>
            <td>$A = U \Sigma V^\top$</td>
            <td>$U, V$ orthogonal, $\Sigma$ diagonal</td>
            <td>Rank, Norms, Approximation, PCA</td>
          </tr>
          <tr>
            <td><b>Spectral</b></td>
            <td>$A = Q \Lambda Q^\top$</td>
            <td>$Q$ orthogonal, $\Lambda$ real diagonal</td>
            <td>Symmetric matrices ($A=A^\top$)</td>
          </tr>
        </tbody>
      </table>

      <h3>Key Identities</h3>
      <ul>
        <li>$\|A\|_2 = \sigma_1$</li>
        <li>$\|A\|_F = \sqrt{\sum \sigma_i^2}$</li>
        <li>$A^+ = V \Sigma^+ U^\top$</li>
        <li>$\kappa(A) = \sigma_1 / \sigma_r$</li>
      </ul>
    </section>

    <!-- SECTION 5: EXERCISES -->
    <section class="section-card" id="section-exercises"><h2><i data-feather="edit-3"></i> 5. Exercises</h2>
      <div class="insight">
        <h4>Recap & Key Concepts</h4>
        <p>These exercises explore the theoretical depths of matrix norms, the geometry of projections, and the algebraic properties of the trace. P1.1 links norms to duality (Lecture 02). P1.14 links SVD to the operator norm.</p>
      </div>

      <h3>P1.1 — Dual of $\ell_p$ Norm</h3>
      <p>Show that for $1 < p < \infty$, the dual norm of $\|\cdot\|_p$ is $\|\cdot\|_q$ where $\frac{1}{p} + \frac{1}{q} = 1$. Use Hölder's Inequality for the upper bound and construct an alignment vector for the lower bound.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>Upper bound ($\le$):</strong> By Hölder's inequality, $x^\top y \le \|x\|_p \|y\|_q$.
          The definition of the dual norm is $\|y\|_* = \sup_{\|x\|_p \le 1} x^\top y$.
          From Hölder, if $\|x\|_p \le 1$, then $x^\top y \le \|y\|_q$. Thus $\|y\|_* \le \|y\|_q$.
        </div>
        <div class="proof-step">
          <strong>Lower bound ($\ge$):</strong> We need to find specific $x$ with $\|x\|_p=1$ such that $x^\top y = \|y\|_q$.
          Let $x_i = \mathrm{sign}(y_i) |y_i|^{q-1}$. Then $x_i y_i = |y_i|^q$.
          $\|x\|_p^p = \sum |y_i|^{p(q-1)}$. Since $1/p + 1/q = 1$, we have $p(q-1) = p(q - q/p) = pq(1 - 1/p) = pq(1/q) = q$.
          So $\|x\|_p^p = \sum |y_i|^q = \|y\|_q^q$. Thus $\|x\|_p = \|y\|_q^{q/p}$.
          Normalize: let $\tilde{x} = x / \|y\|_q^{q/p}$. Then $\|\tilde{x}\|_p = 1$.
          $\tilde{x}^\top y = \frac{1}{\|y\|_q^{q/p}} \sum |y_i|^q = \frac{\|y\|_q^q}{\|y\|_q^{q/p}} = \|y\|_q^{q - q/p} = \|y\|_q^1 = \|y\|_q$.
          Thus the supremum is at least $\|y\|_q$.
        </div>
      </div>

      <h3>P1.2 — Frobenius Cauchy–Schwarz</h3>
      <p>Prove that $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$ by viewing matrices in $\mathbb{R}^{m \times n}$ as vectors in $\mathbb{R}^{mn}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The Frobenius norm corresponds to the standard Euclidean norm of the vectorized matrix: $\|A\|_F = \|\mathrm{vec}(A)\|_2$.
        The inner product of matrices is $\langle A, B \rangle = \mathrm{tr}(A^\top B) = \sum_{i,j} A_{ij} B_{ij}$.
        This is exactly the dot product of the vectorized forms: $\mathrm{vec}(A)^\top \mathrm{vec}(B)$.
        By the standard Cauchy-Schwarz inequality for vectors:
        $$ |\mathrm{vec}(A)^\top \mathrm{vec}(B)| \le \|\mathrm{vec}(A)\|_2 \|\mathrm{vec}(B)\|_2 $$
        Substituting back the matrix notation yields $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.
        </p>
      </div>

      <h3>P1.3 — SVD Computation</h3>
      <p>Compute the Singular Value Decomposition $A = U \Sigma V^\top$ by hand for the matrix $A = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$. Verify that the singular values are the square roots of the eigenvalues of $A^\top A$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>1. Compute $A^\top A$ and its eigenvalues:</strong>
          $A^\top A = \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$.
          Characteristic eq: $(1-\lambda)^2 - 1 = 0 \implies \lambda^2 - 2\lambda = 0 \implies \lambda_1=2, \lambda_2=0$.
          Singular values: $\sigma_1 = \sqrt{2}, \sigma_2 = 0$.
        </div>
        <div class="proof-step">
          <strong>2. Compute $V$ (eigenvectors of $A^\top A$):</strong>
          For $\lambda_1=2$: $\begin{bmatrix} -1 & 1 \\ 1 & -1 \end{bmatrix} v = 0 \implies v_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$.
          For $\lambda_2=0$: $v_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}$.
          $V = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}$.
        </div>
        <div class="proof-step">
          <strong>3. Compute $U$:</strong>
          $u_1 = \frac{1}{\sigma_1} A v_1 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{1}{2} \begin{bmatrix} 2 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$.
          Since $m=2$, we need a second vector $u_2$ orthogonal to $u_1$. $u_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$.
          $U = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$.
        </div>
        <div class="proof-step">
          <strong>Result:</strong> $A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} \sqrt{2} & 0 \\ 0 & 0 \end{bmatrix} \left(\frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}\right)^\top$.
        </div>
      </div>

      <h3>P1.6 — Normal Equations vs. QR</h3>
      <p>Solve the system $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix}, b = \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}$ using (a) Normal Equations and (b) QR factorization. Compare the intermediate steps.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>(a) Normal Equations:</b> $A^\top A x = A^\top b$.
        $A^\top A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$.
        $A^\top b = \begin{bmatrix} 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix} \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.
        Solve $\begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} x = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.
        Multiply by inverse $\frac{1}{3} \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}$:
        $x = \frac{1}{3} \begin{bmatrix} 6-4 \\ -3+8 \end{bmatrix} = \frac{1}{3} \begin{bmatrix} 2 \\ 5 \end{bmatrix}$.</p>
        <p><b>(b) QR Factorization:</b>
        Gram-Schmidt on columns $a_1, a_2$.
        $u_1 = a_1 = (1, 0, 1)^\top$. $q_1 = (1/\sqrt{2}, 0, 1/\sqrt{2})^\top$.
        $v_2 = a_2 - \langle a_2, q_1 \rangle q_1 = (1, 1, 0)^\top - (1/\sqrt{2}) (1/\sqrt{2}, 0, 1/\sqrt{2})^\top = (1, 1, 0)^\top - (1/2, 0, 1/2)^\top = (1/2, 1, -1/2)^\top$.
        Normalize $v_2$: $\|v_2\|^2 = 1/4 + 1 + 1/4 = 1.5 = 3/2$. $\|v_2\| = \sqrt{3/2}$.
        $q_2 = \sqrt{2/3} (1/2, 1, -1/2)^\top$.
        Solve $Rx = Q^\top b$.
        $R = \begin{bmatrix} \sqrt{2} & 1/\sqrt{2} \\ 0 & \sqrt{3/2} \end{bmatrix}$. $Q^\top b = \begin{bmatrix} 3/\sqrt{2} \\ 4\sqrt{2/3}-1/\sqrt{6} \end{bmatrix}$.
        Back substitution yields same $x$. QR avoids forming $A^\top A$ (better condition number).</p>
      </div>

      <h3>P1.7 — Pseudoinverse Projectors</h3>
      <p>Show that $P = AA^+$ is the orthogonal projector onto $\mathcal{R}(A)$. Verify $P^2=P$ and $P^\top=P$ using SVD.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Let $A = U \Sigma V^\top$. Then $A^+ = V \Sigma^+ U^\top$.
        $P = AA^+ = (U \Sigma V^\top)(V \Sigma^+ U^\top) = U (\Sigma \Sigma^+) U^\top$.
        $\Sigma \Sigma^+$ is a diagonal matrix with 1s for non-zero singular values and 0s otherwise.
        Let $U_r$ be the first $r$ columns of $U$. Then $P = U_r U_r^\top$.
        <br><b>Idempotent:</b> $P^2 = (U_r U_r^\top)(U_r U_r^\top) = U_r (U_r^\top U_r) U_r^\top = U_r I U_r^\top = P$.
        <br><b>Symmetric:</b> $P^\top = (U_r U_r^\top)^\top = (U_r^\top)^\top U_r^\top = U_r U_r^\top = P$.
        <br>Since the columns of $U_r$ form a basis for $\mathcal{R}(A)$, $P$ is the orthogonal projector onto $\mathcal{R}(A)$.</p>
      </div>

      <h3>P1.8 — Projector Formula</h3>
      <p>Let $A$ have linearly independent columns. Derive the projection formula $P = A(A^\top A)^{-1}A^\top$ by minimizing $\|Ax - b\|_2^2$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>We want to find the point $p = Ax$ in the column space closest to $b$.
        Problem: $\min_x \|Ax - b\|_2^2$.
        Gradient: $2A^\top (Ax - b) = 0$.
        Normal eqs: $A^\top A x = A^\top b$.
        Since cols are independent, $A^\top A$ is invertible.
        Solution: $x^* = (A^\top A)^{-1} A^\top b$.
        Projection: $p = A x^* = A(A^\top A)^{-1} A^\top b$.
        The matrix mapping $b$ to $p$ is $P = A(A^\top A)^{-1} A^\top$.</p>
      </div>

      <h3>P1.9 — Hyperplane Projection</h3>
      <p>Find the projection of $(1, 2, 3)^\top$ onto $x_1 + x_2 + x_3 = 3$. Verify your result using the standard formula for distance to a plane.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Normal $a = (1, 1, 1)^\top$. Point $y = (1, 2, 3)^\top$. Target $b=3$.
        $p = y - \frac{a^\top y - b}{\|a\|^2} a$.
        $a^\top y = 1+2+3 = 6$. $\|a\|^2 = 3$.
        $p = (1, 2, 3)^\top - \frac{6-3}{3} (1, 1, 1)^\top = (1, 2, 3)^\top - (1, 1, 1)^\top = (0, 1, 2)^\top$.
        Check: $0+1+2=3$. Correct.
        Distance: $\|y-p\| = \|(1, 1, 1)\| = \sqrt{3}$.
        Formula: $\frac{|a^\top y - b|}{\|a\|} = \frac{|6-3|}{\sqrt{3}} = \frac{3}{\sqrt{3}} = \sqrt{3}$. Matches.</p>
      </div>

      <h3>P1.10 — Condition Number Calculation</h3>
      <p>Calculate the condition number $\kappa(A) = \sigma_{\max}(A)/\sigma_{\min}(A)$ for the matrix $A = \begin{bmatrix} 1 & 0 \\ 0 & \epsilon \end{bmatrix}$ where $\epsilon > 0$. What happens as $\epsilon \to 0$?</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The matrix is diagonal, so its singular values are the absolute values of diagonal entries: $1$ and $\epsilon$.
        Assuming $\epsilon < 1$: $\sigma_{\max} = 1, \sigma_{\min} = \epsilon$.
        $\kappa(A) = 1/\epsilon$.
        As $\epsilon \to 0$, $\kappa(A) \to \infty$. The matrix becomes ill-conditioned (nearly singular).</p>
      </div>

      <h3>P1.11 — Pseudoinverse of Rank-1 Matrix</h3>
      <p>Let $A = xy^\top$ where $x, y \in \mathbb{R}^n$ are non-zero vectors. Show that $A^+ = \frac{A^\top}{\|x\|_2^2 \|y\|_2^2}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>SVD of rank-1 matrix: $A = \sigma u v^\top$.
        $\|A\|_F = \|x\|\|y\| = \sigma$.
        $u = x/\|x\|$, $v = y/\|y\|$.
        Check: $A = (\|x\|\|y\|) \frac{x}{\|x\|} \frac{y^\top}{\|y\|} = xy^\top$. Correct.
        Pseudoinverse: $A^+ = \frac{1}{\sigma} v u^\top = \frac{1}{\|x\|\|y\|} \frac{y}{\|y\|} \frac{x^\top}{\|x\|} = \frac{yx^\top}{\|x\|^2 \|y\|^2}$.
        Note that $A^\top = (xy^\top)^\top = yx^\top$.
        Thus $A^+ = \frac{A^\top}{\|x\|^2 \|y\|^2}$.</p>
      </div>

      <h3>P1.12 — Operator Norm Properties</h3>
      <p>Prove that $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$. Use the Rayleigh Quotient definition of eigenvalues.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>$\|A\|_2^2 = \sup_{x \ne 0} \frac{\|Ax\|^2}{\|x\|^2} = \sup_{x \ne 0} \frac{x^\top A^\top A x}{x^\top x}$.
        The term on the right is the Rayleigh quotient for the symmetric matrix $M = A^\top A$.
        By the variational characterization of eigenvalues, the maximum value of the Rayleigh quotient is $\lambda_{\max}(M)$.
        Thus $\|A\|_2^2 = \lambda_{\max}(A^\top A)$, so $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$.</p>
      </div>

      <h3>P1.13 — Orthogonal Invariance</h3>
      <p>Show that $\|U A V^\top\|_F = \|A\|_F$ and $\|U A V^\top\|_2 = \|A\|_2$ for orthogonal $U, V$. This proves that these norms depend only on the singular values.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Frobenius:</b> $\|X\|_F^2 = \mathrm{tr}(X^\top X)$.
        $\|UAV^\top\|_F^2 = \mathrm{tr}(V A^\top U^\top U A V^\top) = \mathrm{tr}(V A^\top A V^\top) = \mathrm{tr}(V^\top V A^\top A) = \mathrm{tr}(A^\top A) = \|A\|_F^2$.
        <br><b>Spectral:</b> $\|UAV^\top\|_2 = \sup_{\|x\|=1} \|UAV^\top x\|$.
        Since $U$ is orthogonal, $\|Uy\| = \|y\|$. So we maximize $\|A (V^\top x)\|$.
        Since $V$ is orthogonal, as $x$ ranges over the unit sphere, $y = V^\top x$ also ranges over the unit sphere.
        Thus $\sup_{\|x\|=1} \|A V^\top x\| = \sup_{\|y\|=1} \|Ay\| = \|A\|_2$.
        </p>
      </div>

      <h3>P1.14 — The Orthogonal Group</h3>
      <p>Show that the set of orthogonal matrices forms a compact group.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Group:</b>
        1. Closure: $(Q_1 Q_2)^\top (Q_1 Q_2) = Q_2^\top Q_1^\top Q_1 Q_2 = Q_2^\top I Q_2 = I$.
        2. Inverse: $Q^{-1} = Q^\top$, which is orthogonal since $(Q^\top)^\top Q^\top = Q Q^\top = I$.
        3. Identity: $I \in O(n)$.
        <br><b>Compactness:</b>
        1. Bounded: The columns are unit vectors, so $\|Q\|_F = \sqrt{n}$. The set is bounded.
        2. Closed: The condition $Q^\top Q = I$ is a continuous equality constraint (level set of a continuous function). Thus the set is closed.
        By Heine-Borel, closed and bounded in finite dimensions implies compact.
        </p>
      </div>
</section>

    <!-- SECTION 6: READINGS -->
    <section class="section-card" id="section-readings">
      <h2>6. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required:</strong> Boyd & Vandenberghe, Appendix A.</li>
        <li><strong>Recommended:</strong> Trefethen & Bau, <em>Numerical Linear Algebra</em>, Lectures 1–5.</li>
        <li><strong>Advanced References:</strong> Golub & Van Loan, <em>Matrix Computations</em>, Chapter 2.</li>
      </ul>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initSvdApproximator } from './widgets/js/svd-approximator.js';
    initSvdApproximator('widget-svd-approximator');
  </script>
  <script type="module">
    import { initConditionNumber } from './widgets/js/condition-number.js';
    initConditionNumber('widget-condition-number');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
