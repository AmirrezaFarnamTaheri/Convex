<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>01. Linear Algebra Advanced — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.128/build/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../00-linear-algebra-basics/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../02-introduction/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>01. Linear Algebra Advanced</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, linear-algebra, svd, qr, conditioning</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture covers advanced linear algebra topics essential for robust numerical optimization. We explore matrix factorizations (QR, SVD) that provide stable alternatives to the normal equations, define the pseudoinverse for rank-deficient problems, and analyze the impact of condition numbers on algorithmic stability.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-basics/index.html">Lecture 00: Linear Algebra Basics</a> (Subspaces, Norms, Least Squares).</p>
        <p><strong>Forward Connections:</strong> The SVD is central to the analysis of the Nuclear Norm in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>. Condition numbers determine the convergence rates of First-Order Methods in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a>.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Factorize Matrices:</b> Compute and interpret the QR decomposition and Singular Value Decomposition (SVD) as fundamental tools for numerical linear algebra.</li>
        <li><b>Solve Robust Least Squares:</b> Apply QR and SVD to solve least squares problems without forming the potentially ill-conditioned matrix $A^\top A$.</li>
        <li><b>Handle Rank Deficiency:</b> Use the Moore-Penrose pseudoinverse to find minimum-norm solutions for rank-deficient or underdetermined systems.</li>
        <li><b>Analyze Stability:</b> Calculate condition numbers and estimate the sensitivity of linear systems to perturbations in data.</li>
        <li><b>Approximate Matrices:</b> Apply the Eckart-Young-Mirsky theorem for optimal low-rank matrix approximations (PCA) using truncated SVD.</li>
      </ul>
    </section>

    <!-- SECTION 1: QR DECOMPOSITION -->
    <section class="section-card" id="section-qr">
      <h2>1. The QR Decomposition</h2>

      <h3>Motivation: Numerical Stability</h3>
      <p>In <a href="../00-linear-algebra-basics/index.html">Lecture 00</a>, we solved the least squares problem $\min \|Ax - b\|_2$ using the normal equations $A^\top A x = A^\top b$. While theoretically correct, this approach can be numerically unstable. If $A$ has condition number $\kappa(A)$, then $A^\top A$ has condition number $\kappa(A)^2$. For ill-conditioned matrices, this squaring can result in a loss of precision that exceeds machine limits (floating-point errors dominate).</p>
      <p>The <b>QR decomposition</b> offers a solution that depends on $\kappa(A)$ rather than $\kappa(A)^2$, providing significantly better stability.</p>

      <h3>Definition and Existence</h3>
      <div class="theorem-box">
        <h4>Theorem: QR Factorization</h4>
        <p>Any matrix $A \in \mathbb{R}^{m \times n}$ with $m \ge n$ and linearly independent columns can be factored as:</p>
        $$ A = QR $$
        <p>where $Q \in \mathbb{R}^{m \times n}$ has orthonormal columns ($Q^\top Q = I_n$) and $R \in \mathbb{R}^{n \times n}$ is upper triangular with positive diagonal entries.</p>
      </div>

      <div class="proof-box">
        <h4>Construction via Gram-Schmidt</h4>
        <p>The existence of the QR decomposition follows directly from the Gram-Schmidt orthogonalization process applied to the columns $a_1, \dots, a_n$ of $A$.</p>

        <div class="proof-step">
          <strong>Step 1: Initialization.</strong>
          Let $u_1 = a_1$. If $u_1 = 0$, we can pick any unit vector or skip; assuming linear independence, $u_1 \neq 0$. Normalize to get the first vector: $q_1 = u_1 / \|u_1\|_2$.
          <br>Set $r_{11} = \|u_1\|_2$. Then $a_1 = r_{11} q_1$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Iteration ($k=2, \dots, n$).</strong>
          We project the current vector $a_k$ onto the subspace spanned by the previous orthonormal vectors $q_1, \dots, q_{k-1}$ and subtract this projection. This process, "Gram-Schmidt orthogonalization", ensures that the remaining vector $v_k$ is orthogonal to all previous $q_j$:
          $$ v_k = a_k - \sum_{j=1}^{k-1} \underbrace{\langle q_j, a_k \rangle}_{r_{jk}} q_j $$
          Here, $\langle q_j, a_k \rangle$ is the projection coefficient of $a_k$ onto $q_j$. Let $r_{jk} = \langle q_j, a_k \rangle$. These are the coefficients in the upper triangle of $R$.
          <br>Normalize the residual to get the next basis vector: $r_{kk} = \|v_k\|_2$ and $q_k = v_k / r_{kk}$. (Note: $r_{kk} \neq 0$ due to linear independence).
        </div>

        <div class="proof-step">
          <strong>Step 3: Matrix Form.</strong>
          Rearranging the definition of $v_k$, we express each column $a_k$ as a linear combination of $q_1, \dots, q_k$:
          $$ a_k = r_{1k} q_1 + r_{2k} q_2 + \dots + r_{kk} q_k $$
          In matrix notation, this is exactly $A = QR$, where $R$ is upper triangular because $a_k$ depends only on the first $k$ columns of $Q$.
        </div>
      </div>

      <h3>Solving Least Squares with QR</h3>
      <p>Given $A = QR$, the least squares objective simplifies due to the orthogonal invariance of the Euclidean norm. The standard efficient approach uses the normal equations:</p>
      $$ A^\top A x = A^\top b \iff (QR)^\top (QR) x = (QR)^\top b \iff R^\top Q^\top Q R x = R^\top Q^\top b $$
      <p>Since $Q^\top Q = I$, this simplifies to $R^\top R x = R^\top Q^\top b$. Since $R$ is invertible (full rank assumption), we multiply by $R^{-\top}$:
      $$ R x = Q^\top b $$
      This is a triangular system, easily solvable by <b>back substitution</b> in $O(n^2)$ time.</p>
    </section>

    <!-- SECTION 2: SVD -->
    <section class="section-card" id="section-svd">
      <h2>2. The Singular Value Decomposition (SVD)</h2>

      <h3>Geometric Intuition</h3>
      <p>The Singular Value Decomposition (SVD) generalizes the concept of diagonalization to all matrices, including non-square ones. It expresses any linear transformation $A$ as a composition of three simple operations: a rotation, a scaling, and another rotation.</p>
      $$ A = U \Sigma V^\top $$
      <p>Given an input vector $x$:</p>
      <ol>
        <li><b>Rotate/Reflect ($V^\top$):</b> The matrix $V^\top$ is orthogonal. It rotates the coordinate system to align the input vector with the "principal axes" of the matrix.</li>
        <li><b>Scale ($\Sigma$):</b> The matrix $\Sigma$ is diagonal with non-negative entries $\sigma_i$. It stretches or shrinks the vector along the coordinate axes. These scaling factors are the <b>singular values</b>.</li>
        <li><b>Rotate/Reflect ($U$):</b> The matrix $U$ is orthogonal. It rotates the stretched vector to its final orientation in the output space.</li>
      </ol>
      <p>This decomposition reveals that the image of the unit sphere under any linear map is a hyperellipse. The singular values are the lengths of the hyperellipse's semi-axes, and the columns of $U$ are the directions of these axes.</p>

      <div class="example">
        <h4>Numerical Example: SVD in 2D</h4>
        <p>Consider the matrix $A = \begin{bmatrix} 3 & 0 \\ 0 & -2 \end{bmatrix}$. It stretches the x-axis by 3 and the y-axis by 2, while reflecting the y-axis.</p>
        <p>Its SVD is $A = U \Sigma V^\top$:</p>
        $$ A = \underbrace{\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}}_{U} \underbrace{\begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix}}_{\Sigma} \underbrace{\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}}_{V^\top} $$
        <ul>
          <li><b>$V^\top = I$:</b> The input basis aligns with the standard axes (no rotation needed).</li>
          <li><b>$\Sigma$:</b> The singular values are $3$ and $2$. The map stretches $e_1$ by 3 and $e_2$ by 2.</li>
          <li><b>$U$:</b> The reflection $\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}$ flips the y-component, accounting for the negative entry in $A$. Note that singular values must be non-negative, so the negative sign is absorbed into $U$ (or $V$).</li>
        </ul>
      </div>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-basics/matrix-multiplication.svg"
             alt="SVD Geometric Interpretation"
             style="max-width: 80%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 2.1:</i> SVD decomposes a transformation into Rotation ($V^\top$) $\to$ Stretch ($\Sigma$) $\to$ Rotation ($U$).</figcaption>
      </figure>

      <h3>Theorem: Existence of SVD</h3>
      <div class="theorem-box">
        <p>Every matrix $A \in \mathbb{R}^{m \times n}$ can be factored as $A = U \Sigma V^\top$, where:</p>
        <ul>
          <li>$U \in \mathbb{R}^{m \times m}$ is orthogonal ($U^\top U = I$).</li>
          <li>$V \in \mathbb{R}^{n \times n}$ is orthogonal ($V^\top V = I$).</li>
          <li>$\Sigma \in \mathbb{R}^{m \times n}$ is diagonal with entries $\sigma_1 \ge \sigma_2 \ge \dots \ge 0$.</li>
        </ul>
      </div>

      <div class="proof-box">
        <h4>Constructive Proof via Spectral Theorem</h4>
        <p>We construct the SVD using the eigendecomposition of the symmetric PSD matrix $A^\top A$.</p>

        <div class="proof-step">
          <strong>Step 1: Spectral Decomposition of $A^\top A$.</strong>
          The matrix $A^\top A \in \mathbb{R}^{n \times n}$ is symmetric and positive semidefinite.
          Therefore, it has real, non-negative eigenvalues $\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_n \ge 0$.
          Let $\{v_1, \dots, v_n\}$ be an orthonormal set of eigenvectors corresponding to these eigenvalues.
          Define the <b>singular values</b> as $\sigma_i = \sqrt{\lambda_i}$.
          Let $r$ be the rank of $A$. Then $\sigma_1 \ge \dots \ge \sigma_r > 0$, and $\sigma_{r+1} = \dots = \sigma_n = 0$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Defining the Left Singular Vectors.</strong>
          For $i = 1, \dots, r$, define the vectors $u_i \in \mathbb{R}^m$ as:
          $$ u_i = \frac{1}{\sigma_i} A v_i $$
          We claim that these $\{u_1, \dots, u_r\}$ form an orthonormal set.
          <br><b>Proof:</b> Compute the inner product $\langle u_i, u_j \rangle$:
          $$ u_i^\top u_j = \left( \frac{1}{\sigma_i} A v_i \right)^\top \left( \frac{1}{\sigma_j} A v_j \right) = \frac{1}{\sigma_i \sigma_j} v_i^\top (A^\top A v_j) $$
          Since $v_j$ is an eigenvector of $A^\top A$ with eigenvalue $\sigma_j^2$:
          $$ = \frac{1}{\sigma_i \sigma_j} v_i^\top (\sigma_j^2 v_j) = \frac{\sigma_j^2}{\sigma_i \sigma_j} (v_i^\top v_j) $$
          Since $\{v_i\}$ are orthonormal, $v_i^\top v_j = \delta_{ij}$. Thus $u_i^\top u_j = \delta_{ij}$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Completing the Bases.</strong>
          <ul>
            <li>Let $V = [v_1 \dots v_n] \in \mathbb{R}^{n \times n}$. This matrix is orthogonal by construction.</li>
            <li>We have orthonormal vectors $\{u_1, \dots, u_r\}$. If $r < m$, we can extend this set to a full orthonormal basis $\{u_1, \dots, u_m\}$ for $\mathbb{R}^m$ (using Gram-Schmidt). Let $U = [u_1 \dots u_m] \in \mathbb{R}^{m \times m}$.</li>
            <li>Let $\Sigma \in \mathbb{R}^{m \times n}$ be a diagonal matrix with entries $\Sigma_{ii} = \sigma_i$ for $i \le r$ and 0 otherwise.</li>
          </ul>
        </div>

        <div class="proof-step">
          <strong>Step 4: Verification.</strong>
          We check that $AV = U\Sigma$. Equality of matrices is equivalent to equality of columns.
          <ul>
            <li><b>For $i \le r$:</b>
              <br>LHS: $(AV)_i = A v_i$.
              <br>RHS: $(U\Sigma)_i = U (\Sigma e_i) = U (\sigma_i e_i) = \sigma_i (U e_i) = \sigma_i u_i$.
              <br>By definition $u_i = \frac{1}{\sigma_i} A v_i$, which implies $A v_i = \sigma_i u_i$. Thus, the columns match.
            </li>
            <li><b>For $i > r$:</b>
              <br>LHS: $A v_i$. Since $v_i$ is an eigenvector of $A^\top A$ with eigenvalue 0, $A^\top A v_i = 0 \implies v_i^\top A^\top A v_i = 0 \implies \|Av_i\|^2 = 0 \implies A v_i = 0$.
              <br>RHS: $(U\Sigma)_i = U (\Sigma e_i) = U(0) = 0$.
              <br>They match.
            </li>
          </ul>
          Since $AV = U\Sigma$ and $V$ is orthogonal ($V^{-1} = V^\top$), we have $A = U \Sigma V^\top$.
        </div>
      </div>

      <h3>Matrix Norms via SVD</h3>
      <p>The SVD provides the most natural characterization of matrix norms:</p>
      <ul>
        <li><b>Spectral Norm (Operator Norm):</b> $\|A\|_2 = \sigma_{\max}(A) = \sigma_1$. It measures the maximum gain.</li>
        <li><b>Frobenius Norm (Energy):</b> $\|A\|_F = \sqrt{\sum \sigma_i^2}$. It measures the total energy.</li>
        <li><b>Nuclear Norm (Trace Norm):</b> $\|A\|_* = \sum \sigma_i$. This is the convex envelope of the rank function on the unit ball, playing the same role for matrices that the $\ell_1$ norm plays for vectors (sparsity promotion).</li>
      </ul>

      <h3>Low-Rank Approximation (Eckart-Young-Mirsky)</h3>
      <p>The SVD allows us to find the best approximation of a matrix $A$ by a matrix of lower rank $k$. This is crucial for compression (keeping signal) and denoising (discarding noise).</p>
      <div class="theorem-box">
        <p>Let $A_k = \sum_{i=1}^k \sigma_i u_i v_i^\top$ be the truncated SVD. Then $A_k$ is the solution to:</p>
        $$ \min_{\text{rank}(B) \le k} \|A - B\| $$
        <p>Intuitively, by keeping the largest singular values, we capture the directions where the matrix has the most energy (action), minimizing the residual energy.</p>
        <p>The truncated SVD $A_k$ is the optimal approximation for both the spectral norm ($\|\cdot\|_2$) and the Frobenius norm ($\|\cdot\|_F$). The errors are:</p>
        <ul>
          <li>Spectral Error: $\|A - A_k\|_2 = \sigma_{k+1}$</li>
          <li>Frobenius Error: $\|A - A_k\|_F = \sqrt{\sum_{i=k+1}^r \sigma_i^2}$</li>
        </ul>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: SVD for Image Compression</h3>
        <p><b>The Power of Low-Rank Approximation:</b> An image can be viewed as a matrix of pixel intensities. By computing its SVD and keeping only the largest singular values, we can reconstruct the image with far less data.</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Concept:</b> $A \approx \sum_{i=1}^k \sigma_i u_i v_i^\top$.</li>
          <li><b>Observation:</b> Notice how the structure (large features) is captured by the first few modes, while details (and noise) reside in the smaller singular values.</li>
        </ul>
        <div id="widget-svd-approximator" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 3: PSEUDOINVERSE -->
    <section class="section-card" id="section-pseudoinverse">
      <h2>3. The Pseudoinverse and Condition Number</h2>

      <h3>The Moore-Penrose Pseudoinverse ($A^+$)</h3>
      <p>When $A$ is not square or not invertible, we cannot define $A^{-1}$. However, we can define a generalized inverse $A^+$ that "inverts" the action of $A$ on its range and sends the orthogonal complement to zero.</p>

      <p><b>Definition via SVD:</b> If $A = U \Sigma V^\top$, then:
      $$ A^+ = V \Sigma^+ U^\top $$
      where $\Sigma^+$ is the $n \times m$ matrix obtained by transposing $\Sigma$ and inverting the non-zero singular values:
      $$
      \Sigma^+ = \begin{bmatrix}
      1/\sigma_1 & & & 0 & \dots \\
      & \ddots & & \vdots & \\
      & & 1/\sigma_r & 0 & \dots \\
      0 & \dots & 0 & 0 & \dots \\
      \vdots & & \vdots & \vdots & \ddots
      \end{bmatrix}
      $$
      (The non-zero block is diagonal with entries $1/\sigma_i$; all other entries are zero).</p>

      <h4>Properties and Applications</h4>
      <ul>
        <li><b>Minimum Norm Solution:</b> For any system $Ax=b$ (or the least squares problem), the vector $x^\star = A^+ b$ is the solution that has the <b>minimum Euclidean norm</b>.
        <br>
        <div class="proof-box">
          <h4>Proof: $x_{LS} = A^+b$ minimizes norm</h4>
          <p>The general solution to the least squares problem $\min \|Ax-b\|_2$ is the set of all $x$ satisfying the normal equations $A^\top A x = A^\top b$. This solution set is affine: $S_{LS} = \{x_p + z \mid z \in \mathcal{N}(A)\}$, where $x_p$ is any particular solution.</p>
          <div class="proof-step">
            <strong>Step 1: $x_{mn} = A^+ b$ is a solution.</strong>
            We check if $x_{mn} = V \Sigma^+ U^\top b$ satisfies the normal equations $A^\top A x = A^\top b$.
            First, recall that $A = U \Sigma V^\top$, so $A^\top = V \Sigma^\top U^\top$.
            Substituting into the expression for $A^\top A x_{mn}$:
            $$ A^\top A x_{mn} = (V \Sigma^\top U^\top) (U \Sigma V^\top) (V \Sigma^+ U^\top) b $$
            Using $U^\top U = I$ and $V^\top V = I$:
            $$ = V \Sigma^\top (\Sigma \Sigma^+) U^\top b $$
            The matrix product $\Sigma^\top \Sigma \Sigma^+$ is an $n \times m$ matrix. For indices $i \le r$, the diagonal entries are $\sigma_i \cdot (\sigma_i \cdot 1/\sigma_i) = \sigma_i$. For $i > r$, they are 0.
            This matrix is exactly $\Sigma^\top$.
            $$ = V \Sigma^\top U^\top b = A^\top b $$
            Thus, $x_{mn}$ satisfies the normal equations and is a least squares solution.
          </div>
          <div class="proof-step">
            <strong>Step 2: Orthogonality.</strong>
            $x_{mn}$ lies in the span of the first $r$ columns of $V$ (since $\Sigma^+$ has zeros for rows $r+1 \dots n$).
            The columns $v_1, \dots, v_r$ span the row space $\mathcal{R}(A^\top)$.
            The nullspace $\mathcal{N}(A)$ is spanned by $v_{r+1}, \dots, v_n$.
            Therefore, $x_{mn} \in \mathcal{R}(A^\top)$ and is orthogonal to any $z \in \mathcal{N}(A)$.
          </div>
          <div class="proof-step">
            <strong>Step 3: Minimality.</strong>
            Any other solution is of the form $x = x_{mn} + z$ with $z \in \mathcal{N}(A)$.
            $$ \|x\|^2 = \|x_{mn} + z\|^2 = \|x_{mn}\|^2 + \|z\|^2 + 2x_{mn}^\top z $$
            Since $x_{mn} \perp z$, the cross term vanishes:
            $$ \|x\|^2 = \|x_{mn}\|^2 + \|z\|^2 $$
            This is minimized when $z=0$, so $x_{mn}$ is the unique minimum norm solution.
          </div>
        </div>
        </li>
        <li><b>Projectors:</b> $AA^+$ is the orthogonal projector onto the column space $\mathcal{R}(A)$. $A^+A$ is the orthogonal projector onto the row space $\mathcal{R}(A^\top)$.</li>
      </ul>

      <h3>Condition Number</h3>
      <p>The condition number $\kappa(A)$ measures the sensitivity of the solution of a linear system to perturbations in the data.</p>
      $$ \kappa(A) = \|A\|_2 \|A^+\|_2 = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} $$
      <p>where $\sigma_{\min}$ is the smallest <i>non-zero</i> singular value.</p>

      <ul>
        <li><b>$\kappa \approx 1$:</b> Well-conditioned. Errors are not amplified. Orthogonal matrices have $\kappa = 1$.</li>
        <li><b>$\kappa \gg 1$:</b> Ill-conditioned. Small errors in $b$ can lead to massive errors in $x$.</li>
      </ul>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Condition Number & Stability</h3>
        <p><b>Visualize Ill-Conditioning:</b> Compare solving $Ax=b$ for a well-conditioned matrix versus an ill-conditioned one. In the ill-conditioned case, the "ellipse" of the matrix is very skinny. A small change in the target vector $b$ (moving the target slightly off the major axis) requires a massive change in $x$ to compensate.</p>
        <div id="widget-condition-number" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 4: REVIEW -->
    <section class="section-card" id="section-review">
      <h2>4. Review & Cheat Sheet</h2>

      <table class="data-table">
        <thead>
          <tr>
            <th>Decomposition</th>
            <th>Formula</th>
            <th>Components</th>
            <th>Primary Use</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>QR</b></td>
            <td>$A = QR$</td>
            <td>$Q$ orthonormal, $R$ upper triangular</td>
            <td>Least squares, Gram-Schmidt</td>
          </tr>
          <tr>
            <td><b>SVD</b></td>
            <td>$A = U \Sigma V^\top$</td>
            <td>$U, V$ orthogonal, $\Sigma$ diagonal</td>
            <td>Rank, Norms, Approximation, PCA</td>
          </tr>
          <tr>
            <td><b>Spectral</b></td>
            <td>$A = Q \Lambda Q^\top$</td>
            <td>$Q$ orthogonal, $\Lambda$ real diagonal</td>
            <td>Symmetric matrices ($A=A^\top$)</td>
          </tr>
        </tbody>
      </table>

      <h3>Key Identities</h3>
      <ul>
        <li>$\|A\|_2 = \sigma_1$</li>
        <li>$\|A\|_F = \sqrt{\sum \sigma_i^2}$</li>
        <li>$A^+ = V \Sigma^+ U^\top$</li>
        <li>$\kappa(A) = \sigma_1 / \sigma_r$</li>
      </ul>
    </section>

    <!-- SECTION 5: EXERCISES -->
    <section class="section-card" id="section-exercises"><h2><i data-feather="edit-3"></i> 5. Exercises</h2>
      <div class="insight">
        <h4>Recap & Key Concepts</h4>
        <p>These exercises explore the theoretical depths of matrix norms, the geometry of projections, and the algebraic properties of the trace. P1.1 links norms to duality (Lecture 02). P1.14 links SVD to the operator norm.</p>
      </div>

      <h3>P1.1 — Dual of $\ell_p$ Norm</h3>
      <p>Show that for $1 < p < \infty$, the dual norm of $\|\cdot\|_p$ is $\|\cdot\|_q$ where $\frac{1}{p} + \frac{1}{q} = 1$. Use Hölder's Inequality for the upper bound and construct an alignment vector for the lower bound.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>Upper bound ($\le$):</strong> By Hölder's inequality, $x^\top y \le \|x\|_p \|y\|_q$.
          The definition of the dual norm is $\|y\|_* = \sup_{\|x\|_p \le 1} x^\top y$.
          From Hölder, if $\|x\|_p \le 1$, then $x^\top y \le \|y\|_q$. Thus $\|y\|_* \le \|y\|_q$.
        </div>
        <div class="proof-step">
          <strong>Lower bound ($\ge$):</strong> We need to construct a vector $x$ that "aligns" with $y$ to maximize the inner product.
          We want $x_i$ to have the same sign as $y_i$, and magnitudes such that equality holds in Hölder's.
          Choose $x_i = \mathrm{sign}(y_i) |y_i|^{q-1}$.
          <br>Check inner product: $x^\top y = \sum \mathrm{sign}(y_i)|y_i|^{q-1} y_i = \sum |y_i|^q = \|y\|_q^q$.
          <br>Check norm of $x$: $\|x\|_p^p = \sum |x_i|^p = \sum |y_i|^{p(q-1)}$.
          Since $(p-1)q = p \implies p(q-1) = q$: $\|x\|_p^p = \sum |y_i|^q = \|y\|_q^q$.
          So $\|x\|_p = \|y\|_q^{q/p}$.
          <br>Now normalize to get a unit vector $\tilde{x} = x / \|x\|_p = x / \|y\|_q^{q/p}$.
          $$ \tilde{x}^\top y = \frac{\|y\|_q^q}{\|y\|_q^{q/p}} = \|y\|_q^{q - q/p} = \|y\|_q^1 = \|y\|_q $$
          Since we found a unit vector achieving this value, $\|y\|_* \ge \|y\|_q$.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Duality of Norms:</b> $\|y\|_* = \sup \{ y^\top x \mid \|x\| \le 1 \}$.
        <br><b>Hölder's Inequality:</b> $|x^\top y| \le \|x\|_p \|y\|_q$ for conjugate exponents $1/p+1/q=1$.
        <br><b>Alignment:</b> Equality holds when $x$ and $y$ are aligned (component-wise powers).</p>
      </div>

      <h3>P1.2 — Frobenius Cauchy–Schwarz</h3>
      <p>Prove that $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$ by viewing matrices in $\mathbb{R}^{m \times n}$ as vectors in $\mathbb{R}^{mn}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The Frobenius norm corresponds to the standard Euclidean norm of the vectorized matrix: $\|A\|_F = \|\mathrm{vec}(A)\|_2$.
        The inner product of matrices is $\langle A, B \rangle = \mathrm{tr}(A^\top B) = \sum_{i,j} A_{ij} B_{ij}$.
        This is exactly the dot product of the vectorized forms: $\mathrm{vec}(A)^\top \mathrm{vec}(B)$.
        By the standard Cauchy-Schwarz inequality for vectors:
        $$ |\mathrm{vec}(A)^\top \mathrm{vec}(B)| \le \|\mathrm{vec}(A)\|_2 \|\mathrm{vec}(B)\|_2 $$
        Substituting back the matrix notation yields $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Vectorization:</b> $\mathrm{vec}(A)$ stacks columns. $\langle A, B \rangle = \mathrm{vec}(A)^\top \mathrm{vec}(B)$.
        <br><b>Trace Inner Product:</b> $\langle A, B \rangle = \mathrm{tr}(A^\top B)$.
        <br><b>Cauchy-Schwarz:</b> $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.</p>
      </div>

      <h3>P1.3 — SVD Computation</h3>
      <p>Compute the Singular Value Decomposition $A = U \Sigma V^\top$ by hand for the matrix $A = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$. Verify that the singular values are the square roots of the eigenvalues of $A^\top A$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>1. Compute $A^\top A$ and its eigenvalues:</strong>
          $A^\top A = \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$.
          Characteristic eq: $(1-\lambda)^2 - 1 = 0 \implies \lambda^2 - 2\lambda = 0 \implies \lambda_1=2, \lambda_2=0$.
          Singular values: $\sigma_1 = \sqrt{2}, \sigma_2 = 0$.
        </div>
        <div class="proof-step">
          <strong>2. Compute $V$ (eigenvectors of $A^\top A$):</strong>
          For $\lambda_1=2$: $\begin{bmatrix} -1 & 1 \\ 1 & -1 \end{bmatrix} v = 0 \implies v_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$.
          For $\lambda_2=0$: $v_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}$.
          $V = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}$.
        </div>
        <div class="proof-step">
          <strong>3. Compute $U$:</strong>
          $u_1 = \frac{1}{\sigma_1} A v_1 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{1}{2} \begin{bmatrix} 2 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$.
          Since $m=2$, we need a second vector $u_2$ orthogonal to $u_1$. $u_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$.
          $U = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$.
        </div>
        <div class="proof-step">
          <strong>Result:</strong> $A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} \sqrt{2} & 0 \\ 0 & 0 \end{bmatrix} \left(\frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}\right)^\top$.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>SVD Algorithm:</b>
        <br>1. Eigenvalues of $A^\top A$ give $\sigma_i^2$.
        <br>2. Eigenvectors of $A^\top A$ give $V$ (Right singular vectors).
        <br>3. Compute $u_i = \frac{1}{\sigma_i} A v_i$ (Left singular vectors).
        <br>4. Form $A = \sum \sigma_i u_i v_i^\top$.</p>
      </div>

      <h3>P1.6 — Normal Equations vs. QR</h3>
      <p>Solve the system $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix}, b = \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}$ using (a) Normal Equations and (b) QR factorization. Compare the intermediate steps.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>(a) Normal Equations:</b> $A^\top A x = A^\top b$.
        $A^\top A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$.
        $A^\top b = \begin{bmatrix} 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix} \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.
        Solve $\begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} x = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.
        Multiply by inverse $\frac{1}{3} \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}$:
        $x = \frac{1}{3} \begin{bmatrix} 6-4 \\ -3+8 \end{bmatrix} = \frac{1}{3} \begin{bmatrix} 2 \\ 5 \end{bmatrix}$.</p>
        <p><b>(b) QR Factorization:</b>
        Gram-Schmidt on columns $a_1, a_2$.
        $u_1 = a_1 = (1, 0, 1)^\top$. $q_1 = (1/\sqrt{2}, 0, 1/\sqrt{2})^\top$.
        $v_2 = a_2 - \langle a_2, q_1 \rangle q_1 = (1, 1, 0)^\top - (1/\sqrt{2}) (1/\sqrt{2}, 0, 1/\sqrt{2})^\top = (1, 1, 0)^\top - (1/2, 0, 1/2)^\top = (1/2, 1, -1/2)^\top$.
        Normalize $v_2$: $\|v_2\|^2 = 1/4 + 1 + 1/4 = 1.5 = 3/2$. $\|v_2\| = \sqrt{3/2}$.
        $q_2 = \sqrt{2/3} (1/2, 1, -1/2)^\top$.
        Solve $Rx = Q^\top b$.
        $R = \begin{bmatrix} \sqrt{2} & 1/\sqrt{2} \\ 0 & \sqrt{3/2} \end{bmatrix}$. $Q^\top b = \begin{bmatrix} 3/\sqrt{2} \\ 4\sqrt{2/3}-1/\sqrt{6} \end{bmatrix}$.
        Back substitution yields same $x$. QR avoids forming $A^\top A$ (better condition number).</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Least Squares Solvers:</b>
        <br>1. <b>Normal Equations:</b> $A^\top A x = A^\top b$. Fast but unstable ($\kappa^2$).
        <br>2. <b>QR Factorization:</b> $A=QR \implies Rx = Q^\top b$. Stable ($\kappa$).
        <br><b>Geometry:</b> QR constructs an orthonormal basis for $\mathcal{R}(A)$.</p>
      </div>

      <h3>P1.7 — Pseudoinverse Projectors</h3>
      <p>Show that $P = AA^+$ is the orthogonal projector onto $\mathcal{R}(A)$. Verify $P^2=P$ and $P^\top=P$ using SVD.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Let $A = U \Sigma V^\top$. Then $A^+ = V \Sigma^+ U^\top$.
        $P = AA^+ = (U \Sigma V^\top)(V \Sigma^+ U^\top) = U (\Sigma \Sigma^+) U^\top$.
        $\Sigma \Sigma^+$ is a diagonal matrix with 1s for non-zero singular values and 0s otherwise.
        Let $U_r$ be the first $r$ columns of $U$. Then $P = U_r U_r^\top$.
        <br><b>Idempotent:</b> $P^2 = (U_r U_r^\top)(U_r U_r^\top) = U_r (U_r^\top U_r) U_r^\top = U_r I U_r^\top = P$.
        <br><b>Symmetric:</b> $P^\top = (U_r U_r^\top)^\top = (U_r^\top)^\top U_r^\top = U_r U_r^\top = P$.
        <br>Since the columns of $U_r$ form a basis for $\mathcal{R}(A)$, $P$ is the orthogonal projector onto $\mathcal{R}(A)$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Pseudoinverse Properties:</b>
        <br>1. $A^+ = V \Sigma^+ U^\top$.
        <br>2. $AA^+$ is the projector onto $\mathcal{R}(A)$.
        <br>3. $A^+A$ is the projector onto $\mathcal{R}(A^\top) = \mathcal{N}(A)^\perp$.
        <br>4. $x = A^+b$ is the minimum-norm least-squares solution.</p>
      </div>

      <h3>P1.8 — Projector Formula</h3>
      <p>Let $A$ have linearly independent columns. Derive the projection formula $P = A(A^\top A)^{-1}A^\top$ by minimizing $\|Ax - b\|_2^2$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>We want to find the point $p = Ax$ in the column space closest to $b$.
        Problem: $\min_x \|Ax - b\|_2^2$.
        Gradient: $2A^\top (Ax - b) = 0$.
        Normal eqs: $A^\top A x = A^\top b$.
        Since cols are independent, $A^\top A$ is invertible.
        Solution: $x^* = (A^\top A)^{-1} A^\top b$.
        Projection: $p = A x^* = A(A^\top A)^{-1} A^\top b$.
        The matrix mapping $b$ to $p$ is $P = A(A^\top A)^{-1} A^\top$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Projection Matrix:</b> If $A$ has full column rank, $P_{\mathcal{R}(A)} = A(A^\top A)^{-1}A^\top$.
        <br><b>Derivation:</b> Set gradient of $\|Ax-b\|^2$ to zero.
        <br><b>Properties:</b> Symmetric ($P^\top = P$) and Idempotent ($P^2 = P$).</p>
      </div>

      <h3>P1.9 — Hyperplane Projection</h3>
      <p>Find the projection of $(1, 2, 3)^\top$ onto $x_1 + x_2 + x_3 = 3$. Verify your result using the standard formula for distance to a plane.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Normal $a = (1, 1, 1)^\top$. Point $y = (1, 2, 3)^\top$. Target $b=3$.
        $p = y - \frac{a^\top y - b}{\|a\|^2} a$.
        $a^\top y = 1+2+3 = 6$. $\|a\|^2 = 3$.
        $p = (1, 2, 3)^\top - \frac{6-3}{3} (1, 1, 1)^\top = (1, 2, 3)^\top - (1, 1, 1)^\top = (0, 1, 2)^\top$.
        Check: $0+1+2=3$. Correct.
        Distance: $\|y-p\| = \|(1, 1, 1)\| = \sqrt{3}$.
        Formula: $\frac{|a^\top y - b|}{\|a\|} = \frac{|6-3|}{\sqrt{3}} = \frac{3}{\sqrt{3}} = \sqrt{3}$. Matches.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Hyperplane Projection:</b> Projecting $y$ onto $a^\top x = b$.
        <br><b>Formula:</b> $p = y - \frac{a^\top y - b}{\|a\|^2}a$.
        <br><b>Residual:</b> Parallel to normal $a$. Length is distance to plane.</p>
      </div>

      <h3>P1.10 — Condition Number Calculation</h3>
      <p>Calculate the condition number $\kappa(A) = \sigma_{\max}(A)/\sigma_{\min}(A)$ for the matrix $A = \begin{bmatrix} 1 & 0 \\ 0 & \epsilon \end{bmatrix}$ where $\epsilon > 0$. What happens as $\epsilon \to 0$?</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The matrix is diagonal, so its singular values are the absolute values of diagonal entries: $1$ and $\epsilon$.
        Assuming $\epsilon < 1$: $\sigma_{\max} = 1, \sigma_{\min} = \epsilon$.
        $\kappa(A) = 1/\epsilon$.
        As $\epsilon \to 0$, $\kappa(A) \to \infty$. The matrix becomes ill-conditioned (nearly singular).</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Condition Number:</b> $\kappa(A) = \sigma_{\max}(A) / \sigma_{\min}(A) = \|A\|_2 \|A^{-1}\|_2$.
        <br><b>Significance:</b> Measures sensitivity of linear systems to error.
        <br><b>Ill-conditioned:</b> $\kappa(A) \gg 1$. Near singular.</p>
      </div>

      <h3>P1.11 — Pseudoinverse of Rank-1 Matrix</h3>
      <p>Let $A = xy^\top$ where $x, y \in \mathbb{R}^n$ are non-zero vectors. Show that $A^+ = \frac{A^\top}{\|x\|_2^2 \|y\|_2^2}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>SVD of rank-1 matrix: $A = \sigma u v^\top$.
        $\|A\|_F = \|x\|\|y\| = \sigma$.
        $u = x/\|x\|$, $v = y/\|y\|$.
        Check: $A = (\|x\|\|y\|) \frac{x}{\|x\|} \frac{y^\top}{\|y\|} = xy^\top$. Correct.
        Pseudoinverse: $A^+ = \frac{1}{\sigma} v u^\top = \frac{1}{\|x\|\|y\|} \frac{y}{\|y\|} \frac{x^\top}{\|x\|} = \frac{yx^\top}{\|x\|^2 \|y\|^2}$.
        Note that $A^\top = (xy^\top)^\top = yx^\top$.
        Thus $A^+ = \frac{A^\top}{\|x\|^2 \|y\|^2}$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Rank-1 Matrices:</b> $A = xy^\top$.
        <br><b>SVD:</b> $\sigma_1 = \|x\|\|y\|$, $u=x/\|x\|$, $v=y/\|y\|$.
        <br><b>Pseudoinverse:</b> $A^+ = \frac{yx^\top}{\|x\|^2\|y\|^2}$. Reverses scaling on the active subspaces.</p>
      </div>

      <h3>P1.12 — Operator Norm Properties</h3>
      <p>Prove that $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$. Use the Rayleigh Quotient definition of eigenvalues.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>$\|A\|_2^2 = \sup_{x \ne 0} \frac{\|Ax\|^2}{\|x\|^2} = \sup_{x \ne 0} \frac{x^\top A^\top A x}{x^\top x}$.
        The term on the right is the Rayleigh quotient for the symmetric matrix $M = A^\top A$.
        By the variational characterization of eigenvalues, the maximum value of the Rayleigh quotient is $\lambda_{\max}(M)$.
        Thus $\|A\|_2^2 = \lambda_{\max}(A^\top A)$, so $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Spectral Norm:</b> $\|A\|_2 = \sup_{x \ne 0} \frac{\|Ax\|}{\|x\|} = \sqrt{\lambda_{\max}(A^\top A)}$.
        <br><b>Rayleigh Quotient:</b> $\lambda_{\max}(M) = \sup_x \frac{x^\top M x}{x^\top x}$.
        <br><b>Connection:</b> $\|Ax\|^2/\|x\|^2$ is the Rayleigh quotient for $A^\top A$.</p>
      </div>

      <h3>P1.13 — Orthogonal Invariance</h3>
      <p>Show that $\|U A V^\top\|_F = \|A\|_F$ and $\|U A V^\top\|_2 = \|A\|_2$ for orthogonal $U, V$. This proves that these norms depend only on the singular values.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Frobenius:</b> $\|X\|_F^2 = \mathrm{tr}(X^\top X)$.
        $\|UAV^\top\|_F^2 = \mathrm{tr}(V A^\top U^\top U A V^\top) = \mathrm{tr}(V A^\top A V^\top) = \mathrm{tr}(V^\top V A^\top A) = \mathrm{tr}(A^\top A) = \|A\|_F^2$.
        <br><b>Spectral:</b> $\|UAV^\top\|_2 = \sup_{\|x\|=1} \|UAV^\top x\|$.
        Since $U$ is orthogonal, $\|Uy\| = \|y\|$. So we maximize $\|A (V^\top x)\|$.
        Since $V$ is orthogonal, as $x$ ranges over the unit sphere, $y = V^\top x$ also ranges over the unit sphere.
        Thus $\sup_{\|x\|=1} \|A V^\top x\| = \sup_{\|y\|=1} \|Ay\| = \|A\|_2$.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Unitary Invariance:</b> Norms depending only on singular values are invariant under orthogonal transformations.
        <br><b>Schatten Norms:</b> $\|A\|_p = (\sum \sigma_i^p)^{1/p}$. Includes Spectral ($p=\infty$) and Frobenius ($p=2$).</p>
      </div>

      <h3>P1.14 — The Orthogonal Group</h3>
      <p>Show that the set of orthogonal matrices forms a compact group.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Group:</b>
        1. Closure: $(Q_1 Q_2)^\top (Q_1 Q_2) = Q_2^\top Q_1^\top Q_1 Q_2 = Q_2^\top I Q_2 = I$.
        2. Inverse: $Q^{-1} = Q^\top$, which is orthogonal since $(Q^\top)^\top Q^\top = Q Q^\top = I$.
        3. Identity: $I \in O(n)$.
        <br><b>Compactness:</b>
        1. Bounded: The columns are unit vectors, so $\|Q\|_F = \sqrt{n}$. The set is bounded.
        2. Closed: The condition $Q^\top Q = I$ is a continuous equality constraint (level set of a continuous function). Thus the set is closed.
        By Heine-Borel, closed and bounded in finite dimensions implies compact.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Orthogonal Group $O(n)$:</b> $\{Q \mid Q^\top Q = I\}$.
        <br><b>Topology:</b> Closed (inverse image of singleton) and Bounded ($|Q_{ij}| \le 1$) $\implies$ Compact.
        <br><b>Implication:</b> Continuous functions on $O(n)$ attain extrema (Extreme Value Theorem).</p>
      </div>
</section>

    <!-- SECTION 6: READINGS -->
    <section class="section-card" id="section-readings">
      <h2>6. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required:</strong> Boyd & Vandenberghe, Appendix A.</li>
        <li><strong>Recommended:</strong> Trefethen & Bau, <em>Numerical Linear Algebra</em>, Lectures 1–5.</li>
        <li><strong>Advanced References:</strong> Golub & Van Loan, <em>Matrix Computations</em>, Chapter 2.</li>
      </ul>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initSvdApproximator } from './widgets/js/svd-approximator.js';
    initSvdApproximator('widget-svd-approximator');
  </script>
  <script type="module">
    import { initConditionNumber } from './widgets/js/condition-number.js';
    initConditionNumber('widget-condition-number');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
