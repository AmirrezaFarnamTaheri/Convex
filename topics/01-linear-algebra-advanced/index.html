<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>01. Linear Algebra Advanced — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.128/build/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../00-linear-algebra-basics/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../02-introduction/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>01. Linear Algebra Advanced</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, linear-algebra, svd, qr, conditioning</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture covers advanced linear algebra topics essential for robust numerical optimization. We explore matrix factorizations (QR, SVD) that provide stable alternatives to the normal equations, define the pseudoinverse for rank-deficient problems, and analyze the impact of condition numbers on algorithmic stability.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-basics/index.html">Lecture 00: Linear Algebra Basics</a> (Subspaces, Norms, Least Squares).</p>
        <p><strong>Forward Connections:</strong> The SVD is central to the analysis of the Nuclear Norm in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>. Condition numbers determine the convergence rates of First-Order Methods in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a>.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Factorize Matrices:</b> Compute and interpret the QR decomposition and Singular Value Decomposition (SVD).</li>
        <li><b>Solve Robust Least Squares:</b> Apply QR and SVD to solve least squares problems without forming $A^\top A$.</li>
        <li><b>Handle Rank Deficiency:</b> Use the Moore-Penrose pseudoinverse to find minimum-norm solutions.</li>
        <li><b>Analyze Stability:</b> Calculate condition numbers and estimate the sensitivity of linear systems to perturbations.</li>
        <li><b>Approximate Matrices:</b> Apply the Eckart-Young-Mirsky theorem for optimal low-rank matrix approximations (PCA).</li>
      </ul>
    </section>

    <!-- SECTION 1: QR DECOMPOSITION -->
    <section class="section-card" id="section-qr">
      <h2>1. The QR Decomposition</h2>

      <h3>Motivation: Numerical Stability</h3>
      <p>In Lecture 00, we solved the least squares problem $\min \|Ax - b\|_2$ using the normal equations $A^\top A x = A^\top b$. While theoretically correct, this approach is numerically hazardous. If $A$ has condition number $\kappa(A)$, then $A^\top A$ has condition number $\kappa(A)^2$. For ill-conditioned matrices, this squaring can result in a loss of precision that exceeds machine limits.</p>
      <p>The <b>QR decomposition</b> offers a solution that depends on $\kappa(A)$ rather than $\kappa(A)^2$, providing significantly better stability.</p>

      <h3>Definition and Existence</h3>
      <div class="theorem-box">
        <h4>Theorem: QR Factorization</h4>
        <p>Any matrix $A \in \mathbb{R}^{m \times n}$ with $m \ge n$ and linearly independent columns can be factored as:</p>
        $$ A = QR $$
        <p>where $Q \in \mathbb{R}^{m \times n}$ has orthonormal columns ($Q^\top Q = I_n$) and $R \in \mathbb{R}^{n \times n}$ is upper triangular with positive diagonal entries.</p>
      </div>

      <div class="proof-box">
        <h4>Construction via Gram-Schmidt</h4>
        <p>The existence of the QR decomposition follows directly from the Gram-Schmidt orthogonalization process applied to the columns $a_1, \dots, a_n$ of $A$.</p>

        <div class="proof-step">
          <strong>Step 1: Initialization.</strong>
          Let $u_1 = a_1$. Normalize to get the first vector: $q_1 = u_1 / \|u_1\|_2$.
          <br>Set $r_{11} = \|u_1\|_2$. Then $a_1 = r_{11} q_1$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Iteration ($k=2, \dots, n$).</strong>
          We remove the projection of $a_k$ onto the subspace spanned by the previous orthonormal vectors $q_1, \dots, q_{k-1}$:
          $$ v_k = a_k - \sum_{j=1}^{k-1} \langle q_j, a_k \rangle q_j $$
          Let $r_{jk} = \langle q_j, a_k \rangle$. These are the coefficients in the upper triangle.
          <br>Normalize the residual: $r_{kk} = \|v_k\|_2$ and $q_k = v_k / r_{kk}$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Matrix Form.</strong>
          Rearranging the definition of $v_k$, we express each column $a_k$ as a linear combination of $q_1, \dots, q_k$:
          $$ a_k = r_{1k} q_1 + r_{2k} q_2 + \dots + r_{kk} q_k $$
          In matrix notation, this is exactly $A = QR$, where $R$ is upper triangular because $a_k$ depends only on the first $k$ columns of $Q$.
        </div>
      </div>

      <h3>Solving Least Squares with QR</h3>
      <p>Given $A = QR$, the least squares objective simplifies due to the orthogonal invariance of the Euclidean norm. The standard efficient approach uses the normal equations:</p>
      $$ A^\top A x = A^\top b \iff (QR)^\top (QR) x = (QR)^\top b \iff R^\top Q^\top Q R x = R^\top Q^\top b $$
      <p>Since $Q^\top Q = I$, this simplifies to $R^\top R x = R^\top Q^\top b$. Since $R$ is invertible (full rank assumption), we multiply by $R^{-\top}$:
      $$ R x = Q^\top b $$
      This is a triangular system, easily solvable by <b>back substitution</b> in $O(n^2)$ time.</p>
    </section>

    <!-- SECTION 2: SVD -->
    <section class="section-card" id="section-svd">
      <h2>2. The Singular Value Decomposition (SVD)</h2>

      <h3>Geometric Intuition</h3>
      <p>The SVD provides the "diagonalization" of any matrix, in addition to square ones. It states that <b>every linear map</b> can be decomposed into a rotation, a scaling, and another rotation.</p>
      $$ A = U \Sigma V^\top $$
      <ul>
        <li><b>$V^\top$ (Rotation/Reflection):</b> Aligns the input basis with the "principal axes" of the data.</li>
        <li><b>$\Sigma$ (Stretch):</b> Scales the axes by the singular values $\sigma_i \ge 0$.</li>
        <li><b>$U$ (Rotation/Reflection):</b> Rotates the stretched axes to their final directions in the output space.</li>
      </ul>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-basics/matrix-multiplication.svg"
             alt="SVD Geometric Interpretation"
             style="max-width: 80%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 2.1:</i> SVD decomposes a transformation into Rotation ($V^\top$) $\to$ Stretch ($\Sigma$) $\to$ Rotation ($U$).</figcaption>
      </figure>

      <h3>Theorem: Existence of SVD</h3>
      <div class="theorem-box">
        <p>Every matrix $A \in \mathbb{R}^{m \times n}$ can be factored as $A = U \Sigma V^\top$, where:</p>
        <ul>
          <li>$U \in \mathbb{R}^{m \times m}$ is orthogonal ($U^\top U = I$).</li>
          <li>$V \in \mathbb{R}^{n \times n}$ is orthogonal ($V^\top V = I$).</li>
          <li>$\Sigma \in \mathbb{R}^{m \times n}$ is diagonal with entries $\sigma_1 \ge \sigma_2 \ge \dots \ge 0$.</li>
        </ul>
      </div>

      <div class="proof-box">
        <h4>Constructive Proof via Spectral Theorem</h4>
        <p>We construct the SVD using the eigendecomposition of the symmetric PSD matrix $A^\top A$.</p>

        <div class="proof-step">
          <strong>Step 1: Eigenvalues of $A^\top A$.</strong>
          Let $A^\top A = V \Lambda V^\top$ be the spectral decomposition, with $\lambda_1 \ge \dots \ge \lambda_n \ge 0$.
          Define the singular values $\sigma_i = \sqrt{\lambda_i}$. Let $r$ be the rank of $A$, so $\sigma_1, \dots, \sigma_r > 0$ and $\sigma_{r+1} = \dots = 0$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Construct $U$.</strong>
          For $i = 1, \dots, r$, define $u_i = \frac{1}{\sigma_i} A v_i$.
          We verify orthonormality:
          $$ u_i^\top u_j = \frac{1}{\sigma_i \sigma_j} v_i^\top A^\top A v_j = \frac{1}{\sigma_i \sigma_j} v_i^\top (\lambda_j v_j) = \frac{\sigma_j^2}{\sigma_i \sigma_j} \delta_{ij} = \delta_{ij} $$
          Extend $\{u_1, \dots, u_r\}$ to an orthonormal basis $\{u_1, \dots, u_m\}$ for $\mathbb{R}^m$ to form $U$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Verification.</strong>
          Compute $U \Sigma V^\top v_i$:
          <ul>
            <li>If $i \le r$: $U \Sigma e_i = \sigma_i u_i = \sigma_i (\frac{1}{\sigma_i} A v_i) = A v_i$.</li>
            <li>If $i > r$: $U \Sigma e_i = 0$. Also $A v_i = 0$ since $v_i \in \mathcal{N}(A^\top A) = \mathcal{N}(A)$.</li>
          </ul>
          Since $U \Sigma V^\top$ and $A$ agree on the basis $V$, they are equal.
        </div>
      </div>

      <h3>Matrix Norms via SVD</h3>
      <p>The SVD provides the most natural characterization of matrix norms:</p>
      <ul>
        <li><b>Spectral Norm (Operator Norm):</b> $\|A\|_2 = \sigma_{\max}(A) = \sigma_1$. It measures the maximum gain.</li>
        <li><b>Frobenius Norm (Energy):</b> $\|A\|_F = \sqrt{\sum \sigma_i^2}$. It measures the total energy.</li>
        <li><b>Nuclear Norm (Trace Norm):</b> $\|A\|_* = \sum \sigma_i$. This is the convex envelope of the rank function on the unit ball, playing the same role for matrices that the $\ell_1$ norm plays for vectors (sparsity promotion).</li>
      </ul>

      <h3>Low-Rank Approximation (Eckart-Young-Mirsky)</h3>
      <p>The SVD allows us to find the best approximation of a matrix $A$ by a matrix of lower rank $k$. This is crucial for compression and denoising.</p>
      <div class="theorem-box">
        <p>Let $A_k = \sum_{i=1}^k \sigma_i u_i v_i^\top$ be the truncated SVD. Then $A_k$ is the solution to:</p>
        $$ \min_{\text{rank}(B) \le k} \|A - B\| $$
        <p>for both the spectral norm ($\|\cdot\|_2$) and the Frobenius norm ($\|\cdot\|_F$). The errors are:</p>
        <ul>
          <li>Spectral Error: $\|A - A_k\|_2 = \sigma_{k+1}$</li>
          <li>Frobenius Error: $\|A - A_k\|_F = \sqrt{\sum_{i=k+1}^r \sigma_i^2}$</li>
        </ul>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: SVD for Image Compression</h3>
        <p><b>The Power of Low-Rank Approximation:</b> An image can be viewed as a matrix of pixel intensities. By computing its SVD and keeping only the largest singular values, we can reconstruct the image with far less data.</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Concept:</b> $A \approx \sum_{i=1}^k \sigma_i u_i v_i^\top$.</li>
          <li><b>Observation:</b> Notice how the structure (large features) is captured by the first few modes, while details (and noise) reside in the smaller singular values.</li>
        </ul>
        <div id="widget-svd-approximator" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 3: PSEUDOINVERSE -->
    <section class="section-card" id="section-pseudoinverse">
      <h2>3. The Pseudoinverse and Condition Number</h2>

      <h3>The Moore-Penrose Pseudoinverse ($A^+$)</h3>
      <p>When $A$ is not square or not invertible, we cannot define $A^{-1}$. However, we can define a generalized inverse $A^+$ that "inverts" the action of $A$ on its range and sends the orthogonal complement to zero.</p>

      <p><b>Definition via SVD:</b> If $A = U \Sigma V^\top$, then:
      $$ A^+ = V \Sigma^+ U^\top $$
      where $\Sigma^+$ is obtained by taking the transpose of $\Sigma$ and replacing every non-zero singular value $\sigma_i$ with $1/\sigma_i$.</p>

      <h4>Properties and Applications</h4>
      <ul>
        <li><b>Minimum Norm Solution:</b> For any system $Ax=b$, the vector $x^\star = A^+ b$ is the solution to the least squares problem $\min \|Ax-b\|_2$ that has the <b>minimum Euclidean norm</b>.</li>
        <li><b>Projectors:</b> $AA^+$ is the orthogonal projector onto the column space $\mathcal{R}(A)$. $A^+A$ is the orthogonal projector onto the row space $\mathcal{R}(A^\top)$.</li>
      </ul>

      <h3>Condition Number</h3>
      <p>The condition number $\kappa(A)$ measures the sensitivity of the solution of a linear system to perturbations in the data.</p>
      $$ \kappa(A) = \|A\|_2 \|A^+\|_2 = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} $$
      <p>where $\sigma_{\min}$ is the smallest <i>non-zero</i> singular value.</p>

      <ul>
        <li><b>$\kappa \approx 1$:</b> Well-conditioned. Errors are not amplified. Orthogonal matrices have $\kappa = 1$.</li>
        <li><b>$\kappa \gg 1$:</b> Ill-conditioned. Small errors in $b$ can lead to massive errors in $x$.</li>
      </ul>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Condition Number & Stability</h3>
        <p><b>Visualize Ill-Conditioning:</b> Compare solving $Ax=b$ for a well-conditioned matrix versus an ill-conditioned one. In the ill-conditioned case, the "ellipse" of the matrix is very skinny. A small change in the target vector $b$ (moving the target slightly off the major axis) requires a massive change in $x$ to compensate.</p>
        <div id="widget-condition-number" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 4: REVIEW -->
    <section class="section-card" id="section-review">
      <h2>4. Review & Cheat Sheet</h2>

      <table class="data-table">
        <thead>
          <tr>
            <th>Decomposition</th>
            <th>Formula</th>
            <th>Components</th>
            <th>Primary Use</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>QR</b></td>
            <td>$A = QR$</td>
            <td>$Q$ orthonormal, $R$ upper triangular</td>
            <td>Least squares, Gram-Schmidt</td>
          </tr>
          <tr>
            <td><b>SVD</b></td>
            <td>$A = U \Sigma V^\top$</td>
            <td>$U, V$ orthogonal, $\Sigma$ diagonal</td>
            <td>Rank, Norms, Approximation, PCA</td>
          </tr>
          <tr>
            <td><b>Spectral</b></td>
            <td>$A = Q \Lambda Q^\top$</td>
            <td>$Q$ orthogonal, $\Lambda$ real diagonal</td>
            <td>Symmetric matrices ($A=A^\top$)</td>
          </tr>
        </tbody>
      </table>

      <h3>Key Identities</h3>
      <ul>
        <li>$\|A\|_2 = \sigma_1$</li>
        <li>$\|A\|_F = \sqrt{\sum \sigma_i^2}$</li>
        <li>$A^+ = V \Sigma^+ U^\top$</li>
        <li>$\kappa(A) = \sigma_1 / \sigma_r$</li>
      </ul>
    </section>

    <!-- SECTION 5: EXERCISES -->
    <section class="section-card" id="section-exercises"><h2><i data-feather="edit-3"></i> 5. Exercises</h2>
      <div class="insight">
        <h4>Recap & Key Concepts</h4>
        <p>These exercises explore the theoretical depths of matrix norms, the geometry of projections, and the algebraic properties of the trace. P1.1 links norms to duality (Lecture 02). P1.14 links SVD to the operator norm.</p>
      </div>

      <h3>P1.1 — Dual of $\ell_p$ Norm</h3>
      <p>Show that for $1 < p < \infty$, the dual norm of $\|\cdot\|_p$ is $\|\cdot\|_q$ where $\frac{1}{p} + \frac{1}{q} = 1$. Use Hölder's Inequality for the upper bound and construct an alignment vector for the lower bound.</p>

      <h3>P1.2 — Frobenius Cauchy–Schwarz</h3>
      <p>Prove that $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$ by viewing matrices in $\mathbb{R}^{m \times n}$ as vectors in $\mathbb{R}^{mn}$.</p>

      <h3>P1.3 — SVD Computation</h3>
      <p>Compute the Singular Value Decomposition $A = U \Sigma V^\top$ by hand for the matrix $A = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$. Verify that the singular values are the square roots of the eigenvalues of $A^\top A$.</p>

      <h3>P1.6 — Normal Equations vs. QR</h3>
      <p>Solve the system $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix}, b = \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}$ using (a) Normal Equations and (b) QR factorization. Compare the intermediate steps.</p>

      <h3>P1.7 — Pseudoinverse Projectors</h3>
      <p>Show that $P = AA^+$ is the orthogonal projector onto $\mathcal{R}(A)$. Verify $P^2=P$ and $P^\top=P$ using SVD.</p>

      <h3>P1.8 — Projector Formula</h3>
      <p>Let $A$ have linearly independent columns. Derive the projection formula $P = A(A^\top A)^{-1}A^\top$ by minimizing $\|Ax - b\|_2^2$.</p>

      <h3>P1.9 — Hyperplane Projection</h3>
      <p>Find the projection of $(1, 2, 3)^\top$ onto $x_1 + x_2 + x_3 = 3$. Verify your result using the standard formula for distance to a plane.</p>

      <h3>P1.10 — Condition Number Calculation</h3>
      <p>Calculate the condition number $\kappa(A) = \sigma_{\max}(A)/\sigma_{\min}(A)$ for the matrix $A = \begin{bmatrix} 1 & 0 \\ 0 & \epsilon \end{bmatrix}$ where $\epsilon > 0$. What happens as $\epsilon \to 0$?</p>

      <h3>P1.11 — Pseudoinverse of Rank-1 Matrix</h3>
      <p>Let $A = xy^\top$ where $x, y \in \mathbb{R}^n$ are non-zero vectors. Show that $A^+ = \frac{A^\top}{\|x\|_2^2 \|y\|_2^2}$.</p>

      <h3>P1.12 — Operator Norm Properties</h3>
      <p>Prove that $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$. Use the Rayleigh Quotient definition of eigenvalues.</p>

      <h3>P1.13 — Orthogonal Invariance</h3>
      <p>Show that $\|U A V^\top\|_F = \|A\|_F$ and $\|U A V^\top\|_2 = \|A\|_2$ for orthogonal $U, V$. This proves that these norms depend only on the singular values.</p>

      <h3>P1.14 — The Orthogonal Group</h3>
      <p>Show that the set of orthogonal matrices forms a compact group.</p>
</section>

    <!-- SECTION 6: READINGS -->
    <section class="section-card" id="section-readings">
      <h2>6. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required:</strong> Boyd & Vandenberghe, Appendix A.</li>
        <li><strong>Recommended:</strong> Trefethen & Bau, <em>Numerical Linear Algebra</em>, Lectures 1–5.</li>
        <li><strong>Advanced References:</strong> Golub & Van Loan, <em>Matrix Computations</em>, Chapter 2.</li>
      </ul>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initSvdApproximator } from './widgets/js/svd-approximator.js';
    initSvdApproximator('widget-svd-approximator');
  </script>
  <script type="module">
    import { initConditionNumber } from './widgets/js/condition-number.js';
    initConditionNumber('widget-condition-number');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
